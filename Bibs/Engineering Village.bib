% Encoding: UTF-8
@inproceedings{20175004516359 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A model for the description of the information system dynamics},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Flory, A. and Kouloumdjian, J.},
volume = {65 LNCS},
year = {1978},
pages = {307 - 318},
issn = {03029743},
address = {Venice, Italy},
abstract = {This paper deals with the description of the information system dynamics at the conceptual level. A model is put forward to take into account both the structural and dynamic aspects of data base. The data model (an entity-relation model) helps to build the data into a structured set of relations that correspond to a specific third normal form. These relations do not play the same role in data base evolution : we propose a classification of &lsquo;event&rsquo; relations &mdash; the tuples of which are the starting point of treatments &mdash; and of &lsquo;permanent&rsquo; relations. The description of processings deriving from a primary event may be divided into elementary chronological steps with the help of a hierarchy graph. Each step can be analyzed in term of &lsquo;states&rsquo; which express the stage of treatment of the triggering event.<br/> &copy; 1978, Springer Verlag. All rights reserved.},
key = {Information systems},
keywords = {Information use;International cooperation;System theory;},
note = {Conceptual levels;Dynamic aspects;Normal form;Relation models;},
URL = {http://dx.doi.org/10.1007/3-540-08934-9_85},
} 


@inproceedings{20064910293358 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolving the implementation of ISA relationships in EER schemas},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
volume = {4231 LNCS},
year = {2006},
pages = {237 - 246},
issn = {03029743},
address = {Tucson, AZ, United states},
abstract = {One of the most severe problems related to database evolution is how to reflect in the data level the changes that have occurred in the conceptual schema of a database. This is specially relevant when evolution operations affect ISA relationships. In this paper we present our view of the evolution of ISA relationships, focusing on the artifacts that generate the sentences for changing the data in a consistent way. &copy; Springer-Verlag Berlin Heidelberg 2006.},
key = {Database systems},
keywords = {Data acquisition;Evolutionary algorithms;Mathematical models;Numerical methods;Problem solving;},
note = {Artifacts;Database evolution;EER schemes;ISA relationship;},
} 


@article{20073410778586 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A virtual type for a multiple-type object and its implementation},
journal = {Systems and Computers in Japan},
author = {Sato, Hideki and Aritsugi, Masayoshi},
volume = {38},
number = {11},
year = {2007},
pages = {25 - 35},
issn = {08821666},
abstract = {To deal with the evolution of data and applications and with the existence of multiple views for the same data, the object data model needs to be extended with the object extension functionality to allow an object to dynamically change its type, and the object viewing functionality to allow an object to be seen as if it had a different structure and behavior. This paper proposes the virtual type facility which provides a persistent object of INADA, an enhanced C++ persistent programming language compliant with the ODMG standard, with a different structure and behavior from those of its base types. Additionally, it presents the implementation details of the facility. The virtual type facility gives a different interface of an object to redefine its structure and behavior without affecting the behavior of a base type, while the multiple-type object of INADA allows an object to be dynamically extended to model an entity which changes its behavior with time. The virtual type facility integrates the object extension functionality and the object viewing functionality for dealing with the database evolution. &copy; 2007 Wiley Periodicals, Inc.},
key = {Data reduction},
keywords = {C (programming language);Computer programming languages;Data structures;Database systems;},
note = {Data evolution;Database evolution;Object views;Persistent programming languages;},
URL = {http://dx.doi.org/10.1002/scj.20781},
} 


@article{2001466728638 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ROVER: Flexible yet consistent evolution of relationships},
journal = {Data and Knowledge Engineering},
author = {Claypool, Kajal T. and Rundensteiner, Elke A. and Heineman, George T.},
volume = {39},
number = {1},
year = {2001},
pages = {27 - 50},
issn = {0169023X},
abstract = {Relationships have been repeatedly identified as an important object-oriented modeling construct. Most emerging modeling standards such as the object database management group (ODMG) object model and UML have some support for relationships. However object-oriented database (OODB) systems have largely ignored the existence of relationships during schema evolution. We are the first to propose comprehensive support for relationship evolution. A complete schema evolution facility for any OODB system must provide primitives to manipulate all object model constructs, and maintenance strategies for the structural and referential integrity of the database under such evolution. We propose a set of basic evolution primitives for relationships as well as a compound set of changes that can be applied to the same. However, given the myriad of possible change semantics a user may desire in the future, any pre-defined set is not sufficient. Rather we present a flexible schema evolution framework that allows the user to define new relationship transformations as well as to extend existing ones. Addressing the second problem, namely of updating schema evolution primitives to conform to the new set of invariants, can be a very expensive re-engineering effort. In this paper we present an approach that de-couples the constraints from the schema evolution code, thereby enabling their update without any re-coding effort. We also present an approach that can be used to verify the correctness of these complex evolution operations using the de-coupled constraints. &copy; 2001 Published by Elsevier Science B.V.},
key = {Relational database systems},
keywords = {Constraint theory;Electronic commerce;Multimedia systems;Object oriented programming;Semantics;},
note = {Object-oriented database (OODB) systems;},
URL = {http://dx.doi.org/10.1016/S0169-023X(01)00029-5},
} 


@inproceedings{20154801623873 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {18th International Conference on Conceptual Modeling, ER 1999},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {1728},
year = {1999},
pages = {1 - 539},
issn = {03029743},
address = {Paris, France},
abstract = {The proceedings contain 36 papers. The special focus in this conference is on Supporting Schema Evolution and Temporal Database Design. The topics include:  Three levels of reuse for supporting the evolution of database schemas; a unified framework for supporting dynamic schema evolution in object databases; an incremental and semi-automatic method inheritance graph hierarchy construction; developing an object-oriented video database system with spatio-temporal reasoning capabilities; entity evolution in IsA hierarchies; temporal ER modeling with description logics; automatic migration and wrapping of database applications - a schema transformation approach; a methodology for clustering entity relationship models - a human information processing approach; designing good semi-structured databases; building views over semistructured data sources; modeling and maintaining multi-view data warehouses; object views through search views of web datasources; understanding and modelling business processes with DEMO; a methodology for building a repository of object-oriented design fragments; an algorithm to extract is a inheritance hierarchies from a relational database; a generic infrastructure for decision-oriented collaborative task support; a model for adding value to content; workflow specification in TRAMs; from CASE to CARE (computer-aided requirements engineering); solving the problem of semantic heterogeneity in defining mediator update translators; a method for requirements elicitation and formal specification; dealing with semantic heterogeneity during data integration; detecting redundancy in data warehouse evolution; modelling data warehouses and OLAP applications by means of dialogue objects; resolving the weak status of weak entity types in entity-relationships schemas; a taxonomy of recursive relationships and their structural validity in ER modeling and extending functional dependencies in indefinite sequence relations.},
} 


@inproceedings{20163002636317 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {5th European Summer School on Business Intelligence, eBISS 2015},
journal = {Lecture Notes in Business Information Processing},
volume = {253},
year = {2016},
pages = {1 - 130},
issn = {18651348},
address = {Barcelona, Spain},
abstract = {The proceedings contain 5 papers. The special focus in this conference is on Business Intelligence. The topics include: schema evolution for databases and data warehouses; publishing OLAP cubes on the semantic web; design issues in social business intelligence projects; context-aware business intelligence and key performance indicators in data warehouses.},
} 


@inproceedings{20163602780144 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Starry vault: Automating multidimensional modeling from data vaults},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Golfarelli, Matteo and Graziani, Simone and Rizzi, Stefano},
volume = {9809 LNCS},
year = {2016},
pages = {137 - 151},
issn = {03029743},
address = {Prague, Czech republic},
abstract = {The data vault model natively supports data and schema evolution, so it is often adopted to create operational data stores. However, it can hardly be directly used for OLAP querying. In this paper we propose an approach called Starry Vault for finding a multidimensional structure in data vaults. Starry Vault builds on the specific features of the data vault model to automate multidimensional modeling, and uses approximate functional dependencies to discover out of data the information necessary to infer the structure of multidimensional hierarchies. The manual intervention by the user is limited to some editing of the resulting multidimensional schemata, which makes the overall process simple and quick enough to be compatible with the situational analysis needs of a data scientist.<br/> &copy; Springer International Publishing Switzerland 2016.},
key = {Data warehouses},
keywords = {Information systems;Information use;},
note = {Data vault;Functional dependency;Multi-dimensional model;Multi-dimensional structure;Multidimensional hierarchies;Multidimensional schemata;Operational data stores;Warehouse design;},
URL = {http://dx.doi.org/10.1007/978-3-319-44039-2_10},
} 


@inproceedings{1997293662769 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Propagation mechanism for populated schema versions},
journal = {Proceedings - International Conference on Data Engineering},
author = {Lautemann, Sven-Eric},
year = {1997},
pages = {67 - 78},
address = {Birmingham, UK},
abstract = {Object-oriented database systems (OODBMS) offer powerful modeling concepts as required by advanced application domains like CAx or office automation. Typical applications have to handle large and complex structured objects which frequently change their value and their structure. As the structure is described in the schema of the database, support for schema evolution is a highly required feature. Therefore, a set of schema update primitives must be provided which can be used to perform the required changes, even in the presence of populated databases and running applications. In this paper, we use the versioning approach to schema evolution to support schema updates as a complex design task. The presented propagation mechanism is based on conversion functions that map objects between different types and can be used to support schema evolution and schema integration.},
key = {Database systems},
keywords = {Data structures;Object oriented programming;},
note = {Populated schema versions;Propagation mechanism;},
} 


@inproceedings{2006289986200 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing collections of XML schemas in microsoft SQL server 2005},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Pal, Shankar and Tomic, Dragan and Berg, Brandon and Xavier, Joe},
volume = {3896 LNCS},
year = {2006},
pages = {1102 - 1105},
issn = {03029743},
address = {Munich, Germany},
abstract = {Schema evolution is of two kinds: (a) those requiring instance transformation because the application is simpler to develop when it works only with one version of the schema, and (b) those in which the old data must be preserved and instance transformation must be avoided. The latter is important in practice but has received scant attention in the literature. Data conforming to multiple versions of the XML schema must be maintained, indexed, and manipulated using the same query. Microsoft's SQL Server 2005 introduces XML schema collections to address both types of schema evolution. &copy; Springer-Verlag Berlin Heidelberg 2006.},
key = {Servers},
keywords = {Data processing;Database systems;Query languages;Schematic diagrams;XML;},
note = {Data manipulation;Literature;Queries;Sequential query language (SQL);},
URL = {http://dx.doi.org/10.1007/11687238_69},
} 


@inproceedings{20151200649844 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CEUR Workshop Proceedings},
journal = {CEUR Workshop Proceedings},
volume = {1008},
year = {2013},
pages = {59 - },
issn = {16130073},
address = {Florence, Italy},
abstract = {The proceedings contain 7 papers. The topics discussed include: merging uncertain multi-version XML documents; identifying change patterns in software history; the concept difference for EL terminologies using hypergraphs; an algorithm for transforming XPath expressions according to schema evolution; concurrency effects over variable size identifiers in distributed collaborative editing; tracking changes through EARMARK: a theoretical perspective and an implementation; and staged evolution with quality gates for model libraries.},
} 


@article{20180204635158 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Entity Attribute Value style modeling approach for archetype based data},
journal = {Information (Switzerland)},
author = {Batra, Shivani and Sachdeva, Shelly and Bhalla, Subhash},
volume = {9},
number = {1},
year = {2017},
issn = {20782489},
abstract = {Entity Attribute Value (EAV) storage model is extensively used to manage healthcare data in existing systems, however it lacks search efficiency. This study examines an entity attribute value style modeling approach for standardized Electronic Health Records (EHRs) database. It sustains qualities of EAV (i.e., handling sparseness and frequent schema evolution) and provides better performance for queries in comparison to EAV. It is termed as the Two Dimensional Entity Attribute Value (2D EAV) model. Support for ad-hoc queries is provided through a user interface for better user-interaction. 2D EAV focuses on how to handle template-centric queries as well as other health query scenarios. 2D EAV is analyzed (in terms of minimum non-null density) to make a judgment about the adoption of 2D EAV over n-ary storage model of RDBMS. The primary aim of current research is to handle sparseness, frequent schema evolution, and efficient query support altogether for standardized EHRs. 2D EAV will benefit data administrators to handle standardized heterogeneous data that demands high search efficiency. It will also benefit both skilled and semi-skilled database users (such as, doctors, nurses, and patients) by providing a global semantic interoperable mechanism of data retrieval.<br/> &copy; 2018 by the authors.},
key = {Search engines},
keywords = {Digital storage;Efficiency;Health;Information management;Query processing;Records management;Semantics;Standardization;User interfaces;},
note = {Archetype based EHRs;Attribute values;Data administrators;Electronic health record (EHRs);Existing systems;Heterogeneous data;Search efficiency;Semantic interoperability;},
URL = {http://dx.doi.org/10.3390/info9010002},
} 


@inproceedings{20170503308204 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - 2016 IEEE 8th International Workshop on Managing Technical Debt, MTD 2016},
journal = {Proceedings - 2016 IEEE 8th International Workshop on Managing Technical Debt, MTD 2016},
year = {2016},
address = {Raleigh, NC, United states},
abstract = {The proceedings contain 7 papers. The topics discussed include: how 'specification by example' and test-driven development help to avoid technical debt; the perception of technical debt in the embedded systems domain: an industrial case study; database design debts through examining schema evolution; towards assessing the technical debt of undesired software behaviors in design patterns; technical debt indexes provided by tools: a preliminary discussion; practical technical debt discovery by matching patterns in assessment graph; and adjusting the balance sheet by appending technical debt.},
} 


@inproceedings{2000085262136 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On-line reorganization in object databases},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
author = {Lakhamraju, Mohana K. and Rastogi, Rajeev and Seshadri, S. and Sudarshan, S.},
volume = {29},
number = {2},
year = {2000},
pages = {58 - 69},
address = {Dallas, TX, United states},
abstract = {Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution.  The high availability requirements (24 &times; 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.  In this paper, we address the problem of on-line reorganization in object databases, where a set of objects have to be migrated from one location to another.  Specifically, we consider the case where objects in the database may contain physical references to other objects.  Relocating an object in this case involves finding the set of objects (parents) that refer to it, and modifying the references in each parent.  We propose an algorithm called the Incremental Reorganization Algorithm (IRA) that achieves the above task with minimal interference to concurrently executing transactions.  The IRA algorithm holds locks on at most two distinct objects at any point of time.  We have implemented IRA on Brahma, a storage manager developed at IIT Bombay, and conducted an extensive performance study.  Our experiments reveal that IRA makes on-line reorganization feasible, with very little impact on the response times of concurrently executing transactions and on overall system throughput.  We also describe how the IRA algorithm can handle system failures.},
} 


@inproceedings{20160902022997 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {International Symposium on Objects and Databases, 2000 held in conjunction with 14th European Conference on Object-Oriented Programming, ECOOP 2000},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {1944},
year = {2001},
pages = {1 - 198},
issn = {03029743},
address = {Sophia Antipolis, France},
abstract = {The proceedings contain 14 papers. The special focus in this conference is on Persistence, Clustering, Schema Evolution, Data Mining and Data Warehouse. The topics include: Parametric polymorphism and orthogonal persistence; towards a consistent viewpoint on consistency for persistent applications; towards scalable and recoverable object evolution for the Pjama persistent platform; dynamic clustering in object-oriented databases; opportunistic prioritised clustering framework; a flexible approach for instance adaptation during class versioning; optimizing performance of schema evolution sequences; benefits of an object-oriented multidimensional data model; networked open database services; pointwise temporal object database browsing and UML as a basis for a generic graphical query language.},
} 


@inproceedings{2003357615230 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application of hierarchical incremental knowledge acquisition (HIKA) using an object-oriented database management system (OODBMS)},
journal = {International Journal of Engineering Intelligent Systems for Electrical Engineering and Communications},
author = {Beydoun, Ghassan and Al-Jadir, Lina},
volume = {11},
number = {2},
year = {2003},
pages = {85 - 93},
issn = {14728915},
abstract = {Motives for using a Database Management System (DBS) to build a Knowledge Base System (KBS), include KBS's lack of ability to manage large sets of rules, to control concurrent access and to manage multiple knowledge bases simultaneously. In this paper, we build a KBS using a database management system DBMS for its schema evolution ability. We use this ability of an Object Oriented DMBS (OODBMS) to manage the consistency of an incrementally built hierarchical Knowledge Base (KB). The underlying knowledge representation scheme, which we use, is our Nested Ripple Down Rules (NRDR), which guides an expert in incrementally expressing his conceptual hierarchical model of the domain during the actual Knowledge Acquisition (KA) process. An NRDR KB evolves into a hierarchy of concepts where each concept is defined as a collection of hierarchical rules with exceptions. To modify a concept definition, exception rules are added only, they are never deleted or modified. This greatly eases maintenance and development of a concept definition, but may cause inconsistencies to occur in the knowledge base. In this paper, we analyse NRDR constructs from an OO perspective. In particular, we explore the dynamic relation between NRDR concepts and world objects. Our analysis leads us to implement NRDR concepts with classes with specialization constraints using the OODBMS, F2. This in turn allows the use of the built-in schema evolution mechanisms to manage the consistency of an evolving NRDR conceptual hierarchy. With the help of a real domain expert, we illustrate our application in a potentially commercial classification task, that of checking whether or not a given air-conditioning system is suitable in a given setting. The significance of this paper is two folds: first, it provides an efficient mechanism maintaining consistency of an evolving classification hierarchy, using built-in schema evolution features of an OODBMS. Second, it enhances the interface of an OODBMS, to allow intelligent classification queries over stored objects.},
key = {Knowledge acquisition},
keywords = {Database systems;Hierarchical systems;Information management;Knowledge based systems;Object oriented programming;},
note = {Schema analysis;},
} 


@inproceedings{20182105225851 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a flexible mediator architecture using fuzzy logic for integration of incomplete and uncertain information},
journal = {ACM International Conference Proceeding Series},
author = {Aggoune, Aicha},
volume = {2018-March},
year = {2018},
pages = {7 - 13},
address = {Rabat, Morocco},
abstract = {The major problems of data integration systems are closely related to the heterogeneity of the data sources, the query processing complexity, and the global schema evolution. These problems are increased when data sources contain both uncertain and incomplete information. The existing works aim to use fuzzy ontology as well as a global schema of mediator system for providing a homogeneous view of semantically heterogeneous data sources and expressing imprecise information. In this paper, we present new flexible mediator architecture based only on fuzzy logic for, representing incomplete and uncertain information, and fuzzy querying of heterogeneous databases using fuzzy predicates. This architecture is split into three layers: Flexible mediation, Flexible wrappers, and sources. At the wrapper layer, we propose a method for fuzzy querying of both incomplete and uncertain information. The flexible mediator architecture, including an example of data integration in food safety field.<br/> &copy; 2018 Association for Computing Machinery.},
key = {Fuzzy logic},
keywords = {Artificial intelligence;Computer circuits;Data integration;Pattern recognition;Query languages;Search engines;},
note = {Flexible mediator;Fuzzy predicates;Incomplete information;Relational Database;Uncertain informations;},
URL = {http://dx.doi.org/10.1145/3177148.3180094},
} 


@inproceedings{20100712716127 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Journal on Data Semantics XIII},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {5530 LNCS},
year = {2009},
issn = {03029743},
abstract = {The proceedings contain 6 papers. The topics discussed include: multidimensional integrated ontologies: a framework for designing semantic data warehouses; a unified object constraint model for designing and implementing multidimensional systems; modeling data warehouse schema evolution over extended hierarchy semantics; an ETL process for OLAP using RDF/OWL ontologies; ontology-driven conceptual design of ETL processes using graph transformations; and policy-regulated management of ETL evolution.},
} 


@inproceedings{20130716007283 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SYRCoDIS 2005 - Proceedings of the Spring Young Researchers' Colloquium on Databases and Information Systems},
journal = {CEUR Workshop Proceedings},
volume = {154},
year = {2005},
issn = {16130073},
address = {Saint Petersburg, Russia},
abstract = {The proceedings contain 8 papers. The topics discussed include: web sites automatic summarization; deductive approach to semistructured schema evolution; using relational operations to express association rules; introducing trigger support for XML database systems; a locking protocol for scheduling transactions on XML data; deductive analysis of large data streams: a new perspective for view technology; automated model transformation in MDA; finite state automata - a concept for data representation; and partitioning inverted lists for efficient evaluation of set-containment joins in main memory.},
} 


@inproceedings{20160401837963 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CEUR Workshop Proceedings},
journal = {CEUR Workshop Proceedings},
volume = {1474},
year = {2015},
issn = {16130073},
address = {Ottawa, ON, Canada},
abstract = {The proceedings contain 9 papers. The topics discussed include: system development at run time; using adaptation plans to control the behavior of models@runtime; on the need for extended transactional models@run.time; using reference attribute grammar-controlled rewriting for energy autotuning; models@run.time for object-relational mapping supporting schema evolution; leveraging models at run-time to retrieve information for feature location; managing distributed context models requires adaptivity too; and scenarios@run.time distributed execution of specifications on IoT-connected robots.},
} 


@article{20075010973717 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic maintenance of an integrated schema},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Motz, Regina},
volume = {3379 LNCS},
year = {2005},
pages = {21 - 30},
issn = {03029743},
abstract = {This work presents a schema evolution methodology able to propagate structural and semantic modifications occurring in the local schemas of a federated database to the integrated schema. Our approach is to regard this problem from a schema integration point of view. Our theoretical framework is based on a declarative schema integration methodology, which reduces schema integration to the resolution of a set of equivalence correspondences between arbitrarily complex local subschemas. &copy; Springer-Verlag Berlin Heidelberg 2005.},
key = {Semantic Web},
keywords = {Computer programming languages;Database systems;Problem solving;},
note = {Dynamic maintenance;Federated database;Integrated schema;Semantic modifications;},
} 


@inproceedings{20104613394934 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Objects and Databases - Third International Conference, ICOODB 2010, Proceedings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {6348 LNCS},
year = {2010},
pages = {Versant Corporation; db4objects; InterSystems Corporation; IBM Deutschland; sones - },
issn = {03029743},
address = {Frankfurt/Main, Germany},
abstract = {The proceedings contain 12 papers. The topics discussed include: search computing challenges and directions; searching the web of objects; unifying remote data, remote procedures, and web services; revisiting schema evolution in object databases in support of agile development; the case for object databases in cloud data management; query optimization by result caching in the stack-based approach; a flexible object model and algebra for uniform access to object databases; data model driven implementation of web cooperation systems with tricia; object-oriented constraints for XML schema; closing schemas in object-relational databases; and a comparative study of the features and performance of ORM tools in a .NET environment.},
} 


@article{20083311461389 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application of Hierarchical Incremental Knowledge Acquisition (HIKA) using an Object-Oriented Database Management System (OODBMS)},
journal = {Engineering Intelligent Systems},
author = {Beydoun, Ghassan and Al-Jadir, Lina},
volume = {11},
number = {2},
year = {2003},
pages = {85 - 93},
issn = {14728915},
abstract = {Motives for using a Database Management System (DBS) to build a Knowledge Base System (KBS), include KBS's lack of ability to manage large sets of rules, to control concurrent access and to manage multiple knowledge bases simultaneously. In this paper, we build a KBS using a database management system DBMS for its schema evolution ability. We use this ability of an Object Oriented. DMBS (OODBMS) to manage the consistency of an incrementally built hierarchical Knowledge Base (KB). The underlying knowledge representation scheme, which we use, is our Nested Ripple Down Rules (NRDR), which guides an expert in incrementally expressing his conceptual hierarchical model of the domain during the actual Knowledge Acquisition (KA) process. An NRDR KB evolves into a hierarchy of concepts where each concept is defined as a collection of hierarchical rules with exceptions. To modify a concept definition, exception rules are added only, they are never deleted or modified. This greatly eases maintenance and development of a concept definition, but may cause inconsistencies to occur in the knowledge base. In this paper, we analyse NRDR constructs from an OO perspective. In particular, we explore the dynamic relation between NRDR concepts and world objects. Our analysis leads us to implement NRDR concepts with classes with specialization constraints using the OODBMS, F2. This in turn allows the use of the built-in schema evolution mechanisms to manage the consistency of an evolving NRDR conceptual hierarchy. With the help of a real domain expert, we illustrate our application in a potentially commercial classification task, that of checking whether or not a given airconditioning system is suitable in a given setting. The significance of this paper is two folds: first, it provides an efficient mechanism maintaining consistency of an evolving classification hierarchy, using built-in schema evolution features of an OODBMS. Second, it enhances the interface of an OODBMS, to allow intelligent classification queries over stored objects. &copy; 2003 CRL Publishing Ltd.<br/>},
key = {Object-oriented databases},
keywords = {Access control;Concurrency control;Hierarchical systems;Knowledge acquisition;Knowledge based systems;Knowledge representation;Management information systems;},
note = {Classification hierarchies;Database management;Hierarchical knowledge;HIKA;Incremental knowledge acquisition;Intelligent classification;Knowledge base system;Object-oriented syystems;},
} 


@inproceedings{20184305978191 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {QUDOS 2016 - Proceedings of the 2nd International Workshop on Quality-Aware DevOps, co-located with ISSTA 2016},
journal = {QUDOS 2016 - Proceedings of the 2nd International Workshop on Quality-Aware DevOps, co-located with ISSTA 2016},
year = {2016},
pages = {ACM SIGSOFT - },
address = {Saarbrucken, Germany},
abstract = {The proceedings contain 12 papers. The topics discussed include: coverage-based metrics for cloud adaptation; a software architecture framework for quality-aware DevOps; towards a UML profile for data intensive applications; a systematic approach for performance evaluation using process mining: the POSIDONIA operations case study; DICE fault injection tool; datalution: a tool for continuous schema evolution in NoSQL-backed web applications; model-driven continuous deployment for quality DevOps; PET: continuous performance evaluation tool; a tool for verification of big-data applications; and TemPerf: temporal correlation between performance metrics and source code.<br/>},
} 


@inproceedings{2006099727537 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Composition of mappings given by embedded dependencies},
journal = {Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
author = {Nash, Alan and Bernstein, Philip A. and Melnik, Sergey},
year = {2005},
pages = {172 - 183},
address = {Baltimore, MD, United states},
abstract = {Composition of mappings between schemas is essential to support schema evolution, data exchange, data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this paper, we study the issues involved in composing such mappings. Our algorithms and results extend those of Fagin et al. [8] who studied composition of mappings given by several kinds of constraints. In particular, they proved that full source-to-target tuple-generating dependencies (tgds) are closed under composition, but embedded source-to-target tgds are not. They introduced a class of second-order constraints, SO tgds, that is closed under composition and has desirable properties for data exchange. We study constraints that need not be source-to-target and we concentrate on obtaining (first-order) embedded dependencies. As part of this study, we also consider full dependencies and second-order constraints that arise from Skolemizing embedded dependencies. For each of the three classes of mappings that we study, we provide (a) an algorithm that attempts to compute the composition and (b) sufficient conditions on the input mappings that guarantee that the algorithm will succeed. In addition, we give several negative results. In particular, we show that full dependencies are not closed under composition, and that second-order dependencies that are not limited to be source-to-target are not closed under restricted composition. Furthermore, we show that determining whether the composition can be given by these kinds of dependencies is undecidable. Copyright 2005 ACM.},
key = {Database systems},
keywords = {Computation theory;Data transfer;Mapping;},
note = {Embedded dependencies;Second-order dependencies;},
URL = {http://dx.doi.org/10.1145/1065167.1065189},
} 


@inproceedings{20161402187486 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - 3rd International Workshop on Release Engineering, RELENG 2015},
journal = {Proceedings - 3rd International Workshop on Release Engineering, RELENG 2015},
year = {2015},
pages = {Association for Computing Machinery's Special Interest Group on Software Engineering (ACM SIGSOFT); IEEE Computer Society Technical Council on Software Engineering (TCSE) - },
address = {Florence, Italy},
abstract = {The proceedings contain 9 papers. The topics discussed include: release engineering as a force multiplier; research opportunities in continuous delivery; towards definitions for release engineering and DevOps; securing a deployment pipeline; performance of defect prediction in rapidly evolving software; predicting field reliability; continuous deployment and schema evolution in SQL databases; extracting configuration knowledge from build files with symbolic analysis; and continuous delivery with Jenkins.},
} 


@inproceedings{20151300693742 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CEUR Workshop Proceedings},
journal = {CEUR Workshop Proceedings},
volume = {1020},
year = {2013},
pages = {88 - },
issn = {16130073},
address = {Ilmenau, Germany},
abstract = {The proceedings contain 13 papers. The topics discussed include: WattDB - a rocky road to energy proportionality; optimizing database architecture for machine architecture: is there still hope?; adaptive prejoin approach for performance optimization in MapReduce-based warehouses; consistency models for cloud-based online games: the storage system's perspective; analysis of DDOS detection systems; a conceptual model for the XML schema evolution; semantic enrichment of ontology mappings: detecting relation types and complex correspondences; MVAL: addressing the insider threat by valuation-based query processing; TrIMPI: a data structure for efficient pattern matching on moving objects; and XQuery processing over NoSQL stores.},
} 


@inproceedings{20144600188141 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Query maintenance with hierarchical updates: Using graph modeling approach},
journal = {International Conference on Recent Advances and Innovations in Engineering, ICRAIE 2014},
author = {Gosain, Anjana and Heena, H.},
year = {2014},
address = {Jaipur, India},
abstract = {Data warehouse must evolve with changes in the schema, software components, user requirements or organization's business rules. Various authors have proposed different schema evolution operators for various level updates but none of them handled the syntactic and semantic adaptation of queries automatically. Few authors discussed this problem considering few changes in data sources schema and data warehouse structure schema. But they did not deal with the problem due to changes in the hierarchy and its levels in a dimension of the schema. In this paper, we perform what-if analysis for change in the hierarchy and its levels in the data warehouse schema. Queries, views and relations are modeled in a graph annotated with policies to manage the change event. Framework detects the affected part of graph due to change in hierarchical levels and readjusts according to the specified action without making the queries invalid.<br/> &copy; 2014 IEEE.},
key = {Data warehouses},
keywords = {Graph theory;Public policy;Semantics;},
note = {Data warehouse evolutions;Data warehouse structure;Graph model;Hierarchical level;hierarchical update;Semantic adaptation;Software component;User requirements;},
URL = {http://dx.doi.org/10.1109/ICRAIE.2014.6909169},
} 


@article{1997173561516 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Normalization of class hierarchy in databases},
journal = {Journal of Computer Science and Technology},
author = {Li, Tianzhu and Xiao, Jitian and Sun, Zhaohao and Bian, Xiaofan},
volume = {11},
number = {4},
year = {1996},
pages = {356 - 364},
issn = {10009000},
abstract = {In complex object oriented databases, the purpose of introducing class hierarchy is to express ISA semantics, to realize inheriting and to reuse schema definition codes. The schema definition and schema evolution, based on the partial order of lattice, often cause the loss of information inheriting and the redundance of schema definition. Based on the fullness of the inheritance shown by class hierarchy, three normal forms of class hierarchy are given in this paper, and a general algorithm of normalization of class hierarchy is presented, following the Boolean algebra model of class hierarchy. The loss of information inheritance can be avoided when they are applied to schema design and schema evolution.},
key = {Database systems},
keywords = {Algorithms;Boolean algebra;Computational linguistics;Hierarchical systems;Mathematical models;Object oriented programming;},
note = {Class hierarchy normalization;Object oriented database;},
} 


@inproceedings{2005058811823 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Goal-focused self-modifying workflow in the healthcare domain},
journal = {Proceedings of the Hawaii International Conference on System Sciences},
author = {Browne, Eric D. and Schrefl, Michael and Warren, James R.},
volume = {37},
year = {2004},
pages = {2295 - 2304},
issn = {10603425},
address = {Big Island, HI., United states},
abstract = {This paper introduces the concept of self-modifying workflow in the context of health care planning. Certain tasks in the workflow schema are devoted to modifying the downstream workflow on an instance by instance basis. Such self-modifying schemas provide the necessary flexibility to suit the evolving diagnostic and therapeutic processes Encountered in Chronic Disease Management (CDM), particularly in complex areas requiring significant individualisation. The management of Diabetes Mellitus in a community care setting provides an example to illustrate this complexity. Over the past few years, object-oriented modelling tools of inheritance and specialisation have been applied to workflow modelling to assist in schema evolution and workflow migration, thereby potentially empowering new Workflow Management Systems (WfMSs) with the functionality to allow the tailoring of Guideline-based Care Plans to individual patient requirements. However, schema evolution is a necessary but insufficient requirement for such tailoring. Healthcare WfMSs need to support a paradigm whereby schema evolution becomes a de facto operation for each workflow instance (i.e. patient episode of care). Self-modifying workflows provide this paradigm. In order to facilitate self-modification of workflow schemas, we annunciate a set of valid operations that can be applied to downstream components of a workflow schema. These operations are primarily concerned with turning abstract subworkflows into concrete ones through completion and alteration of template primitives.},
key = {Medical computing},
keywords = {Database systems;Diagnosis;Diseases;Health care;Insulin;Mathematical models;Medical problems;Object oriented programming;Patient monitoring;Software engineering;},
note = {Chronic disease management (CDM);Enhanced primary care (EPC);Medical benefits schedule (MBS);Workflow management systems (WfMS);},
} 


@inproceedings{20173704139789 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {5th International Conference on Advanced Information Systems Engineering, CAiSE 1993},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {685 LNCS},
year = {1993},
pages = {1 - 650},
issn = {03029743},
address = {Paris, France},
abstract = {The proceedings contain 34 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: a procedural approach to schema evolution; an active meta-model for knowledge evolution in an object-oriented database; schema integration in object-oriented databases; towards a model for persistent data integration; using explanations to improve the validation of executable models; validating conceptual models by transformational prototyping; partial evaluation and symbolic computation for the understanding of fortran programs; dealing with security requirements during the development of information systems; elicitafing and formalising requirements for C.I.M. information systems; concepts for real-world modelling; the semantics of parts versus aggregates in data/knowledge modelling; object interaction in object-oriented deductive conceptual models; towards reliable information systems: the KorSo approach; similarity for analogical software reuse: a conceptual modelling approach; temporal aspects in reuse of requirement specifications; animation and verification computer-aided verification of software process model properties; an animation facility to simulate an information and communication system; design of user-driven interfaces using petri nets and objects; development environments perspectives on software development environments; an object-oriented database approach for supporting hypertext and estimation process of performance constraints during the design of real-time and embedded systems.},
} 


@inproceedings{1987080129291 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ENHANCING THE OBJECT-ORIENTED CONCEPTS FOR DATABASE SUPPORT.},
author = {Kim, Won and Woelk, Darrell and Garza, Jorge and Chou, Hong-Tai and Banerjee, Jay and Ballou, Nat},
year = {1987},
pages = {291 - 292},
address = {Los Angeles, CA, USA},
abstract = {The authors elaborate on three major enhancements to the conventional object-oriented data model, namely, schema evolution, composite objects, and versions. Schema evolution is the ability to dynamically make changes to the class definitions and the structure of the class lattice. Composite objects are recursive collections of exclusive components that are treated as units of storage, retrieval, and integrity enforcement. Versions are variations of the same object that are related by the history of their derivation. These additional features are strongly motivated by data management requirements of object-oriented applications from the AI, CAD/CAM, and OIS (office information systems with multimedia documents) domains. An object-oriented data model, with these enhancements, has been incorporated into ORION, a prototype database system developed at MCC as a vehicle of research into object-oriented databases.},
key = {DATABASE SYSTEMS},
keywords = {ARTIFICIAL INTELLIGENCE;COMPUTER AIDED DESIGN;COMPUTER AIDED MANUFACTURING;OFFICE AUTOMATION;},
note = {OBJECT-ORIENTED DATA MODELS;OFFICE INFORMATION SYSTEMS;},
} 


@inproceedings{20154801629717 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {14th British National Conference on Databases, BNCOD 1996},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {1094},
year = {1996},
pages = {1 - 233},
issn = {03029743},
address = {Edinburgh, United kingdom},
abstract = {The proceedings contain 18 papers. The special focus in this conference is on OODB Issues and Integrity Issues. The topics include: Schema integration meta-knowledge classification and reuse; view mechanism for schema evolution in object-oriented DBMS; an active rule language for ROCK and ROLL; integrity constraints in multiversion databases; the development of a semantic integrity constraint subsystem for a distributed database; understanding the tension between transition rules and confidentiality; extending ER for dynamic behaviour and refinement; speeding up knowledge discovery in large relational databases by means of a new discretization algorithm; integration of load measurement parameters into the cost evaluation of database queries; high performance OO traversals in monet; a modular compiler architecture for a data manipulation language; querying graph databases using a functional language extended with second order facilities; adding temporal indeterminacy to the database language SQL; interface research inside a multinational bank; dissemination-based information systems; microsoft database technologies; an active OODBMS for financial applications and universal data management.},
} 


@inproceedings{20133116549191 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On brewing fresh espresso: LinkedIn's distributed data serving platform},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
author = {Qiao, Lin and Surlaker, Kapil and Das, Shirshanka and Quiggle, Tom and Schulman, Bob and Ghosh, Bhaskar and Curtis, Antony and Seeliger, Oliver and Zhang, Zhen and Auradkar, Aditya and Beavers, Chris and Brandt, Gregory and Gandhi, Mihir and Gopalakrishna, Kishore and Ip, Wai and Jagadish, Swaroop and Lu, Shi and Pachev, Alexander and Ramesh, Aditya and Sebastian, Abraham and Shanbhag, Rupa and Subramaniam, Subbu and Sun, Yun and Topiwala, Sajid and Tran, Cuong and Westerman, Jemiah and Zhang, David},
year = {2013},
pages = {1135 - 1146},
issn = {07308078},
address = {New York, NY, United states},
abstract = {Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn's requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, realtime secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. This paper describes the motivation and design principles involved in building Espresso, the data model and capabilities exposed to clients, details of the replication and secondary indexing implementation and presents a set of experimental results that characterize the performance of the system along various dimensions. When we set out to build Espresso, we chose to apply best practices in industry, already published works in research and our own internal experience with different consistency models. Along the way, we built a novel generic distributed cluster management framework, a partition-aware change-capture pipeline and a high-performance inverted index implementation. Copyright &copy; 2013 ACM.<br/>},
key = {Social networking (online)},
keywords = {Indexing (of information);},
note = {Cluster management;Consistency model;Design Principles;Distributed clusters;Hierarchical document;Large database;MySQL;Transactions;},
URL = {http://dx.doi.org/10.1145/2463676.2465298},
} 


@inproceedings{20105213520211 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {FlexTable: Using a dynamic relation model to store RDF data},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wang, Yan and Du, Xiaoyong and Lu, Jiaheng and Wang, Xiaofang},
volume = {5981 LNCS},
number = {PART 1},
year = {2010},
pages = {580 - 594},
issn = {03029743},
abstract = {Efficient management of RDF data is an important factor in realizing the Semantic Web vision. The existing approaches store RDF data based on triples instead of a relation model. In this paper, we propose a system called FlexTable, where all triples of an instance are coalesced into one tuple and all tuples are stored in relation schemas. The main technical challenge is how to partition all the triples into several tables, i.e. it is needed to design an effective and dynamic schema structure to store RDF triples. To deal with this challenge, we firstly propose a schema evolution method called LBA, which is based on a lattice structure to automatically evolve schemas while new triples are inserted. Secondly, we propose a novel page layout with an interpreted storage format to reduce the physical adjustment cost during schema evolution. Finally we perform comprehensive experiments on two practical RDF data sets to demonstrate that FlexTable is superior to the state-of-the-art approaches. &copy; Springer-Verlag Berlin Heidelberg 2010.<br/>},
key = {Semantic Web},
keywords = {Digital storage;},
note = {Efficient managements;FlexTable;Lattice;RDF data;Relational Model;Semantic web vision;State-of-the-art approach;Technical challenges;},
URL = {http://dx.doi.org/10.1007/978-3-642-12026-8_44},
} 


@inproceedings{20124515650292 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Advances in Conceptual Modeling - ER 2012 Workshops: CMS, ECDM-NoCoDA, MoDIC, MORE-BI, RIGiM, SeCoGIS, WISM, Proceedings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {7518 LNCS},
year = {2012},
issn = {03029743},
address = {Florence, Italy},
abstract = {The proceedings contain 42 papers. The topics discussed include: a rule based approach for mapping sensor data to ontological models in AAL environments; parameters for service level agreements generation in cloud computing: a client-centric vision; a client-centric ASM-based approach to identity management in cloud computing; static analysis of XML document adaptations; supporting database provenance under schema evolution; contextual recommendations for groups; a scientific hypothesis conceptual model; an integrated multidimensional modeling approach to access big data in business intelligence platforms; towards discovering ontological models from big RDF data; towards scalable information modeling of requirements architectures; OLAP-like analysis of time point-based sequential data; multi-dimensional navigation modeling using bi analysis graphs; and ontology alignment for semantic data integration through foundational ontologies.},
} 


@article{20071410523123 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Composition of mappings given by embedded dependencies},
journal = {ACM Transactions on Database Systems},
author = {Nash, Alan and Bernstein, Philip A. and Melnik, Sergey},
volume = {32},
number = {1},
year = {2007},
issn = {03625915},
abstract = {Composition of mappings between schemas is essential to support schema evolution, data exchange data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this article, we study the issues nvolved in composing such mappings. Our algorithms and results extend those of Fagin et al. [2004], who studied the composition of mappings given by several kinds of constraints. In particular, they proved that full source-to-target tuple-generating dependencies(tgds) are closed under composition, but embedded source-to-target tgds are not. They introduced a class of second-order constraints, SO tgds, that is closed under composition and has desirable ro perties for data exchange. We study constraints that need not be source-to-target and we concentrate on obtaining (firstorder) embedded dependencies. As part of this study, we also consider full dependencies and secondorder constraints that arise from kolemizing embedded dependencies. For each of the three classes of mappings that we study, we provide: (a) an algorithm that attempts to compute the composition; and (b) sufficient conditions on the input mappings which guarantee that the algorithm will succeed. In addition, we give several negative results. In particular, we show that full and second-order dependencies that are not limited to be source-to-target are not closed under composition (for the latter, under the additional restriction that no new function symbols are introduced). Furthermore, we show that determining whether the composition can be given by these kinds of dependencies is undecidable. &copy; 2007 ACM.},
key = {Relational database systems},
keywords = {Algorithms;Computational complexity;Electronic data interchange;Information dissemination;Information management;},
note = {Dependencies;Metadata management;Second-order dependencies;},
URL = {http://dx.doi.org/10.1145/1206049.1206053},
} 


@inproceedings{20110113540958 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling concept evolution: A historical perspective},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Rizzolo, Flavio and Velegrakis, Yannis and Mylopoulos, John and Bykau, Siarhei},
volume = {5829 LNCS},
year = {2009},
pages = {331 - 345},
issn = {03029743},
abstract = {The world is changing, and so must the data that describes its history. Not surprisingly, considerable research effort has been spent in Databases along this direction, covering topics such as temporal models and schema evolution. A topic that has not received much attention, however, is that of concept evolution. For example, Germany (instance-level concept) has evolved several times in the last century as it went through different governance structures, then split into two national entities that eventually joined again. Likewise, a caterpillar is transformed into a butterfly, while a mother becomes two (maternally-related) entities. As well, the concept of Whale (a class-level concept) changed over the past two centuries thanks to scientific discoveries that led to a better understanding of what the concept entails. In this work, we present a formal framework for modeling, querying and managing such evolution. In particular, we describe how to model the evolution of a concept, and how this modeling can be used to answer historical queries of the form "How has concept X evolved over period Y". Our proposal extends an RDF-like model with temporal features and evolution operators. Then we provide a query language that exploits these extensions and supports historical queries. &copy; Springer-Verlag 2009.<br/>},
key = {Query processing},
keywords = {Data mining;Query languages;},
note = {Concept evolutions;Evolution operator;Governance structures;Historical perspective;Historical queries;Modeling concepts;Scientific discovery;Temporal features;},
URL = {http://dx.doi.org/10.1007/978-3-642-04840-1_25},
} 


@inproceedings{20183705813420 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - IDEAS 1998: International Database Engineering and Applications Symposium},
journal = {Proceedings - IDEAS 1998: International Database Engineering and Applications Symposium},
year = {1998},
pages = {Cardiff University; Concordia University; University of Bradford - },
address = {Cardiff, Wales, United kingdom},
abstract = {The proceedings contain 33 papers. The topics discussed include: stratum approaches to temporal DBMS implementation; logical data modeling of spatiotemporal applications: definitions and a model; dealing with version pertinence to design an efficient schema evolution framework; transitive dependencies in transaction closures; interactive query for image contents by semantic descriptors and multi-agent; storage sub-systems to support large functional databases; scalable mining for classification rules in relational databases; mining association rules with weighted items; on-line analytical processing in distributed data warehouses; and the design and implementation of an infrastructure for multimedia digital libraries.<br/>},
} 


@inproceedings{20100812724563 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 2nd International Workshop on Hot Topics in Software Upgrades, HotSWUp '09},
journal = {Proceedings of the 2nd International Workshop on Hot Topics in Software Upgrades, HotSWUp '09},
year = {2009},
pages = {ACM Special Interest Group on Programming Languages, SIGPLAN - },
address = {Orlando, FL, United states},
abstract = {The proceedings contain 9 papers. The topics discussed include: cooperative update: a new model for dependable live update; dynamic software updates for real-time systems; on performance of delegation in java; online application upgrade using edition-based redefinition; automating database schema evolution in information system upgrades; an implementation of the Linux software repository model for other operating systems; dynamic software updates: the state mapping problem; migrating protocols in multi-threaded message-passing systems; and efficient systematic testing for dynamically updatable software.},
} 


@inproceedings{20122515123420 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {How clean is your sandbox? Towards a unified theoretical framework for incremental bidirectional transformations},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Terwilliger, James F. and Cleve, Anthony and Curino, Carlo A.},
volume = {7307 LNCS},
year = {2012},
pages = {1 - 23},
issn = {03029743},
address = {Prague, Czech republic},
abstract = {Bidirectional transformations (bx) constitute an emerging mechanism for maintaining the consistency of interdependent sources of information in software systems. Researchers from many different communities have recently investigated the use of bxto solve a large variety of problems, including relational view update, schema evolution, data exchange, database migration, and model co-evolution, just to name a few. Each community leveraged and extended different theoretical frameworks and tailored their use for specific sub-problems. Unfortunately, the question of how these approaches actually relate to and differ from each other remains unanswered. This question should be addressed to reduce replicated efforts among and even within communities, enabling more effective collaboration and fostering cross-fertilization. To effectively move forward, a systematization of these many theories and systems is now required. This paper constitutes a first, humble yet concrete step towards a unified theoretical framework for a tractable and relevant subset of bx approaches and tools. It identifies, characterizes, and compares tools that allow the incremental definition of bidirectional mappings between software artifacts. Identifying similarities between such tools yields the possibility of developing practical tools with wide-ranging applicability; identifying differences allows for potential new research directions, applying the strengths of one tool to another whose strengths lie elsewhere. &copy; 2012 Springer-Verlag.<br/>},
key = {Electronic data interchange},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Bidirectional mapping;Bidirectional transformation;Cross fertilization;Database migrations;Software artifacts;Software systems;Sources of informations;Theoretical framework;},
URL = {http://dx.doi.org/10.1007/978-3-642-30476-7_1},
} 


@inproceedings{20093912331442 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Pivoted table index for querying product-property-value information},
journal = {Proceedings of the 3rd International Conference on Ubiquitous Information Management and Communication, ICUIMC'09},
author = {Lee, Hyunja and Shim, Junho},
year = {2009},
pages = {58 - 62},
address = {Suwon, Korea, Republic of},
abstract = {The query for triple information on product-attribute (property)-value is one of the most frequent queries in e-commerce. Storing the triple, schema vertically is effective for avoidance of sparse data and schema evolution. However, conventional horizontal schema often shows better query performance when properties are queried as groups clustered by product. Therefore, we propose vertical schema as a primary table structure for the triple information in RDBMS and a pivoted table index created from the basic vertical table. This index is beneficial to performance of the frequent pattern query on the group properties associated with each product class. Copyright 2009 ACM.<br/>},
key = {Electronic commerce},
keywords = {Information management;Ontology;},
note = {Frequent pattern queries;Group properties;Index;Pivot;Product attributes;Query performance;RDBMS;Vertical schema;},
URL = {http://dx.doi.org/10.1145/1516241.1516253},
} 


@inproceedings{20183905873155 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {7th International Provenance and Annotation Workshop, IPAW 2018},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {11017 LNCS},
year = {2018},
issn = {03029743},
address = {London, United kingdom},
abstract = {The proceedings contain 33 papers. The special focus in this conference is on Provenance and Annotation. The topics include: Belief propagation through provenance graphs; Using provenance to efficiently propagate SPARQL updates on RDF source graphs; implementing data provenance in health data analytics software; quine: A temporal graph system for provenance storage and analysis; capturing provenance for runtime data analysis in computational science and engineering applications; UniProv - Provenance management for UNICORE workflows in HPC environments; Towards a PROV ontology for simulation models; capturing the provenance of internet of things deployments; towards transparency of IoT message brokers; provenance of dynamic adaptations in user-steered dataflows; provenance-based root cause analysis for revenue leakage detection: A telecommunication case study; Case base reasoning decision support using the DecPROV ontology for decision modelling; bottleneck patterns in provenance; architecture for template-driven provenance recording; combining provenance management and schema evolution; provenance for entity resolution; where provenance in database storage; streaming provenance compression; structural analysis of whole-system provenance graphs; a graph testing framework for provenance network analytics; classification of provenance triples for scientific reproducibility: A comparative evaluation of deep learning models in the provcare project; provenance for astrophysical data; data Provenance in Agriculture; extracting provenance metadata from privacy policies; Provenance-enabled stewardship of human data in the GDPR Era; a provenance model for the european union general data protection regulation; Automating provenance capture in software engineering with UML2PROV; simulated domain-specific provenance; Versioned-PROV: A PROV extension to support mutable data entities; using the provenance from astronomical workflows to increase processing efficiency.<br/>},
} 


@inproceedings{20071210495628 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution - The other side of the XML update coin},
journal = {Proceedings - International Workshop on Biomedical Data Engineering, BMDE2005},
author = {Klettke, Meike and Meyer, Holger and Hansel, Birger},
volume = {2005},
year = {2005},
pages = {The IEEE Computer Society; The Database Society of Japan, DBSJ; Information Processing Society of Japan, IPSJ; The Inst. of Elec., Info. and Com. Engineers, IEICE - },
address = {Tokyo, Japan},
abstract = {Updates for XML are a quite new research area. Many applications need not only to transform or search XML documents but to alter them. Nowadays, there are some suggestions for update languages, and some XML database systems support updates operations via special APIs. In all update languages the content and the structure of documents can be changed. That means, documents may invalidate the XML schema after or during accomplishing an update. In this article, we suggest different architectures for processing such updates. Some of them reject updates that violate the schema, others entail a schema evolution that generalizes the schema. We point out necessary evolution steps and show implications of the schema evolution. With the discussion of architectures for executing updates we will focus on problems that can occur by updating XML documents. These questions have to be considered in all implementations that update XML documents. &copy; 2005 IEEE.},
key = {XML},
keywords = {Database systems;Problem solving;Search engines;Software architecture;},
note = {Evolution of XML schemas;Incremental XML document validation;Transactions;Update languages;XML updates;},
URL = {http://dx.doi.org/10.1109/ICDE.2005.219},
} 


@article{20145200371805 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema change operations for full support of schema versioning in the xSchema framework},
journal = {International Journal of Information Technology and Web Engineering},
author = {Brahmia, Zouhaier and Grandi, Fabio and Oliboni, Barbara and Bouaziz, Rafik},
volume = {9},
number = {2},
year = {2014},
pages = {20 - 46},
issn = {15541045},
abstract = {&tau;XSchema (Currim et al., 2004) is a framework (a language and a suite of tools) for the creation and validation of time-varying XML documents. A &tau;XSchema schema is composed of a conventional XML Schema annotated with physical and logical annotations. All components of a &tau;XSchema schema can evolve over time to reflect changes in the real-world. Since many applications need to keep track of both data and schema evolution, schema versioning has been long advocated to be the best solution to do this. In this paper, we complete the &tau;XSchema framework, which is predisposed from the origin to support schema versioning, with the definition of the operations which are necessary to exploit such feature and make schema versioning functionalities available to final users. Moreover, we propose a new technique for schema versioning in &tau;XSchema, allowing a complete and safe management of schema changes. It supports both versioning of conventional schema and versioning of annotations, in an integrated manner. For each component of a &tau;XSchema schema, our approach provides a complete and sound set of change primitives and a set of high-level change operations, for the maintenance of such a component and defines their operational semantics.<br/> Copyright &copy; 2014, IGI Global.},
key = {XML},
keywords = {Semantics;},
note = {Conventional schema;Logical annotations;Physical annotations;Schema versioning;Temporal Database;Temporal schema;XML schemas;},
URL = {http://dx.doi.org/10.4018/ijitwe.2014040102},
} 


@inproceedings{20065010296923 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Management of executable schema mappings for XML data exchange},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Pankowski, Tadeusz},
volume = {4254 LNCS},
year = {2006},
pages = {264 - 277},
issn = {03029743},
address = {Munich, Germany},
abstract = {Executable schema mappings between XML schemas are essential to support numerous data management tasks such as data exchange, data integration and schema evolution. The novelty of this paper consists in a method for automatic generation of automappings (automorphisms) from key constraints and value dependencies over XML schemas, and designing algebraic operations on mappings and schemas represented by automappings. During execution of mappings some missing or incomplete data may be inferred. A well-defined executable semantics for mappings and operations on mappings are proposed. A mapping language XDMap to specify XML schema mappings is discussed. The language allows to specify executable mappings that can be used to compute target instances from source instances preserving key constraints and value dependencies. The significance of mappings and operators over mappings is discussed on a scenario of data exchange in a P2P setting. &copy; Springer-Verlag Berlin Heidelberg 2006.},
key = {XML},
keywords = {Automation;Conformal mapping;Constraint theory;Information dissemination;Information management;Semantics;},
note = {Automappings;Data exchange;Data integration;Data management;},
} 


@inproceedings{20145200373092 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
volume = {P-234},
year = {2014},
issn = {16175468},
address = {Luxembourg, Luxembourg},
abstract = {The proceedings contain 9 papers. The topics discussed include: flexibility and evolution in process-aware information systems: all problems solved ?; on the role of process models in risk and disaster information management; towards an analysis driven approach for adapting enterprise architecture languages; outlining a graphical model query approach based on graph matching; choosing an adequate level of detail in business process modelling; designing and implementing a framework for event-based predictive modelling of business processes; BPMN extension for business process monitoring; towards schema evolution in object-aware process management systems; on the usability of business process modelling tools - a review and future research directions; visual analytics for supporting manufacturers and distributors in online sales; and business process as a service - status and architecture.},
} 


@inproceedings{20171203452941 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Accelerating data queries on Hadoop framework by using compact data formats},
journal = {2016 IEEE 4th Workshop on Advances in Information, Electronic and Electrical Engineering, AIEEE 2016 - Proceedings},
author = {Plase, Daiga and Niedrite, Laila and Taranovs, Romans},
year = {2016},
pages = {IEEE Lithuania and Latvia Sections; Riga Technical University (RTU) - },
address = {Vilnius, Lithuania},
abstract = {There are massive amounts of data generated from IoT, online transactions, click streams, emails, logs, posts, social networking interactions, sensors, mobile phones and their applications etc. The question is where and how to store these data in order to provide faster data access. Understanding and handling Big Data is a big challenge. The research direction in Big Data projects using Hadoop Technology, MapReduce kind of framework and compact data formats such as RCFile, SequenceFile, ORC, Avro, Parquet shows that only two data formats (Avro and Parquet) support schema evolution and compression in order to utilize less storage space. In this paper, file formats like Avro and Parquet are compared with text formats to evaluate the performance of the data queries. Different data query patterns have been evaluated. Cloudera's open-source Apache Hadoop distribution CDH 5.4 has been chosen for the experiments presented in this article. The results show that compact data formats (Avro and Parquet) take up less storage space when compared with plain text data formats because of binary data format and compression advantage. Furthermore data queries from the column based data format Parquet are faster when compared with text data formats and Avro.<br/> &copy; 2016 IEEE.},
key = {Big data},
keywords = {Computer software;Data handling;Digital storage;},
note = {Avro;Hadoop;HDFS;Hive;Parquet;},
URL = {http://dx.doi.org/10.1109/AIEEE.2016.7821807},
} 


@inproceedings{20154701592943 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An oo model for incremental hierarchical KA},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Beydoun, Ghassan},
volume = {2473},
year = {2002},
pages = {14 - 20},
issn = {03029743},
address = {Siguenza, Spain},
abstract = {Using a database management system (DBS) to build a knowledge base system (KBS) is sometimes desirable because DBS systems allow management of large sets of rules, control of concurrent access and managing multiple knowledge bases simultaneously. In this paper, we describe how to build a KBS using a database management system DBMS for its schema evolution ability. This allows the use of an Object Oriented DMBS (OODBMS) to manage the consistency of an incrementally built hierarchical knowledge base (KB). The underlying knowledge representation scheme, which we use, is our Nested Ripple Down Rules (NRDR). An NRDR KB evolves into a hierarchy of concepts where each concept is defined as a collection of hierarchical rules with exceptions. To modify a concept definition, exception rules are added by a domain expert, they are never deleted or modified. This eases maintenance and development of a concept definition, but may cause inconsistencies to occur in the KB. We analyse the relation between cases and rules as the knowledge base evolves and as these inconsistencies occur. We explore what specific features an OO database model should accommodate to be used to implement an NRDR KB. The aim is that this in turn allows the use of the built-in mechanisms to manage the consistency of an evolving NRDR conceptual hierarchy. The significance of this paper is two folds: first, it describes an efficient mechanism maintaining consistency of an evolving classification hierarchy, using built-in schema evolution features of an OODBMS. Second, it proposes an intelligent interface for an OODBMS, to allow intelligent classification queries over stored objects.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Object-oriented databases},
keywords = {Access control;Concurrency control;Knowledge based systems;Knowledge management;Knowledge representation;Management information systems;},
note = {Classification hierarchies;Concept definitions;Conceptual hierarchy;Database modeling;Hierarchical knowledge;Intelligent classification;Intelligent interface;Knowledge base system;},
} 


@article{1991030090597 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object-oriented database support for CAD},
journal = {CAD Computer Aided Design},
author = {Kim, Won and Banerjee, Jay and Chou, Hong-Tai and Garza, Jorge F.},
volume = {22},
number = {8},
year = {1990},
pages = {469 - 479},
issn = {00104485},
abstract = {ORION is a prototype object-oriented database system built in the Advanced Computer Technology (ACT) Program at MCC. It is intended to support the data management needs of applications in such domains as computer-aided design, artificial intelligence, and office information systems. The paper describes features of ORION which have been implemented specifically to support CAD environments. These include dynamic schema evolution, version control and change notification, and transaction management.},
key = {Database Systems},
keywords = {Computer Aided Design;},
note = {Object Orientism;Object-Oriented Databases;Orion Database System;Schema Evolution;Transaction Management;Version Control;},
URL = {http://dx.doi.org/10.1016/0010-4485(90)90063-I},
} 


@inproceedings{20164603023032 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards truly flexible and adaptive process-aware information systems},
journal = {Lecture Notes in Business Information Processing},
author = {Dadam, Peter and Reichert, Manfred and Rinderle, Stefanie and Jurisch, Martin and Acker, Hilmar and Goser, Kevin and Kreher, Ulrich and Lauer, Markus},
volume = {5},
year = {2008},
pages = {72 - 83},
issn = {18651348},
address = {Klagenfurt, Austria},
abstract = {If current process management systems shall be applied to a broad spectrum of applications, they will have to be significantly improved with respect to their technological capabilities. Particularly, in dynamic environments it must be possible to quickly implement and deploy new processes, to enable ad-hoc modifications of running process instances on-the-fly (e.g., to dynamically add, delete or move process steps), and to support process schema evolution with instance migration (i.e., to propagate process schema changes to already running instances if desired). These requirements must be met without affecting process consistency and by preserving the robustness of the process management system. In this paper we describe how these challenges have been addressed and solved in the ADEPT2 Process Management System. Our overall vision is to provide a next generation process management technology which can be used in a variety of application domains.<br/> &copy; Springer-Verlag Berlin Heidelberg 2008.},
key = {Management information systems},
keywords = {Computer system recovery;Information systems;Information use;},
note = {Adaptive process;Dynamic environments;Generation process;Instance migrations;Process consistency;Process management systems;Schema evolution;Technological capability;},
} 


@inproceedings{20085211816993 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A service-oriented architecture for virtual laboratory integration and management},
journal = {Proceedings of the 7th International Conference on Machine Learning and Cybernetics, ICMLC},
author = {Pan, Ding},
volume = {2},
year = {2008},
pages = {649 - 654},
address = {Kunming, China},
abstract = {Virtual Laboratory Information Management System (VLIMS) is crucial to a remote virtual laboratory integration and management. An architecture for the remote VLIMS was proposed. The architecture integrates heterogeneous experimental system, using expanded Web services mechanism, based on a real-time metadata management. In implementation, the schema evolution with version was proposed in the abstract by model management operators; Web services composition was shown by a business process example. It satisfies the requirements for information management of a virtual laboratory. &copy; 2008 IEEE.<br/>},
key = {Information management},
keywords = {Artificial intelligence;Information services;Laboratories;Learning systems;Metadata;Service oriented architecture (SOA);Web services;Websites;},
note = {Business Process;Experimental system;Meta model;Metadata management;Remote virtual laboratories;Schema evolution;Virtual laboratories;Web services composition;},
URL = {http://dx.doi.org/10.1109/ICMLC.2008.4620485},
} 


@inproceedings{20154801619487 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Instances evolution vs classes evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Tamzalit, Dalila and Oussalah, Chabane},
volume = {1677},
year = {1999},
pages = {16 - 25},
issn = {03029743},
address = {Florence, Italy},
abstract = {we propose a model developed to have inherent capabilities for auto-adaptation between classes and instances. Our two main objectives are: to allow objects to evolve their structures dynamically, with all necessary impacts on the database schema; to allow, similarly, the creation and display of different plans for evolving the design, like ways of schema evolution, giving in this way a simulation tool for database design and maintenance. Artificial Life and Genetic Algorithms inspired the idea of objects evolving and adapting to their environment. A model evolution is then considered in an auto-adaptive loop between classes and instances. Change is two-way: that coming down from class to object instantiates in development processes; that coming up from object to class in emergence processes which concern evolved instances which become not conform to any existing class.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Database systems},
keywords = {Computer control;Expert systems;Genetic algorithms;},
note = {Adaptive loops;Artificial Life;Auto adaptation;Database design;Database schemas;Development process;Model evolution;Schema evolution;},
} 


@article{2000335227827 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Consistent schema version removal: An optimization technique for object-oriented views},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Crestana-Jensen, Viviane M. and Lee, Amy J. and Rundensteiner, Elke A.},
volume = {12},
number = {2},
year = {2000},
pages = {261 - 280},
issn = {10414347},
abstract = {Powerful solutions enabling interoperability must allow applications to evolve and requirements of shared databases to change, while minimizing such changes on other integrated applications. Several approaches, such as the transparent schema evolution system (TSE) by Ra et al., schema versions by Lautemann, and integrated views by Bertino, have been proposed to make interoperability possible by using object-oriented techniques. These approaches may generate a large number of schema versions over time resulting in an excessive build-up of classes and underlying object instances, not all being necessarily still in use. This results in degradation of system performance due to the view maintenance and the storage overhead costs. In this paper, we address the problem of removing obsolete view schemas. We characterize four potential problems of schema consistency that could be caused by removal of a single derived class. We demonstrate that schema version removal is sensitive to the order in which individual classes are processed, and present a formal dependency model that captures all dependencies between classes as logic clauses and manipulates them to make decisions on class deletions and nondeletions while guaranteeing the consistency of the schema. We have also developed and proven consistent a dependency graph (DG) representation of the formal model. Lastly, we present a cost model for evaluating alternative removal patterns on DG to assure selection of the optimal solution. The proposed techniques have been implemented in our Schema View Removal (SVR) tool. Lastly, we report experimental findings for applying our techniques for consistent schema version removal on the MultiView/TSE system.},
key = {Database systems},
keywords = {Computer systems programming;Interoperability;Object oriented programming;Response time (computer systems);},
note = {Dependency graph (DG);Transparent schema evolution (TSE) systems;},
URL = {http://dx.doi.org/10.1109/69.842266},
} 


@inproceedings{20093012219270 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {AutonomousDB: A tool for autonomic propagation of schema updates in heterogeneous multi-database environments},
journal = {Proceedings of the 5th International Conference on Autonomic and Autonomous Systems, ICAS 2009},
author = {Moraes, Arlei Calazans and Salgado, Ana Carolina and Tedesco, Patricia Azevedo},
year = {2009},
pages = {251 - 256},
address = {Valencia, Spain},
abstract = {One of the biggest challenges of building and maintaining applications with long life cycles, is dealing with the inevitable changes in requirements that occur over time. Many of these applications depend on DBMS that most often suffer direct consequences in their schemas due to changes in the reality that they model. In this work, we propose a new alternative to the issue of schema evolution in multi-database environments, which uses concepts of Autonomic Computing in DBMS to propagate updates in schemas replicated within the same environment, thus ensuring their consistency. Our prototype counts with a Multi-Agent System, which executes events for updating schemas in the target DBMS, which may be of different platforms. Repetitive tasks which are performed in different databases to ensure the evolution of replicated schemas are thus eliminated, freeing the DBA to do other tasks that are more important, such as analysis, database design and strategic management of data. &copy; 2009 IEEE.<br/>},
key = {Database systems},
keywords = {Autonomous agents;Life cycle;Multi agent systems;},
note = {Autonomic Computing;Autonomy;Database design;Long life cycles;Propagated updates;Repetitive task;Schema evolution;Strategic management;},
URL = {http://dx.doi.org/10.1109/ICAS.2009.14},
} 


@inproceedings{1992094150345 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Versions and change notification in an object-oriented database system.},
journal = {Proceedings - Design Automation Conference},
author = {Chou, Hong-Tai and Kim, Won},
year = {1988},
pages = {275 - 281},
issn = {01467123},
address = {Anaheim, CA, USA},
abstract = {The authors have built a prototype object-oriented database system called ORION to support applications from the CAD/CAM (computer-aided-design/computer-aided-manufacturing), AI (artificial-intelligence), and office-information-system domains. Advanced functions supported in ORION include versions, change notification, composite objects, dynamic schema evolution, and multimedia data. The versions and change notification features are based on a model that the authors developed earlier. They have integrated their model of versions and change notification into the ORION object-oriented data model, and also provide an insight into system overhead that versions and change notification incur.},
key = {DATABASE SYSTEMS},
keywords = {ARTIFICIAL INTELLIGENCE;COMPUTER AIDED DESIGN;OFFICE AUTOMATION;},
note = {CHANGE NOTIFICATION;COMPOSITE OBJECTS;DYNAMIC SCHEMA EVOLUTION;MULTIMEDIA DATA;OBJECT-ORIENTED DATABASE;},
} 


@inproceedings{20154201386663 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A temporal query language for OLAP: Implementation and a case study},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vaisman, Alejandro A. and Mendelzon, Alberto O.},
volume = {2397},
year = {2002},
pages = {78 - 96},
issn = {03029743},
address = {Frascati, Italy},
abstract = {Commercial OLAP systems usually treat OLAP dimensions as static entities. In practice, dimension updates are often necessary in order to adapt the multidimensional database to changing requirements. In earlier work we proposed a temporal multidimensional model and TOLAP, a query language supporting it, accounting for dimension updates and schema evolution at a high level of abstraction. In this paper we present our implementation of the model and the query language. We show how to translate a TOLAP program to SQL, and present a real-life case study, a medical center in Buenos Aires. We apply our implementation to this case study in order to show how our approach can address problems that occur in real situations and that current non-temporal commercial systems cannot deal with. We present results on query and dimension update performance, and briefly describe a visualization tool that allows editing and running TOLAP queries, performing dimension updates, and browsing dimensions across time.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Query processing},
keywords = {High level languages;Query languages;},
note = {Commercial systems;Dimension updates;High level of abstraction;Multi-dimensional model;Multidimensional database;Schema evolution;Temporal queries;Visualization tools;},
} 


@inproceedings{20073910822469 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fast identification of relational constraint violations},
journal = {Proceedings - International Conference on Data Engineering},
author = {Chandel, Amit and Koudas, Nick and Pu, Ken Q. and Srivastava, Divesh},
year = {2007},
pages = {776 - 785},
issn = {10844627},
address = {Istanbul, Turkey},
abstract = {Logical constraints, (e.g., 'phone numbers in toronto can have prefixes 416, 647, 905 only'), am ubiquitous in relational databases. Traditional integrity constraints, such as functional dependencies, are examples of such logical constraints as well. However, under frequent database updates, schema evolution and transformations, they can be easily violated. As a result, tables become inconsistent and data quality is degraded. In this paper we study the problem of validating collections of user defined constraints on a number of relational tables. Our primary goal is to quickly identify which tables violate such constraints. Logical constraints are potentially complex logical formuli, and we demonstrate that they cannot be efficiently evaluated by SQL queries. In order to enable fast identification of constraint violations, we propose to build and maintain specialized logical indices on the relational tables. We choose Boolean Decision Diagrams (BDD) as the index structure to aid in this task. We first propose efficient algorithms to construct and maintain such indices in a space efficient manner. We then describe a set of query re-write rules that aid in the efficient utilization of logical Indices during constraint validation. We have implemented our approach on top of a relational database and tested our techniques using large collections of real and synthetic data sets. Our results indicate that utilizing our techniques in conjunction with logical indices during constraint validation offers very significant performance advantages. &copy; 2007 IEEE.},
key = {Constraint theory},
keywords = {Boolean functions;Functional analysis;Identification (control systems);Indexing (of information);Mathematical transformations;Problem solving;Query processing;Relational database systems;Ubiquitous computing;},
note = {Boolean Decision Diagrams (BDD);Fast identification;Logical constraints;Relational constraint violations;Relational databases;Schema evolution;},
URL = {http://dx.doi.org/10.1109/ICDE.2007.367923},
} 


@inproceedings{20154801619644 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {If we refuse the inheritance},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Al-Jadir, Lina and Leonard, Michel},
volume = {1677},
year = {1999},
pages = {560 - 572},
issn = {03029743},
address = {Florence, Italy},
abstract = {Specialization is an abstract concept which expresses the IS-A relationship while inheritance is a mechanism which implements specialization. Our experiences in extended entity-relationship DBMSs and object-oriented DBMSs have shown that specialization can be implemented by several mechanisms. We propose in this paper a mechanism which is more flexible than inheritance with respect to object dynamics and schema evolution. In our &ldquo;hologram&rdquo; approach, an object is implemented by multiple instances which represent its many faceted nature. Those instances are linked together through aggregation links in a specialization hierarchy. Objects are dynamic since they can migrate between the classes of a hierarchy. Attributes and methods are not inherited but reached by navigating in a specialization hierarchy. Class views provide customized interfaces of classes. Our approach makes schema changes more pertinent and easier to understand.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Expert systems},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Abstract concept;Entity-relationship;Multiple instances;Object dynamics;Object oriented;Schema changes;Schema evolution;},
} 


@inproceedings{1994031196488 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object prototyping: Concept and specification language},
journal = {Proceedings - IEEE Computer Society's International Computer Software &amp; Applications Conference},
author = {Lee, Horng-Juing and Tsai, Wei-Tek},
year = {1993},
pages = {209 - 215},
issn = {07306512},
address = {Phoenix, AZ, USA},
abstract = {Object-Oriented (OO) database systems are becoming popular for CAD, VLSI, and CASE applications. One of the reasons is the support of prototyping through inheritance and/or schema evolution. In this paper, we propose another rapid prototyping technique: object prototyping. It assumes that a database schema is fixed and rapid prototyping is achieved by applying alternative values into attributes of an object. This is useful especially for design applications in which determining a design object's properties is not straightforward. We design a specification language based on the what-for-if construct for modeling the object prototyping. This specification language can 1) help the designer to simulate alternative design parameters to find out the optimal design parameters, 2) reduce the need for writing database programs, 3) support the development and construction of complex applications on top of the database, and 4) test the functionality and performance of design objects.},
key = {Database systems},
keywords = {Computational linguistics;Computer hardware description languages;Computer programming languages;Data structures;Object oriented programming;Query languages;},
note = {Database programs;Database schema;Design objects;Design parameters;Inheritance evolution;Object oriented database systems;Object prototyping;Schema evolution;Specification language;What-for-if expressions;},
} 


@inproceedings{20103213142277 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Experience with the CMS event data model},
journal = {Journal of Physics: Conference Series},
author = {Elmer, P. and Hegner, B. and Sexton-Kennedy, L.},
volume = {219},
number = {1 PART 3},
year = {2010},
issn = {17426588},
address = {Prague, Czech republic},
abstract = {The re-engineered CMS EDM was presented at CHEP in 2006. Since that time we have gained a lot of operational experience with the chosen model. We will present some of our findings, and attempt to evaluate how well it is meeting its goals. We will discuss some of the new features that have been added since 2006 as well as some of the problems that have been addressed. Also discussed is the level of adoption throughout CMS, which spans the trigger farm up to the final physics analysis. Future plans, in particular dealing with schema evolution and scaling, will be discussed briefly. &copy; 2010 IOP Publishing Ltd.<br/>},
key = {High energy physics},
keywords = {Physics;},
note = {Operational experience;Physics analysis;Schema evolution;},
URL = {http://dx.doi.org/10.1088/1742-6596/219/3/032022},
} 


@article{20141917689293 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multiple wide tables with vertical scalability in multitenant sensor cloud systems},
journal = {International Journal of Distributed Sensor Networks},
author = {Ma, Kun and Yang, Bo},
volume = {2014},
year = {2014},
issn = {15501329},
abstract = {Software-as-a-service (SaaS) has emerged as a new computing paradigm to provide reliable software on demand. With such an inspiring motivation, sensor cloud system can benefit from this infrastructure. Generally, sharing database and schema is the most commonly used data storage model in the cloud. However, the data storage of tenants in the cloud is approaching schema null and evolution issues. To address these limitations, this paper proposes multitenant multiple wide tables with vertical scalability by analyzing the features of multitenant data. To solve schema null issue, extended vertical part is used to trim down the amount of schema null values. To reduce probability of schema evolution, wide table is divided into multiple clusters that we called multiple wide tables. This design reaches the balance between tenant customizing and its performance. Besides, the partition and correctness of multiple wide tables with vertical scalability are discussed in detail. The experimental results indicate that the solution of our multiple wide tables with vertical scalability is superior to single wide table, and single wide table with vertical scalability in the aspects of spatial intensity and read performance. &copy; 2014 Kun Ma and Bo Yang.<br/>},
key = {Software as a service (SaaS)},
keywords = {Digital storage;Scalability;},
note = {Computing paradigm;Data storage models;Multi tenants;Multiple clusters;Read performance;Schema evolution;Spatial intensity;Vertical scalabilities;},
URL = {http://dx.doi.org/10.1155/2014/583686},
} 


@inproceedings{20082811366450 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On representing instance changes in adaptive process management systems},
journal = {Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE},
author = {Rinderle, Stefanie and Kreher, Ulrich and Lauer, Markus and Dadam, Peter and Reichert, Manfred},
year = {2006},
pages = {297 - 302},
issn = {15244547},
address = {Manchester, United kingdom},
abstract = {By separating the process logic from the application code process management systems (PMS) offer promising perspectives for automation and management of business processes. However, the added value of PMS strongly depends on their ability to support business process changes which can affect the process type as well as the process instance level. This does not only impose challenging conceptual issues (e.g., correctness of process schemata after changes) but also requires sophisticated implementation concepts with respect to efficient algorithms, flexible architectures, and reasonable treatment of resources. In this paper we sketch the general implementation concepts for representing process type and process instance data as well as for realizating process schema evolution. All these concepts have been developed and are currently implemented in the ADEPT2 prototype within the AristaFlow project. &copy;2006 IEEE.<br/>},
note = {Adaptive process management systems;Application codes;Business process change;Flexible architectures;Process instances;Process management systems;Process-instance level;Schema evolution;},
URL = {http://dx.doi.org/10.1109/WETICE.2006.51},
} 


@article{20152100875873 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Growing up with stability: How open-source relational databases evolve},
journal = {Information Systems},
author = {Skoulis, Ioannis and Vassiliadis, Panos and Zarras, Apostolos V.},
volume = {53},
year = {2015},
pages = {363 - 385},
issn = {03064379},
abstract = {Like all software systems, databases are subject to evolution as time passes. The impact of this evolution can be vast as a change to the schema of a database can affect the syntactic correctness and the semantic validity of all the surrounding applications. In this paper, we have performed a thorough, large-scale study on the evolution of databases that are part of larger open source projects, publicly available through open source repositories. Lehman's laws of software evolution, a well-established set of observations on how the typical software systems evolve (matured during the last forty years), has served as our guide towards providing insights on the mechanisms that govern schema evolution. Much like software systems, we found that schemata expand over time, under a stabilization mechanism that constraints uncontrolled expansion with perfective maintenance. At the same time, unlike typical software systems, the growth is typically low, with long periods of calmness interrupted by bursts of maintenance and a surprising lack of complexity increase.<br/> &copy; 2015 Elsevier Ltd. All rights reserved.},
key = {Open source software},
keywords = {Computer software maintenance;Semantics;},
note = {Lehman's laws;Open source projects;Open source repositories;Perfective maintenance;Relational Database;Schema evolution;Software Evolution;Stabilization mechanisms;},
URL = {http://dx.doi.org/10.1016/j.is.2015.03.009},
} 


@inproceedings{20180104614000 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Change management with roles},
journal = {Proceedings - 6th International Conference on Database Systems for Advanced Applications, DASFAA 1999},
author = {Lautemann, Sven-Eric},
year = {1999},
pages = {291 - 300},
address = {Hsinchu, Taiwan},
abstract = {Various proposals have been made to extend object-oriented languages and database systems with roles because they allow to weaken strict typing concepts and therefore can provide mechanisms for an object to change its type during its lifetime. This so-called object migration is not the only advantage offered by a suitable role model. This paper studies the possibilities to use roles for a general change management system that also includes support for schema evolution. OPAQUE, as the selected role model, can be extended with schema update mechanisms in a surprisingly clean and straightforward way including the concepts of versions and views as well.<br/> &copy; 1999 IEEE.},
key = {Object-oriented databases},
keywords = {Object oriented programming;},
note = {Change management;Object migration;Role model;Schema evolution;Update mechanisms;},
URL = {http://dx.doi.org/10.1109/DASFAA.1999.765763},
} 


@article{20144700216115 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database queries and constraints via lifting problems},
journal = {Mathematical Structures in Computer Science},
author = {Spivak, David I.},
volume = {24},
number = {6},
year = {2013},
issn = {09601295},
abstract = {Previous work has demonstrated that categories are useful and expressive models for databases. In the current paper we build on that model, showing that certain queries and constraints correspond to lifting problems, as found in modern approaches to algebraic topology. In our formulation, each SPARQL graph pattern query corresponds to a category-theoretic lifting problem, whereby the set of solutions to the query is precisely the set of lifts. We interpret constraints within the same formalism, and then investigate some basic properties of queries and constraints. In particular, to any database &pi;, we can associate a certain derived database Qry(&pi;) of queries on &pi;. As an application, we explain how giving users access to certain parts of Qry(&pi;), rather than direct access to &pi;, improves the ability to manage the impact of schema evolution.<br/> Copyright &copy; 2013 Cambridge University Press.},
key = {Query languages},
keywords = {Query processing;Topology;},
note = {Algebraic topology;Database queries;Graph patterns;Lifting problems;Schema evolution;Users access;},
URL = {http://dx.doi.org/10.1017/S0960129513000479},
} 


@article{1997433807409 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object-centered approach for modelling engineering design products: Combining description logic and object-oriented modelling},
journal = {Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM},
author = {Hakim, M.Maher and Garrett Jr., James H.},
volume = {11},
number = {3},
year = {1997},
pages = {187 - 198},
issn = {08900604},
abstract = {Class-centered data models, such as the object-oriented data model, are inadequate for supporting engineering design product models because of their lack of support for object evolution, schema evolution, and semantic and user-defined relationships. Description logic overcomes these limitations by providing constructs for intentional description of classes, relationships, and objects. By combining description logic with object-oriented modelling concepts, design product schemas and data can be uniformly represented and modified throughout the design process.},
key = {Data structures},
keywords = {Computational linguistics;Computer aided design;Computer simulation;Formal logic;Object oriented programming;Product design;},
note = {Engineering design;Object evolution;Schema evolution;},
} 


@inproceedings{20175004516844 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An approach to deriving object hierarchies from database schema and contents},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Chen, I-Min Amy and Lee, Rei-Chi},
volume = {542 LNAI Part F2},
year = {1991},
pages = {112 - 121},
issn = {03029743},
address = {Charlotte, NC, United states},
abstract = {Database design involves a set of activities, which include requirements analysis, conceptual database design, selection of a database management system, mapping from conceptual model to physical model, and the physical database design. Two problems that often confuse database designers in the conceptual database design phase are: (1) how to construct object hierarchies, and (2) how to specify object hierarchies. In this paper, we propose an adaptive approach utilizing a schema evolution process to address these two problems. The approach provides a set of heuristics to analyze the database schema as well as the database contents to provide substantial information to database designers for constructing and specifying object hierarchies.<br/> &copy; Springer-Verlag Berlin Heidelberg 1991.},
key = {Database systems},
keywords = {Intelligent systems;},
note = {Adaptive approach;Conceptual database design;Conceptual model;Database contents;Object hierarchy;Physical database;Requirements analysis;Schema evolution;},
URL = {http://dx.doi.org/10.1007/3-540-54563-8_75},
} 


@article{20162202448516 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Synchronization of queries and views upon schema evolutions: A survey},
journal = {ACM Transactions on Database Systems},
author = {Caruccio, Loredana and Polese, Giuseppe and Tortora, Genoveffa},
volume = {41},
number = {2},
year = {2016},
issn = {03625915},
abstract = {One of the problems arising upon the evolution of a database schema is that some queries and views defined on the previous schema version might no longer work properly. Thus, evolving a database schema entails the redefinition of queries and views to adapt them to the new schema. Although this problem has been mainly raised in the context of traditional information systems, solutions to it are also advocated in other database-related areas, such as Data Integration, Web Data Integration, and Data Warehouses. The problem is a critical one, since industrial organizations often need to adapt their databases and data warehouses to frequent changes in the real world. In this article, we provide a survey of existing approaches and tools to the problem of adapting queries and views upon a database schema evolution; we also propose a classification framework to enable a uniform comparison method among many heterogeneous approaches and tools.<br/>},
key = {Data integration},
keywords = {Classification (of information);Data warehouses;Query languages;Query processing;Surveys;Synchronization;},
note = {Classification framework;Comparison methods;Database schemas;Industrial organization;Real-world;Schema evolution;View synchronizations;Web data integration;},
URL = {http://dx.doi.org/10.1145/2903726},
} 


@inproceedings{20155201738491 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Current, legacy, and invalid tuples in conditionally evolving databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Jensen, Ole Guttorm and Bohlen, Michael},
volume = {2457},
year = {2002},
pages = {65 - 82},
issn = {03029743},
address = {Izmir, Turkey},
abstract = {After the schema of a relation has evolved some tuples no longer fit the current schema. The mismatch between the schema a tuple is supposed to have and the schema a tuple actually has is inherent to evolving schemas, and is the defining property of legacy tuples. Handling this mismatch is at the very core of a DBMS that supports schema evolution. The paper proposes tuple versioning as a structure for evolving databases that permits conditional schema changes and precisely keeps track of schema mismatches at the level of individual tuples. Together with the change history this allows the DBMS to correctly identify current, legacy, and invalid tuples. We give an algorithm that classifies tuples, in time and space proportional to the length of the change history. We show how tuple versioning supports a flexible semantics needed to accurately answer queries over evolving databases.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Database systems},
keywords = {Information systems;Information use;Query processing;Semantics;},
note = {Change history;Evolving database;Schema changes;Schema evolution;Versioning;},
} 


@inproceedings{1996363246833 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 1996 6th International Workshop on Research Issues in Data Engineering, RIDE},
journal = {Proceedings of the IEEE International Workshop on Research Issues in Data Engineering},
year = {1996},
pages = {IEEE - },
address = {New Orleans, LA, USA},
abstract = {The proceedings contains 17 papers. Topics discussed include multidatabase systems, real time agent systems, transaction management, query processing, transparent schema evolution systems, distributed main memory.},
key = {Database systems},
keywords = {Algorithms;Computer architecture;Computer software;Data storage equipment;Management;Online systems;Query languages;Real time systems;Storage allocation (computer);},
note = {Distributed geoscientific query processing;EiRev;Interoperability;Software Package SKIPPER;Transaction management;Transparent schema evolution systems;Workflow management;},
} 


@article{20102913079411 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Metadata version management for DW 2.0 environment},
journal = {Journal of Convergence Information Technology},
author = {Pan, Ding},
volume = {5},
number = {3},
year = {2010},
pages = {54 - 61},
issn = {19759320},
abstract = {As a new paradigm for data warehousing demanded by today's decision support community, DW 2.0 recognized the life cycle of data with it, that make metadata evolution mechanism became one of the important research issues. The requirements of multi-version management for four data sectors in DW 2.0 environment are described. Then a novel metadata versioning meta-model is proposed, that is capable of storing and managing schemas versions, comparing and interpreting the results of versions queries, and tracing the version evolution. In implementation, the schema evolution with version is discussed in the abstract by model management operators; a verification engine to resolve the evolution inconsistencies is represented. The prototype has verified its feasibility and validity.<br/>},
key = {Environmental management},
keywords = {Data warehouses;Decision support systems;Life cycle;Metadata;Structural integrity;},
note = {Decision supports;DW 2.0;Evolution mechanism;Metadata versioning;Model management;Research issues;Schema evolution;Version management;},
} 


@inproceedings{20115214647163 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Organic databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Jagadish, H.V. and Nandi, Arnab and Qian, Li},
volume = {7108 LNCS},
year = {2011},
pages = {49 - 63},
issn = {03029743},
address = {Aizu-Wakamatsu, Japan},
abstract = {Databases today are carefully engineered: there is an expensive and deliberate design process, after which a database schema is defined; during this design process, various possible instance examples and use cases are hypothesized and carefully analyzed; finally, the schema is ready and then can be populated with data. All of this effort is a major barrier to database adoption. In this paper, we explore the possibility of organic database creation instead of the traditional engineered approach. The idea is to let the user start storing data in a database with a schema that is just enough to cove the instances at hand. We then support efficient schema evolution as new data instances arrive. By designing the database to evolve, we can sidestep the expensive front-end cost of carefully engineering the design of the database. The same set of issues also apply to database querying. Today, databases expect queries to be carefully specified, and to be valid with respect to the database schema. In contrast, the organic query specification model would allow users to construct queries incrementally, with little knowledge of the database. We also examine this problem in this paper. &copy; 2011 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Query processing},
keywords = {Cost engineering;Information systems;Information use;Query languages;},
note = {Construct queries;Database creation;Database querying;Database schemas;Design process;Front end;Query Specification;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-25731-5_5},
} 


@article{20105213531267 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Inconsistency - Tolerant integrity checking},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Decker, Hendrik and Martinenghi, Davide},
volume = {23},
number = {2},
year = {2011},
pages = {218 - 234},
issn = {10414347},
abstract = {All methods for efficient integrity checking require all integrity constraints to be totally satisfied, before any update is executed. However, a certain amount of inconsistency is the rule, rather than the exception in databases. In this paper, we close the gap between theory and practice of integrity checking, i.e., between the unrealistic theoretical requirement of total integrity and the practical need for inconsistency tolerance, which we define for integrity checking methods. We show that most of them can still be used to check whether updates preserve integrity, even if the current state is inconsistent. Inconsistency-tolerant integrity checking proves beneficial both for integrity preservation and query answering. Also, we show that it is useful for view updating, repairs, schema evolution, and other applications. &copy; 2006 IEEE.<br/>},
key = {Computational methods},
keywords = {Information systems;},
note = {Inconsistency tolerances;Integrity checking;Integrity constraints;Integrity preservations;Query answering;Schema evolution;Theory and practice;View updating;},
URL = {http://dx.doi.org/10.1109/TKDE.2010.87},
} 


@inproceedings{20155001665502 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Inferring the principal type and the schema requirements of an OQL query},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Trigoni, A. and Bierman, G.M.},
volume = {2097},
year = {2001},
pages = {185 - 201},
issn = {03029743},
address = {Chilton, United kingdom},
abstract = {In this paper, we present an inference algorithm for OQL which both identifies the most general type of a query in the absence of schema type information, and derives the minimum type requirements a schema should satisfy to be compatible with this query. Our algorithm is useful in any database application where heterogeneity is encountered, for example, schema evolution, queries addressed against multiple schemata, inter-operation or reconciliation of heterogeneous schemata. Our inference algorithm is technically interesting as it concerns an object functional language with a rich semantics and complex type system. More precisely, we have devised a set of constraints and an algorithm to resolve them. Our resulting type inference system for OQL should be useful in any open distributed, or even semi-structured, database environment.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Distributed database systems},
keywords = {Inference engines;Query languages;Query processing;Semantics;},
note = {Complex type systems;Database applications;Functional languages;Inference algorithm;Schema evolution;Semi-structured;Type inferences;Type information;},
} 


@inproceedings{20083011404949 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An effective chromosome representation for optimising product quality},
journal = {Proceedings of the 2007 11th International Conference on Computer Supported Cooperative Work in Design, CSCWD},
author = {Tsai, Chen-Fang and Chao, Kuo-Ming},
year = {2007},
pages = {1032 - 1037},
address = {Melbourne, VIC, Australia},
abstract = {Optimising variables in the quality control of production can be a complex issue, as it may involve many different constraints and expectations on the quality of products which are normally provided from different organisations to form a multiple supply chain. The challenge of measuring product interdependence across various supply chains and identifying a trade-off between quality and cost is not trivial. In this research, which applies dynamic Genetic Algorithms, we propose a new approach to representing the problem domain within the chromosome which takes advantage of schema evolution and domain knowledge to refine the chromosome structure. As a result, different weightings can be derived and applied to the genes in order to improve searching efficiency of the Genetic Algorithms (GA). An example of multiple supply chains has been used to evaluate the proposed approach. The results show that the proposed approach outperforms traditional GA approaches. &copy;2007 IEEE.<br/>},
key = {Quality control},
keywords = {Chromosomes;Economic and social effects;Genetic algorithms;Interactive computer systems;Supply chains;},
note = {Chromosome structure;Dynamic genetic algorithms;Multiple supplies;Multiple supply chain optimizations;Quality of product;Schema evolution;Searching efficiency;Weight rankings and contribution ratio;},
URL = {http://dx.doi.org/10.1109/CSCWD.2007.4281581},
} 


@inproceedings{20160801978015 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution of schema and individuals of configurable products},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Mannisto, Tomi and Sulonen, Reijo},
volume = {1727},
year = {1999},
pages = {12 - 23},
issn = {03029743},
address = {Paris, France},
abstract = {The increasing importance of better customisation of industrial products has led to development of configurable products. They allow companies to provide product families with a large number, typically millions of variants. Description and management of a large product variety within a single data model is challenging and requires a solid conceptual basis. This management differs from the schema evolution of traditional databases and the crucial distinction is the role of schema. For configurable products, the schema changes more frequently and more radically. In this paper, the characteristics that distinguish configurable products from traditional data modelling and management are investigated taking into account both the evolution of the schema and the instances. In this respect, the existing data modelling approaches and product data management systems are inadequate. Therefore, a new conceptual framework for product data management systems of configurable products that allows relatively independent evolution of schema and instances is proposed.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Information management},
keywords = {Data mining;Information systems;Information use;Management information systems;Reverse engineering;World Wide Web;},
note = {Conceptual frameworks;Independent evolutions;Industrial product;Product data management systems;Product families;Product variety;Schema changes;Schema evolution;},
URL = {http://dx.doi.org/10.1007/3-540-48054-4_2},
} 


@inproceedings{20100412655289 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A mapreduce framework for change propagation in geographic databases},
journal = {ICEIS 2009 - 11th International Conference on Enterprise Information Systems, Proceedings},
author = {Di Martino, Ferdinando and Sessa, Salvatore and Polese, Giuseppe and Vacca, Mario},
volume = {DISI},
year = {2009},
pages = {31 - 36},
address = {Milan, Italy},
abstract = {Updating a schema is a very important activity which occurs naturally during the life cycle of database systems, due to different causes. A challenging problem arising when a schema evolves is the change propagation problem, i.e. the updating of the database ground instances to make them consistent with the evolved schema. Spatial datasets, a stored representation of geographical areas, are VLDBs and so the change propagation process, involving an enormous mass of data among geographical distributed nodes, is very expensive and call for efficient processing. Moreover, the problem of designing languages and tools for spatial data sets change propagation is relevant, for the shortage of tools for schema evolution, and, in particular, for the limitations of those for spatial data sets. In this paper, we take in account both efficiency and limitations and we propose an instance update language, based on the efficient and popular Map-Reduce Google programming paradigm, which allows to perform in a parallel way a wide category of schema changes. A system embodying the language has been implementing.<br/>},
key = {Information systems},
keywords = {Database systems;Information use;Life cycle;},
note = {Change propagation;Geographic database;Map-reduce;Schema evolution;Spatial datasets;Update languages;},
} 


@article{20124515651600 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Binary XML storage and query processing in Oracle 11g},
journal = {Proceedings of the VLDB Endowment},
author = {Zhang, Ning and Agarwal, Nipun and Chandrasekar, Sivasankaran and Idicula, Sam and Medi, Vijay and Petride, Sabina and Sthanikam, Balasubramanyam},
volume = {2},
number = {2},
year = {2009},
pages = {1354 - 1365},
issn = {21508097},
abstract = {Oracle RDBMS has supported XML data management for more than six years since version 9i. Prior to 11g, textcentric XML documents can be stored as-is in a CLOB column and schema-based data-centric documents can be shredded and stored in object-relational (OR) tables mapped from their XML Schema. However, both storage formats have intrinsic limitations-XML/CLOB has unacceptable query and update performance, and XML/OR requires XML schema. To tackle this problem, Oracle 11g introduces a native Binary XML storage format and a complete stack of data management operations. Binary XML was designed to address a wide range of real application problems encountered in XML data management-schema flexibility, amenability to XML indexes, update performance, schema evolution, just to name a few. In this paper, we introduce the Binary XML storage format based on Oracle SecureFiles System[21]. We propose a lightweight navigational index on top of the storage and an NFA-based navigational algorithm to provide efficient streaming processing. We further optimize query processing by exploiting XML structural and schema information that are collected in database dictionary. We conducted extensive experiments to demonstrate high performance of the native Binary XML in query processing, update, and space consumption. &copy; 2009 VLDB Endowment.<br/>},
key = {XML},
keywords = {Digital storage;Query processing;},
note = {Database dictionaries;Management operation;Object-relational;Real applications;Schema evolution;Schema information;Space consumption;Streaming processing;},
URL = {http://dx.doi.org/10.14778/1687553.1687561},
} 


@inproceedings{20162502520433 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Storage and querying of high dimensional sparsely populated data in compressed representation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Latiful Hoque, Abu Sayed M.},
volume = {2510},
year = {2002},
pages = {418 - 425},
issn = {03029743},
address = {Shiraz, Iran},
abstract = {Storage and querying of high dimensional sparsely populated data creates new challenge to conventional horizontal model. It requires supportinglarg e number of columns and frequently alteringof database schema. The sparsity of data degrades performance in both time and space. A 3-ary vertical representation [5] can be used. But the cardinality of the vertical table grows exponentially when the density of the non-null values increases. It is also difficult to support multiple data types usinga single vertical table. In this paper, we have presented a compressed 1-ary vertical representation where schema evolution is easy and size grows linearly with non-null density. Queries can be processed on compressed form of data without decompression. Decompression is done only when the result is necessary. We have considered three alternative representations: 3-ary uncompressed vertical, 1-ary compressed bit-array and 1-ary compressed offset. Experimental results show the superiority of 1-ary offset representation in both space and time.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Digital storage},
keywords = {Query processing;},
note = {Bit arrays;Cardinalities;Database schemas;High-dimensional;Horizontal models;Multiple data types;Schema evolution;Space and time;},
} 


@inproceedings{20141417541167 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Storage and querying of high dimensional sparsely populated data in compressed representation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Hoque, Abu Sayed M. Latiful},
volume = {2510 LNCS},
year = {2002},
pages = {418 - 425},
issn = {03029743},
address = {Shiraz, Iran},
abstract = {Storage and querying of high dimensional sparsely populated data creates new challenge to conventional horizontal model. It requires supporting large number of columns and frequently altering of database schema. The sparsity of data degrades performance in both time and space. A 3-ary vertical representation [5] can be used. But the cardinality of the vertical table grows exponentially when the density of the non-null values increases. It is also difficult to support multiple data types usinga single vertical table. In this paper, we have presented a compressed 1-ary vertical representation where schema evolution is easy and size grows linearly with non-null density. Queries can be processed on compressed form of data without decompression. Decompression is done only when the result is necessary. We have considered three alternative representations: 3-ary uncompressed vertical, 1-ary compressed bit-array and 1-ary compressed offset. Experimental results show the superiority of 1-ary offset representation in both space and time. &copy; Springer-Verlag Berlin Heidelberg 2002.<br/>},
key = {Digital storage},
keywords = {Query processing;},
note = {Cardinalities;Database schemas;High-dimensional;Horizontal models;Multiple data types;Null value;Schema evolution;Space and time;},
} 


@article{2006169824387 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Formal approach to modelling a multiversion data warehouse},
journal = {Bulletin of the Polish Academy of Sciences: Technical Sciences},
author = {Bebel, B. and Krolikowski, Z. and Wrembel, R.},
volume = {54},
number = {1},
year = {2006},
pages = {51 - 62},
issn = {02397528},
abstract = {A data warehouse (DW) is a large centralized database that stores data integrated from multiple, usually heterogeneous external data sources (EDSs). DW content is processed by so called On-Line Analytical Processing applications, that analyze business trends, discover anomalies and hidden dependencies between data. These applications are part of decision support systems. EDSs constantly change their content and often change their structures. These changes have to be propagated into a DW, causing its evolution. The propagation of content changes is implemented by means of materialized views. Whereas the propagation of structural changes is mainly based on temporal extensions and schema evolution, that limits the application of these techniques. Our approach to handling the evolution of a DW is based on schema and data versioning. This mechanism is the core of, so called, a multiversion data warehouse. A multiversion DW is composed of the set of its versions. A single DW version is in turn composed of a schema version and the set of data described by this schema version. Every DW version stores a DW state which is valid within a certain time period. In this paper we present: (1) a formal model of a multiversion data warehouse, (2) the set of operators with their formal semantics that support a DW evolution, (3) the impact analysis of the operators on DW data and user analytical queries. The presented formal model was a basis for implementing a multiversion DW prototype system.},
key = {Formal languages},
keywords = {Computer simulation;Data reduction;Decision support systems;Mathematical models;Online systems;Semantics;Software prototyping;},
note = {Data evolution;Data versioning;Formal model;Multiversion data warehouse;Schema evolution;Schema versioning;},
} 


@inproceedings{20154801614445 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Safely Managing Data Variety in Big Data Software Development},
journal = {Proceedings - 1st International Workshop on Big Data Software Engineering, BIGDSE 2015},
author = {Cerqueus, Thomas and Almeida, Eduardo Cunha De and Scherzinger, Stefanie},
year = {2015},
pages = {4 - 10},
address = {Florence, Italy},
abstract = {We consider the task of building Big Data software systems, offered as software-as-a-service. These applications are commonly backed by NoSQL data stores that address the proverbial Vs of Big Data processing: NoSQL data stores can handle large volumes of data and many systems do not enforce a global schema, to account for structural variety in data. Thus, software engineers can design the data model on the go, a flexibility that is particularly crucial in agile software development. However, NoSQL data stores commonly do not yet account for the veracity of changes when it comes to changes in the structure of persisted data. Yet this is an inevitable consequence of agile software development. In most NoSQL-based application stacks, schema evolution is completely handled within the application code, usually involving object mapper libraries. Yet simple code refactorings, such as renaming a class attribute at the source code level, can cause data lossor runtime errors once the application has been deployed to production. We address this pain point by contributing type checking rules that we have implemented within an IDE plug in. Our plug in Contro Vol statically type checks the object mapper class declarations against the code release history. Contro Vol is thus capable of detecting common yet risky cases of mismatched data and schema, and can even suggest automatic fixes.<br/> &copy; 2015 IEEE.},
key = {Big data},
keywords = {Agile manufacturing systems;Codes (symbols);Computer programming languages;Data handling;Software as a service (SaaS);Software design;},
note = {Agile software development;Application codes;Data store;Release history;Run-time errors;Schema evolution;Software systems;Typechecking;},
URL = {http://dx.doi.org/10.1109/BIGDSE.2015.9},
} 


@inproceedings{20142717879882 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An adapter-based approach to co-evolve generated SQL in model-to-text transformations},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Garcia, Jokin and Diaz, Oscar and Cabot, Jordi},
volume = {8484 LNCS},
year = {2014},
pages = {518 - 532},
issn = {03029743},
address = {Thessaloniki, Greece},
abstract = {Forward Engineering advocates for code to be generated dynamically through model-to-text transformations that target a specific platform. In this setting, platform evolution can leave the transformation, and hence the generated code, outdated. This issue is exacerbated by the perpetual beta phenomenon in Web 2.0 platforms where continuous delta releases are a common practice. Here, manual co-evolution becomes cumbersome. This paper looks at how to automate - fully or in part - the synchronization process between the platform and the transformation. To this end, the transformation process is split in two parts: the stable part is coded as a MOFScript transformation whereas the unstable side is isolated through an adapter that is implicitly called by the transformation at generation time. In this way, platform upgrades impact the adapter but leave the transformation untouched. The work focuses on DB schema evolution, and takes MediaWiki as a vivid case study. A first case study results in the upfront cost of using the adapter paying off after three releases MediaWiki upgrades. &copy; 2014 Springer International Publishing.<br/>},
key = {Computer software},
keywords = {Information systems;Information use;Systems engineering;},
note = {Co-evolution;Common practices;Forward engineerings;Generation time;Model to text transformations;Schema evolution;Synchronization process;Transformation process;},
URL = {http://dx.doi.org/10.1007/978-3-319-07881-6_35},
} 


@inproceedings{20173704142197 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution towards, in, and beyond object databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Scholl, Marc H. and Tresch, Markus},
volume = {777 LNCS},
year = {1994},
pages = {64 - 82},
issn = {03029743},
address = {Hamburg, Germany},
abstract = {There is a manifold of meanings we could associate with the term &ldquo;evolution&rdquo; in the database arena. This paper tries to categorize some of these into a unique framework, showing similarities and differences. Among the topics touched upon are: extending traditional data models to become &ldquo;object-oriented&rdquo;, migrating existing data to (not necessarily OO) databases, schema extension and modification in a populated database, integration of federated systems, and the use of &ldquo;external services&rdquo; to enrich DBMS functionalities. The following are presented in more detail: first, we describe the necessity of object evolution over time; second, we discuss schema evolution; and third, we present evolutionary database interoperability by identifying different coupling levels. A few basic mechanisms, such as views (derived information) and a uniform treatment of data and meta data, and type and/or class hierarchies, allow for a formal description of (most of) the relevant problems. Beyond presenting our own approach, we try to provide a platform to solicit further discussion.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Information management},
keywords = {Artificial intelligence;Data handling;Information systems;Information use;Object-oriented databases;},
note = {Basic mechanism;Class hierarchies;Database interoperability;Federated systems;Formal Description;Object oriented;Schema evolution;},
} 


@inproceedings{20124815726139 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact analysis of the schema changes on distributed data warehouse systems},
journal = {WMSCI 2007 - The 11th World Multi-Conference on Systemics, Cybernetics and Informatics, Jointly with the 13th International Conference on Information Systems Analysis and Synthesis, ISAS 2007 - Proc.},
author = {Italiano, Isabel Cristina and De Almeida, Jorge Rady},
volume = {1},
year = {2007},
pages = {65 - 70},
address = {Orlando, FL, United states},
abstract = {As the use of data warehouse is becoming more and more important in almost all organizations around the world, business requirements demand analytical information at anytime and anywhere. In order to address these business users' requirements, it is useful to keep data warehouse information stored in the user workstation, thus allowing local information analysis. In general, users' workstations have limited resources and users just need specific subsets of the data warehouse. This paper introduces a framework, describing the data structures and the processes needed to implement data marts at mobile platforms (laptops) synchronized with a central data warehouse, considering different users characteristics and needs. One critical issue of the framework must be carefully examined: the replication of schema evolution between the central data warehouse and local data marts at mobile platforms. As business requirements demand new functionalities, data warehouse schema must evolve and be updated and consequently local data marts schema need to reflect all these changes. Therefore, this paper also introduces a classification of schema changes usually found at data warehouse systems, evaluates the schema change complexity and estimates its impact on other data warehouse objects.<br/>},
key = {Data warehouses},
keywords = {Cybernetics;Information systems;Information use;Management information systems;Mobile phones;Systems analysis;},
note = {Business requirement;Critical issues;Data warehouse systems;Distributed data warehouse;Impact analysis;Mobile platform;Schema evolution;Specific subset;},
} 


@inproceedings{20173504106719 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application and system prototyping via an extensible object-oriented environment},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Morsi, Magdi M. A. and Navathe, Shamkant B.},
volume = {823 LNCS},
year = {1994},
pages = {24 - 33},
issn = {03029743},
address = {Arlington, TX, United states},
abstract = {Due to the various, requirements of nontraditional applications, extensibility of database management systems has become a theme for several research projects. In this paper, we present an object-oriented design and implementation of an extensible environment for object-oriented databases. At the outset, we classify extensibility into logical and physical extensibility. We further categorize the three predominent ways to achieve the above extensibility features. Our approach can be categorized as a mixture of the open architecture and open class hierarchy approaches. The schema level is captured as classes and the instance level as objects. Logical extensibility arises because the database schema components, such as Class, InstanceVariable, and Method, are manipulated as user classes and are subject to schema evolution operations. Physical extensibility in the system is provided by making the system implementation information available to the database engineer for adding a new subclass, such as a new access method, to the implementation class hierarchy. Also the methods of the implementation classes can be modified, such as query optimization, resulting in changing the behavior of the system for supporting new operators.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Object-oriented databases},
keywords = {Data mining;Query processing;},
note = {Class hierarchies;Extensible objects;Object oriented design;Open architecture;Query optimization;Schema evolution;System implementation;System prototyping;},
} 


@inproceedings{20141117451566 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {LabFlow-l: A database benchmark for high-throughput workflow management},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bonnet, Anthony and Shrufi, Adel and Rozen, Steve},
volume = {1057 LNCS},
year = {1996},
pages = {463 - 478},
issn = {03029743},
address = {Avignon, France},
abstract = {Workflow management is a ubiquitous task faced by many organizations, and entails the coordination of various activities. This coordination is increasingly carried out by software systems called workflow management systems (WFMS). An important component of many WFMSs is a DBMS for keeping track of workflow activity. This DBMS maintains an audit trail, or event history, that records the results of each activity. Like other data, the event history can be indexed and queried, and views can be defined on top of it. In addition, a WFMS must accommodate frequent workflow changes, which result from a rapidly evolving business environment. Since the database schema depends on the workflow, the DBMS must also support dynamic schema evolution. These requirements are especially challenging in high-throughput WFMSs-i.e., systems for managing high-volume, mission-critical workflows. Unfortunately, existing database benchmarks do not capture the combination of flexibility and performance required by these systems. To address this issue, we have developed LabFlow-1, the first version of a benchmark that concisely captures the DBMS requirements of high-throughput WFMSs. LabFlow-1 is based on the data and workflow management needs of a large genome-mapping laboratory, and reflects their real-world experience. In addition, we use LabFlow-1 to test the usability and performance of two object storage managers. These tests revealed substantial differences between these two systems, and highlighted the critical importance of being able to control locality of reference to persistent data.<br/>},
key = {Database systems},
keywords = {Benchmarking;Digital storage;Throughput;Work simplification;},
note = {Business environments;Locality of reference;Mission critical;Real-world experience;Schema evolution;Software systems;Workflow management systems;Workflow managements;},
} 


@inproceedings{20154701594946 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {LabFlow-1: A database benchmark for high-throughput workflow management},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bonner, Anthony and Shrufi, Adel and Rozen, Steve},
volume = {1057},
year = {1996},
pages = {463 - 478},
issn = {03029743},
address = {Avignon, France},
abstract = {Workflow management is a ubiquitous task faced by many organizations, and entails the coordination of various activities. This coordination is increasingly carried out by software systems called workflow management systems (WFMS). An important component of many WFMSs is a DBMS for keeping track of workflow activity. This DBMS maintains an audit trail, or event history, that records the results of each activity. Like other data, the event history can be indexed and queried, and views can be defined on top of it. In addition, a WFMS must accommodate frequent workflow changes, which result from a rapidly evolving business environment. Since the database schema depends on the workflow, the DBMS must also support dynamic schema evolution. These requirements are especially challenging in high-throughput WFMSs&mdash;i.e., systems for managing high-volume, mission-critical workflows. Unfortunately, existing database benchmarks do not capture the combination of flexibility and performance required by these systems. To address this issue, we have developed LabFlow-1, the first version of a benchmark that concisely captures the DBMS requirements of high-throughput WFMSs. LabFlow-1 is based on the data and workflow management needs of a large genome-mapping laboratory, and reflects their real-world experience. In addition, we use LabFlow-1 to test the usability and performance of two object storage managers. These tests revealed substantial differences between these two systems, and highlighted the critical importance of being able to control locality of reference to persistent data.<br/> &copy; Springer-Verlag Berlin Heidelberg 1996.},
key = {Database systems},
keywords = {Benchmarking;Digital storage;Throughput;Work simplification;},
note = {Business environments;Locality of reference;Mission critical;Real-world experience;Schema evolution;Software systems;Workflow management systems;Workflow managements;},
} 


@inproceedings{20154101373248 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Visualisation of RDF(S)-based information},
journal = {Proceedings of the International Conference on Information Visualisation},
author = {Telea, Alexandru and Frasincar, Flavius and Houben, Geert-Jan},
volume = {2003-January},
year = {2003},
pages = {294 - 299},
issn = {10939547},
address = {London, United kingdom},
abstract = {As Resource Description Framework (RDF) reaches maturity, there is an increasing need for tools that support it. A common and natural representation for RDF data is a directed labeled graph. Although there are tools to edit and/or browse RDF graph representations, we found their architecture rigid and not easily amenable to producing effective visual representations, especially for large RDF graphs. We discuss here how GViz, a general purpose graph visualisation tool, allows the easy construction and fine-tuning of various visual exploratory scenarios for RDF data. GViz's extended ability of customizing the visualisation's icons showed to be very useful in the context of RDF graph structures visualisation. Among the presented applications, we mention customizable selections, schema-instance comparison, instances comparison, and schemas comparison (schema evolution). GViz proved to be able not only to visualize large RDF data models, but also to be very flexible in designing scenario-specific queries to support the exploration process.<br/> &copy; 2003 IEEE.},
key = {Visualization},
keywords = {Directed graphs;Information analysis;Information systems;Semantic Web;},
note = {Customizable;Exploration process;Fine tuning;Labeled graphs;Natural representation;Resource description framework;Schema evolution;Visual representations;},
URL = {http://dx.doi.org/10.1109/IV.2003.1217993},
} 


@inproceedings{20085011777971 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relational-style XML query},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
author = {Saito, Taro L. and Morishita, Shinichi},
year = {2008},
pages = {303 - 314},
issn = {07308078},
address = {Vancouver, BC, Canada},
abstract = {We study the problem of querying relational data embedded in XML. Relational data can be represented by various free structures in XML. However, current XML query methods, such as XPath and XQuery, demand explicit path expressions, and thus it is quite difficult for users to produce correct XML queries in the presence of structural variations. To solve this problem, we introduce a novel query method that automatically discovers various XML structures derived from relational data. A challenge in implementing our method is to reduce the cost of enumerating all possible tree structures that match the query. We show that the notion of functional dependencies has an important role in generating efficient query schedules that avoid irrelevant tree structures. Our proposed method, the relational-style XML query, has several advantages over traditional XML data management. These include removing the burden of designing strict tree-pattern schemas, enhancing the descriptions of relational data with XML's rich semantics, and taking advantage of schema evolution capability of XML. In addition, the independence of query statements from the underlying XML structure is advantageous for integrating XML data from several sources. We present extensive experimental results that confirm the scalability and tolerance of our query method for various sizes of XML data containing structural variations. Copyright 2008 ACM.<br/>},
key = {XML},
keywords = {Design;Forestry;Management;Semantics;Trees (mathematics);},
note = {Free structures;Functional dependency;Path expressions;Query statements;Relational data;Schema evolution;Structural variations;Tree structures;},
URL = {http://dx.doi.org/10.1145/1376616.1376650},
} 


@inproceedings{20113314233954 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparative study on data warehouse evolution techniques},
journal = {Communications in Computer and Information Science},
author = {Thakur, Garima and Gosain, Anjana},
volume = {190 CCIS},
number = {PART 1},
year = {2011},
pages = {691 - 703},
issn = {18650929},
abstract = {Data warehouse integrates data from various heterogeneous information sources under a unified structure to facilitate reporting &amp; analysis done by the organizations to provide strategic information to the decision support systems. These information sources are autonomous in nature and they frequently change their data owing to transactions being carried out within the organization and may change their schema due to evolving requirements. The existing requirements are updated and some new requirements are added in order to cope up with the latest business scenarios. In fact, data warehouse never ceases to evolve. Thus, appropriate techniques should be devised in order to handle the evolving data and schema changes so that the DW can be stored in its most updated version with all types of modifications being incorporated accurately to reflect the correct form of data subject to analysis. This paper provides a comprehensive comparison of various approaches, techniques and tools being developed by various researchers in order to resolve these issues. We have examined four techniques that address the DW evolution namely schema evolution, schema versioning, temporal warehousing and view maintenance and presented a brief tabular comparison of the explored methodologies based on various parameters. &copy; 2011 Springer-Verlag.<br/>},
key = {Data warehouses},
keywords = {Artificial intelligence;Decision support systems;},
note = {Data warehouse evolutions;Materialized view;Schema evolution;schema versions;View maintenance;},
URL = {http://dx.doi.org/10.1007/978-3-642-22709-7_67},
} 


@inproceedings{20180204625451 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ad-hoc recovery in workflows, a formal model and some system support aspects},
journal = {Proceedings - 1999 IFCIS International Conference on Cooperative Information Systems, CoopIS 1999},
author = {Tang, Jian},
year = {1999},
pages = {222 - 233},
address = {Edinburgh, United kingdom},
abstract = {How to increase flexibility has always been an issue relating to the applicability of any workflow system. We address this issue in one case where an agent needs to alter the control flow prescribed in the original definition to some previous executions. This phenomenon is termed ad-hoc recovery. Ad-hoc recovery is usually caused by unpredictable reasons, such as unexpected output of some individual tasks, events or exceptions due to the changing environment. Due to its irregularity in nature, ad-hoc recovery in general cannot be dealt with by approaches to handling failure recoveries. On the other hand, since it allows normal control flows to be diverged from at run time without modifying workflow schema, it enjoys higher flexibility than the fixed schema systems while pays a lower cost than schema evolution. In this paper, we introduce the concept of ad-hoc recovery, and characterize its main features. We also discuss some implementation issues.<br/>},
key = {Recovery},
keywords = {Information systems;Information use;Multi agent systems;},
note = {Changing environment;Control flows;Failure recovery;Formal model;Normal controls;Schema evolution;System supports;Work-flow systems;},
URL = {http://dx.doi.org/10.1109/COOPIS.1999.792172},
} 


@inproceedings{20153401195155 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards data warehouse schema design from social networks: Dynamic discovery of multidimensional concepts},
journal = {ICEIS 2015 - 17th International Conference on Enterprise Information Systems, Proceedings},
author = {Yangui, Rania and Nabli, Ahlem and Gargouri, Faiez},
volume = {1},
year = {2015},
pages = {338 - 345},
address = {Barcelona, Spain},
abstract = {This research work is conducting as part of the project BWEC (Business for Women in Women of Emerging Country) that aims to improve the socio-economic situation of handicraft women by providing true technological means. In fact, since few years, the Web has been transformed into an exchange platform where users have become the main suppliers of information through social media. User-generated data are usually rich and thus need to be analyzed to enhance decision. The storage and the centralization of these data in a data warehouse (DW) are highly required. Nevertheless, the growing complexity and volumes of the data to be analyzed impose new requirements on DW. In order to address these issues, in this paper, we propose four stages methodology to define a DW schema from social networks. Firstly we design the initial DW schema based on the existing approaches. Secondly, we apply a set of transformation rules to prepare the creation of the NOSQL(Not Only SQL) data warehouse. Then, based on user's requirement, clustering of social networks profiling data will be performed which allows the dynamic discovery of multidimensional concepts. Finally, the enrichment of the NoSQL DW schema by the discovered MC will be realized to ensure the DW schema evolution.<br/>},
key = {Metadata},
keywords = {Data warehouses;Digital storage;Employment;Information systems;Ontology;Scalability;Social networking (online);},
note = {Clustering;Dynamic discovery;Dynamicity;Emerging countries;Flexibility;Schema evolution;Socio-economics;Transformation rules;},
} 


@inproceedings{20190606485192 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The design and implementation of a database for human genome research},
journal = {Proceedings - 8th International Conference on Scientific and Statistical Data Base Management, SSDBM 1996},
author = {Sargent, R. and Fuhrman, D. and Critchlow, T. and Di Sera, T. and Mecklenburg, R. and Lindstrom, G. and Cartwright, P.},
year = {1996},
pages = {220 - 225},
address = {Stockholm, Sweden},
abstract = {The Human Genome Project poses severe challenges in database design and implementation. These include comprehensive coverage of diverse data domains and user constituencies; robustness in the presence of incomplete, inconsistent and multi-version data; accessibility through many levels of abstraction, and scalability in content and organizational complexity. The paper presents a new data model developed to meet these challenges by the Utah Center for Human Genome Research. The central characteristics are: (i) a high level data model comprising five broadly applicable workflow notions; (ii) representation of those notions as objects in an extended relational model; (iii) expression of working database schemas as meta data in administration tables; (iv) population of the database through tables dependent on the meta data tables; and (v) implementation via a conventional relational database management system. The authors explore two advantages of this approach: the resulting representational flexibility, and the reflective use of meta data to accomplish schema evolution by ordinary updates. Implementation and performance pragmatics of this work are sketched, as well as implications for future database development.<br/> &copy; 1996 IEEE.},
key = {Information management},
keywords = {Genes;Metadata;Population statistics;Reflection;},
note = {Design and implementations;Genome informatics;Implications for futures;Levels of abstraction;Organizational complexity;Relational data models;Relational database management systems;Schema evolution;},
URL = {http://dx.doi.org/10.1109/SSDM.1996.506064},
} 


@inproceedings{20190606477023 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The design and implementation of a database for human genome research},
journal = {Proceedings - 1996 Australian Software Engineering Conference, ASWEC 1996},
author = {Sargent, R. and Fuhrman, D. and Critchlow, T. and Di Sera, T. and Mecklenburg, R. and Lindstrom, G. and Cartwright, P.},
year = {1996},
pages = {220 - 225},
address = {Melbourne, VIC, Australia},
abstract = {The Human Genome Project poses severe challenges in database design and implementation. These include comprehensive coverage of diverse data domains and user constituencies; robustness in the presence of incomplete, inconsistent and multi-version data; accessibility through many levels of abstraction, and scalability in content and organizational complexity. The paper presents a new data model developed to meet these challenges by the Utah Center for Human Genome Research. The central characteristics are: (i) a high level data model comprising five broadly applicable workflow notions; (ii) representation of those notions as objects in an extended relational model; (iii) expression of working database schemas as meta data in administration tables; (iv) population of the database through tables dependent on the meta data tables; and (v) implementation via a conventional relational database management system. The authors explore two advantages of this approach: The resulting representational flexibility, and the reflective use of meta data to accomplish schema evolution by ordinary updates. Implementation and performance pragmatics of this work are sketched, as well as implications for future database development.<br/> &copy; 1996 IEEE.},
key = {Information management},
keywords = {Genes;Metadata;Population statistics;Reflection;Software engineering;},
note = {Design and implementations;Genome informatics;Implications for futures;Levels of abstraction;Organizational complexity;Relational data models;Relational database management systems;Schema evolution;},
URL = {http://dx.doi.org/10.1109/SSDM.1996.506064},
} 


@inproceedings{1996393261301 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design and implementation of a database for human genome research},
journal = {Scientific and Statistical Database Management - Proceedings of the International Working Conference},
author = {Sargent, Rob and Fuhrman, Dave and Critchlow, Terence and Di Sera, Tony and Mecklenburg, Robert and Lindstrom, Gary and Cartwright, Peter},
year = {1996},
pages = {220 - 225},
address = {Stockholm, Swed},
abstract = {The Human Genome Project poses severe challenges in database design and implementation. These include comprehensive coverage of diverse data domains and user constituencies; robustness in the presence of incomplete, inconsistent and multi-version data; accessibility through many levels of abstraction, and scalability in content and organizational complexity. This paper presents a new data model developed to meet these challenges by the Utah Center for Human Genome Research. The central characteristics are (i) a high level data model comprising five broadly applicable workflow notions; (ii) representation of those notions as objects in an extended relational model; (iii) expression of working database schemas as meta data in administration tables; (iv) population of the database through tables dependent on the meta data tables; and (v) implementation via a conventional relational database management system. We explore two advantages of this approach: the resulting representational flexibility, and the reflective use of meta data to accomplish schema evolution by ordinary updates. Implementation and performance pragmatics of this work are sketched, as well as implications for future database development.},
key = {Relational database systems},
keywords = {Data structures;Genetic engineering;Human engineering;Natural sciences computing;Systems analysis;},
note = {Data evolvability;Database schemas;Extended relational data model;Genome informatics;Human genome research;Meta data;Schema evolution;},
} 


@article{1997033427711 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Logic programming framework for modeling temporal objects},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Kesim, F.Nihan and Sergot, Marek},
volume = {8},
number = {5},
year = {1996},
pages = {724 - 741},
issn = {10414347},
abstract = {We present a general approach for modeling temporal aspects of objects in a logic programming framework. Change is formulated in the context of a database which stores explicitly a record of all changes that have occurred to objects and thus (implicitly) all states of objects in the database. A snapshot of the database at any given time is an object-oriented database, in the sense that it supports an object-based data model. An object is viewed as a collection of simple atomic formulas, with support for an explicit notion of object identity, classes and inheritance. The event calculus is a treatment of time and change in first-order classical logic augmented with negation as failure. The paper develops a variant of the event calculus for representing changes to objects, including change in internal state of objects, creation and deletion of objects, and mutation of objects over time. The concluding sections present two natural and straightforward extensions, to deal with versioning of objects and schema evolution, and a sketch of implementation strategies for practical application to temporal object-oriented databases.},
key = {Logic programming},
keywords = {Data structures;Database systems;Formal logic;Object oriented programming;},
note = {Event calculus;Object versioning;Schema evolution;Temporal aspect models;Temporal reasoning;},
URL = {http://dx.doi.org/10.1109/69.542026},
} 


@inproceedings{20101212784363 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual Universal Database Language (CUDL) and enterprise medical information systems},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Karanikolas, Nikitas N. and Skourlas, Christos and Nitsiou, Maria and Yannakoudakis, Emmanuel J.},
volume = {DISI},
year = {2008},
pages = {362 - 367},
address = {Barcelona, Spain},
abstract = {Today, there is an increased trend for Electronic Patient Records (EPR) incorporating and correlating heterogeneous information imported from various sources and from different medical applications. New possibilities are also given by the rapid technological progress and the development of independent software applications and tools that handle multimedia medical data. Moreover, users (e.g. doctors, nurses) often prefer to use general purpose software (e.g. word processors) and specific applications and tools for organizing and accessing medical data and they only partially use Hospital Information Systems (HIS). Therefore, it is necessary for the HIS to provide the capability for encapsulating in their EPR externally created information. Dynamic evolution of the HIS must also be supported by flexible Database schemata. In this paper, we conclude that modern HIS should be designed and implemented using database management systems offering new enhanced database models and manipulation languages. Eventually, we describe and discuss how to use the Frame Database Model (FDB) and the new Conceptual Universal Database Language (CUDL) for supporting dynamic schema evolution and covering the needs of the users.<br/>},
key = {Medical information systems},
keywords = {Application programs;Database systems;Hospitals;Information use;Management information systems;Medical applications;Medical computing;Object oriented programming;},
note = {Electronic patient record;General purpose software;Heterogeneous data;Heterogeneous information;Hospital information systems;Schema evolution;Software applications;Technological progress;},
} 


@inproceedings{20100412653638 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Generating SQL/XML query and update statements},
journal = {International Conference on Information and Knowledge Management, Proceedings},
author = {Nicola, Matthias and Kiefer, Tim},
year = {2009},
pages = {1187 - 1196},
address = {Hong Kong, China},
abstract = {The XML support in relational databases and the SQL/XML language are still relatively new as compared to purely relational databases and traditional SQL. Today, most database users have a strong relational and SQL background. SQL/XML enables users to perform queries and updates across XML and relational data, but many struggle with writing SQL/XML statements or XQuery update expressions. One reason is the novelty of SQL/XML and of the XQuery expressions that must be included. Another problem is that the tree structure of the XML data may be unknown or difficult to understand for the user. Evolving XML Schemas as well as hybrid XML/relational schemas make it even harder to write SQL/XML statements. Also, legacy applications use SQL but may require access to XML data without costly code changes. Motivated by these challenges, we developed a method to generate SQL/XML query and update statements automatically. The input is either a GUI or a regular SQL statement that uses logical data item names irrespective of their actual location in relational or XML columns in the database. The output is a SQL/XML statement that queries or updates relational and XML data as needed to carry out the original user statement. This relieves the user and simplifies schema evolution and integration. We have prototyped and tested the proposed method on top of DB2 9.5. Copyright 2009 ACM.<br/>},
key = {XML},
keywords = {Knowledge management;Mapping;Query processing;Translation (languages);Trees (mathematics);},
note = {Generation;Legacy applications;Relational data;Relational Database;Schema evolution;SQL/XML;XQuery;XQuery expressions;},
URL = {http://dx.doi.org/10.1145/1645953.1646104},
} 


@inproceedings{20102913089895 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Foundations of schema mapping management},
journal = {Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
author = {Arenas, Marcelo and Perez, Jorge and Reutter, Juan L. and Riveros, Cristian},
year = {2010},
pages = {227 - 238},
address = {Indianapolis, IN, United states},
abstract = {In the last few years, a lot of attention has been paid to the specification and subsequent manipulation of schema mappings, a problem which is of fundamental importance in metadata management. There have been many achievements in this area, and semantics have been defined for operators on schema mappings such as composition and inverse. However, little research has been pursued towards providing formal tools to compare schema mappings, in terms of their ability to transfer data and avoid storing redundant information, which has hampered the development of foundations for more complex operators as many of them involve these notions. In this paper, we address the problem of providing foundations for metadata management by developing an order to compare the amount of information transferred by schema mappings. From this order we derive several other criteria to compare mappings, we provide tools to deal with these criteria, and we show their usefulness in defining and studying schema mapping operators. More precisely, we show how the machinery developed can be used to study the extract and merge operators, that have been identified as fundamental for the development of a metadata management framework. We also use our machinery to provide simpler proofs for some fundamental results regarding the inverse operator, and we give an effective characterization for the decidability of the well-known schema evolution problem. &copy; 2010 ACM.<br/>},
key = {Information management},
keywords = {Data integration;Electronic data interchange;Inverse problems;Machinery;Metadata;Semantics;},
note = {Amount of information;Formal tools;Inverse operators;Merge operators;Metadata management;Model management;Schema evolution;Schema mappings;},
URL = {http://dx.doi.org/10.1145/1807085.1807116},
} 


@inproceedings{20160801978014 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Handling evolving data through the use of a description driven systems architecture},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Estrella, F. and Kovacs, Z. and Le Goff, J.-M. and McClatchey, R. and Zsenei, M.},
volume = {1727},
year = {1999},
pages = {1 - 11},
issn = {03029743},
address = {Paris, France},
abstract = {Traditionally product data and their evolving definitions, have been handled separately from process data and their evolving definitions. There is little or no overlap between these two views of systems even though product and process data arc inextricably linked over the complete software lifecycle from design to production. The integration of product and process models in an unified data model provides the means by which data could be shared across an enterprise throughout the lifecycle, even while that data continues to evolve. In integrating these domains, an object oriented approach to data modelling has been adopted by the CRISTAL (Cooperating Repositories and an Information System for Tracking Assembly Lifecycles) project. The model that has been developed is description-driven in nature in that it captures multiple layers of product and process definitions and it provides object persistence, flexibility, reusability, schema evolution and versioning of data elements. This paper describes the model that has been developed in CRISTAL and how descriptive meta-objects in that model have their persistence handled. It concludes that adopting a description-driven approach to modelling, aligned with a use of suitable object persistence, can lead to an integration of product and process models which is sufficiently flexible to cope with evolving data definitions.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Information management},
keywords = {Data integration;Data mining;Information systems;Information use;Life cycle;Object oriented programming;Product design;Reusability;Reverse engineering;World Wide Web;},
note = {Driven system;Evolving datum;Multiple layers;Object oriented approach;Process definition;Schema evolution;Software life cycles;Versioning;},
URL = {http://dx.doi.org/10.1007/3-540-48054-4_1},
} 


@inproceedings{20094912523163 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Collateral evolution of applications and databases},
journal = {International Workshop on Principles of Software Evolution (IWPSE)},
author = {Lin, Dien-Yen and Neamtiu, Iulian},
year = {2009},
pages = {31 - 40},
address = {Amsterdam, Netherlands},
abstract = {Separating the evolution of an application from the evolution of its persistent data, or from the evolution of the database system used to store the data can have collateral effects, such as data loss, program failure, or decreased performance. In this paper, we use empirical evidence to identify challenges and solutions associated with the collateral evolution of application programs and databases. We first perform an evolution study that identifies changes to database schemas in two popular open source applications. Next, we study the evolution of database file formats for three widely-used database management systems. We then investigate how application programs and database management systems cope with these changes, and point out how collateral evolution can lead to potential problems. Finally, we sketch solutions for facilitating and ensuring the safety of application and database evolution, hence minimizing collateral effects. Copyright 2009 ACM.<br/>},
key = {Application programs},
keywords = {Database systems;Management information systems;Open source software;},
note = {Collateral evolution;Empirical studies;Mozilla;Schema evolution;Schema migration;Software Evolution;Software transformation;Sqlite;},
URL = {http://dx.doi.org/10.1145/1595808.1595817},
} 


@inproceedings{20140917365709 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Optimising conceptual data models through profiling in object databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zaschke, Tilmann and Leone, Stefania and Gmunder, Tobias and Norrie, Moira C.},
volume = {8217 LNCS},
year = {2013},
pages = {284 - 297},
issn = {03029743},
address = {Hong Kong, China},
abstract = {Agile methods promote iterative development with short cycles, where user feedback from the previous iteration is used to refactor and improve the current version. For information systems development, we propose to extend this feedback loop by using database profiling information to propose adaptations to the conceptual model to improve performance. For every software release, our database profiler identifies and analyses navigational access patterns, and proposes model optimisations based on data characteristics, access patterns and a cost-benefit model. The proposed model optimisations are based on common database and data model refactoring patterns. The database profiler has been implemented as part of an open-source object database and integrated into an existing agile development environment, where the model optimisations are presented as part of the IDE. We evaluate our approach based on an example of agile development of a research publication system. &copy; Springer-Verlag 2013.<br/>},
key = {Agile manufacturing systems},
keywords = {Cost benefit analysis;Data mining;Information use;Iterative methods;Object-oriented databases;Open source software;},
note = {Agile development environments;Conceptual data models;Data characteristics;Improve performance;Information systems development;Iterative development;Optimisations;Profiling informations;},
URL = {http://dx.doi.org/10.1007/978-3-642-41924-9_24},
} 


@inproceedings{1983080124224 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {EVENT DATABASE SPECIFICATION MODEL.},
journal = {Proceedings of the International Conference on Databases},
author = {King, Roger and McLeod, Dennis},
year = {1982},
pages = {299 - 322},
address = {Jerusalem, Isr},
key = {DATABASE SYSTEMS},
note = {CONCEPTUAL SCHEMA DESIGN;DATA MANIPULATION;EVENT DATABASE SPECIFICATION MODEL;SCHEMA EVOLUTION CONSTRUCTS;},
} 


@inproceedings{20184806149231 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Curating variational data in application development},
journal = {Proceedings - IEEE 34th International Conference on Data Engineering, ICDE 2018},
author = {Storl, Uta and Muller, Daniel and Tekleab, Alexander and Tolale, Stephane and Stenzel, Julian and Klettke, Meike and Scherzinger, Stefanie},
year = {2018},
pages = {1605 - 1608},
address = {Paris, France},
abstract = {Building applications for processing data lakes is a software engineering challenge. We present Darwin, a middleware for applications that operate on variational data. This concerns data with heterogeneous structure, usually stored within a schema-flexible NoSQL database. Darwin assists application developers in essential data and schema curation tasks: Upon request, Darwin extracts a schema description, discovers the history of schema versions, and proposes mappings between these versions. Users of Darwin may interactively choose which mappings are most realistic. Darwin is further capable of rewriting queries at runtime, to ensure that queries also comply with legacy data. Alternatively, Darwin can migrate legacy data to reduce the structural heterogeneity. Using Darwin, developers may thus evolve their data in sync with their code. In our hands-on demo, we curate synthetic as well as real-life datasets.<br/> &copy; 2018 IEEE.},
key = {Data handling},
keywords = {Application programs;Mapping;Middleware;Query processing;},
note = {Data migration;Nosql database;Query rewritings;Schema evolution;Schema management;Variational data;},
URL = {http://dx.doi.org/10.1109/ICDE.2018.00187},
} 


@inproceedings{20173003975354 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Taming size and cardinality of OLAP data cubes over big data},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Cuzzocrea, Alfredo and Moussa, Rim and Laabidi, Achref},
volume = {10365 LNCS},
year = {2017},
pages = {113 - 125},
issn = {03029743},
address = {London, United kingdom},
abstract = {In this paper, we provide three authoritative application scenarios of TPC-H*d. The latter is a suitable transformation of TPC-H benchmark. The three application scenarios are (i) OLAP cube calculus on top of columnar relational DBMS, (ii) parallel OLAP data cube processing and (iii) virtual OLAP data cube design. We assess the effectiveness and the efficiency of our proposal, using open source systems, namely, Mondrian ROLAP server and its OLAP4j driver, MySQL - row oriented relational database management system and MonetDB -a column-oriented relational database management system.<br/> &copy; Springer International Publishing AG 2017.},
key = {Big data},
keywords = {Calculations;Data handling;Data warehouses;Geometry;Management information systems;Open systems;Relational database systems;},
note = {Application scenario;Column-oriented;Multidimensional database;Open source system;Relational database management systems;Relational DBMS;Schema evolution;Tpc-h benchmarks;},
URL = {http://dx.doi.org/10.1007/978-3-319-60795-5_12},
} 


@inproceedings{20170703341771 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An algorithm for correcting XSLT rules according to DTD updates},
journal = {Proceedings of the 4th International Workshop on Document Changes: Modeling, Detection, Storage and Visualization, DChanges 2016 - Part of the ACM Symposium on Document Engineering, DocEng 2016},
author = {Wu, Yang and Suzuki, Nobutaka},
year = {2016},
pages = {ACM SIGWEB - },
address = {Vienna, Austria},
abstract = {DTDs are continuously updated according to changes in the real world. Updates to a DTD affect the behavior of XSLT stylesheets as well as XML documents under the DTD. To maintain the consistencies of XSLT stylesheets with an updated DTD, we have to detect the XSLT rules affected by DTD updates and correct the affected XSLT rules so that the XSLT stylesheets transform documents under the updated DTD appropriately. However, correcting such affected rules manually are a difficult and time-consuming task, since recent DTDs are becoming larger and more complex and users do not always fully understand the dependencies between XSLT stylesheets and old/updated DTDs. In this paper, we propose an algorithm for correcting XSLT rules affected by DTD updates. We also present some experimental results.<br/> &copy; 2016 ACM.},
key = {XML},
keywords = {Visualization;},
note = {Real-world;Schema evolution;Time-consuming tasks;XSLT;},
URL = {http://dx.doi.org/10.1145/2993585.2993588},
} 


@inproceedings{20160501857947 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reflective constraint writing a symbolic viewpoint of modeling languages},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Draheim, Dirk},
volume = {9510},
year = {2016},
pages = {1 - 60},
issn = {03029743},
address = {Munich, Germany},
abstract = {In this article we show how to extend object constraint languages by reflection. We choose OCL (Object Constraint Language) and extend it by operators for reification and reflection. We show how to give precise semantics to the extended language OCLR by elaborating the necessary type derivation rules and value specifications. A driving force for the introduction of reflection capabilities into a constraint language is the investigation of semantics and pragmatics of modeling constructs. We exploit the resulting reflective constraint language in modeling domains including sets of sets of domain objects. We give precise semantics to UML power types. We carve out the notion of sustainable constraint writing which is about making models robust against unwanted updates. Reflective constraints are an enabler for sustainable constraint writing. We discuss the potential of sustainable constraint writing for emerging tools and technologies. For this purpose, we need to introduce a symbolic viewpoint of information system modeling.<br/> &copy; Springer-Verlag Berlin Heidelberg 2016.},
key = {Modeling languages},
keywords = {Expert systems;Object oriented programming;Semantics;},
note = {Clabjects;Database migrations;Generative programming;Genoupe;Meta model;Modeling tool;Multilevel model;Object Constraint Language;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-662-49214-7_1},
} 


@inproceedings{20111213769537 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An evolutionary design for software systems},
journal = {Proceedings - 2010 2nd World Congress on Nature and Biologically Inspired Computing, NaBIC 2010},
author = {Nguyen, Thuy-Linh},
year = {2010},
pages = {255 - 260},
abstract = {This paper explains Life Design, a bio-inspired approach towards building evolvable software, that is, software that can evolve itself similar to natural life forms. The design is informed by Darwinian evolution, gleaned from natural life, and based on object-orientation, meta-modelling and a web system. It can open a new and systematic way to develop customisable and adaptable systems, or evolve open source systems. &copy; 2010 IEEE.<br/>},
key = {Software design},
keywords = {Boron compounds;Iodine compounds;Open source software;Open systems;Sodium compounds;},
note = {Bio-inspired;Evolvable;Meta-modelling;Object orientation;Schema evolution;},
URL = {http://dx.doi.org/10.1109/NABIC.2010.5716356},
} 


@inproceedings{20142517846097 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution of the application and database with aspects},
journal = {ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems},
author = {Pereira, Rui Humberto R. and Perez-Schofield, J. Baltasar Garcia},
volume = {1},
year = {2014},
pages = {308 - 313},
address = {Lisbon, Portugal},
abstract = {Generally, the evolution process of applications has impact on their underlining data models, thus becoming a time-consuming problem for programmers and database administrators. In this paper we address this problem within an aspect-oriented approach, which is based on a meta-model for orthogonal persistent programming systems. Applying reflection techniques, our meta-model aims to be simpler than its competitors. Furthermore, it enables database multi-version schemas. We also discuss two case studies in order to demonstrate the advantages of our approach.<br/>},
key = {Aspect oriented programming},
keywords = {Database systems;Information systems;Information use;},
note = {Aspect-oriented;Case-studies;Database administrators;Evolution process;Instance adaptation;Multi-version;Programming system;Schema evolution;},
} 


@inproceedings{20155301741075 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {How is life for a table in an evolving relational schema? Birth, death and everything in between},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vassiliadis, Panos and Zarras, Apostolos V. and Skoulis, Ioannis},
volume = {9381},
year = {2015},
pages = {453 - 466},
issn = {03029743},
address = {Stockholm, Sweden},
abstract = {In this paper, we study the version history of eight databases that are part of larger open source projects, and report on our observations on how evolution-related properties, like the possibility of deletion, or the amount of updates that a table undergoes, are related to observable table properties like the number of attributes or the time of birth of a table. Our findings indicate that (i) most tables live quiet lives; (ii) few top-changers adhere to a profile of long duration, early birth, medium schema size at birth; (iii) tables with large schemata or long duration are quite unlikely to be removed, and, (iv) early periods of the database life demonstrate a higher level of evolutionary activity compared to later ones.<br/> &copy; Springer International Publishing Switzerland 2015.},
key = {Data mining},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Long duration;Open source projects;Patterns of change;Relational schemas;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-319-25264-3_34},
} 


@article{20143600040948 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Efficient adaptation of XML data using a conceptual model},
journal = {Information Systems Frontiers},
author = {Maly, Jakub and Neasky, Martin and Mlynkova, Irena},
volume = {16},
number = {4},
year = {2014},
pages = {663 - 696},
issn = {13873326},
abstract = {One of the prominent characteristics of XML applications is their dynamic nature. Changes in user requirements cause changes in schemas used in the systems and changes in the schemas subsequently make existing documents invalid. In this work, we study two tightly coupled problems&mdash;schema evolution and document adaptation. The presented approach extends an existing conceptual model for evolution of XML applications towards document adaptation, by introducing a formal framework for detecting changes between two versions of a schema. From the detected changes it is possible to create a script that transforms documents valid against the old version of the schema to documents valid against its new version.<br/> &copy; 2012, Springer Science+Business Media, LLC.},
key = {XML},
keywords = {Computer networks;Information systems;},
note = {Change management;Conceptual model;Document adaptation;Formal framework;Schema evolution;User requirements;XML applications;XML schema evolutions;},
URL = {http://dx.doi.org/10.1007/s10796-012-9375-8},
} 


@article{20144400143596 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual independence: A design principle for the construction of adaptive information systems},
journal = {Information Systems},
author = {McGinnes, Simon and Kapros, Evangelos},
volume = {47},
year = {2015},
pages = {33 - 50},
issn = {03064379},
abstract = {This paper examines the problem of conceptual dependence, the coupling of software applications internal structures and logic with their underlying conceptual models. Although conceptual dependence is almost universal in information system design, it produces a range of unintended negative consequences including system inflexibility and increased maintenance costs. Many information systems contain components, such as database tables and classes, whose design reflects the entity types and relationships in underlying, domain-oriented conceptual models. When the models change, work is involved in altering the software components. For example, an e-commerce system might include tables and classes representing product types, customers and orders, with associated code in methods, stored procedures and other scripts. The structure of the entity types and their relationships will be implicit in the tables, classes and code, coupling the system to its conceptual model. Any change to the model (such as the introduction of a new entity type, representing order lines) invalidates existing structures and code, causing rework. In large systems, this rework can be time-consuming and expensive. Research shows that schema change is common, and that it contributes significantly to the high cost of software maintenance. We argue that much of the cost may be avoidable if alternative design strategies are used. The paper describes an alternative design approach based on the principle of conceptual independence, which can be used to produce adaptive information systems (AIS). It decouples the internal structures and logic of information systems from the domain-specific entity types and relationships in the conceptual models they implement. An architecture for AIS is presented which includes soft schemas (conceptual models stored as data), an end-user conceptual modelling tool, a set of archetypal categories (predefined semantic categories), and an adaptive data model which allows data to be stored without conceptual dependence. The archetypal categories allow domain-specific run time behaviour to be provided, despite the absence of domain-specific software structure and logic. An advantage of AIS over conventionally-designed applications is that each AIS can be used in a wide variety of domains. AIS offer the prospect of significantly reduced maintenance costs, as well as increased scope for the development and modification of systems by end users. Work to date on implementation of the AIS architecture is discussed, and an agenda for future research is outlined including development and evaluation of a fully-featured AIS. The paper discusses challenges to be overcome and barriers to adoption.<br/> &copy; 2014 Elsevier Ltd.},
key = {Software design},
keywords = {Application programs;Codes (symbols);Computer circuits;Costs;Information systems;Information use;Maintenance;Semantics;},
note = {Adaptive information systems;Conceptual independence;Conceptual model;Data independence;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.is.2014.06.001},
} 


@inproceedings{20142717879872 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Open-source databases: Within, outside, or beyond Lehman's laws of software evolution?},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Skoulis, Ioannis and Vassiliadis, Panos and Zarras, Apostolos},
volume = {8484 LNCS},
year = {2014},
pages = {379 - 393},
issn = {03029743},
address = {Thessaloniki, Greece},
abstract = {Lehman's laws of software evolution is a well-established set of observations (matured during the last forty years) on how the typical software systems evolve. However, the applicability of these laws on databases has not been studied so far. To this end, we have performed a thorough, large-scale study on the evolution of databases that are part of larger open source projects, publicly available through open source repositories, and report on the validity of the laws on the grounds of properties like size, growth, and amount of change per version. &copy; 2014 Springer International Publishing.<br/>},
key = {Open source software},
keywords = {Database systems;Information systems;Information use;Open systems;Systems engineering;},
note = {Large-scale studies;Lehman's laws;Open source database;Open source projects;Open source repositories;Schema evolution;Software Evolution;Software systems;},
URL = {http://dx.doi.org/10.1007/978-3-319-07881-6_26},
} 


@inproceedings{20143618132128 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Storing Long-Lived Concurrent Schema and Data Versions in Relational Databases},
journal = {Advances in Intelligent Systems and Computing},
author = {Wall, Bob and Angryk, Rafal},
volume = {312},
year = {2015},
pages = {97 - 108},
issn = {21945357},
address = {Ohrid, Macedonia},
abstract = {Although there is a strong focus on NoSQL databases for cloud computing environments, traditional relational data bases are still an integral part of many computing services in the cloud. Two significant issues in managing a relational database in a cloud environment are handling the inevitable evolution of the database schema and managing changes to system configuration and other data stored in the database as the system evolves over time. Techniques for handling these issues in on-premise databases are much less feasible in cloud computing environments, which demand efficiency, elasticity, and scalability. We propose a versioning system that can be used in relational databases to allow new versions of the database schema and data to be maintained within the same database as the production data. Past research on versioning either handles data versioning but not schema changes, or handles both but is focused on OLAP or XML databases. In this paper, we describe a mechanism for storing concurrent versions of data in an OLTP database. We explore two different implementation alternatives for versioned data storage and evaluate their relative merits given different workloads. We provide a concrete description of how this can be implemented within the InnoDB storage engine, which is the default data store for MySQL databases, and we present a quantitative comparison of the two implementations in InnoDB. &copy; Springer International Publishing Switzerland 2015.<br/>},
key = {Cloud computing},
keywords = {Information systems;Information use;Virtual storage;},
note = {Cloud computing environments;Data versioning;Database as a service;Quantitative comparison;Relational Database;Schema evolution;System configurations;Versioning systems;},
URL = {http://dx.doi.org/10.1007/978-3-319-10518-5_8},
} 


@inproceedings{20121014841386 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A flexible graph-based data model supporting incremental schema design and evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Braunschweig, Katrin and Thiele, Maik and Lehner, Wolfgang},
volume = {7059 LNCS},
year = {2012},
pages = {302 - 306},
issn = {03029743},
address = {Paphos, Cyprus},
abstract = {Web data is characterized by a great structural diversity as well as frequent changes, which poses a great challenge for web applications based on that data. We want to address this problem by developing a schema-optional and flexible data model that supports the integration of heterogenous and volatile web data. Therefore, we want to rely on graph-based models that allow to incrementally extend the schema by various information and constraints. Inspired by the on-going web 2.0 trend, we want users to participate in the design and management of the schema. By incrementally adding structural information, users can enhance the schema to meet their very specific requirements. &copy; 2012 Springer-Verlag.<br/>},
key = {Data integration},
keywords = {Graph theory;Graphic methods;},
note = {Graph-based models;Schema design;Schema evolution;schema flexibility;Structural diversity;Structural information;WEB application;Web data;},
URL = {http://dx.doi.org/10.1007/978-3-642-27997-3_29},
} 


@inproceedings{20142717879888 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a form based dynamic database schema creation and modification system},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Malhotra, Kunal and Medhekar, Shibani and Navathe, Shamkant B. and Laborde, M. D. David},
volume = {8484 LNCS},
year = {2014},
pages = {595 - 609},
issn = {03029743},
address = {Thessaloniki, Greece},
abstract = {The traditional approach to relational database design starts with the conceptual design of an application based schema in a model like the Entity-relationship model, then mapping that to a logical design and eventually representing it as a set of related normalized tables. The project we present has been motivated by needs of healthcare-IT where small group practices are currently in need of systems that will cater to their dynamic requirements without depending on EMR (Electronic Medical Record) systems. It is also relevant for researchers for mining huge repositories of data such as social networks, etc. and create extracts of data on the fly for data analytics. Based on user characteristics and needs, the data is likely to vary and hence, a dynamic back-end database must be created. This paper addresses a form-based approach to schema creation and modification. &copy; 2014 Springer International Publishing.<br/>},
key = {User interfaces},
keywords = {Conceptual design;Information systems;Information use;Medical computing;Systems engineering;},
note = {Dynamic user interface;Electronic medical record;Entity-Relationship modeling;Relational Database;Schema evolution;Schema modification;Traditional approaches;User characteristics;},
URL = {http://dx.doi.org/10.1007/978-3-319-07881-6_40},
} 


@inproceedings{20121014841390 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML document versioning, revalidation and constraints},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Maly, Jakub and Necasky, Martin},
volume = {7059 LNCS},
year = {2012},
pages = {317 - 321},
issn = {03029743},
address = {Paphos, Cyprus},
abstract = {One of the prominent characteristics of XML applications is their dynamic nature. When a system grows and evolves, old user requirements change and/or new requirements accumulate. Apart from changes in the interfaces used/provided by the system or its components, it is also necessary to modify the existing documents with each new version, so they are valid against the new specification. In this doctoral work we will extend an existing conceptual modeling approach with the support for multiple versions of the model. Thanks to this extension, it will be possible to detect changes between two versions of a schema and generate revalidation script for the existing data. By adding integrity constraints to the model, it will be able to revalidate changes in semantics besides changes in structure. &copy; 2012 Springer-Verlag.<br/>},
key = {XML},
keywords = {Data mining;Semantics;},
note = {Conceptual model;constraints;Dynamic nature;Integrity constraints;Schema evolution;User requirements;XML applications;XML schemas;},
URL = {http://dx.doi.org/10.1007/978-3-642-27997-3_32},
} 


@article{20143218023134 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual independence: A design principle for the construction of adaptive information systems},
journal = {Information Systems},
author = {McGinnes, Simon and Kapros, Evangelos},
volume = {47},
year = {2015},
pages = {33 - 50},
issn = {03064379},
abstract = {This paper examines the problem of conceptual dependence, the coupling of software applications' internal structures and logic with their underlying conceptual models. Although conceptual dependence is almost universal in information system design, it produces a range of unintended negative consequences including system inflexibility and increased maintenance costs. Many information systems contain components, such as database tables and classes, whose design reflects the entity types and relationships in underlying, domain-oriented conceptual models. When the models change, work is involved in altering the software components. For example, an e-commerce system might include tables and classes representing product types, customers and orders, with associated code in methods, stored procedures and other scripts. The structure of the entity types and their relationships will be implicit in the tables, classes and code, coupling the system to its conceptual model. Any change to the model (such as the introduction of a new entity type, representing order lines) invalidates existing structures and code, causing rework. In large systems, this rework can be time-consuming and expensive. Research shows that schema change is common, and that it contributes significantly to the high cost of software maintenance. We argue that much of the cost may be avoidable if alternative design strategies are used. The paper describes an alternative design approach based on the principle of conceptual independence, which can be used to produce adaptive information systems (AIS). It decouples the internal structures and logic of information systems from the domain-specific entity types and relationships in the conceptual models they implement. An architecture for AIS is presented which includes soft schemas (conceptual models stored as data), an end-user conceptual modelling tool, a set of archetypal categories (predefined semantic categories), and an adaptive data model which allows data to be stored without conceptual dependence. The archetypal categories allow domain-specific run time behaviour to be provided, despite the absence of domain-specific software structure and logic. An advantage of AIS over conventionally-designed applications is that each AIS can be used in a wide variety of domains. AIS offer the prospect of significantly reduced maintenance costs, as well as increased scope for the development and modification of systems by end users. Work to date on implementation of the AIS architecture is discussed, and an agenda for future research is outlined including development and evaluation of a fully-featured AIS. The paper discusses challenges to be overcome and barriers to adoption. &copy; 2014 Elsevier Ltd.<br/>},
key = {Software design},
keywords = {Application programs;Codes (symbols);Computer circuits;Costs;Information systems;Information use;Maintenance;Semantics;},
note = {Adaptive information systems;Conceptual independence;Conceptual modelling;Data independence;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.is.2014.06.001},
} 


@inproceedings{20160601888212 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A datalog-based protocol for lazy data migration in agile NoSQL application development},
journal = {DBPL 2015 - Proceedings of the 15th Symposium on Database Programming Languages},
author = {Scherzinger, Stefanie and Storl, Uta and Klettke, Meike},
year = {2015},
pages = {41 - 44},
address = {Pittsburgh, PA, United states},
abstract = {We address a practical challenge in agile web development against NoSQL data stores: Upon a new release of the web application, entities already persisted in production no longer match the application code. Rather than migrating all legacy entities eagerly (prior to the release) and at the cost of application downtime, lazy data migration is a popular alternative: When a legacy entity is loaded by the application, all pending structural changes are applied. Yet correctly migrating legacy data from several releases back, involving more than one entity at-a-time, is not trivial. In this paper, we propose a holistic Datalog: non-rec model for reading, writing, and migrating data. In implementing our model, we may blend established Datalog evaluation algorithms, such as an incremental evaluation with certain rules evaluated bottom-up, and certain rules evaluated topdown with sideways information passing. Our systematic approach guarantees that from the viewpoint of the application, it remains transparent whether data is migrated eagerly or lazily.<br/> &copy; 2015 ACM.},
note = {Application codes;Application development;Data store;Datalog;Evaluation algorithm;Incremental evaluation;Schema evolution;Web development;},
URL = {http://dx.doi.org/10.1145/2815072.2815078},
} 


@inproceedings{20162102407369 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Controvol: Let yesterday's data catch up with today's application code},
journal = {WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web},
author = {Cerqueus, Thomas and De Almeida, Eduardo Cunha and Scherzinger, Stefanie},
year = {2015},
pages = {15 - 16},
address = {Florence, Italy},
abstract = {In building software-as-a-service applications, a exible development environment is key to shipping early and often. Therefore, schema-flexible data stores are becoming more and more popular. They can store data with heterogeneous structure, allowing for new releases to be pushed frequently, without having to migrate legacy data first. However, the current application code must continue to work with any legacy data that has already been persisted in production. To let legacy data structurally catch up" with the latest application code, developers commonly employ object mapper libraries with life-cycle annotations. Yet when used without caution, they can cause runtime errors and even data loss. We present ControVol, an IDE plugin that detects evolutionary changes to the application code that are incompatible with legacy data. ControVol warns developers already at development time, and even suggests automatic fixes for lazily migrating legacy data when it is loaded into the application. Thus, ControVol ensures that the structure of legacy data can catch up with the structure expected by the latest software release.<br/>},
key = {Application programs},
keywords = {Codes (symbols);Life cycle;Software as a service (SaaS);World Wide Web;},
note = {Application codes;Development environment;Development time;Evolutionary changes;Heterogeneous structures;Schema evolution;Software release;Web development;},
URL = {http://dx.doi.org/10.1145/2740908.2742719},
} 


@inproceedings{20142517846070 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A proposal to maintain the semantic balance in cluster-based data integration systems},
journal = {ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems},
author = {Silva, Edemberg Rocha and Loscio, Bernadette Farias and Salgado, Ana Carolina},
volume = {1},
year = {2014},
pages = {90 - 98},
address = {Lisbon, Portugal},
abstract = {With the large volume of data sources on the Web, we need a system that integrates them, so that the user can query them transparently. For efficiency in queries, integration systems can group these sources in clusters according to the semantic similarity of their schemas. However, the sources have autonomy to evolve their schema, and to join or to leave the integration system at any time. This autonomy may cause a problem which we define as semantic unbalance of clusters. The semantic unbalance can compromise the formation of clusters and hence the efficiency of the submitted queries. In this paper, we propose a solution to the semantic balance of clusters in dynamic data integration systems based on self-organization. We also introduce a measure to evaluate how much the clusters are semantically unbalanced.<br/>},
key = {Data integration},
keywords = {Efficiency;Information systems;Information use;Search engines;Semantics;},
note = {Clustering measure;Dynamic data;Schema evolution;Semantic balance;Semantic clusters;},
} 


@inproceedings{20124215583726 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Toward automated schema-directed code revision},
journal = {DocEng 2012 - Proceedings of the 2012 ACM Symposium on Document Engineering},
author = {Oliveira, Raquel and Geneves, Pierre and Layaida, Nabil},
year = {2012},
pages = {103 - 106},
address = {Paris, France},
abstract = {Updating XQuery programs in accordance with a change of the input XML schema is known to be a time-consuming and error-prone task. We propose an automatic method aimed at helping developers realign the XQuery program with the new schema. First, we introduce a taxonomy of possible problems induced by a schema change. This allows to differentiate problems according to their severity levels, e.g. errors that require code revision, and semantic changes that should be brought to the developer's attention. Second, we provide the necessary algorithms to detect such problems using a solver that checks satisfiability of XPath expressions. Copyright &copy; 2012 by the Association for Computing Machinery, Inc. (ACM).<br/>},
key = {XML},
keywords = {Semantics;},
note = {Automatic method;Error prone tasks;Satisfiability;Schema changes;Schema evolution;Schemas;XPath expressions;XQuery;},
URL = {http://dx.doi.org/10.1145/2361354.2361377},
} 


@article{20143600033736 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An interactive tool for UML class model evolution in database applications},
journal = {Software and Systems Modeling},
author = {Milovanovic, Vukasin and Milicev, Dragan},
volume = {14},
number = {3},
year = {2015},
pages = {1273 - 1295},
issn = {16191366},
abstract = {In the context of model-driven development of database applications with UML, the (usually relational) database schema is obtained automatically from the application&rsquo;s structural (class) UML model. Changes in requirements often lead to modifications of the application&rsquo;s structural model. Such changes, in turn, have to be propagated to the underlying database schema. Very often, especially when the system is in production with a large volume of users&rsquo; live data, the data is considered to be valuable enough to be preserved through these changes. This paper describes an approach to cope with the problem of model evolution with the ultimate requirement to preserve the data stored in the database. The algorithm interactively determines differences between structural UML models before and after the changes and resolves those differences into transformations in the relational database domain.<br/> &copy; 2013, Springer-Verlag Berlin Heidelberg.},
key = {Unified Modeling Language},
keywords = {XML;},
note = {Database applications;Model driven development;Model evolution;Object-relational mapping;Relational Database;Schema evolution;Schema matching;Structural modeling;},
URL = {http://dx.doi.org/10.1007/s10270-013-0378-9},
} 


@inproceedings{20160701946626 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Propagation of structural modifications to an integrated schema},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Motz, Regina},
volume = {1475},
year = {1998},
pages = {163 - 174},
issn = {03029743},
address = {Poznan, Poland},
abstract = {This paper addresses the problem of propagating local structural schema changes to an already acquired integrated schema of a federation. Our approach is to regard this problem from a schema integration point of view. The main contribution is the development of a framework for performing evolution of an integrated schema without information loss and avoiding as much as possible re-integration steps.<br/> &copy; Springer-Verlag Berlin Heidelberg 1998.},
key = {Database systems},
keywords = {Information systems;Information use;Integration;},
note = {Federated Databases;Information loss;Re-integration;Schema changes;Schema evolution;Schema integration;Structural modifications;},
URL = {http://dx.doi.org/10.1007/BFb0057730},
} 


@inproceedings{1997173561768 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 1996 3rd Biennial Joint Conference on Engineering Systems Design and Analysis, ESDA. Part 2 (of 9)},
journal = {American Society of Mechanical Engineers, Petroleum Division (Publication) PD},
editor = {Czejdo, B.D.;Esat, I.I.;Trousse, B.;Shirazi, B.;},
volume = {74},
number = {2},
year = {1996},
pages = {ASME PD - },
address = {Montpellier, Fr},
abstract = {The proceedings contains 30 papers. Topics discussed include object oriented database management systems, query decomposition and processing, object interoperable systems, hypermedia applications, graphical object manipulation interface, robot control, intelligent robots, virtual reality based telesensation systems, and industrial robots.},
key = {Database systems},
keywords = {Computer simulation;Graphical user interfaces;Industrial robots;Information technology;Intelligent robots;Mechanical variables control;Object oriented programming;Query languages;Virtual reality;},
note = {EiRev;Fault tolerance;Hypermedia;Object interoperable systems;Object oriented database management systems;Schema evolution;},
} 


@inproceedings{20160701936621 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing grid schemas globally},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Kuramitsu, Kimio},
volume = {3226},
year = {2004},
pages = {296 - 308},
issn = {03029743},
address = {Paris, France},
abstract = {Sharing schemas is a shortcut to data interoperability, while in grid environments there are many difficulties such as schema disagreements and schema evolutions. We propose a new&rdquo;mappings first, schemas later&rdquo; schema model, named Grid Schema. The Grid Schema uses the idea of context-free mapping to modularize schemas and its translation rules. This is incorporated into its schema validation mechanism, which enables us to check the compatibility of different formed data. We show the flexibility of the Grid Schema in maintaining global schemas in a distributed, evolving, and multi-cultural environment.<br/> &copy; IFIP International Federation for Information Processing 2004.},
key = {Mapping},
keywords = {Semantics;XML;},
note = {Context-free mapping;Cultural environment;Data interoperability;Global schemas;Grid environments;Schema evolution;Schema validation;Translation rules;},
} 


@inproceedings{20093012213183 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A conceptual model for digital libraries evolution},
journal = {WEBIST 2009 - Proceedings of the 5th International Conference on Web Information Systems and Technologies},
author = {Baruzzo, Andrea and Casoto, Paolo and Dattolo, Antonina and Tasso, Carlo},
year = {2009},
pages = {299 - 304},
address = {Lisbon, Portugal},
abstract = {The evolution and preservation of digital libraries are not simply a matter of technological decisions, but they can be better understood if treated as the integration of three complementary dimensions (based on the informational, technological and social domains). These dimensions together form a conceptual framework suitable to characterize the whole digital library concept.In this paper, starting from the experience and the lessons learned in the realization of the EU-India E-Dvara project, we propose such framework, providing motivational examples and discussing opportune solutions. More in particular, we discuss the issues concerned the technical infrastructure adaptation, the coordination of different user roles, and the data evolution in order to select the dimensions along which we base our framework.<br/>},
key = {Digital libraries},
keywords = {Information services;Information systems;Information use;Metadata;Multi agent systems;Service oriented architecture (SOA);},
note = {Conceptual frameworks;Conceptual model;Data evolution;Schema evolution;Social domains;Technical infrastructure;User roles;},
} 


@inproceedings{20140317197112 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Semantic exploration of archived product lifecycle metadata under schema and instance evolution},
journal = {CEUR Workshop Proceedings},
author = {Brunsmann, Jorg},
volume = {801},
year = {2011},
pages = {37 - 47},
issn = {16130073},
address = {Berlin, Germany},
abstract = {The product lifecycle spans from idea generation, design, manufacturing and service to disposal. During all these phases, engineers use their tacit knowledge to fulfill their tasks. If engineers retire or leave a company, their embodied knowledge also resigns. To circumvent such loss of important company's intellectual property, the engineer's knowledge is captured as linked data and then used as annotation for product lifecycle data models. To enable the reuse of data not only in the near-term, the product data and its annotated metadata are ingested into special long-term archives. However, achieving full preservation of semantically enriched product data requires the consideration of the linked data lifecycle which includes the evolution of schemas and instances. Such conceptualization and terminology changes pose the threat of semantic obsolescence of archived product data. Therefore, this paper describes dedicated metadata preservation functionality which respects knowledge evolution of the linked data lifecycle.<br/>},
key = {Life cycle},
keywords = {Data handling;Engineers;Linked data;Metadata;Obsolescence;Product design;Semantics;},
note = {Embodied knowledge;Knowledge evolution;Long term archives;Long-term preservation;Product life cycle management;Product lifecycle data;Product-life-cycle;Schema evolution;},
} 


@article{20083311456670 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Instruction-matrix-based genetic programming},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
author = {Li, Gang and Wang, Jin Feng and Lee, Kin Hong and Leung, Kwong-Sak},
volume = {38},
number = {4},
year = {2008},
pages = {1036 - 1049},
issn = {10834419},
abstract = {In genetic programming (GP), evolving tree nodes separately would reduce the huge solution space. However, tree nodes are highly interdependent with respect to their fitness. In this paper, we propose a new GP framework, namely, instruction-matrix (IM)-based GP (IMGP), to handle their interactions. IMGP maintains an IM to evolve tree nodes and subtrees separately. IMGP extracts program trees from an IM and updates the IM with the information of the extracted program trees. As the IM actually keeps most of the information of the schemata of GP and evolves the schemata directly, IMGP is effective and efficient. Our experimental results on benchmark problems have verified that IMGP is not only better than those of canonical GP in terms of the qualities of the solutions and the number of program evaluations, but they are also better than some of the related GP algorithms. IMGP can also be used to evolve programs for classification problems. The classifiers obtained have higher classification accuracies than four other GP classification algorithms on four benchmark classification problems. The testing errors are also comparable to or better than those obtained with well-known classifiers. Furthermore, an extended version, called condition matrix for rule learning, has been used successfully to handle multiclass classification problems. &copy; 2008 IEEE.<br/>},
key = {Genetic programming},
keywords = {Benchmarking;Classification (of information);Dynamic programming;Forestry;Genetic algorithms;Quality control;Reinforcement learning;},
note = {Bench-mark problems;Benchmark classification;Classification accuracy;Classification algorithm;Condition matrix for rule learning;Instruction-matrix-based genetic programming;Multiclass classification problems;Schema evolution;},
URL = {http://dx.doi.org/10.1109/TSMCB.2008.922054},
} 


@article{20074510911873 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Regular expression transformations to extend regular languages (with application to a Datalog XML schema validator)},
journal = {Journal of Algorithms},
author = {da Luz, Robson and Halfeld Ferrari, Mirian and Musicante, Martin A.},
volume = {62},
number = {3-4},
year = {2007},
pages = {148 - 167},
issn = {01966774},
abstract = {An XML schema is a set of rules for defining the allowed sub-elements of any element in an XML document. These rules use regular expressions to define the language of the element's children. Updates to an XML schema are updates to the regular expressions defined by the schema rules. We consider an interactive, data administration tool for XML databases. In this tool, changes on an XML schema are activated by updates that violate the validity of an XML document. Our schema validator is a Datalog program, resulting from the translation of a given XML schema. Changing the schema implies changing the validator. The main contribution of this paper is an algorithm allowing the evolution of XML schemas. This algorithm is based on the computation of new regular expressions to extend a given regular language in a conservative way, trying to foresee the needs of an application. A translation function from schema constraints to Datalog programs is introduced. The validation of an XML tree corresponds to the evaluation of the Datalog program over the tree. Our method allows the maintenance of the Datalog program in an incremental way, i.e., without redoing the entire translation. &copy; 2007 Elsevier Inc. All rights reserved.},
key = {XML},
keywords = {Computer software;Data structures;Logic programming;Mathematical transformations;Translation (languages);},
note = {Datalog programs;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.jalgor.2007.04.004},
} 


@article{20123415368475 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Multiversion trajectory data warehouse to handle structure changes},
journal = {International Journal of Database Theory and Application},
author = {Oueslati, Wided and Akaichi, Jalel},
volume = {4},
number = {2},
year = {2011},
pages = {35 - 50},
issn = {20054270},
abstract = {The data warehouse (DW) technology was developed to integrate heterogeneous information sources for analysis purposes. Information sources are more and more autonomous and they often change their content due to perpetual transactions (data changes) and may change their structure due to continual users' requirements evolving (schema changes). Handling properly all type of changes is a must. In fact, the DW which is considered as the core component of the modern decision support systems has to be update according to different type of evolution of information sources to reflect the real world subject to analysis.. The goal of this paper is to propose a solution, based on versioning approach, able to handle structure changes in order to keep track of the DW evolution.},
key = {Data warehouses},
keywords = {Artificial intelligence;Decision support systems;},
note = {Alternative version;Real version;Schema evolution;Trajectory data;Versioning;},
} 


@article{20101412825739 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An XML transformation algorithm inferred from an update script between DTDs},
journal = {IEICE Transactions on Information and Systems},
author = {Suzuki, Nobutaka and Fukushima, Yuji},
volume = {E92-D},
number = {4},
year = {2009},
pages = {594 - 607},
issn = {09168532},
abstract = {Finding an appropriate data transformation between two schemas has been an important problem. In this paper, assuming that an update script between original and updated DTDs is available, we consider inferring a transformation algorithm from the original DTD and the update script such that the algorithm transforms each document valid against the original DTD into a document valid against the updated DTD. We first show a transformation algorithm inferred from a DTD and an update script. We next show a su.cient condition under which the transformation algorithm inferred from a DTD d and an update script is unambiguous, i.e., for any document t valid against d, elements to be deleted/inserted can unambiguously be determined. Finally, we show a polynomial-time algorithm for testing the su.cient condition. Copyright &copy; 2009 The Institute of Electronics, Information and Communication Engineers.<br/>},
key = {Metadata},
keywords = {Polynomial approximation;XML;},
note = {Data transformation;Polynomial-time algorithms;Schema evolution;Transformation algorithm;XML transformation;},
URL = {http://dx.doi.org/10.1587/transinf.E92.D.594},
} 


@inproceedings{20100312643513 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A framework for modeling, building and maintaining enterprise information systems software},
journal = {SBES 2009 - 23rd Brazilian Symposium on Software Engineering},
author = {De Almeida, Alexandre Claudio and Boff, Glauber and De Oliveira, Juliano Lopes},
year = {2009},
pages = {115 - 125},
address = {Fortaleza, Ceara, Brazil},
abstract = {An Enterprise Information System (EIS) software has three main aspects: data, which are processed to generate business information; application functions, which transform data into information; and business rules, which control and restrict the manipulation of data by functions. Traditional approaches to EIS software development consider data and application functions. Rules are second class citizens, embedded on the specification of either data (as database integrity constraints) or on the EIS functions (as a part of the application software). This work presents a new, integrated approach for the development and maintenance of EIS software. The main ideas are to focus on the conceptual modeling of the three aspects of the EIS software - application functions, business rules, and database schema - and to automatically generate code for each of these software aspects. This improves software quality, reducing redundancies by centralizing EIS definitions on a single conceptual model. Due to automatic generation of code, this approach increases the software engineering staff productivity, making it possible to respond to the continuous changes in the business domain. &copy; 2009 IEEE.<br/>},
key = {Software design},
keywords = {Automatic programming;Computer software selection and evaluation;Enterprise software;Information systems;Information use;},
note = {Automatic generation of codes;Business rules;Database integrity constraints;Enterprise information system;Model driven development;Schema evolution;Schema generation;Traditional approaches;},
URL = {http://dx.doi.org/10.1109/SBES.2009.24},
} 


@inproceedings{20101812898254 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Rule-based management of schema changes at ETL sources},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Papastefanatos, George and Vassiliadis, Panos and Simitsis, Alkis and Sellis, Timos and Vassiliou, Yannis},
volume = {5968 LNCS},
year = {2010},
pages = {55 - 62},
issn = {03029743},
address = {Riga, Latvia},
abstract = {In this paper, we visit the problem of the management of inconsistencies emerging on ETL processes as results of evolution operations occurring at their sources. We abstract Extract-Transform-Load (ETL) activities as queries and sequences of views. ETL activities and its sources are uniformly modeled as a graph that is annotated with rules for the management of evolution events. Given a change at an element of the graph, our framework detects the parts of the graph that are affected by this change and highlights the way they are tuned to respond to it. We then present the system architecture of a tool called Hecataeus that implements the main concepts of the proposed framework. &copy; 2010 Springer-Verlag.<br/>},
key = {Information use},
keywords = {Information systems;},
note = {ETL process;Evolution operation;Extract transform loads;Hecataeus;Rule based;Schema changes;Schema evolution;System architectures;},
URL = {http://dx.doi.org/10.1007/978-3-642-12082-4_8},
} 


@inproceedings{20130616005720 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An XML document transformation algorithm inferred from an edit script between DTDs},
journal = {Conferences in Research and Practice in Information Technology Series},
author = {Suzuki, Nobutaka and Fukushima, Yuji},
volume = {75},
year = {2008},
pages = {175 - 184},
issn = {14451336},
address = {Wollongong, NSW, Australia},
abstract = {Finding an appropriate data transformation between two schemas has been an important problem. In this paper, assuming that an edit script between original and updated DTDs is available, we consider inferring a transformation algorithm, which transforms each document valid against the original DTD into a document valid against the updated DTD, from the original DTD and the edit script. We first show a transformation algorithm inferred from a DTD and an edit script. We next show a sufficient condition under which the transformation algorithm inferred from a DTD D and an edit script is unambiguous, i.e., for any document valid against D, elements to be deleted/inserted can unambiguously be determined. Finally, we show a polynomial-time algorithm for testing the sufficient condition. &copy; 2008, Australian Computer Society, Inc.<br/>},
key = {Metadata},
keywords = {Polynomial approximation;XML;},
note = {Data transformation;Document transformation;Edit operation;Glushkov automaton;Polynomial-time algorithms;Schema evolution;Transformation algorithm;},
} 


@inproceedings{20124615675380 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML document versioning and revalidation},
journal = {CEUR Workshop Proceedings},
author = {Maly, Jakub and Klimek, Jakub and Mlynkova, Irena and Necasky, Martin},
volume = {706},
year = {2011},
pages = {49 - 60},
issn = {16130073},
address = {Pisek, Czech republic},
abstract = {One of the prominent characteristics of XML applications is their dynamic nature. When a system grows and evolves, old user requirements change and/or new requirements accumulate. Apart from changes in the interface, it is also necessary to modify the existing documents with each new version, so they are valid against the new specification. The approach presented in this work extends an existing conceptual model with the support for multiple versions of the model. Thanks to this extension, it is possible to define a set of changes between two versions of a schema. This work contains an outline of an algorithm that compares two versions of a schema and produces a revalidation script in XSL.<br/>},
key = {XML},
keywords = {Specifications;},
note = {Conceptual model;Dynamic nature;Revalidation;Schema;Schema evolution;User requirements;Versioning;XML applications;},
} 


@article{20063510093220 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ontology-based verification of core model conformity in conceptual modeling},
journal = {Computers, Environment and Urban Systems},
author = {Hess, Claudia and Schlieder, Christoph},
volume = {30},
number = {5},
year = {2006},
pages = {543 - 561},
issn = {01989715},
abstract = {Reference models, often called core models are developed in various application domains. Until now, no computational support exists for the task of verifying the conformity between such core models and their domain models. The approach developed at Bamberg University uses Semantic Web technologies to examine whether or not a domain model is a derivation of a core model. This ontology-based conformity verification supports an iterative modeling process in which core or domain models are modified. Inference services as provided by ontologies can be used to analyze the relationships between core and domain models. For example, it is possible to formally prove which specific relations hold between two types of models and compare the result with the intentions of the domain experts involved in the modeling. As a consequence, knowledge not explicitly represented is revealed. In case that the domain model does not conform to the core model, an interpretation of the inference results is provided in ordinary language giving the domain experts hints on how to modify either the core model, the domain model or both. We evaluated our approach by applying it to a core model and a domain, hence national model from the cadastral domain. Conformity was verified between the core cadastral model proposed by [Lemmen, C., van der Molen, P., van Oosterom, P., Ploeger, H., Quak, W., Stoter, J., et al. (2003). A modular standard for the cadastral domain. In Proceedings of digital earth 2003: Information resources for global sustainability: knowledge, networks, technology, economy, society, natural and human resources, policy and strategy. Brno, Czech Republic, pp. 108-117] and the Greek cadastral model [Tzani, A. (2003). Object-oriented modeling of the Greek Cadastre. Master's thesis, School of Rural and Surveying Engineering of Aristotle University of Thessaloniki], which both are results of research activities related to the European COST Action G9 "Modeling Real Property Transactions". Although our approach to conformity verification was only evaluated with the cadastral models, it can be used for conformity verification in various applications domains due to its generality. &copy; 2005 Elsevier Ltd. All rights reserved.},
key = {Geographic information systems},
keywords = {Iterative methods;Models;Modification;},
note = {Conformity verification;Core models;Ontological modeling;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.08.009},
} 


@article{20094212372434 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Online reorganization of databases},
journal = {ACM Computing Surveys},
author = {Sockut, Gary H. and Iyer, Balakrishna R.},
volume = {41},
number = {3},
year = {2009},
issn = {03600300},
abstract = {In practice, any database management system sometimes needs reorganization, that is, a change in some aspect of the logical and/or physical arrangement of a database. In traditional practice, many types of reorganization have required denying access to a database (taking the database offline) during reorganization. Taking a database offline can be unacceptable for a highly available (24-hour) database, for example, a database serving electronic commerce or armed forces, or for a very large database. A solution is to reorganize online (concurrently with usage of the database, incrementally during users' activities, or interpretively). This article is a tutorial and survey on requirements, issues, and strategies for online reorganization. It analyzes the issues and then presents the strategies, which use the issues. The issues, most of which involve design trade-offs, include use of partitions, the locus of control for the process that reorganizes (a background process or users' activities), reorganization by copying to newly allocated storage (as opposed to reorganizing in place), use of differential files, references to data that has moved, performance, and activation of reorganization. The article surveys online strategies in three categories of reorganization. The first category, maintenance, involves restoring the physical arrangement of data instances without changing the database definition. This category includes restoration of clustering, reorganization of an index, rebalancing of parallel or distributed data, garbage collection for persistent storage, and cleaning (reclamation of space) in a log-structured file system. The second category involves changing the physical database definition; topics include construction of indexes, conversion between B<sup>+</sup>-trees and linear hash files, and redefinition (e.g., splitting) of partitions. The third category involves changing the logical database definition. Some examples are changing a column's data type, changing the inheritance hierarchy of object classes, and changing a relationship from one-to-many to many-to-many. The survey encompasses both research and commercial implementations, and this article points out several open research topics. As highly available or very large databases continue to become more common and more important in the world economy, the importance of online reorganization is likely to continue growing. &copy; 2009 ACM.<br/>},
key = {Trees (mathematics)},
keywords = {Database systems;Digital storage;Economic and social effects;File organization;Maintenance;Online systems;Surveys;},
note = {Clustering;Concurrent reorganization;Indexes;Log structured file systems;On-line reorganization;Redefinition;Reorganization;Restructuring;Schema evolution;Very large database;},
URL = {http://dx.doi.org/10.1145/1541880.1541881},
} 


@inproceedings{20115014601074 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Minimal data sets vs. synchronized data copies in a schema and data versioning system},
journal = {International Conference on Information and Knowledge Management, Proceedings},
author = {Wall, Bob and Angryk, Rafal},
year = {2011},
pages = {67 - 73},
address = {Glasgow, United kingdom},
abstract = {In this paper, we describe a key component of our proposed data-base schema and data versioning system, ScaDaVer. The versioning system is based on common practices used to manage source code changes in software development. It allows users of a data-base to create branches in which changes to the database are isolated from the main database and from other sandboxes. Schema and data versioning techniques are used to isolate changes made within the branches. There are two different approaches we are investigating to handle the schema and data versioning; the first is to store the minimal set of changes from the base schema and data for each branch, and to map queries in the branch back to the primary database to retrieve most data. These query results would be merged with the results from the branch data. The second is to create a copy of each table modified in a branch and map any updates to the primary database table into the branch. We are investigating the qualitative and quantitative differences between these two techniques given different usage patterns, and for the query mapping technique, we are working to prove the correctness of the mapped queries. This is done by expressing queries using multi-relational algebra and showing equivalence of the mapped queries to the same queries against a database without versioning. &copy; 2011 ACM.<br/>},
key = {Query languages},
keywords = {Algebra;Facsimile;Knowledge management;Mapping;Query processing;Software design;},
note = {Common practices;Mapping techniques;Relational algebra;Schema evolution;Schema versioning;Source code changes;Synchronized data;Versioning systems;},
URL = {http://dx.doi.org/10.1145/2065003.2065017},
} 


@inproceedings{20101212784395 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {K-means based approach for olap dimension updates},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Bentayeb, Fadila},
volume = {DISI},
year = {2008},
pages = {531 - 534},
address = {Barcelona, Spain},
abstract = {Actual data warehouses models usually consider OLAP dimensions as static entities. However, in practice, structural changes of dimensions schema are often necessary to adapt the multidimensional database to changing requirements. This paper presents a new structural update operator for OLAP dimensions, named Rollup-WithKmeans based on k-means clustering method. This operator allows to create a new level to which, a pre-existent level in an OLAP dimension hierarchy rolls up. To define the domain of the new level and the aggregation function from an existing level to the new level, our operator classifies all instances of an existing level into k clusters with the k-means clustering algorithm. To choose features for k-means clustering, we propose two solutions. The first solution uses descriptors of the pre-existent level in its dimension table while the second one proposes to describe the new level by measures attributes in the fact table. Moreover, we carried out some experimentations within Oracle 10 g DBMS which validated the relevance of our approach.<br/>},
key = {Clustering algorithms},
keywords = {Data warehouses;Information systems;Information use;},
note = {Analysis level;Clustering;Dimension hierarchies;K-means;OLAP;Schema evolution;},
} 


@article{2004348317122 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Flexible support of team processes by adaptive workflow systems},
journal = {Distributed and Parallel Databases},
author = {Rinderle, Stefanie and Reichert, Manfred and Dadam, Peter},
volume = {16},
number = {1},
year = {2004},
pages = {91 - 116},
issn = {09268782},
abstract = {Process-oriented support of collaborative work is an important challenge today. At first glance, Workflow Management Systems (WfMS) seem to be very suitable tools for realizing team-work processes. However, such processes have to be frequently adapted, e.g., due to process optimizations or when process goals change. Unfortunately, runtime adaptability still seems to be an unsolvable problem for almost all existing WfMS. Usually, process changes can be accomplished by modifying a corresponding (graphical) workflow (WF) schema. Especially for long-running processes, however, it is extremely important that such changes can be propagated to already running WF instances as well, but without causing inconsistencies and errors. The paper presents a general and comprehensive correctness criterion for ensuring compliance of in-progress WF instances with a modified WF schema. For different kinds of WF schema changes, it is precisely stated, which rules and which information are needed at mininum for satisfying this criterion.},
key = {Computer supported cooperative work},
keywords = {Adaptive systems;Computational methods;Distributed computer systems;Error analysis;Graph theory;Optimization;Problem solving;},
note = {Compliance checks;Schema evolution;Workflow management systems (WfMS);},
URL = {http://dx.doi.org/10.1023/B:DAPD.0000026270.78463.77},
} 


@inproceedings{20084711716074 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution of information systems with data hierarchies},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Czejdo, Bogdan Denny},
volume = {1 ISAS},
year = {2008},
pages = {351 - 356},
address = {Barcelona, Spain},
abstract = {Recently, the research and practical efforts have intensified in the area of Information Systems (IS) supporting data and application evolution. The need to support IS evolution is caused by a variety of reasons including dynamicity of data sources, changing processing requirements, and using new technologies. In this paper we concentrate on evolution of IS data repositories caused by dynamicity of data sources. Our approach is to capture changes of various data hierarchies and use them as rules to implement evolution of IS data repository. Evolution of hierarchies can be categorized into hierarchy creation, hierarchy deletion, and hierarchy modification.<br/>},
key = {Information systems},
keywords = {Information use;Management information systems;Systems analysis;},
note = {Data repositories;Data-sources;IS evolution;New technologies;Schema evolution;System evolution;},
} 


@inproceedings{20164603023052 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual modelling in an evolving world},
journal = {Lecture Notes in Business Information Processing},
author = {Mittermeir, Roland T.},
volume = {5},
year = {2008},
pages = {246 - 257},
issn = {18651348},
address = {Klagenfurt, Austria},
abstract = {Conceptual models serve as models of indirection linking the information system and the section of the real world represented by this system. They are established during the analysis phase of system construction and might be changed in discrete steps if the application necessitates further (or fewer) aspects to be covered. This suffices for static domains. In dynamic domains however, the quality of this link between an information system and reality is progressively deteriorating due to shifts in the semantics of the information captured in the system. The paper shows example domains where shifts of semantics would require gradual evolution of the conceptual model and proposes an approach for accommodating these requirements.<br/> &copy; Springer-Verlag Berlin Heidelberg 2008.},
key = {Information systems},
keywords = {Information use;Knowledge representation;Semantics;},
note = {Conceptual model;Conceptual modelling;Discrete step;Dynamic domains;Fuzzy modelling;Schema evolution;Static domain;System construction;},
} 


@article{20100912743824 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Management of evolving semantic grid metadata within a collaborative platform},
journal = {Information Sciences},
author = {Hartung, Michael and Loebe, Frank and Herre, Heinrich and Rahm, Erhard},
volume = {180},
number = {10},
year = {2010},
pages = {1837 - 1849},
issn = {00200255},
abstract = {Grid environments, providing distributed infrastructures, computing resources and data storage, usually show a high degree of heterogeneity and change in their metadata. We propose a platform for collaborative management and maintenance of common metadata for grids. As the conceptual foundation of this platform, a meta model is presented which distinguishes structured descriptions and classification structures that both are modifiable. On this basis, the system allows for the creation and editing of grid relevant metadata and provides various search and navigation facilities for grid participants. We applied the platform to the German D-Grid initiative by establishing the D-Grid Ontology (DGO). &copy; 2009 Elsevier Inc. All rights reserved.<br/>},
key = {Distributed computer systems},
keywords = {Digital storage;Metadata;Semantics;},
note = {Collaborative environments;Data migration;Grids;Schema evolution;Semantic metadata;},
URL = {http://dx.doi.org/10.1016/j.ins.2009.08.008},
} 


@inproceedings{20094012359048 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {RoK: Roll-up with the k-means clustering method for recommending OLAP queries},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bentayeb, Fadila and Favre, Cecile},
volume = {5690 LNCS},
year = {2009},
pages = {501 - 515},
issn = {03029743},
address = {Linz, Austria},
abstract = {Dimension hierarchies represent a substantial part of the data warehouse model. Indeed they allow decision makers to examine data at different levels of detail with On-Line Analytical Processing (OLAP) operators such as drill-down and roll-up. The granularity levels which compose a dimension hierarchy are usually fixed during the design step of the data warehouse, according to the identified analysis needs of the users. However, in practice, the needs of users may evolve and grow in time. Hence, to take into account the users' analysis evolution into the data warehouse, we propose to integrate personalization techniques within the OLAP process. We propose two kinds of OLAP personalization in the data warehouse: (1) adaptation and (2) recommendation. Adaptation allows users to express their own needs in terms of aggregation rules defined from a child level (existing level) to a parent level (new level). The system will adapt itself by including the new hierarchy level into the data warehouse schema. For recommending new OLAP queries, we provide a new OLAP operator based on the K-means method. Users are asked to choose K-means parameters following their preferences about the obtained clusters which may form a new granularity level in the considered dimension hierarchy. We use the K-means clustering method in order to highlight aggregates semantically richer than those provided by classical OLAP operators. In both adaptation and recommendation techniques, the new data warehouse schema allows new and more elaborated OLAP queries. Our approach for OLAP personalization is implemented within Oracle 10 g as a prototype which allows the creation of new granularity levels in dimension hierachies of the data warehouse. Moreover, we carried out some experiments which validate the relevance of our approach. &copy; 2009 Springer Berlin Heidelberg.<br/>},
key = {Data warehouses},
keywords = {Cluster analysis;Clustering algorithms;Decision making;Expert systems;},
note = {Adaptative system;Analysis level;Clustering;Dimension hierarchies;K-means;OLAP;Personalizations;Recommendation;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-03573-9_43},
} 


@article{20063110039282 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interoperability mapping from XML schemas to ER diagrams},
journal = {Data and Knowledge Engineering},
author = {Della Penna, Giuseppe and Marco, Antinisca Di and Intrigila, Benedetto and Melatti, Igor and Pierantonio, Alfonso},
volume = {59},
number = {1},
year = {2006},
pages = {166 - 188},
issn = {0169023X},
abstract = {The eXtensible Markup Language (XML) is a de facto standard on the Internet and is now being used to exchange a variety of data structures. This leads to the problem of efficiently storing, querying and retrieving a great amount of data contained in XML documents. Unfortunately, XML data often need to coexist with historical data. At present, the best solution for storing XML into pre-existing data structures is to extract the information from the XML documents and adapt it to the data structures' logical model (e.g., the relational model of a DBMS). In this paper, we introduce a technique called Xere (XML entity-relationship exchange) to assist the integration of XML data with other data sources. To this aim, we present an algorithm that maps XML schemas into entity-relationship diagrams, discuss its soundness and completeness and show its implementation in XSLT. &copy; 2005 Elsevier B.V. All rights reserved.},
key = {Interoperability},
keywords = {Conformal mapping;Data structures;Information retrieval;Information retrieval systems;Mathematical models;World Wide Web;XML;},
note = {Entity-relationship diagrams;Interoperability and heterogeneity;Schema evolution and maintenance;Web applications/XML;},
URL = {http://dx.doi.org/10.1016/j.datak.2005.08.002},
} 


@inproceedings{20072010600715 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Semantics of persistence in the glib programming language},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
author = {Libicki, Daniel Gakh},
volume = {2006},
year = {2006},
pages = {645 - 646},
address = {Portland, OR, United states},
abstract = {The cornerstone of object-oriented programming is the representation of data as a set of objects. In all of the widely-adopted languages that claim to support object-oriented programming, however, the lifetime of an object is bound by the lifetime of the process that instantiated it. In real applications, the lifetime of data is almost never related to the lifetime of the process that created it. This impedance mismatch necessitates a great deal of repetitive, error-prone labor. A true object-oriented design language must be a persistent language; in other words, the lifetime of an object must be independent of the lifetime of the process.Many persistent languages have been developed in research settings. Most of these languages, however, have attempted to maintain backwards compatibility with some previous, non-persistent language, such as Modula-3 or Java. Glib, on the other hand, is a programming language designed from the outset to support object persistence. I propose that Glib's constructs are simpler and more powerful than those of its predecessors, and now that I have an OOPLSA poster displaying those constructs, you can judge for yourself.},
key = {Java programming language},
keywords = {Computer simulation;Error analysis;Object oriented programming;Query languages;Real time systems;Semantics;Software engineering;},
note = {Glib programming language;Modula-3;Object queries;Schema evolution;Type systems;},
URL = {http://dx.doi.org/10.1145/1176617.1176653},
} 


@inproceedings{20113514283966 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Metadata to support data warehouse evolution},
journal = {Information Systems Development: Towards a Service Provision Society},
author = {Solodovnikova, Darja},
year = {2009},
pages = {627 - 635},
abstract = {The focus of this chapter is metadata necessary to support data warehouse evolution. We present the data warehouse framework that is able to track evolution process and adapt data warehouse schemata and data extraction, transformation, and loading (ETL) processes. We discuss the significant part of the framework, the metadata repository that stores information about the data warehouse, logical and physical schemata and their versions. We propose the physical implementation of multiversion data warehouse in a relational DBMS. For each modification of a data warehouse schema, we outline the changes that need to be made to the repository metadata and in the database. &copy; 2009 Springer Science+Business Media, LLC.<br/>},
key = {Metadata},
keywords = {Data warehouses;},
note = {Changes;Data warehouse evolutions;Evolution process;Metadata repositories;Multiversion data warehouse;Relational DBMS;Repository;Schema evolution;},
URL = {http://dx.doi.org/10.1007/b137171_65},
} 


@article{1994011138707 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On the design and maintenance of optimized relational representations of entity-relationship schemas},
journal = {Data and Knowledge Engineering},
author = {Casanova, Macro A. and Tucherman, Luiz and Laender, H.F.},
volume = {11},
number = {1},
year = {1993},
pages = {1 - 20},
issn = {0169023X},
abstract = {A method for obtaining optimized relational representations of database conceptual schemas in an extended entity-relationship model is first proposed. The method incorporates and generalizes a familiar heuristics to obtain good relational representations and also produces, for each relational structure, an explanation indicating which concepts it represents. Then, a redesign method that, given changes to the conceptual schema, generates a plan to modify the original representation and to organize the database state is described.},
key = {Optimization},
keywords = {Computer aided design;Database systems;Heuristic methods;Knowledge based systems;Maintainability;},
note = {Conceptual database design;Database schema evolution;Database schema optimization;Entity relationship model;Extended entity relationship model;Relational schema optimization;},
URL = {http://dx.doi.org/10.1016/0169-023X(93)90043-O},
} 


@article{1994041233827 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Nonrestrictive concurrency control protocol for object-oriented databases},
journal = {Distributed and Parallel Databases},
author = {Agrawal, D. and Abbadi, A.El.},
volume = {2},
number = {1},
year = {1994},
pages = {7 - 31},
issn = {09268782},
abstract = {We propose an algorithm for executing transactions in object-oriented databases. The object-oriented database model generalizes the classical model of database concurrency control by permitting accesses to class and instance objects, by permitting arbitrary operations on objects as opposed to traditional read and write operations, and by allowing nested execution of transactions on objects. In this paper, we first develop a uniform methodology for treating both classes and instances. We then develop a two-phase locking protocol with a new relationship between locks called ordered sharing for an object-oriented database. Ordered sharing does not restrict the execution of conflicting operations. Finally, we extend the protocol to handle objects that execute methods on other objects thus resulting in the nested execution of transactions. The resulting protocol permits more concurrency than other known locking-based protocols.},
key = {Database systems},
keywords = {Algorithms;Data handling;Data structures;Data transfer;Network protocols;Object oriented programming;Phase locked loops;Telecommunication control;},
note = {Arbitrary operations;Locking protocols;Nested transactions;Nonrestrictive concurrency control protocol;Object oriented databases;Schema evolution;Serializability;Typed objects;},
} 


@inproceedings{20154401460567 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A framework for generating and maintaining global schemas in heterogeneous multidatabase systems},
journal = {Proceedings of the 2003 IEEE International Conference on Information Reuse and Integration, IRI 2003},
author = {Duwairi, Rehab M.},
year = {2003},
pages = {200 - 207},
address = {Las Vegas, NV, United states},
abstract = {The problem of creating a global schema over a set of heterogeneous databases is important due the availability of multiple databases within organizations. The global schema should provide a unified representation of local heterogeneous schemas. In this paper, we provide a general framework that supports the integration of local schemas into a global one. The framework takes into consideration the fact that local schemas are autonomous and may evolve over time, which makes the definition of the global schema obsolete. We define a set of integration operators that integrates local schemas based on the semantic relevance of their classes, and provide a model-independent representation of virtual classes of the global schema. We also define a set of modifications that can be applied to local schemas as a consequence of their local autonomy. For every local modification, we define a propagation rule that will automatically disseminate the effects of that modification to the global schema without having to regenerate it from scratch via integration.<br/> &copy; 2003 IEEE.},
key = {Information use},
keywords = {Database systems;Integration;Semantics;},
note = {Heterogeneous database;Heterogeneous schemas;Integration operators;Multi-database systems;Multidatabases;Schema evolution;Schema integration;Semantic relevance;},
URL = {http://dx.doi.org/10.1109/IRI.2003.1251414},
} 


@inproceedings{20144600203636 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling a multiversion data warehouse: A formal approach},
journal = {ICEIS 2003 - Proceedings of the 5th International Conference on Enterprise Information Systems},
author = {Morzy, Tadeusz and Wrembel, Robert},
volume = {1},
year = {2003},
pages = {120 - 127},
address = {Angers, France},
abstract = {A data warehouse is a large centralized repository that stores a collection of data integrated from external data sources (EDSs). The purpose of building a data warehouse is among others: to provide an integrated access to distributed and usually heterogeneous information, to provide a platform for data analysis and decision making. EDSs are autonomous in most of the cases. In a consequence, their content and structure change in time. In order to keep the content of a data warehouse up to date, after source data changed, various warehouse refreshing techniques have been developed, mainly based on an incremental view maintenance. A data warehouse will also need refreshing after a schema of an EDS changed. This problem has, however, received little attention so far. Few approaches have been proposed and they tackle the problem by using mainly temporal extensions to a data warehouse. Such techniques expose their limitations in multi-period querying. Moreover, in order to support predictions of trends by decision makers, the what-if analysis is often required. For these purposes, multiversion data warehouses seem to be very promising. In this paper we propose a model of a multiversion data warehouse, and show our prototype implementation of such a multiversion data warehouse.<br/>},
key = {Data warehouses},
keywords = {Decision making;Information systems;Large scale systems;Query languages;},
note = {Content and structure;External data sources;Heterogeneous information;Incremental view maintenance;Multiversion data warehouse;Prototype implementations;Schema evolution;Versioning;},
} 


@article{1990076080906 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Architecture of the ORION next-generation database system},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Kim, Won and Garza, Jorge F. and Ballou, Nathaniel and Woelk, Darrell},
volume = {2},
number = {1},
year = {1990},
pages = {109 - 124},
issn = {10414347},
abstract = {Various architectural components of ORION-1 and ORION-1SX are described, and, where appropriate, a review of the current implementation is provided. The message handler receives all messages sent to the ORION system. The object subsystem provides high-level data management functions, including query optimization, schema management, long data management (including text search), and support for versionable objects, composite objects, and multimedia objects. The transaction management subsystem coordinates concurrent object accesses and provides recovery capabilities, using locking and logging techniques, respectively. The storage subsystem manages persistent storage of objects and controls the flow of objects between the secondary storage device and main memory buffers. In ORION-1, all subsystems reside in one computer. The ORION-1SX architecture is significantly different from ORION-1 in the management of shared data structures and distribution of these subsystems and their components (henceforth to be called a module). An ORION-1SX module may reside completely in the clients or in the server, or some parts of the module may reside in the clients and the remainder of the module in the server. This architecture is the consequence of the separation of memory address spaces between the server and clients, the dual-buffer management scheme, and the caching of objects and system structures.},
key = {Database Systems},
keywords = {Computer Metatheory--Programming Theory;Computer Operating Systems--Storage Allocation;},
note = {Object-Oriented Database Systems;Query Processing;Schema Evolution;},
URL = {http://dx.doi.org/10.1109/69.50909},
} 


@inproceedings{20141517548724 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Maximizing reusability: Seeking appropriate positions for derived classes within the class hierarchy},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Alhajj, Reda and Polat, Faruk},
volume = {1},
year = {2000},
pages = {351 - 355},
address = {Como, Italy},
abstract = {This paper addresses reusability maximization by investigating the proper location of a derived class in the class hierarchy. We categorize derived classes into four groups, depending on whether their superclasses and subclasses are known, and present two algorithms that investigate reusability maximization. The first algorithm decides on the possibility of adapting some existing related classes in the lists of superclasses of derived classes. The second algorithm checks the possibility of adding derived classes to the lists of superclasses of some other existing related classes to have the latter classes inheriting things instead of duplication. &copy; 2000 ACM.<br/>},
key = {Reusability},
keywords = {Algorithms;Object-oriented databases;},
note = {Class hierarchies;Four-group;Schema evolution;},
URL = {http://dx.doi.org/10.1145/335603.335833},
} 


@article{1995041503878 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {EVORM: a conceptual modelling technique for evolving application domains},
journal = {Data and Knowledge Engineering},
author = {Proper, H.A. and van der Weide, Th.P.},
volume = {12},
number = {3},
year = {1994},
pages = {313 - 359},
issn = {0169023X},
abstract = {In this paper we present EVORM, a data modelling technique for evolving application domains. EVORM is the result of applying a general theory for the evolution of application domains to the object role modelling technique PSM, a generalisation of ER, EER, FORM and NIAM. First the general theory is presented. This theory describes a general approach to the evolution of application domains, abstracting from details of specific modelling techniques. This theory makes a distinction between the underlying information structure and its evolution on the one hand, and the description and semantics of operations on the information structure and its population on the other hand. Main issues within this theory are object typing, type relatedness and identification of objects. After a (short) introduction to PSM, this general theory is applied, resulting in EVORM. Besides having a right of its own, the usefulness of the general theory is demonstrated by interpreting its abstract results, resulting in more intuitive rules for EVORM.},
key = {Data structures},
keywords = {Abstracting;Computational linguistics;Computer simulation;Data description;Data reduction;Database systems;Information retrieval systems;Information theory;Information use;},
note = {Conceptual modelling;Evolving information systems;Information structure;Predicator set model;Schema evolution;Temporal information systems;},
URL = {http://dx.doi.org/10.1016/0169-023X(94)90031-0},
} 


@article{1996113007554 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {General theory for evolving application models},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Proper, H.A. and van der Weide, T.P.},
volume = {7},
number = {6},
year = {1995},
pages = {984 - 996},
issn = {10414347},
abstract = {In this article we provide a general theory for evolving information systems. This theory makes a distinction between the underlying information structure at the conceptual level, its evolution on the one hand, and the description and semantics of operations on the information structure and its population on the other hand. Main issues within this theory are object typing, type relatedness and identification of objects. In terms of these concepts, we propose some axioms on the well-formedness of evolution. In this general theory, the underlying data model is a parameter, making the theory applicable for a wide range of modelling techniques, including object-role modelling and object oriented techniques.},
key = {Database systems},
keywords = {Computational linguistics;Computer simulation;Data processing;Data structures;Hierarchical systems;Object oriented programming;},
note = {Data modelling;Evolving information systems;Object role modelling;Predicator set model;Schema evolution;Temporal information systems;},
URL = {http://dx.doi.org/10.1109/69.476503},
} 


@inproceedings{20160801978016 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Updates and application migration support in an ODMG temporal extension},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Dumas, Marlon and Fauvet, Marie-Christine and Scholl, Pierre-Claude},
volume = {1727},
year = {1999},
pages = {24 - 35},
issn = {03029743},
address = {Paris, France},
abstract = {A substantial number of temporal extensions to data models and query languages have been proposed. However, little attention has been paid to the migration of data and applications from a "snapshot" DBMS to a temporal extension of it. In this paper, we analyze this issue and precisely formulate some requirements related to it. We then present a temporal extension of the ODMG's object database standard fulfilling these requirements. Throughout this presentation, we underscore the importance of providing adequate update and interpolation modalities in achieving application migration support.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Object-oriented databases},
keywords = {Data mining;Information management;Information systems;Information use;Object oriented programming;Query languages;Query processing;Reverse engineering;World Wide Web;},
note = {Application migrations;ODMG;Schema evolution;Temporal Database;Temporal updates;},
URL = {http://dx.doi.org/10.1007/3-540-48054-4_3},
} 


@inproceedings{20175004539164 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Self-adaptive semantic schema mechanism for multimedia databases},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Yang, Jun and Li, Qing and Zhuang, Yueting},
volume = {4925},
year = {2002},
pages = {69 - 79},
issn = {0277786X},
address = {Shanghai, China},
abstract = {In the context of multimedia retrieval, the goal of accuracy is to a certain extent contradictory with that of efficiency. The former relies on exploiting sophisticated features, whereas the latter favors using simple features with reduced dimensionality. As an endeavor to strike the balance between these two goals, this paper presents a self-adaptive semantic schema mechanism (SSM) for multimedia databases. The SSM is implemented based on an object-oriented data model, with classes being organized into a semantic hierarchy. As its most distinguishable feature, when the conditions of certain ECA-rules are satisfied, SSM supports adaptive evolution of a schema in the form of expansion with new classes and/or compaction by removing inefficient ones. This self-adaptive evolution strategy allows a schema to optimize for the requirements of each specific application, thereby achieving a dynamic, application-specific balance between accuracy and efficiency. A prototype system for multimedia retrieval, 2M2Net, has been built based on this mechanism and validated for its feasibility.<br/> &copy; 2017 SPIE.},
key = {Semantics},
keywords = {Database systems;Efficiency;Multimedia systems;},
note = {ECA rule;Multimedia database;Schema evolution;Self-Adaptive;Semantic schemata;},
URL = {http://dx.doi.org/10.1117/12.481571},
} 


@inproceedings{20173704139813 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An active meta-model for knowledge evolution in an object-oriented database},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bellahsene, Z.},
volume = {685 LNCS},
year = {1993},
pages = {39 - 53},
issn = {03029743},
address = {Paris, France},
abstract = {It is not reasonable to consider that a single data model can be adequate for any application/There are many data models; each of them has its advantages and drawbacks. Building up an other model does not make sense in the actual slate of research in the database field. Our approach consists of a proposal of a meta-model providing an open environment to allow knowledge evolution in object-oriented database systems. Knowledge evolution means updates on database schema: propagation of updates from schema to instances and dynamic propagation of views update operations. Furthermore this meta-model enables the extension of data model concepts by modifying their semantics. By modifying and extending these model, it can be tailored or customised to suit various application domains.<br/> &copy; Springer-Verlag Berlin Heidelberg 1993.},
key = {Object-oriented databases},
keywords = {Crack propagation;Information systems;Information use;Semantics;Systems engineering;},
note = {Active database;Dynamic propagation;Inheritance;Knowledge evolution;Meta model;Modeling concepts;Open environment;Schema evolution;},
} 


@inproceedings{20174404317541 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object-oriented schema extension and abstraction},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Hursch, Walter and Lieberherr, Karl and Mukherjea, Sougata},
volume = {Part F129680},
year = {1993},
pages = {54 - 62},
address = {Indianapolis, IN, United states},
abstract = {An algorithm is presented that abstracts out the "largest" common substructure of two given object-oriented class structures. This abstraction algorithm is based on two concepts: (1) a mathematical formulation of extension for class structures containing part-of and inheritance relationships, and (2) a definition of similarity on the class level. The algorithm shows how class structures can be optimized with respect to the extension relation, and how it can be used to abstract out a candidate parameterized class structure. The algorithm has been implemented as a schema transformation and design tool in the Demeter System.<br/> &copy; 1993 ACM.},
key = {Object oriented programming},
keywords = {Abstracting;Arts computing;Computer software reusability;},
note = {Abstraction algorithms;Inheritance relationships;Mathematical formulation;Object-oriented class;Parameterized;Schema evolution;Schema transformation;View integrations;},
URL = {http://dx.doi.org/10.1145/162754.162807},
} 


@inproceedings{20161602243729 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A generalized modeling framework for schema versioning support},
journal = {Proceedings - 11th Australasian Database Conference, ADC 2000},
author = {Grandi, F. and Mandreoli, F. and Scalas, M.R.},
year = {2000},
pages = {33 - 40},
address = {Canberra, ACT, Australia},
abstract = {Advanced object-oriented applications require the management of schema versions, in order to cope with changes in the structure of the stored data. Two types of versioning have been separately considered so far: branching and temporal. The former arose in application domains like CAD/CAM and software engineering, where different solutions have been proposed to support design schema versions (consolidated versions). The latter concerns temporal databases, where some works have considered temporal schema versioning to fulfil advanced needs of other typical object-oriented applications like GIS and multimedia. In this work, we propose a general model which integrates the two approaches by supporting both design and temporal schema versions. The model is provided with a complete set of schema change primitives for fully-fledged version manipulation whose semantics is described in the paper.<br/> &copy; 2000 IEEE.},
key = {Object-oriented databases},
keywords = {Application programs;Computer aided design;Information management;Object oriented programming;Semantics;},
note = {General model;Generalized models;Object oriented application;Schema changes;Schema evolution;Schema versioning;Support design;Temporal Database;},
URL = {http://dx.doi.org/10.1109/ADC.2000.819811},
} 


@inproceedings{20160902008684 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating the evolution of object-oriented systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Hursch, Walter L. and Seiter, Linda M.},
volume = {1049},
year = {1996},
pages = {2 - 21},
issn = {03029743},
address = {Kanazawa, Japan},
abstract = {A formal framework is presented for maintaining behavior and consistency of object-oriented systems during software evolution. The framework effectively couples a change avoidance approach based on Adaptive Object-Oriented Software with a change management mechanism to fully automate evolution. Schema transformations may render existing objects and programs inconsistent. The framework identifies the introduced inconsistencies and provides the necessary object and program transformations to reinstate consistency while maintaining the behavior of the system. A formal definition of behavioral equivalence is given. To prove behavioral equivalence of adaptive programs, the paper includes a formal semantics for adaptive software. Finally, the feasibility of the framework is demonstrated for a representative set of primitive schema transformations.<br/> &copy; Springer-Verlag Berlin Heidelberg 1996.},
key = {Object oriented programming},
keywords = {Bioinformatics;Empowerment of personnel;Formal methods;Semantics;Systems analysis;},
note = {Behavioral equivalence;Management of change;Object oriented software;Object-oriented system;Program transformations;Schema evolution;Schema transformation;Software Evolution;},
URL = {http://dx.doi.org/10.1007/3-540-60954-7_40},
} 


@article{1994101372241 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual clustering algorithm for database schema design},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Beck, Howard W. and Anwar, Tarek and Navathe, Shamkant B.},
volume = {6},
number = {3},
year = {1994},
pages = {396 - 411},
issn = {10414347},
abstract = {Conceptual clustering techniques based on current theories of categorization provide a way to design database schemas that more accurately represent classes. An approach is presented in which classes are treated as complex clusters of concepts rather than as simple predicates. An important service provided by the database is determining whether a particular instance is a member of a class. A conceptual clustering algorithm based on theories of categorization aids in building classes by grouping related instances and developing class descriptions. The resulting database schema addresses a number of properties of categories, including default values and prototypes, analogical reasoning, exception handling, and family resemblance. Class cohesion results from trying to resolve conflicts between building generalized class descriptions and accommodating members of the class that deviate from these descriptions. This is achieved by combining techniques from machine learning, specifically explanation-based learning and case-based reasoning. A subsumption function is used to compare two class descriptions. A realization function is used to determine whether an instance meets an existing class description. A new function, INTERSECT, is introduced to compare the similarity of two instances. INTERSECT is used in defining an exception condition. Exception handling results in schema modification. This approach is applied to the database problems of schema integration, schema generation, query processing, and view creation.},
key = {Algorithms},
keywords = {Classification (of information);Data handling;Data reduction;Data structures;Database systems;Inference engines;Learning systems;Mathematical models;},
note = {Case based reasoning;Conceptual clustering algorithm;Database schema design;Exception handling;Explanation based reasoning;Machine learning;Schema evolution;},
URL = {http://dx.doi.org/10.1109/69.334862},
} 


@inproceedings{1993050744870 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting exploratory CSCW with the EGRET framework},
journal = {Proceedings of the Conference on Computer-Supported Cooperative Work},
author = {Johnson, Philip},
year = {1992},
pages = {298 - 305},
address = {Toronto, Ont, Can},
abstract = {Exploratory collaboration occurs in domains where the structure and process of group work evolves as an intrinsic part of the collaborative activity. Traditional database and hypertext structural models do not provide explicit support for collaborative exploration. The EGRET framework defines both a data and a process model along with supporting analysis techniques that provide novel support for exploratory collaboration. To do so, the EGRET framework breaks with traditional notions of the relationship between schema and instance structure. In EGRET, schema structure is viewed as a representation of the current state of consensus among collaborators, from which instance structure is allowed to depart in a controlled fashion. This paper discusses the issues of exploratory collaboration, the EGRET approach to its support, and the current status of this research.},
key = {Distributed computer systems},
keywords = {Computer networks;Data processing;Data structures;Decision support systems;},
note = {Exploratory computer supported cooperative work;Hypertext;Schema evolution;},
} 


@article{20072810695771 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The case for mesodata: An empirical investigation of an evolving database system},
journal = {Information and Software Technology},
author = {de Vries, Denise and Roddick, John F.},
volume = {49},
number = {9-10},
year = {2007},
pages = {1061 - 1072},
issn = {09505849},
abstract = {Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute's specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system. &copy; 2006 Elsevier B.V. All rights reserved.},
key = {Data structures},
keywords = {Computer simulation;Information systems;Relational database systems;Value engineering;},
note = {Database evolution;Domain evolution;Mesodata;},
URL = {http://dx.doi.org/10.1016/j.infsof.2006.11.001},
} 


@inproceedings{20142717906038 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Refactoring database to improve queries performance},
journal = {21st Italian Symposium on Advanced Database Systems, SEBD 2013},
author = {Domingues, Marcia Beatriz Pereira and Almeira Jr., Jorge Rady and Costa, Wilian Franca and Saraiva, Antonio},
year = {2013},
pages = {159 - 166},
address = {Roccella Jonica, Reggio Calabria, Italy},
abstract = {The project and implementation of database evolution are important challenges to the development of computer systems, aiming at the frequent requirement of changes in the applications and the need to make updates keeping the databases available to the transactions. The challenge is even bigger when the databases have to respond simultaneously to several applications. Nowadays, in order to evolve the database, there are synchronous and asynchronous solutions in the literature about software development agile methods. A refactoring represents small structural, architectural, integrity or quality changes of data that do not include new system functionalities. Large changes requirements in databases are very common. Both synchronous and asynchronous solutions have as main problem the absence of a complete refactoring process that permits tasks serialization for larger changes. This work aims at showing how refactoring tasks can be made for larger processes in order to optimize the queries times in database. As case study, we have an existing Information Portal System for Precision Agriculture that dynamically generates geoprocessed crop field models based on input data. In this context, we redesigned this database, applying refactoring techniques to improve queries performance. The defined reference processes to database refactoring were represented using BPMN notation.<br/>},
key = {Query languages},
keywords = {Query processing;Software design;},
note = {BPMN;Database schemas;Query performance;Refactorings;Spatial database;},
} 


@inproceedings{20105113494574 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Program analysis and transformation for data-intensive system evolution},
journal = {IEEE International Conference on Software Maintenance, ICSM},
author = {Cleve, Anthony},
year = {2010},
abstract = {Data-intensive software systems are generally made of a database and a collection of application programs in strong interaction with the former. They constitute critical assets in most enterprises, since they support business activities in all production and management domains. Data-intensive systems form most of the so-called legacy systems: they typically are one or more decades old, they are very large, heterogeneous and highly complex. Many of them significantly resist modifications and change due to the lack of documentation, to the use of aging technologies and to inflexible architectures. Therefore, the evolution of data-intensive systems clearly calls for automated support. This thesis explores the use of automated program analysis and transformation techniques in support to the evolution of the database component of the system. The program analysis techniques aim to ease the database evolution process, by helping the developers to understand the data structures that are to be changed, despite the lack of precise and up-to-date documentation. The objective of the program transformation techniques is to support the adaptation of the application programs to the new database. This adaptation process is studied in the context of two realistic database evolution scenarios, namely database database schema refactoring and database platform migration. &copy; 2010 IEEE.<br/>},
key = {Application programs},
keywords = {Legacy systems;Metadata;Program documentation;},
note = {Adaptation process;Business activities;Data-intensive systems;Database components;Management domains;Program transformation techniques;Strong interaction;Transformation techniques;},
URL = {http://dx.doi.org/10.1109/ICSM.2010.5609724},
} 


@article{20182705396155 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Verifying equivalence of database-driven applications},
journal = {Proceedings of the ACM on Programming Languages},
author = {Wang, Yuepeng and Dillig, Isil and Lahiri, Shuvendu K. and Cook, William R.},
volume = {2},
number = {POPL},
year = {2018},
issn = {24751421},
abstract = {This paper addresses the problem of verifying equivalence between a pair of programs that operate over databases with different schemas. This problem is particularly important in the context of web applications, which typically undergo database refactoring either for performance or maintainability reasons. While web applications should have the same externally observable behavior before and after schema migration, there are no existing tools for proving equivalence of such programs. This paper takes a first step towards solving this problem by formalizing the equivalence and refinement checking problems for database-driven applications. We also propose a proof methodology based on the notion of bisimulation invariants over relational algebra with updates and describe a technique for synthesizing such bisimulation invariants.We have implemented the proposed technique in a tool called Mediator for verifying equivalence between database-driven applications written in our intermediate language and evaluate our tool on 21 benchmarks extracted from textbooks and real-world web applications. Our results show that the proposed methodology can successfully verify 20 of these benchmarks.<br/> &copy; 2018 Copyright held by the owner/author(s).},
key = {Application programs},
keywords = {Algebra;Benchmarking;Equivalence classes;Problem solving;},
note = {Database- driven applications;Equivalence checking;Intermediate languages;Observable behavior;Program Verification;Refinement checking;Relational algebra;Relational Database;},
URL = {http://dx.doi.org/10.1145/3158144},
} 


@inproceedings{20140117154610 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Sql-schema-comparer: Support of multi-language refactoring with relational databases},
journal = {IEEE 13th International Working Conference on Source Code Analysis and Manipulation, SCAM 2013},
author = {Schink, Hagen},
year = {2013},
pages = {173 - 178},
address = {Eindhoven, Netherlands},
abstract = {Refactoring is a method to change a source-code's structure without modifying its semantics and was first introduced for object-oriented code. Since then refactorings were defined for relational databases too. But database refactorings must be treated differently because a database schema's structure defines semantics used by other applications to access the data in the schema. Thus, many database refactorings may break interaction with other applications if not treated appropriately. We discuss problems of database refactoring in regard to Java code and present sql-schema-comparer, a library to detect refactorings of database schemes. The sql-schema-comparer library is our first step to more advanced tools supporting developers in their database refactoring efforts. &copy; 2013 IEEE.<br/>},
key = {Codes (symbols)},
keywords = {Computer programming languages;Semantics;},
note = {Database schemas;Java codes;Multi languages;Object-oriented code;Refactorings;Relational Database;Source codes;},
URL = {http://dx.doi.org/10.1109/SCAM.2013.6648199},
} 


@inproceedings{2006229914410 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An integrated environment for reengineering},
journal = {IEEE International Conference on Software Maintenance, ICSM},
author = {De Guzman, Ignacio Garcia-Rodriguez and Polo, Macario and Piattini, Mario},
volume = {2005},
year = {2005},
pages = {165 - 174},
address = {Budapest, Hungary},
abstract = {This paper presents a tool specifically designed for database reengineering. As is well known, reengineering is the process of (1) applying reverse engineering to a software product to get higher-level specifications, and (2) using these specifications as the starting point for the development of a new version of the system. Thus, the complete process can be seen as a sequence of transformation functions that operate on the different sets of artifacts involved in the whole process. The starting point of the reengineering process is the physical schema of the database, which is translated into a vendor-independent metamodel; then, this is translated into a class diagram representing the possible conceptual schema used during the development of the database. This diagram is then taken as the starting point for the code generation process, which produces an executable application for four possible different platforms. &copy; 2005 IEEE.},
key = {Reengineering},
keywords = {Codes (symbols);Computer software;Database systems;Integrated control;Reverse engineering;},
note = {Class diagram;Code generation process;Metamodel;Model-driven reengineering;},
URL = {http://dx.doi.org/10.1109/ICSM.2005.21},
} 


@article{20063710104629 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Survey says: Agile works in practice},
journal = {Dr. Dobb's Journal},
author = {Ambler, Scott W.},
volume = {31},
number = {9},
year = {2006},
pages = {62 - 64},
issn = {1044789X},
abstract = {A survey conducted on the agile software development methods and techniques, which are gaining increasing attention within the IT industry is discussed. The survey reports show that organizations such as Shine Technologies have adopted the agile method such as Extreme Programming (EP), Scrum, Agile MSF, AUP, and in particular FDD. The organization has also adopted agile development techniques such as Test Driven Development (TDD) or pair programming. Agile database development techniques including database refactoring and database regression testing are also beginning to attract attention. The survey shows that the adoption on agile approaches to software development has successfully affected the overall productivity and the quality of the systems that they delivered. Agile software development's focus on collaborative techniques, such as active stakeholder participation and increased feedback, have also helped to improve stakeholder satisfaction.},
key = {Software engineering},
keywords = {Computer software;Computer supported cooperative work;Database systems;Feedback;Information technology;Surveying;},
note = {Agile method;Shine Technologies (CO);Stakeholders;Test Driven Development (TDD);},
} 


@inproceedings{2001065454531 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Method and process for iterative reengineering of data in a legacy system},
journal = {Reverse Engineering - Working Conference Proceedings},
author = {Bianchi, Alessando and Caivano, Danilo and Visaggio, Giuseppe},
year = {2000},
pages = {86 - 96},
address = {Brisbane, Australia},
abstract = {This paper presents an iterative approach to database reengineering, starting from the assumption that for the user organization, the data are the most important asset in a legacy system. The most innovative feature of the proposed approach, in comparison with other rival approaches, is that it can eliminate all the ageing symptoms of the legacy data base. The new database can therefore be readily used to integrate data used by new functions introduced in the legacy software. Moreover, the approach allows all the services offered by modern data base management systems to be exploited. To test the effectiveness of the process described in this paper, it was experimented on a real legacy system; the results reported in the paper confirm its effectiveness.},
key = {Reverse engineering},
keywords = {COBOL (programming language);Computer software maintenance;Data structures;Database systems;Legacy systems;Software engineering;},
note = {Data reverse engineering;Iterative reengineering;},
} 


@inproceedings{20124615673725 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual foundations of interrogative agents},
journal = {WOA 2007 - 8th AI*IA/TABOO Joint Workshop "From Objects to Agents": Agents and Industry: Technological Applications of Software Agents},
author = {Deufemia, Vincenzo and Polese, Giuseppe and Tortora, Genoveffa and Vacca, Mario},
year = {2007},
pages = {26 - 33},
address = {Genova, Italy},
abstract = {Reasoning by interrogation is one of the most ancient and experimented ways of reasoning. Originated by the Aristotelian elenchus, it has been used for many purposes, such as the resolution of mathematical and daily problems [25], [26], the discovery of new knowledge [19], [34], [36], the realization of questioning/answering processes [23]. In this paper we present the conceptual foundations of interrogative agents, a new model of BDI architecture based on interrogative logic. This model allows us to express the properties of agents in a natural way, and to use heuristics for reasoning. Finally, in order to explicate the whole approach and to highlight its main features we describe the application of interrogative agents in the context of database refactoring.<br/>},
key = {Software agents},
keywords = {Application programs;},
note = {BDI architecture;Conceptual foundations;Refactorings;},
} 


@inproceedings{20134016814423 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Integration of heterogeneous BPM schemas: The case of XPDL and BPEL},
journal = {CEUR Workshop Proceedings},
author = {Hornung, Thomas and Koschmider, Agnes and Mendling, Jan},
volume = {231},
year = {2006},
issn = {16130073},
address = {Luxembourg, Luxembourg},
abstract = {Heterogeneous Business Process Modeling (BPM) schemas have been a problem for business process management throughout the last couple of years. Methodological guidance is needed in order to consolidate concurrent schema proposals especially in the BPM area. This paper discusses the applicability of schema integration for this purpose. We use the case of integrating XPDL 2.0 and BPEL 2.0 to highlight that schema integration is not able to cope with heterogeneous control flow representation of BPM schemas. We introduce a schema refactoring step that leads to integrated BPM schemas with less constructs.<br/>},
key = {Information systems},
keywords = {Administrative data processing;Enterprise resource management;Information use;Integration;Systems engineering;},
note = {Business process management;Business process modeling;Heterogeneous control;Refactorings;Schema integration;},
} 


@article{20064910287156 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ensuring database quality},
journal = {Dr. Dobb's Journal},
author = {Ambler, Scott W.},
volume = {31},
number = {12},
year = {2006},
pages = {63 - 65},
issn = {1044789X},
abstract = {The major reasons for the development of a comprehensive testing strategies for the Relational Database Syatem (RDBMS) are described. The three major reasons presented are related to the security, quality, maintenance, and management of the databases handled in an organization. Data is considered to be an corporate asset and its quality must be maintained at a required level. The second reason is for the validation of the functionality within a database in the form of stored procedures. The third reason relates to a suitable regression test of the database that enables modern evolutionary development practices, such as database refactoring. A database testing involves three steps, such as setting up the test, running the test, and checking the results. Several test procedures are also available for managing test data, which can be used alone or in combination. The testing tools enable developers to perform common data management tasks in shorter time and with greater efficiency.},
key = {Relational database systems},
keywords = {Computer aided software engineering;Data handling;Data reduction;Regression analysis;Security of data;Societies and institutions;},
note = {Data management;Data quality;Database handling;},
} 


@inproceedings{1998033938477 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Developing a data warehouse as the first step for information systems reengineering},
journal = {Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
author = {Huang, Shi-Ming and Lour, Wei-Chen and Huang, Hsiu-Feng and Huang, Chien-Ming and Li, Shing-Han and Fu, Chung-Da},
volume = {3},
year = {1997},
pages = {2403 - 2408},
issn = {08843627},
address = {Orlando, FL, USA},
abstract = {All practitioners of information system know that building an industrial-strength information system is both a long-term commitment and a long-term investment for an organization. Any surviving system should encounter many times of changes either major or minor; and these changes might induce the system serious problems and a lot of maintenance cost. According to the recent development of the database reengineering, data warehousing and business reengineering, many organizations have their information system operated for years. They are facing the problems of introducing a new database system and the re-designation of the data processing procedure. For the information system to serve effectively and efficiently as time goes by, there are two ways to improve it. Firstly, re-design the whole system by using new technology and new database models; secondly, introduce a method that will make use of the existing systems to the maximum extent while the new requirements are still satisfied, i.e. information systems reengineering. In this paper, we propose a framework for practitioners to complete the reengineering work more smoothly. The authors have applied this framework to the system reengineering work for a manufacturer. An EIS prototype is developed, according to the new requirements of high-level managers, and the system is operating currently.},
key = {Distributed database systems},
keywords = {Information retrieval systems;Systems engineering;},
note = {Data warehousing;Executive information system (EIS);Information systems reengineering;},
} 


@article{20124115549650 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Language-integrated querying of XML data in SQL server},
journal = {Proceedings of the VLDB Endowment},
author = {Terwilliger, James F. and Melnik, Sergey and Bernstein, Philip A.},
volume = {1},
number = {2},
year = {2008},
pages = {1396 - 1399},
issn = {21508097},
abstract = {Developers need to access persistent XML data program- matically. Object-oriented access is often the preferred method. Translating XML data into objects or vice-versa is a hard problem due to the data model mismatch and the difficulty of query translation. Our prototype addresses this problem by transforming object-based queries and updates into queries and updates on XML using declarative mappings between classes and XML schema types. Our prototype extends the ADO.NET Entity Framework and leverages its object-relational mapping capabilities. We demonstrate how a developer can interact with stored relational and XML data using the Language Integrated Query (LINQ) feature of .NET. We show how LINQ queries are translated into a combination of SQL and XQuery. Finally, we illustrate how explicit mappings facilitate data in- dependence upon database refactoring. &copy; 2008 VLDB Endowment.<br/>},
key = {XML},
keywords = {Mapping;Query processing;Translation (languages);},
note = {Hard problems;Integrated querying;Model mismatch;Object based;Object oriented;Object-relational mapping;Query translations;Refactorings;},
URL = {http://dx.doi.org/10.14778/1454159.1454182},
} 


@inproceedings{20175004524549 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Metacrate: Organize and analyze millions of data profiles},
journal = {International Conference on Information and Knowledge Management, Proceedings},
author = {Kruse, Sebastian and Hahn, David and Walter, Marius and Naumann, Felix},
volume = {Part F131841},
year = {2017},
pages = {2483 - 2486},
address = {Singapore, Singapore},
abstract = {Databases are one of the great success stories in IT. However, they have been continuously increasing in complexity, hampering operation, maintenance, and upgrades. To face this complexity, sophisticated methods for schema summarization, data cleaning, information integration, and many more have been devised that usually rely on data profiles, such as data statistics, signatures, and integrity constraints. Such data profiles are offen extracted by automatic algorithms, which entails various problems: .e profiles can be unfiltered and huge in volume; different profile types require different complex data structures; and the various profile types are not integrated with each other. We introduce Metacrate, a system to store, organize, and analyze data profiles of relational databases, thereby following the proven design of databases. In particular, we (i) propose a logical and a physical data model to store all kinds of data profiles in a scalable fashion; (ii) describe an analytics layer to query, integrate, and analyze the profiles efficiently; and (iii) implement on top a library of established algorithms to serve use cases, such as schema discovery, database refactoring, and data cleaning<br/> &copy; 2017 ACM.},
key = {Knowledge management},
keywords = {Query processing;},
note = {Automatic algorithms;Complex data structures;Data cleaning;Data statistics;Information integration;Integrity constraints;Physical data;Relational Database;},
URL = {http://dx.doi.org/10.1145/3132847.3133180},
} 


@article{20064010155406 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database application evolution: A transformational approach},
journal = {Data and Knowledge Engineering},
author = {Hick, Jean-Marc and Hainaut, Jean-Luc},
volume = {59},
number = {3 SPEC. ISS.},
year = {2006},
pages = {534 - 558},
issn = {0169023X},
abstract = {While recent data management technologies, such as object oriented techniques, address the problem of database schema evolution, standard information systems currently in use raise challenging evolution problems. This paper examines database evolution from the developer point of view. It shows how requirements changes are propagated to database schemas, to data and to programs through a general strategy. This strategy requires the documentation of database design. When absent, such documentation has to be rebuilt through reverse engineering techniques. Our approach, called DB-MAIN, relies on a generic database model and on transformational paradigm that states that database engineering processes can be modeled by schema transformations. Indeed, a transformation provides both structural and instance mappings that formally define how to modify database structures and contents. We describe both the complete and a simplified approaches, and compare their merits and drawbacks. We then analyze the problem of program modification and describe a CASE tool that can assist developers in their task of system evolution. We illustrate our approach with Biomaze, a biochemical knowledge-based the database of which is rapidly evolving. &copy; 2005 Elsevier B.V. All rights reserved.},
key = {Database systems},
keywords = {Biochemistry;Computer aided software engineering;History;Knowledge based systems;Mathematical transformations;Program documentation;Reverse engineering;},
note = {Database conversion;Evolution;Schema transformation;},
URL = {http://dx.doi.org/10.1016/j.datak.2005.10.003},
} 


@inproceedings{20154701577003 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Strategy for database application evolution: The DB-MAIN approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Hick, Jean-Marc and Hainaut, Jean-Luc},
volume = {2813},
year = {2003},
pages = {291 - 306},
issn = {03029743},
address = {Chicago, IL, United states},
abstract = {While recent data management technologies, e.g., object-oriented, address the problem of databases schema evolution, standard information systems currently in use raise challenging problems when evolution is concerned. This paper studies database evolution from the developer point of view. It shows how requirements changes are propagated to the database schemas, to the data and to the programs through a general strategy. This strategy requires the documentation of the database design. When absent, this documentation has to be rebuilt through reverse engineering techniques. The approach relies on a generic database model and on the transformational paradigm that states that database engineering processes can be modelled by schema transformations. Indeed, a transformation provides both structural and instance mappings that formally define how to modify database structures and contents. The paper then analyses the problem of program modification and describes a CASE tool that can assist developers in their task of system evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
key = {Object-oriented databases},
keywords = {Information management;Information use;Reverse engineering;},
note = {Database applications;Database engineering;Management technologies;Program modifications;Requirements change;Reverse engineering techniques;Schema transformation;Standard information;},
} 


@inproceedings{20155001657592 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {16th International Conference on Conceptual Modeling, ER 1997},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {1331},
year = {1997},
pages = {1 - 478},
issn = {03029743},
address = {Los Angeles, CA, United states},
abstract = {The proceedings contain 37 papers. The special focus in this conference is on Keynote Address and Automated Design. The topics include: From conceptual modeler to university president; an ontology for database design automation; exploiting domain knowledge during the automated design of object-oriented databases; intelligent support for retrieval and synthesis of patterns for object-oriented design; a conceptual development framework for temporal information systems; temporal features of class populations and attributes in conceptual models; managing schema evolution using a temporal object model; extended SQL support for uncertain data; conceptual queries using conquer-II; transaction-based specification of database evolution; well-behaving rule systems for entity-relationship and object-oriented models; behavior consistent refinement of object life cycles; towards incremental specification and flexible coordination of workflow activities; a multi-level architecture for representing enterprise data models; data model for customizing db schemas based on business policies; explaining conceptual models; extending an object-oriented model; formal approach to metamodeling; associations and roles in object-oriented modeling; inheritance graph hierarchy construction using rectangular decomposition of a binary relation and designer feedback; towards an object database approach for managing concept lattices; an experience of integration of conceptual schemas in the italian public administration; application-oriented design of behavior; a java-based framework for processing distributed objects; fragmentation techniques for distributing object-oriented databases; an agent based mobile system; is the future of conceptual modeling bleak or bright and successful practices in developing a complex information model.},
} 


@inproceedings{20141717602413 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The evolution process of geographical database within self-organized topological propagation area},
journal = {ESM 2006 - 2006 European Simulation and Modelling Conference: Modelling and Simulation 2006},
author = {Kadri-Dahmani, Hakima and Bertelle, Cyrille and Duchamp, Gerard H.E and Osmani, Aomar},
year = {2006},
pages = {415 - 419},
address = {Toulouse, France},
abstract = {The paper deals with Geographical Data Base evolution which is a major aspect of the actual development in Geographical Information System (GIS). In a more practical aspect, GIS has now to evolve to manage updating. We will explain how the updating processes can be described as an evolution processus for GIS and make them transform from complicated systems to complex systems.<br/>},
key = {Geographic information systems},
keywords = {Large scale systems;Modal analysis;},
note = {Complicated systems;Emergence;Evolution;Evolution process;Geographical data;Geographical database;},
} 


@article{20183605766162 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A scalable model based approach for data model evolution: Application to space missions data models},
journal = {Computer Languages, Systems and Structures},
author = {Oubelli, Lynda Ait and Ait Ameur, Yamine and Bedouet, Judicael and Kervarc, Romain and Chausserie-Lapree, Benoit and Larzul, Beatrice},
volume = {54},
year = {2018},
pages = {358 - 385},
issn = {14778424},
abstract = {During the development of a complex system, data models are the key to a successful engineering process, as they contain and organize all the information manipulated by the different functions involved in the design of the system. Moreover, these data models evolve throughout the design, as the development raises issues that have to be solved through a restructuration of data organization. But any such data model evolution has a deep impact on the functions that have already being defined. Recent research tries to deal with this issue by studying how complex industrial data models evolve from one version to another and how their data instances co-evolve. Complexity and scalability issues make this problem a major scientific challenge, leading to huge gains in development efficiency. This problem is of particular interest in the field of aeronautics and space systems. Indeed, the development of these systems produces many complex data models associated to the designed systems and/or to the systems under design, hence on the one hand data models are available. On the other hand, it is well known that these systems are developed in the context of collaborative projects that may last for decades. In such projects, specifications together with the associated data models are bound to evolve and engineering processes shall take into account this evolution. Our work addresses the problem of data model evolution in a model-driven engineering setting. We focus on minimizing the impact of model evolution on the system development processes in the specific context on the space engineering area, where data models may involve thousands of concepts and relationships, and we investigate the performance of the model-based development (MBD) approach we propose for data model evolution over two space missions, namely PHARAO and MICROSCOPE.<br/> &copy; 2018 Elsevier Ltd},
key = {Industrial research},
keywords = {Computer networks;Software engineering;},
note = {Data migration;Evolution operator;Model comparison;Model evolution;Model-driven Engineering;},
URL = {http://dx.doi.org/10.1016/j.cl.2018.08.001},
} 


@inproceedings{20124715681820 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database evolution on an orthogonal persistent programming system: A semi-transparent approach},
journal = {Iberian Conference on Information Systems and Technologies, CISTI},
author = {Pereira, Rui Humberto R. and Perez-Schofield, J. Baltasar Garcia},
year = {2012},
issn = {21660727},
address = {Madrid, Spain},
abstract = {In this paper the problem of the evolution of an object-oriented database in the context of orthogonal persistent programming systems is addressed. We have observed two characteristics in that type of systems that offer particular conditions to implement the evolution in a semi-transparent fashion. That transparency can further be enhanced with the obliviousness provided by the Aspect-Oriented Programming techniques. Was conceived a meta-model and developed a prototype to test the feasibility of our approach. The system allows programs, written to a schema, access semi-transparently to data in other versions of the schema. &copy; 2012 AISTI.<br/>},
key = {Object oriented programming},
keywords = {Aspect oriented programming;Information systems;Information use;Object-oriented databases;},
note = {Meta model;orthogonal persistence;Particular condition;Programming system;Schema evolution;Semi-transparent;},
} 


@inproceedings{20162502510129 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data model evolution using object-NoSQL mappers: Folklore or state-of-the-art?},
journal = {Proceedings - 2nd International Workshop on BIG Data Software Engineering, BIGDSE 2016},
author = {Ringlstetter, Andreas and Scherzinger, Stefanie and Bissyande, Tegawende F.},
year = {2016},
pages = {33 - 36},
address = {Austin, TX, United states},
abstract = {In big data software engineering, the schema flexibility of NoSQL document stores is a major selling point: When the document store itself does not actively manage a schema, the data model is maintained within the application. Just like object-relational mappers for relational databases, object-NoSQL mappers are part of professional software development with NoSQL document stores. Some mappers go beyond merely loading and storing Java objects: Using dedicated evolution annotations, developers may conveniently add, remove, or rename attributes from stored objects, and also conduct more complex transformations. In this paper, we analyze the dissemination of this technology in Java open source projects. While we find evidence on GitHub that evolution annotations are indeed being used, developers do not employ them so much for evolving the data model, but to solve different tasks instead. Our observations trigger interesting questions for further research.<br/> &copy; 2016 ACM.},
key = {Big data},
keywords = {Application programs;Java programming language;Open source software;Software design;},
note = {Complex transformations;Model evolution;Object-NoSQL mappers;Object-relational;Open source projects;Professional software;Relational Database;State of the art;},
URL = {http://dx.doi.org/10.1145/2896825.2896827},
} 


@article{20072010604775 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Test-driven development of relational databases},
journal = {IEEE Software},
author = {Ambler, Scott W.},
volume = {24},
number = {3},
year = {2007},
pages = {37 - 43},
issn = {07407459},
abstract = {Developers can use a test-driven development with database schema just as they use it with application code. Implementing test-driven database development (TDDD) involves three relatively simple steps: database refactoring, database regression testing, and continuous database integration. In database refactoring, developers make a simple change to a database to improve the design without changing its semantics. In database regression testing, they run a comprehensive test suite that validates the database regularly-ideally, whenever developers change the database schema or access the database in a different way. In continuous database integration, developers rebuild and retest the database schema whenever it changes. From a technical viewpoint, TDDD is straightforward. However, cultural challenges can make it difficult to adopt. &copy; 2007 IEEE.},
key = {Relational database systems},
keywords = {Software design;Software engineering;Software testing;},
note = {Database refactoring;Database testing;Test-driven database design;Test-driven development;},
URL = {http://dx.doi.org/10.1109/MS.2007.91},
} 


@inproceedings{20154801603007 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data model evolution as a basis of business process management},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Gruhn, Volker and Pahl, Claus and Wever, Monika},
volume = {1021},
year = {1995},
pages = {270 - 281},
issn = {03029743},
address = {Gold Coast, QLD, Australia},
abstract = {In this article we propose an approach to business process management which meets the demands of business process evolution. This approach allows for on-the fly modifications of business processes. In contrast to many other approaches, we do not only concentrate on activities to be carried out in business processes, but also on the data created and manipulated by these activities. We propose to apply data model analysis and improvement strategies well-known from the information system field in the context of business process management.<br/> &copy; Springer-Verlag Berlin Heidelberg 1995.},
key = {Information management},
keywords = {Administrative data processing;Data mining;Data structures;Enterprise resource management;},
note = {Business Process;Business process management;Evolution;Improvement strategies;Model analysis;Model evolution;On the flies;},
} 


@inbook{1996013012128 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data model evolution as a basis of business process management},
journal = {Lecture Notes in Computer Science},
author = {Gruhn, V. and Pahl, C. and Wever, M.},
number = {1021},
year = {1995},
pages = {270 - 270},
issn = {03029743},
address = {Gold Coast, Aust},
} 


@article{2006239927088 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Where did all the positions go?},
journal = {Dr. Dobb's Journal},
author = {Ambler, Scott W.},
volume = {31},
number = {6},
year = {2006},
pages = {90 - 92},
issn = {1044789X},
abstract = {The features and the issues regarding the agile software development in the IT and QA professionals, are discussed. Many traditional IT professionals are struggling to see how they fit into an agile software development project. Some people have the attitude that agilist have adopted techniques such as test driven development (TDD), acceptance testing, pair programming code refactoring, and database refactoring, all of which promote creation of high quality work products. The agile software development lifecycle (SDLC) looks very much like traditional SDLC, but the the former is highly collaborative, iterative, and incremental, and the roles that people take are more robust than on traditional projects. On an agile project, developers work closely with their stakeholders to understand their needs, to implement and test their solution, and the solution is shown to their stakeholders for quick feedback.},
key = {Software engineering},
keywords = {Codes (symbols);Database systems;Distributed computer systems;Feedback;Information technology;Life cycle;Problem solving;Professional aspects;Project management;},
note = {Agile software development project;Database refactoring;Software development lifecycle (SDLC);Test driven development (TDD);},
} 


@inproceedings{20160801961095 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A model-driven approach to data structure conceptualization},
journal = {Proceedings of the 2015 Federated Conference on Computer Science and Information Systems, FedCSIS 2015},
author = {Risti, Sonja and Kordi, Slavica and elikovi, Milan and Dimitrieski, Vladimir and Lukovi, Ivan},
year = {2015},
pages = {977 - 984},
address = {Lodz, Poland},
abstract = {Reengineering of an existing information system can be carried out: to improve its maintainability, to migrate to a new technology, to improve quality or to prepare for functional enhancement. An important phase of a data-oriented software system reengineering is a database reengineering process and, in particular, its subprocess - a database reverse engineering process. he reverse engineering process contains two main phases: data structure extraction and data structure conceptualization. In the paper we present a blueprint of a model-driven approach to database reengineering process that is one of the results of our research project on model-driven intelligent systems for software system development, maintenance and evolution. Within that process hereinafter we focus on the data structure conceptualization process and propose a model-driven approach to data structure conceptualization. Proposed process is based on model-tomodel transformations implemented by means of Atlas ransformation Language. 0<br/> &copy; 2015, IEEE.},
key = {Data structures},
keywords = {Computer software;Database systems;Information systems;Information use;Intelligent systems;Reengineering;Reverse engineering;},
note = {Data structure extraction;Database reengineering;Functional enhancements;Model driven approach;Model-driven;New technologies;Reverse engineering process;Software systems;},
URL = {http://dx.doi.org/10.15439/2015F224},
} 


@article{2005379362592 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Co-transformations in information system reengineering},
journal = {Electronic Notes in Theoretical Computer Science},
author = {Cleve, Anthony and Henrard, Jean and Hainaut, Jean-Luc},
volume = {137},
number = {3},
year = {2005},
pages = {5 - 15},
issn = {15710661},
abstract = {Database reengineering consists of deriving a new database from a legacy database and adapting the software components accordingly. This migration process involves three main steps, namely schema conversion, data conversion and program conversion. This paper explores the feasibility of transforming the application programs through code transformation patterns that are automatically derived from the database transformations. It presents the principles of a new transformational approach coupling database and program transformations and it describes a prototype CASE tool based on this approach. &copy; 2005 Elsevier B.V. All rights reserved.},
key = {Database systems},
keywords = {Computer aided software engineering;Computer software;Information science;Systems engineering;},
note = {Code transformation;Database reengineering;Program transformation;Schema transformation;},
URL = {http://dx.doi.org/10.1016/j.entcs.2005.07.001},
} 


@inproceedings{20130615985402 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ORM Logic-based English (OLE) and the ORM ReDesigner Tool: Fact-based reengineering and migration of relational databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Balsters, Herman},
volume = {7567 LNCS},
year = {2012},
pages = {358 - 367},
issn = {03029743},
address = {Rome, Italy},
abstract = {The problem of database reengineering stems from (legacy) databases that are hard to understand (incorrect-, incomplete- or missing semantics) or that perform inefficiently. Reengineering is often cumbersome due to lack of semantics of the original database, and often the data migration target is also unclear. This paper addresses those two problems. We shall show how fact-based modeling, in particular ORM and its representation in (sugared) Sorted Logic, can help in reengineering (relational) databases. We reconstruct the semantics of the source database by offering a set of natural-language sentences capturing conceptual structure and constraints of the source. These sentences are written in a structured natural language format, coined as OLE: ORM Logic-based English. OLE is then used to define the mappings from the original source to a reengineered and restructured target database. We shall also discuss the ORMReDesigner: a semi-automatic tool, based on OLE and NORMA, available as a research prototype, used for reengineering and migrating relational databases. &copy; 2012 Springer-Verlag.<br/>},
key = {Computer circuits},
keywords = {Reengineering;Semantics;},
note = {Conceptual structures;Data migration;Database reengineering;Derivation rules;Relational Database;Research prototype;Semi-automatic tools;structured English;},
URL = {http://dx.doi.org/10.1007/978-3-642-33618-8_50},
} 


@inproceedings{20165203193731 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A model-To-model transformation of a generic relational database schema into a form type data model},
journal = {Proceedings of the 2016 Federated Conference on Computer Science and Information Systems, FedCSIS 2016},
author = {Ristic, Sonja and Kordic, Slavica and Celikovic, Milan and Dimitrieski, Vladimir and Lukovic, Ivan},
year = {2016},
pages = {1577 - 1580},
address = {Gdansk, Poland},
abstract = {An important phase of a data-oriented software system reengineering is a database reengineering process and, in particular, its subprocess-a database reverse engineering process. In this paper we present one of the model-To-model transformations from a chain of transformations aimed at transformation of a generic relational database schema into a form type data model. The transformation is a step of the data structure conceptualization phase of a model-driven database reverse engineering process that is implemented in IIS&lowast;Studio development environment.<br/> &copy; 2016 Polish Information Processing Society.},
key = {Metadata},
keywords = {Information systems;Information use;Reengineering;Reverse engineering;},
note = {Database reengineering;Development environment;Model to model transformation;Model-driven;Relational database schemata;Reverse engineering process;Software systems;},
URL = {http://dx.doi.org/10.15439/2016F408},
} 


@inproceedings{20190606485468 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A language abstraction layer for relational DBMS},
journal = {Proceedings - 1996 International Conference Software Engineering: Education and Practice, SEEP 1996},
author = {Stanger, N.},
year = {1996},
pages = {93 - 96},
address = {Dunedin, New zealand},
abstract = {Current database definition techniques tend to be either purely language based or purely graphical. There is little or no integration between the two. This places artificial limits on the environment that developers must work in; ideally, they should be able to switch paradigms as the need arises. This lack of integration also causes problems with database reengineering. The paper describes an architecture for a relational abstraction layer, which isolates the front end &raquo;dialect&raquo; used from the underlying relational implementation. This allows tighter integration between different database definition techniques.<br/> &copy; 1996 IEEE.},
key = {Software engineering},
keywords = {Abstracting;Database systems;Integration;},
note = {Abstraction layer;Database reengineering;Front end;Relational DBMS;},
URL = {http://dx.doi.org/10.1109/SEEP.1996.533986},
} 


@inproceedings{20064310195626 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Co-transformations in database applications evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Cleve, Anthony and Hainaut, Jean-Luc},
volume = {4143 LNCS},
year = {2006},
pages = {409 - 421},
issn = {03029743},
address = {Braga, Portugal},
abstract = {The paper adresses the problem of consistency preservation in data intensive applications evolution. When the database structure evolves, the application programs must be changed to interface with the new schema. The latter modification can prove very complex, error prone and time consuming. We describe a comprehensive transformation/generative approach according to which automated program transformation can be derived from schema transformation. The proposal is illustrated in the particular context of database reengineering, for which a specific methodology and a prototype tool are presented. Some results of two case studies are described. &copy; Springer-Verlag Berlin Heidelberg 2006.},
key = {Database systems},
keywords = {Computer software;Data structures;Error analysis;Problem solving;Software prototyping;User interfaces;},
note = {Application programs;Consistency preservation;Database reengineering;Prototype tools;},
} 


@inproceedings{20095312599893 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Model-Driven reengineering of database},
journal = {2009 WRI World Congress on Software Engineering, WCSE 2009},
author = {Wang, Hanzhe and Shen, Beijun and Chen, Cheng},
volume = {3},
year = {2009},
pages = {113 - 117},
address = {Xiamen, China},
abstract = {A lot of work has been done applying Model-Driven Approach to those business domain concerned software development. These researches mostly show how to transform business domain models to software application with different paradigms, rather than how to transform specific software artifacts generally regarding of business domain factor, such as database, the common infrastructure of nowadays software system. The later kind of work can make more contribution to general software development rather than some specific business domains. In this paper, we present a MDA based approach to perform database reengineering and also build a framework based on current framework (EMF, Operational-QVT). &copy; 2009 IEEE.<br/>},
key = {Software design},
keywords = {Application programs;Database systems;Reengineering;Software architecture;},
note = {Business domain;Common infrastructures;Database reengineering;MDA-based approach;Model driven approach;Software applications;Software artifacts;Software systems;},
URL = {http://dx.doi.org/10.1109/WCSE.2009.384},
} 


@inproceedings{2002427140755 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Science of computa programming},
journal = {Science of Computer Programming},
editor = {Chikofsky, E;Verhoef, C;},
volume = {45},
number = {2-3},
year = {2002},
issn = {01676423},
address = {Amsterdam, Netherlands},
abstract = {The proceedings contains six papers from the Conference of Science of Computer Programming. Topics discussed include: supporting iterations in exploratory database reengineering processes; a change impact model for changeability assessment in object-oriented software systems; towards a user-controlled software renovation factory; and extracting Java library subsets for deployment on embedded systems.},
key = {Software engineering},
keywords = {Client server computer systems;Computer aided engineering;Computer software maintenance;Computer workstations;Data structures;Database systems;Decision support systems;Digital libraries;Graphical user interfaces;Legacy systems;Mathematical models;Reengineering;Reverse engineering;Semantics;Telecommunication systems;World Wide Web;},
note = {Database reengineering processes;Database reverse engineering (DBRE);EiRev;Information systems;Java library;Object-oriented system;Relational schema;Software renovation factory;Software systems;Teleprocessing system;},
} 


@inproceedings{20162402503079 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Architecture for reengineering legacy databases},
journal = {ICEIS 2001 - Proceedings of the 3rd International Conference on Enterprise Information Systems},
author = {Sivalanka, Prasad N. and Subrahmanya, S.V. and Agarwal, Rakesh},
volume = {1},
year = {2001},
pages = {145 - 150},
address = {Setubal, Portugal},
abstract = {There exist different methods to facilitate database design recovery under the framework of software engineering and reengineering. These tools and methods are usually limited to a particular scenario and requirement, and thus, not generic. In most cases, new tools and methods need to be redeveloped to suit these scenarios. This can result in a significant waste of effort and increased costs. In this paper we describe a generic architecture for reengineering legacy databases, which is an outcome of working on a real software project for one of our customers. The goal of this research is to formalize a process that is applicable to different database reengineering scenarios and requirements. We elaborate the steps that were actually done for implementing the project.<br/>},
key = {Information systems},
keywords = {Application programs;Database systems;Information use;Legacy systems;Reengineering;},
note = {Application development;Database design;Database reengineering;Generic architecture;Legacy database;Real softwares;Software project;Tools and methods;},
} 


@inbook{20172603820996 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Migration of legacy information systems},
journal = {Software Evolution},
author = {Hainaut, Jean-Luc and Cleve, Anthony and Henrard, Jean and Hick, Jean-Marc},
year = {2008},
pages = {105 - 138},
abstract = {This chapter addresses the problem of platform migration of large business applications, that is, complex software systems built around a database and comprising thousands of programs. More specifically, it studies the substitution of a modern data management technology for a legacy one. Platform migration raises two major issues. The first one is the conversion of the database to a new data management paradigm. Recent results have shown that automated lossless database migration can be achieved, both at the schema and data levels. The second problem concerns the adaptation of the application programs to the migrated database schema and to the target data management system. This chapter first poses the problem and describes the State of the Art in information system migration. Then, it develops a two-dimensional reference framework that identifies six representative migration strategies. The latter are further analysed in order to identify methodological requirements. In particular, it appears that transformational techniques are particularly suited to drive the whole migration process. We describe the database migration process, which is a variant of database reengineering. Then, the problem of program conversion is studied. Some migration strategies appear to minimise the program understanding effort, and therefore are sound candidates to develop practical methodologies. Finally, the chapter describes a tool that supports such methodologies and discusses some real-size case studies. &copy; 2008 Springer-Verlag.<br/>},
key = {Information management},
keywords = {Application programs;Database systems;Digital storage;Information systems;Information use;Legacy systems;Management information systems;},
note = {Complex software systems;Data management system;Database migrations;Database reengineering;Legacy information systems;Management technologies;Migration strategy;Program understanding;},
URL = {http://dx.doi.org/10.1007/978-3-540-76440-3_6},
} 


@inproceedings{20154301437081 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CoDEL  A relationally complete language for database evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Herrmann, Kai and Voigt, Hannes and Behrend, Andreas and Lehner, Wolfgang},
volume = {9282},
year = {2015},
pages = {63 - 76},
issn = {03029743},
address = {Poitiers, France},
abstract = {Software developers adapt to the fast-moving nature of software systems with agile development techniques. However, database developers lack the tools and concepts to keep pace. Data, already existing in a running product, needs to be evolved accordingly, usually by manually written SQL scripts. A promising approach in database research is to use a declarative database evolution language, which couples both schema and data evolution into intuitive operations. Existing database evolution languages focus on usability but did not aim for completeness. However, this is an inevitable prerequisite for reasonable database evolution to avoid complex and error-prone workarounds. We argue that relational completeness is the feasible expressiveness for a database evolution language. Building upon an existing language, we introduce CoDEL. We define its semantic using relational algebra, propose a syntax, and show its relational completeness.<br/> &copy; Springer International Publishing Switzerland 2015.},
key = {Database systems},
keywords = {Algebra;Information systems;Information use;Semantics;Software engineering;},
note = {Agile development;Descriptive database evolution;Evolution language;Intuitive operations;Nature of software;Relational algebra;Relational completeness;Software developer;},
URL = {http://dx.doi.org/10.1007/978-3-319-23135-8_5},
} 


@article{1995031501928 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Conceptual database evolution through learning in object databases},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Li, Qing and McLeod, Dennis},
volume = {6},
number = {2},
year = {1994},
pages = {205 - 224},
issn = {10414347},
abstract = {Changes to the conceptual structure (meta-data) of a database are common in many application environments and are in general inadequately supported by existing database systems. An approach to supporting such meta-data evolution in a simple, extensible, object database environment is presented. Machine learning techniques are the basis for a cooperative user/system database design and evolution methodology. An experimental end-user database evolution tool based on this approach has been designed and implemented.},
key = {Database systems},
keywords = {Data structures;Design;Learning systems;Object oriented programming;},
note = {Applied machine learning;Conceptual database evolution;End user database tools;Evolution through learning;Object databases;},
URL = {http://dx.doi.org/10.1109/69.277766},
} 


@inproceedings{1986070106397 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {DYNAMIC CONSTRAINTS AND DATABASE EVOLUTION.},
author = {Vianu, Victor},
year = {1983},
pages = {389 - 399},
address = {Atlanta, GA, USA},
abstract = {A simple extension of the relational model is introduced to study the effects of dynamic constraints on database evolution. Both static and dynamic constraints are used in conjunction with this 'dynamic' extension of the relational model. The static constraints considered here are functional dependencies (fd's). The dynamic constraints involve global updates and are restricted to certain analogs of fd's, called 'dynamic' fd's. The results concern the interaction between the static and dynamic constraints. The effect of the past history of the database on the static constraints is investigated using the notions of age and age-closure. The connection between the static constraints and the future evolution of the database is described through the notions of survivability and survivability-closure.},
key = {DATABASE SYSTEMS},
keywords = {COMPUTER METATHEORY - Formal Logic;COMPUTER PROGRAMMING - Structured Programming;DATA PROCESSING - Data Handling;},
note = {DATABASE EVOLUTION;DYNAMIC CONSTRAINTS;FUNCTIONAL DEPENDENCIES;RELATIONAL DATABASE MODEL;},
URL = {http://dx.doi.org/10.1145/588058.588105},
} 


@inproceedings{20125015800409 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic adaptation of software applications to database evolution by graph differencing and AOP-based dynamic patching},
journal = {Proceedings - International Computer Software and Applications Conference},
author = {Song, Yang and Peng, Xin and Xing, Zhenchang and Zhao, Wenyun},
year = {2012},
pages = {111 - 118},
issn = {07303157},
address = {Izmir, Turkey},
abstract = {Modern information systems, such as enterprise applications and e-commerce applications, often consist of databases surrounded by a large variety of software applications depending on the databases. During the evolution and deployment of such information systems, developers have to ensure the global consistency between database schemas and surrounding software applications. However, in such situations as Enterprise Application Integration (EAI), databases are shared by a number of software applications contributed by different independent parties, and the developers of those applications often have little or no control on when and how database schema evolves over time. As a result, databases and software applications may not always remain in sync. Such inconsistency may lead to data loss, program failures, or decreased performance. The fundamental challenge in evolving and deploying such database-centric information systems is the fact that databases and their surrounding software applications are subject to independent, asynchronous, and potentially conflicting evolution processes. In this paper, we present an approach to automatically adapting software applications to the evolution of their underlying databases by graph-based schema differencing and aspect-oriented dynamic patching. Our empirical study shows that our approach can automatically adapt software applications to a number of common types of database schema evolution, which accounts for over 87.5% of all schema evolution in the subject system. Our approach allows database schema maintainer to evolve database schema more freely without being afraid of breaking surrounding software applications; it also allows application developers to catch up database schema evolution more quickly without diverting too much from their main business concerns. &copy; 2012 IEEE.<br/>},
key = {Enterprise software},
keywords = {Computer software maintenance;Database systems;Graphic methods;Information systems;Information use;},
note = {Application developers;Automatic adaptation;E-Commerce applications;Enterprise application integration;Enterprise applications;Global consistency;Software applications;Software Evolution;},
URL = {http://dx.doi.org/10.1109/COMPSAC.2012.21},
} 


@inproceedings{20102613038089 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fuzzy XML model for representing fuzzy relational databases in fuzzy XML format},
journal = {ICEIS 2006 - 8th International Conference on Enterprise Information Systems, Proceedings},
author = {Jiwani, Alnaar and Alimohamed, Yasin and Spence, Krista and Ozyer, Tansel and Alhajj, Reda},
volume = {DISI},
year = {2006},
pages = {163 - 168},
address = {Paphos, Cyprus},
abstract = {This paper describes a fuzzy XML schema model for representing a fuzzy relational database in XML format. It outlines a simple translation algorithm to include fuzzy relations and similarity matrices with their associated conventional relation. We also describe an example implementation of a fuzzy relational database and the XML document resulting from the translation according to our schema.<br/>},
key = {Information systems},
keywords = {Information use;XML;},
note = {Conventional relations;Database reengineering;Fuzzy data;Fuzzy relational database;Fuzzy xml models;Translation algorithms;Xml schema modeling;XML schemas;},
} 


@inproceedings{2003037321012 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A database evolution approach for object-oriented databases},
journal = {IEEE International Conference on Software Maintenance, ICSM},
author = {Rashid, Awais},
year = {2001},
pages = {561 - 564},
address = {Florence, Italy},
abstract = {This paper describes a composite evolution approach which integrates the evolution of the various types of entities in an object-oriented database into one model. The approach provides maintainers with a coherent and comprehensible view of the system and at the same time maintains change histories at a fine granularity. Links among meta-objects are implemented using dynamic relationships which are semantic constructs and first-class objects. Referential integrity is maintained by the relationships architecture reducing the evolution complexity at the meta-object level. A customisable and exchangeable instance adaptation approach is proposed. The approach is based on separating the instance adaptation code from class versions using aspects, abstractions used in Aspect-Oriented Programming to localise crosscutting concerns. A high level object-oriented model offering transparent access to the proposed evolution functionality is provided.},
key = {Database systems},
keywords = {C (programming language);Computational complexity;Object oriented programming;Semantics;Software engineering;},
note = {Aspect oriented programming;Database evolution approach;Evolution complexity;Object oriented databases;Referential integrity;Semi-Autonomous Database Evolution System;},
URL = {http://dx.doi.org/10.1109/ICSM.2001.972772},
} 


@article{2002467213450 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Generating three-tier applications from relational databases: A formal and practical approach},
journal = {Information and Software Technology},
author = {Polo, Macario and Gomez, Juan Angel and Piattini, Mario and Ruiz, Francisco},
volume = {44},
number = {15},
year = {2002},
pages = {923 - 941},
issn = {09505849},
abstract = {This article describes a method for building applications with a three-tier structure (presentation, business, persistence) from an existing relational database. The method works as a transformation function that takes the relational schema as its input, producing three sets of classes (which depend on the actual system being reengineered) to represent the final application, as well as some additional auxiliary classes (which are 'constant' and always generated, such as an 'About' dialog, for example). All the classes generated are adequately placed along the three-tiers. The method is based on (1) the formalization of all the sets involved in the process, and (2) the mathematical formulation of the required functions to get the final application. For this second step, we have taken into account several well-known, widely used design and transformation patterns that produce high quality designs and highly maintainable software. The method is implemented in a tool that we have successfully used in several projects of medium size. Obviously, it is quite difficult for the obtained software to fulfill all the requirements desired by the customer, but the uniformity and understandability of its design makes very easy its modification. &copy; 2002 Elsevier Science B.V. All rights reserved.},
key = {Relational database systems},
keywords = {Computer aided software engineering;Computer software;Mathematical transformations;Reverse engineering;},
note = {Database reengineering;},
URL = {http://dx.doi.org/10.1016/S0950-5849(02)00130-1},
} 


@article{2000104981213 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Hybrid approach to convert relational schema to object-oriented schema},
journal = {Information sciences},
author = {Kwan, Irene and Li, Qing},
volume = {117},
number = {3},
year = {1999},
pages = {201 - 241},
issn = {00200255},
abstract = {In the paradigm of DataBase Re-Engineering (DBRE), reverse-engineering data semantics by schema translation from lower level of abstraction such as relational schema to a higher level of abstraction such as Extended Entity Relational (EER) model has been, in the past, extensively studied with due to its relative simplicity in matching. However, schema transformation from EER model to Object-Oriented DataBase (OODB) schema is not straightforward due to its missing of dynamic semantic representation, since an Object-Oriented (OO) schema should contain both the structure and operations of the data objects. In this paper, we describe a hybrid approach which applies both heuristic learning techniques in discovering the behavioural semantics from relational schema and knowledge-based approach in recovering static and structural semantics, to reach a complete conversion. A practical case is applied through an implemented prototype to validate the effectiveness of our methodology.},
key = {Computational linguistics},
keywords = {Heuristic methods;Knowledge based systems;Learning algorithms;Object oriented programming;Reengineering;Relational database systems;},
note = {Database reengineering;Heuristic learning techniques;},
URL = {http://dx.doi.org/10.1016/S0020-0255(99)00009-2},
} 


@inproceedings{2002397101571 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Elicitation and conversion of hidden objects and restrictions in a database schema},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Rivero, Laura C. and Doorn, Jorge H. and Ferraggine, Viviana E.},
year = {2002},
pages = {463 - 469},
address = {Madrid, Spain},
abstract = {Mapping a database schema from one model into another, with a higher semantic capacity, is a current research subject with application in several development fields, such as schema integration and translation, migration from legacy systems and reengineering of poor quality or no-longer accurate data models. Inclusion dependencies are one of the most important concepts in relational databases and they are the key to perform some reengineering of database schemas. Referential integrity restrictions (rir), a particular case of an inclusion constraint, requires that the set of distinct values occurring in some specified column, simple or composite (foreign key), must be a subset of the set of distinct primary key values drawn from the same domain. Pure inclusion dependencies (id), however, may apply between other pairs of attributes also (alternate keys or non-keys). Database schemas containing ids frequently reveal the presence of hidden objects and misrepresented relationships and, as a consequence, increase the effort to develop program applications and maintain the integrity. This work presents a heuristics for the conversion of schemas with ids into equivalent schemas with only rirs. In case some irreducible ids remain, a semantic interpretation of their necessity and maintenance is given.},
key = {Object oriented programming},
keywords = {Constraint theory;Data structures;Heuristic programming;Knowledge acquisition;Legacy systems;Reengineering;Relational database systems;Semantics;},
note = {Database conceptual schema reengineering;Objects conversion;Objects elicitation;Referential integrity restriction;},
URL = {http://dx.doi.org/10.1145/508791.508878},
} 


@article{1987010003982 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {DECISION SUPPORT SYSTEM FOR DATABASE EVOLUTION USING DATA MODEL INDEPENDENT ARCHITECTURE.},
journal = {Computers and Operations Research},
author = {Hsu, Cheng},
volume = {13},
number = {4},
year = {1986},
pages = {427 - 436},
issn = {03050548},
abstract = {This paper presents methods for the control of large-scale databases - the repository of information pertaining to an enterprise using computers. In particular, the paper proposes a set of quantitative models for monitoring the performance of database systems when the changing nature of applications of the systems is explicitly considered. Using these models, the system's performance will be assessed constantly according to the criteria set forth for them. Also, decision rules will be derived to determine the optimal policies of control. The methods that are proposed in the paper are especially useful for operating settings where both the interrelationships of data and the uses envisioned for them are evolving rapidly. Some database constructs unavailable previously are developed to facilitate the implementation of such a decision framework for database systems.},
key = {DATABASE SYSTEMS},
keywords = {DECISION THEORY AND ANALYSIS;INFORMATION RETRIEVAL SYSTEMS;},
note = {DATA MODEL INDEPENDENT ARCHITECTURE;DATABASE EVOLUTION;DECISION SUPPORT SYSTEM;FOUR-SCHEME ARCHITECTURE;},
URL = {http://dx.doi.org/10.1016/0305-0548(86)90030-4},
} 


@inproceedings{20111613914178 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Efficient and scalable data evolution with column oriented databases},
journal = {ACM International Conference Proceeding Series},
author = {Liu, Ziyang and He, Bin and Hsiao, Hui-I and Chen, Yi},
year = {2011},
pages = {105 - 116},
abstract = {Database evolution is the process of updating the schema of a database or data warehouse (schema evolution) and evolving the data to the updated schema (data evolution). It is often desired or necessitated when changes occur to the data or the query workload, the initial schema was not carefully designed, or more knowledge of the database is known and a better schema is concluded. The Wikipedia database, for example, has had more than 170 versions in the past 5 years [8]. Unfortunately, although much research has been done on the schema evolution part, data evolution has long been a prohibitively expensive process, which essentially evolves the data by executing SQL queries and re-constructing indexes. This prevents databases from being flexibly and frequently changed based on the need and forces schema designers, who cannot afford mistakes, to be highly cautious. Techniques that enable efficient data evolution will undoubtedly make life much easier. In this paper, we study the efficiency of data evolution, and discuss the techniques for data evolution on column oriented databases, which store each attribute, rather than each tuple, contiguously. We show that column oriented databases have a better potential than traditional row oriented databases for supporting data evolution, and propose a novel data-level data evolution framework on column oriented databases. Our approach, as suggested by experimental evaluations on real and synthetic data, is much more efficient than the query-level data evolution on both row and column oriented databases, which involves unnecessary access of irrelevant data, materializing intermediate results and re-constructing indexes.<br/>},
key = {Query processing},
keywords = {Data warehouses;},
note = {Bitmap indexes;Column-oriented database;Data evolution;Experimental evaluation;Intermediate results;Schema;Schema evolution;Synthetic data;},
URL = {http://dx.doi.org/10.1145/1951365.1951380},
} 


@inproceedings{20154201410050 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Facilitating database attribute domain evolution using mesodata},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {de Vries, Denise and Roddick, John F.},
volume = {3289},
year = {2004},
pages = {429 - 440},
issn = {03029743},
address = {Shanghai, China},
abstract = {Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute&rsquo;s specification, semantics and/or range of allowable values changes. We present a model in which mesodata &ndash; an additional domain definition layer containing domain structure and intelligence &ndash; is used to alleviate and in some cases obviate the need for data conversion or coercion. We present the nature and use of mesodata as it affects domain evolution, such as when a domain changes, when the semantics of a domain alter and when the attribute&rsquo;s specification is modified.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
key = {Data handling},
keywords = {Semantics;Specifications;},
note = {Data conversion;Domain evolution;Domain structure;Schema evolution;},
} 


@article{20123615397621 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CODS: Evolving data efficiently and scalably in column oriented databases},
journal = {Proceedings of the VLDB Endowment},
author = {Liu, Ziyang and Natarajan, Sivaramakrishnan and He, Bin and Hsiao, Hui-I and Chen, Yi},
volume = {3},
number = {2},
year = {2010},
pages = {1521 - 1524},
issn = {21508097},
abstract = {Database evolution is the process of updating the schema of a database or data warehouse (schema evolution) and evolving the data to the updated schema (data evolution). Database evolution is often necessitated in relational databases due to the changes of data or workload, the suboptimal initial schema design, or the availability of new knowledge of the database. It involves two steps: updating the database schema, and evolving the data to the new schema. Despite the capability of commercial RDBMSs to well optimize query processing, evolving the data during a database evolution through SQL queries is shown to be prohibitively costly. We designed and developed CODS, a platform for efficient data level data evolution in column oriented databases, which evolves the data to the new schema without materializing query results or unnecessary compression/decompression as occurred in traditional query level approaches. CODS ameliorates the efficiency of data evolution by orders of magnitude compared with commercial or open source RDBMSs. &copy; 2010 VLDB Endowment.<br/>},
key = {Query processing},
keywords = {Data warehouses;Query languages;},
note = {Column-oriented database;Data evolution;Database schemas;Evolving datum;Orders of magnitude;Relational Database;Schema design;Schema evolution;},
} 


@article{20170503302903 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Robust and simple database evolution},
journal = {Information Systems Frontiers},
author = {Herrmann, Kai and Voigt, Hannes and Rausch, Jonas and Behrend, Andreas and Lehner, Wolfgang},
volume = {20},
number = {1},
year = {2018},
pages = {45 - 61},
issn = {13873326},
abstract = {Software developers adapt to the fast-moving nature of software systems with agile development techniques. However, database developers lack the tools and concepts to keep the pace. Whenever the current database schema is evolved, the already existing data needs to be evolved as well. This is usually realized with manually written SQL scripts, which is error-prone and explains significant costs in software projects. A promising solution are declarative database evolution languages, which couple both schema and data evolution into intuitive operations. Existing database evolution languages focus on usability but do not strive for completeness. However, this is an inevitable prerequisite to avoid complex and error-prone workarounds. We present CoDEL which is based on an existing language but is relationally complete. We precisely define its semantic using relational algebra, propose a syntax, and formally validate its relational completeness. Having a complete and comprehensive database evolution language facilitates valuable support throughout the whole evolution of a database. As an instance, we present VaCo, a tool supporting developers with variant co-evolution. Given a variant schema derived from a core schema, VaCo uses the richer semantics of CoDEL to semi-automatically co-evolve this variant with the core.<br/> &copy; 2017, Springer Science+Business Media New York.},
key = {Database systems},
keywords = {Algebra;Semantics;Software engineering;},
note = {Agile development;Co-evolution;Evolution language;Intuitive operations;Nature of software;Relational algebra;Relational completeness;Software developer;},
URL = {http://dx.doi.org/10.1007/s10796-016-9730-2},
} 


@inproceedings{20161902358283 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database evolution for software product lines},
journal = {DATA 2015 - 4th International Conference on Data Management Technologies and Applications, Proceedings},
author = {Herrmann, Kai and Reimann, Jan and Voigt, Hannes and Demuth, Birgit and Fromm, Stefan and Stelzmann, Robert and Lehner, Wolfgang},
year = {2015},
pages = {125 - 133},
address = {Colmar, Alsace, France},
abstract = {Software product lines (SPLs) allow creating a multitude of individual but similar products based on one common software model. Software components can be developed independently and new products can be generated easily. Inevitably, software evolves, a new version has to be deployed, and the data already existing in the database has to be transformed accordingly. As independently developed components are compiled into an individual SPL product, the local evolution script of every involved component has to be weaved into a single global database evolution script for the product. In this paper, we report on the database evolution toolkit DAVE in the context of an industry project. DAVE solves the weaving problem and provides a feasible solution for database evolution in SPLs.<br/>},
key = {Database systems},
keywords = {Computer software;Information management;},
note = {Common software;Evolution;Feasible solution;Global database;Industry project;Software component;Software Product Line;Software product line (SPLs);},
} 


@inproceedings{20134016814427 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adaptive query formulation to handle database evolution},
journal = {CEUR Workshop Proceedings},
author = {Papastefanatos, George and Vassiliadis, Panos and Vassiliou, Yannis},
volume = {231},
year = {2006},
issn = {16130073},
address = {Luxembourg, Luxembourg},
abstract = {Databases are continuously evolving environments, where design constructs are added, removed or updated rather often. Research has extensively dealt with the problem of database evolution. Nevertheless, problems arise with existing queries and applications, mainly due to the fact that, in most cases, their role as integral parts of the environment is not given the proper attention. Furthermore, the queries are not designed to handle database evolution. In this paper, we introduce a graph-based model that uniformly captures relations, views, constraints and queries. For several cases of database evolution we present rules so that both syntactical and semantic correctness of queries are retained.<br/>},
key = {Query languages},
keywords = {Graphic methods;Information systems;Information use;Query processing;Semantics;Systems engineering;},
note = {Graph-based modeling;Integral part;Not given;Query formulation;},
} 


@inproceedings{20172603847722 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Living in parallel realities - Co-existing schema versions with a bidirectional database evolution language},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
author = {Herrmann, Kai and Voigt, Hannes and Behrend, Andreas and Rausch, Jonas and Lehner, Wolfgang},
volume = {Part F127746},
year = {2017},
pages = {1101 - 1116},
issn = {07308078},
address = {Chicago, IL, United states},
abstract = {We introduce end-to-end support of co-existing schema versions within one database. While it is state of the art to run multiple versions of a continuously developed application concurrently, it is hard to do the same for databases. In order to keep multiple co-existing schema versions alive-which are all accessing the same data set-developers usually employ handwritten delta code (e.g. views and triggers in SQL). This delta code is hard to write and hard to maintain: if a database administrator decides to adapt the physical table schema, all handwritten delta code needs to be adapted as well, which is expensive and error-prone in practice. In this paper, we present InVerDa: developers use the simple bidirectional database evolution language BiDEL, which carries enough information to generate all delta code automatically. Without additional effort, new schema versions become immediately accessible and data changes in any version are visible in all schema versions at the same time. InVerDa also allows for easily changing the physical table design without affecting the availability of co-existing schema versions. This greatly increases robustness (orders of magnitude less lines of code) and allows for significant performance optimization. A main contribution is the formal evaluation that each schema version acts like a common full-fledged database schema independently of the chosen physical table design.<br/> &copy; 2017 Copyright held by the owner/author(s).},
key = {Database systems},
keywords = {Codes (symbols);},
note = {Co-existing;Database administrators;Database schemas;Developed applications;Lines of code;Orders of magnitude;Performance optimizations;State of the art;},
URL = {http://dx.doi.org/10.1145/3035918.3064046},
} 


@article{20081911248829 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {MeDEA: A database evolution architecture with traceability},
journal = {Data and Knowledge Engineering},
author = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
volume = {65},
number = {3},
year = {2008},
pages = {419 - 441},
issn = {0169023X},
abstract = {One of the most important challenges that software engineers (designers, developers) still have to face in their everyday work is the evolution of working database systems. As a step for the solution of this problem in this paper we propose MeDEA, which stands for Metamodel-based Database Evolution Architecture. MeDEA is a generic evolution architecture that allows us to maintain the traceability between the different artifacts involved in any database development process. MeDEA is generic in the sense that it is independent of the particular modeling techniques being used. In order to achieve this, a metamodeling approach has been followed for the development of MeDEA. The other basic characteristic of the architecture is the inclusion of a specific component devoted to storing the translation of conceptual schemas to logical ones. This component, which is one of the most noteworthy contributions of our approach, enables any modification (evolution) realized on a conceptual schema to be traced to the corresponding logical schema, without having to regenerate this schema from scratch, and furthermore to be propagated to the physical and extensional levels. &copy; 2007 Elsevier B.V. All rights reserved.},
key = {Database systems},
keywords = {Data acquisition;Mathematical models;Problem solving;Software architecture;},
note = {Conceptual modeling;Database development process;Extensional levels;Logical schema;},
URL = {http://dx.doi.org/10.1016/j.datak.2007.12.001},
} 


@inproceedings{20173904213166 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Meta object management and its application to database evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Tresch, Markus and Scholl, Marc H.},
volume = {645 LNCS},
year = {1992},
pages = {299 - 321},
issn = {03029743},
address = {Karlsruhe, Germany},
abstract = {In this paper, we address the problem of supporting more flexibility on the schema of object-oriented databases. We describe a general framework based on an object-oriented data model, where three levels of objects are distinguished: data objects, schema objects, and meta-schema objects. We discuss the prerequisites for applying the query and update operations of an object algebra uniformly on all three levels. As a sample application of the framework, we focus on database evolution, that is, realizing incremental changes to the database schema and their propagation to data instances. We show, how each schema update of a given taxonomy is realized by direct updating of schema objects, and how this approach can be used to build a complete tool for database evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 1992.},
key = {Object-oriented databases},
keywords = {Algebra;Data mining;Query processing;},
note = {Data objects;Database schemas;Incremental changes;ITS applications;Meta-objects;Object oriented data;Sample applications;},
URL = {http://dx.doi.org/10.1007/3-540-56023-8_19},
} 


@article{2004057946445 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object database evolution using separation of concerns},
journal = {SIGMOD Record (ACM Special Interest Group on Management of Data)},
author = {Rashid, Awais and Sawyer, Peter},
volume = {29},
number = {4},
year = {2000},
pages = {26 - 33},
issn = {01635808},
abstract = {This paper proposes an object database evolution approach based on separation of concerns. The lack of customisability and extensibility in existing evolution frameworks is a consequence of using attributes at the meta-object level to implement links among meta-objects and the injection of instance adaptation code directly into the class versions. The proposed approach uses dynamic relationships to separate the connection code from meta-objects and aspects - abstractions used by Aspect-Oriented Programming to localise cross-cutting concerns - to separate the instance adaptation code from class versions. The result is a customisable and extensible evolution framework with low maintenance overhead.},
} 


@inproceedings{20155001657596 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transaction-based specification of database evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Baekgaard, Lars},
volume = {1331},
year = {1997},
pages = {127 - 140},
issn = {03029743},
address = {Los Angeles, CA, United states},
abstract = {We present a two-layer language for the specification of database evolution in terms of transaction-based, dynamic integrity constraints. The first language layer is based on first-order logic and it is used to express dynamic constraints in terms of queries on the transaction history of a database. The second layer uses a customizable combination of text and graphics and its semantics are defined in terms of the first order language. Our language is orthogonal to state-based constraint languages and it can be used as a supplement to these. Also, our language can be used in combination with all object-based or entity-based data models. We use examples to illustrate the use of the specification language.<br/> &copy; 1997, Springer Verlag. All rights reserved.},
key = {Query languages},
keywords = {Data mining;Formal logic;Semantics;Specification languages;Specifications;},
note = {Based specification;Constraint language;Constraint specifications;Database integrity;Dynamic constraints;Dynamic integrity;First-order language;Transaction history;},
} 


@inproceedings{20160801959482 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An architecture for managing database evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Dom I Nguez, Eladio and Lloret, Jorge and Zapata, MarIA Antonia},
volume = {2784},
year = {2003},
pages = {63 - 74},
issn = {03029743},
address = {Tampere, Finland},
abstract = {This paper presents an architecture for managing database evolution when all the components of the database (conceptual schema, logical schema and extension) are available. The strategy of evolution in which our architecture is based is that of &lsquo;forward database maintenance&rsquo;, that is, changes are applied to the conceptual schema and propagated automatically down to the logical schema and to the extension. In order to put into practice this strategy, each component of a database is seen under this architecture as the information base of an information system. Furthermore, a translation information system is considered in order to manage the translation of conceptual elements into logical schema elements. A current Oracle implementation of this architecture is also presented.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
key = {Information management},
keywords = {Architecture;Data mining;Database systems;Information systems;Information use;Web services;},
note = {Conceptual elements;Conceptual schemas;Database maintenance;Information base;Meta-modelling;},
URL = {http://dx.doi.org/10.1007/978-3-540-45275-1_6},
} 


@inproceedings{20183305684198 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards an efficient and effective framework for the evolution of scientific databases},
journal = {ACM International Conference Proceeding Series},
author = {Schuler, Robert E. and Kesselman, Carl},
year = {2018},
pages = {Alpin; EOS Solutions; Systems; Wurth Phoenix - },
address = {Bolzano-Bozen, Italy},
abstract = {Database systems are well suited to scientific data management and analysis workloads, however, a database must evolve to keep pace with changing requirements and adjust to changes in the domain conceptualization as applications mature. Evolving a database (i.e., updating its schema and instance data) is one of the greatest challenges in database maintenance and the difficulties are compounded by the lack of sufficient tools to support scientists. This paper presents a schema evolution framework based on an algebraic approach that introduces extended and higher-level composite relational operators tailored to the task of schema evolution. These higher-level operators simplify the task of evolving a database for non-expert users, while enabling efficient evaluation of schema evolution expressions.<br/> &copy; 2018 Copyright held by the owner/author(s).},
key = {Information management},
keywords = {Algebra;Database systems;Management information systems;},
note = {Algebraic approaches;Database maintenance;Non-experts;Relational algebra;Relational operator;Schema evolution;Scientific data management;Scientific database;},
URL = {http://dx.doi.org/10.1145/3221269.3221300},
} 


@inproceedings{20083811549386 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema versioning in multi-temporal XML databases},
journal = {Proceedings - 7th IEEE/ACIS International Conference on Computer and Information Science, IEEE/ACIS ICIS 2008, In conjunction with 2nd IEEE/ACIS Int. Workshop on e-Activity, IEEE/ACIS IWEA 2008},
author = {Brahmia, Zouhaier and Bouaziz, Rafik},
year = {2008},
pages = {158 - 164},
address = {Portland, OR, United states},
abstract = {Schema evolution keeps only the current data and the schema version after applying schema changes. On the contrary, schema versioning creates new schema versions and preserves old schema versions and their corresponding data. Much research work has recently focused on the problem of schema evolution in XML databases, but less attention has been devoted to schema versioning in such databases. In this paper, we present an approach for schema versioning in multi-temporal XML databases. This approach is based on the XML Schema language for describing XML schema, and is database consistency-preserving. &copy; 2008 IEEE.<br/>},
key = {Database systems},
keywords = {XML;},
note = {Database consistency;Multi-temporal;Schema evolution;Schema versioning;Temporal Database;XML database;XML schema languages;XML schemas;},
URL = {http://dx.doi.org/10.1109/ICIS.2008.89},
} 


@inproceedings{20153001064051 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ROVER: A framework for the evolution of relationships},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Claypool, Kajal T. and Rundensteiner, Elke A. and Heineman, George T.},
volume = {1920},
year = {2000},
pages = {409 - 422},
issn = {03029743},
address = {Salt Lake City, UT, United states},
abstract = {Relationships have been repeatedly identified as an important object-oriented modeling construct. Today most emerging modeling standards such as the ODMG object model and UML have some support for relationships. However while dealing with schema evolution, OODB systems have largely ignored the existence of relationships. We are the first to propose comprehensive support for relationship evolution. A complete schema evolution facility for any OODB system must provide (1) primitives to manipulate all object model constructs; (2) and also maintenance strategies for the structural and referential integrity of the database under such evolution. We hence propose a set of basic evolution primitives for relationships as well as a compound set of changes that can be applied to the same. However, given the myriad of possible change semantics a user may desire in the future, any pre-defined set is not sufficient. Rather we present a flexible schema evolution framework which allows the user to define new relationship transformations as well as to extend the existing ones. Addressing the second problem, namely of updating the schema evolution primitives to conform to the new set of invariants, can be a very expensive re-engineering effort. In this paper we present an approach that de-couples the constraints from the schema evolution code, thereby enabling their update without any re-coding effort.<br/> &copy; Springer-Verlag Berlin Heidelberg 2000.},
key = {Object-oriented databases},
keywords = {Data mining;Semantics;},
note = {Consistency management;Important object;Maintenance strategies;Object model;Referential integrity;Relationship evolutions;Relationships;Schema evolution;},
} 


@inproceedings{20103213142296 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Support for significant evolutions of the user data model in ROOT files},
journal = {Journal of Physics: Conference Series},
author = {Canal, Ph. and Brun, R. and Fine, V. and Janyst, L. and Lauret, J. and Russo, P.},
volume = {219},
number = {1 PART 3},
year = {2010},
issn = {17426588},
address = {Prague, Czech republic},
abstract = {One of the main strengths of ROOT input and output (I/O) is its inherent support for schema evolution. Two distinct modes are supported, one manual via a hand coded streamer function and one fully automatic via the ROOT StreamerInfo. One draw back of the streamer functions is that they are not usable by TTree objects in split mode. Until now, the user could not customize the automatic schema evolution mechanism and the only mechanism to go beyond the default rules was to revert to using the streamer function. In ROOT 5.22/00, we introduced a new mechanism which allows user provided extensions of the automatic schema evolution that can be used in object-wise, member-wise and split modes. This paper will describe the many possibilities ranging from the simple assignment of transient members to the complex reorganization of the user's object model. &copy; 2010 IOP Publishing Ltd.<br/>},
key = {High energy physics},
keywords = {Physics;},
note = {AND splits;Default rule;Input and outputs;New mechanisms;Object model;Schema evolution;},
URL = {http://dx.doi.org/10.1088/1742-6596/219/3/032004},
} 


@inproceedings{20150900588954 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bidirectional transformations in database evolution: A case study at scale},
journal = {CEUR Workshop Proceedings},
author = {Beine, Mathieu and Hames, Nicolas and Weber, Jens H. and Cleve, Anthony},
volume = {1133},
year = {2014},
pages = {100 - 107},
issn = {16130073},
address = {Athens, Greece},
abstract = {Bidirectional transformations (BX) play an important role in database schema/application co-evolution. In earlier work, Terwilliger introduced the theoretical concept of a Channel as a BX-based mechanism to de-couple &iexcl;&deg;virtual databases&iexcl;&plusmn; used by the application code from the actual representation of the data maintained within the DBMS. In this paper, we report on considerations and experiences implementing such Channels in practice in the context of a complex real-world application, and with generative tool support. We focus on Channels implementing Pivot/Unpivot transformations. We present di.erent alternatives for generating such Channels and discuss their performance characteristics at scale. We also present a transformational tool to generate these Channels.<br/>},
key = {Database systems},
note = {Application codes;Bidirectional transformation;Co-evolution;Database schemas;Generative tools;Performance;Performance characteristics;Virtual database;},
} 


@inproceedings{20163102662252 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Extended change identification system},
journal = {ENASE 2016 - Proceedings of the 11th International Conference on Evaluation of Novel Software Approaches to Software Engineering},
author = {Parimala, N. and Gautam, Vinay},
year = {2016},
pages = {51 - 58},
address = {Rome, Italy},
abstract = {Schema evolution leads to multiple versions of the data warehouse schema. We address the issue of whether the information required by the decision maker is present in some version of the data warehouse or not by checking all the versions for the existence or the absence of the required information. The user specifies the sought information using business terms. We build Delta Ontology which captures the ontology information in terms of mapping between business terms and schema terms. The Delta Ontology is built for the modifications to the schema as it evolves. We propose an algorithm to search for the information in the latest version of E-Metadata and the Delta Ontology. Our algorithm lists all the versions where the information is available giving precedence to finding the information in a single version over across versions. The decision maker is also informed if the information is totally missing.<br/> Copyright &copy; 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
key = {Software engineering},
keywords = {Data warehouses;Decision making;Metadata;Ontology;},
note = {Change identification;Decision makers;E metadatas;Multi-version metadatas;Schema evolution;},
} 


@inproceedings{20133716742394 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XTCE and XML database evolution and lessons from JWST, landsat, and constellation},
journal = {SpaceOps 2008 Conference},
author = {Gal-Edd, Jonathan and Kreisler, Stephen and Fatig, Curtis and Jones, Ronald},
year = {2008},
pages = {European Organisation for the Exploitation; European Space Agency (ESA); of Meteorological Satellites - },
address = {Heidelberg, Germany},
abstract = {The database organizations within three different NASA projects have advanced current practices by creating database synergy between the various spacecraft life cycle stakeholders and educating users in the benefits of the Consultative Committee for Space Data Systems (CCSDS) XML Telemetry and Command Exchange (XTCE) format.<br/>},
key = {Database systems},
keywords = {Life cycle;NASA;Telemetering;XML;},
note = {Consultative committee for space data systems;Current practices;Database organization;LANDSAT;NASA projects;Telemetry and command;XML database;},
} 


@inproceedings{20154201415762 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting flexible object database evolution with aspects},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Rashid, Awais and Leidenfrost, Nicholas},
volume = {3286},
year = {2004},
pages = {75 - 94},
issn = {03029743},
address = {Vancouver, BC, Canada},
abstract = {Object database management systems (ODBMSs) typically offer fixed approaches to evolve the schema of the database and adapt existing instances accordingly. Applications, however, have very specialised evolution requirements that can often not be met by the fixed approach offered by the ODBMS. In this paper, we discuss how aspect-oriented programming (AOP) has been employed in the AspOEv evolution framework, which supports flexible adaptation and introduction of evolution mechanisms - for dynamic evolution of the schema and adaptation of existing instances - governing an object database. We argue that aspects support flexibility in the framework by capturing crosscutting hot spots (customisation points in the framework) and establishing their causality relationships with the custom evolution approaches. Furthermore, aspects help in information hiding by screening the database programmer from the complexity of the hot spots manipulated by custom evolution mechanisms. They also make it possible to preserve architectural constraints and specify custom version polymorphism policies.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
key = {Object-oriented databases},
keywords = {Aspect oriented programming;Object oriented programming;},
note = {Architectural constraints;Aspect-Oriented Programming (AOP);Customisation;Database programmers;Dynamic evolution;Evolution mechanism;Flexible object;Information hiding;},
} 


@inproceedings{20101212784353 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An approach for schema versioning in multi-temporal XML databases},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Brahmia, Zouhaier and Bouaziz, Rafik},
volume = {DISI},
year = {2008},
pages = {290 - 297},
address = {Barcelona, Spain},
abstract = {Schema evolution keeps only the current data and the schema version after applying schema changes. On the contrary, schema versioning creates new schema versions and preserves old schema versions and their corresponding data. These two techniques have been investigated widely, both in the context of static and temporal databases. With the growing interest in XML and temporal XML data as well as the mechanisms for holding such data, the XML context within which data items are formatted also becomes an issue. Whereas much research work has recently focused on the problem of schema evolution in XML databases, less attention has been devoted to schema versioning in such databases. In this paper, we propose an approach for schema versioning in multi-temporal XML databases. This approach is based on the XML Schema language for describing XML schema, and is database consistency-preserving.<br/>},
key = {XML},
keywords = {Database systems;Information systems;Information use;},
note = {Database consistency;Multi-temporal;Schema evolution;Schema versioning;Temporal Database;XML database;XML schema languages;XML schemas;},
} 


@inproceedings{1993101069218 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards more flexible schema management in object bases},
journal = {Proceedings - International Conference on Data Engineering},
author = {Moerkotte, Guido and Zachmann, Andreas},
year = {1993},
pages = {174 - 181},
address = {Vienna, Austria},
abstract = {There exists a current trend in database technology to make databases more extensible and flexible, or even to generate databases for specific customer needs. So far, schema management and especially schema evolution have been excluded from this trend. In this paper, we propose a new approach to schema management and topics centered around it, like schema consistency and schema evolution. This approach allows easy tailoring of schema management, high-level specification of schema consistency and development of advanced tools supporting the user during schema evolution.},
key = {Database systems},
keywords = {File organization;High level languages;Object oriented programming;},
note = {Database generation;Object bases;Schema consistency;Schema evolution;Schema management;},
} 


@inproceedings{20184806149246 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {In for a surprise when migrating NoSQL data},
journal = {Proceedings - IEEE 34th International Conference on Data Engineering, ICDE 2018},
author = {Storl, Uta and Tekleab, Alexander and Klettke, Meike and Scherzinger, Stefanie},
year = {2018},
pages = {1662 - },
address = {Paris, France},
abstract = {Schema-flexible NoSQL data stores lend themselves nicely for storing versioned data, a product of schema evolution. In this lightning talk, we apply pending schema changes to records that have been persisted several schema versions back. We present first experiments with MongoDB and Cassandra, where we explore the trade-off between applying chains of pending changes stepwise (one after the other), and as composite operations. Contrary to intuition, composite migration is not necessarily faster. The culprit is the computational overhead for deriving the compositions. However, caching composition formulae achieves a speed up: For Cassandra, we can cut the runtime by nearly 80%. Surprisingly, the relative speedup seems to be system-dependent. Our take away message is that in applying pending schema changes in NoSQL data stores, we need to base our design decisions on experimental evidence rather than on intuition alone.<br/> &copy; 2018 IEEE.},
key = {Economic and social effects},
keywords = {Engineering;Industrial engineering;},
note = {Cassandras;Computational overheads;Data migration;Design decisions;Experimental evidence;Nosql database;Schema changes;Schema evolution;},
URL = {http://dx.doi.org/10.1109/ICDE.2018.00202},
} 


@inproceedings{20160801959477 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Change management for a temporal versioned object-oriented database},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {de Matos Galante, Renata and Edelweiss, Nina and dos Santos, Clesio Saraiva},
volume = {2784},
year = {2003},
pages = {1 - 12},
issn = {03029743},
address = {Tampere, Finland},
abstract = {In this paper, we propose a schema versioning mechanism to manage the schema evolution in temporal object-oriented databases. The schema evolution management uses an object-oriented data model that supports temporal features and versions definition - the Temporal Versions Model - TVM. One interesting feature of our proposal is that TVM is used to control not only the schema versioning, but also the storage of extensional database and propagation of the changes performed on the objects. The extensional data level supports integration with the existing database, allowing the maintenance of conventional and temporal versioned objects. The instance propagation approach is proposed through the specification of propagation and conversion functions. These functions assure the correct instance propagation and allow the user to handle all instances consistently in both backward and forward schema versions. Finally, the initial requirements concerning data management in the temporal versioning environment, during schema evolution, are presented.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
key = {Information management},
keywords = {Data mining;Digital storage;Object oriented programming;Object-oriented databases;Relational database systems;Web services;},
note = {Change management;Conversion function;Object oriented data;Schema evolution;Schema versioning;Temporal features;Temporal object-oriented database;Temporal versioning;},
URL = {http://dx.doi.org/10.1007/978-3-540-45275-1_1},
} 


@inproceedings{1993041562732 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Formalizing database evolution in the situation calculus},
author = {Reiter, Raymond},
year = {1992},
pages = {600 - 600},
address = {Tokyo, Jpn},
} 


@inproceedings{20101712872327 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolving schemas for streaming XML},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Shoaran, Maryam and Thomo, Alex},
volume = {5956 LNCS},
year = {2010},
pages = {266 - 285},
issn = {03029743},
address = {Sofia, Bulgaria},
abstract = {In this paper we model schema evolution for XML by defining formal language operators on Visibly Pushdown Languages (VPLs). Our goal is to provide a framework for efficient validation of streaming XML in the realistic setting where the schemas of the exchanging parties evolve and thus diverge from one another. We show that Visibly Pushdown Languages are closed under the defined language operators and this enables us to expand the schemas (for XML) in order to account for flexible or constrained evolution. &copy; 2010 Springer Berlin Heidelberg.<br/>},
key = {XML},
keywords = {Formal languages;},
note = {Evolution;Schema evolution;Streaming data;Streaming XML;Visibly pushdown languages;XML schemas;},
URL = {http://dx.doi.org/10.1007/978-3-642-11829-6_18},
} 


@inproceedings{20173504082373 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database evolution: The DB-MAIN approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Hainaut, J.-L. and Englebert, V. and Henrard, J. and Hick, J.-M. and Roland, D.},
volume = {881 LNCS},
year = {1994},
pages = {112 - 131},
issn = {03029743},
address = {Manchester, United kingdom},
abstract = {The paper analyses some of the practical problems that arise when the requirements of an information system evolve, and when the database and its application programs are to be modified accordingly. It presents four important strategies to cope with this evolution, namely forward maintenance, backward maintenance, reverse engineering and anticipating design. A common, generic, framework that can support these strategies is described. It is based on a generic data structure model, on a transformational approach to database engineering, and on a design process model. The paper discusses how this framework allows formalizing these evolution strategies, and describes a generic CASE tool that supports database applications maintenance.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Professional aspects},
keywords = {Application programs;Computer aided software engineering;Data mining;Database systems;Maintenance;Product design;Reengineering;Reverse engineering;Systems engineering;},
note = {Database applications;Database engineering;Design Process Model;Evolution strategies;Generic data structures;Practical problems;Process Modeling;Transformational approach;},
} 


@article{20113014172440 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolving schemas for streaming XML},
journal = {Theoretical Computer Science},
author = {Shoaran, M. and Thomo, A.},
volume = {412},
number = {35},
year = {2011},
pages = {4545 - 4557},
issn = {03043975},
abstract = {In this paper we model schema evolution for XML by defining formal language operators on Visibly Pushdown Languages (VPLs). Our goal is to provide a framework for efficient validation of streaming XML in the realistic setting where the schemas of the exchanging parties evolve and thus diverge from one another. We show that Visibly Pushdown Languages are closed under the defined language operators and this enables us to expand the schemas (for XML) in order to account for flexible or constrained evolution. &copy; 2011 Elsevier B.V. All rights reserved.},
key = {XML},
keywords = {Formal languages;},
note = {Evolution;Schema evolution;Schemas;Streaming data;Streaming XML;Visibly pushdown languages;XML schemas;},
URL = {http://dx.doi.org/10.1016/j.tcs.2011.04.037},
} 


@inproceedings{20160801978588 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting database evolution: Using ontologies matching},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Lammari, Nadira and Akoka, Jacky and Comyn-Wattiau, Isabelle},
volume = {2817},
year = {2003},
pages = {284 - 288},
issn = {03029743},
address = {Geneve, Switzerland},
abstract = {In this paper, we propose a methodology for managing database evolutions by exploiting two ontologies. The first ontology describes the changes occurring in a database application. The second one aims at characterizing techniques and tools useful for database change management. We propose an algorithm performing ontologies matching and its application to identify appropriate techniques and tools for a given database change.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
key = {Database systems},
keywords = {Ontology;},
note = {Appropriate techniques;Change management;Database applications;ITS applications;Techniques and tools;},
URL = {http://dx.doi.org/10.1007/978-3-540-45242-3_27},
} 


@inproceedings{1997053659224 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object-oriented database evolution},
journal = {Lecture Notes in Computer Science},
author = {Lagorce, J.-B. and Stockus, A. and Waller, E.},
volume = {1186},
year = {1997},
pages = {379 - 379},
issn = {03029743},
address = {Delphi, Greece},
} 


@inproceedings{2002287013724 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A feature-based database evolution approach in the design process},
journal = {Robotics and Computer-Integrated Manufacturing},
author = {Da Cunha, Raimundo R.M. and Dias, A.},
volume = {18},
number = {3-4},
year = {2002},
pages = {275 - 281},
issn = {07365845},
address = {Dublin, Ireland},
abstract = {Design data are assigned in geometric and non-geometric form in order to meet design requirements. These data and information must be encapsulated in a data structure that has significance for design applications in each design process phase. The main goal of this research is to find design data groups that represent each mechanical design phase, which will be called phase's design signature. In addition, current data should be an evolution of the geometric and non-geometric information of the previous design phase. In this paper, the purpose is to identify and model a set of design features that encapsulate the design data and their transformations which occurred during the mechanical design phases. This database must capture the designer's intents that can be modeled and implemented using feature-based model in the conventional CAD systems, object-oriented modeling, and Java classes. &copy; 2002 Elsevier Science Ltd. All rights reserved.},
key = {Database systems},
keywords = {Computer aided design;Computer integrated manufacturing;Data structures;Object oriented programming;Product design;},
note = {Object-oriented modeling;},
URL = {http://dx.doi.org/10.1016/S0736-5845(02)00018-2},
} 


@inproceedings{20155001652902 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Object-oriented database evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Lagorce, Jean-Bernard and Stokus, Arnas and Waller, Emmanuel},
volume = {1186},
year = {1997},
pages = {379 - 393},
issn = {03029743},
address = {Delphi, Greece},
abstract = {An evolution languageis composed of an instance update language, a schema update language, and a mechanism to combine them. We present a formal evolution language for object-oriented database management systems. This language allows to write programs to update simultaneously both the schema and the instance. Static checking of these programs insures that the resulting database is consistent. We propose an autonomous instance update language, based on an adequate specific query language and a pure instance update language. The main features of the query language are a formal type inference system including disjunctive types, and the decidability of the satisfiability problem, despite a negation operator. The pure instance update language allows objects migration, and objects and references creation and deletion; its semantics is declarative, and an algorithm to compute it is presented. We propose an evolution mechanism for combining this instance update language with a classical schema update language, and use it to obtain an evolution language. Decidability of consistency is shown for a fragment of this language, by reduction to first-order logic with two variables.<br/> &copy; Springer-Verlag Berlin Heidelberg 1997.},
key = {Object-oriented databases},
keywords = {Computability and decidability;Query languages;Query processing;Semantics;},
note = {Evolution mechanism;First order logic;Satisfiability problems;Static checking;Type inferences;Update languages;},
} 


@article{2000104980457 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ontology of object-oriented database evolution},
journal = {Computer Systems Science and Engineering},
author = {Hines, M.L.},
volume = {14},
number = {6},
year = {1999},
pages = {371 - 377},
issn = {02676192},
abstract = {The need for expanded data representation and semantic capture has resulted in the emergence of object-oriented databases. Representation of relationships beyond inheritance relationships, provision of separate development and working environments, and control of evolution for developing database systems are areas not yet adequately addressed in current object-oriented database research. This work defines: a specification hierarchy which provides expanded relationship representation and a separate, yet connected, development environment for database systems; and an ontological system which constrains the migration of classes and relationships from the development environment to the working environment.},
key = {Database systems},
keywords = {Computational linguistics;Data structures;Object oriented programming;Specifications;},
note = {Object oriented databases;Semantic capture;Specification hierarchy;},
} 


@inbook{1998024084674 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transaction-based specification of database evolution},
journal = {Lecture Notes in Computer Science},
author = {Baekgaard, L.},
volume = {1331},
year = {1997},
pages = {127 - 127},
issn = {03029743},
address = {Los Angeles, CA, United states},
} 


@article{20180804819761 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An integration-oriented ontology to govern evolution in Big Data ecosystems},
journal = {Information Systems},
author = {Nadal, Sergi and Romero, Oscar and Abello, Alberto and Vassiliadis, Panos and Vansummeren, Stijn},
volume = {79},
year = {2019},
pages = {3 - 19},
issn = {03064379},
abstract = {Big Data architectures allow to flexibly store and process heterogeneous data, from multiple sources, in their original format. The structure of those data, commonly supplied by means of REST APIs, is continuously evolving. Thus data analysts need to adapt their analytical processes after each API release. This gets more challenging when performing an integrated or historical analysis. To cope with such complexity, in this paper, we present the Big Data Integration ontology, the core construct to govern the data integration process under schema evolution by systematically annotating it with information regarding the schema of the sources. We present a query rewriting algorithm that, using the annotated ontology, converts queries posed over the ontology to queries over the sources. To cope with syntactic evolution in the sources, we present an algorithm that semi-automatically adapts the ontology upon new releases. This guarantees ontology-mediated queries to correctly retrieve data from the most recent schema version as well as correctness in historical queries. A functional and performance evaluation on real-world APIs is performed to validate our approach.<br/> &copy; 2018 Elsevier Ltd},
key = {Big data},
keywords = {Data integration;Ontology;Semantic Web;},
note = {Analytical process;Evolution;Heterogeneous data;Historical analysis;Historical queries;Integration process;Performance evaluations;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.is.2018.01.006},
} 


@inproceedings{20155001665211 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Facilitating virtual representation of CAD data through a learning based approach to conceptual database evolution employing direct instance sharing},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Rashid, Awais and Sawyer, Peter},
volume = {1460},
year = {1998},
pages = {384 - 393},
issn = {03029743},
address = {Vienna, Austria},
abstract = {This paper presents a framework for a learning based approach to dynamically evolve the conceptual structure of a database in order to facilitate virtual representation of data in a CAD environment. A generic object model is presented which spans applications from a wide range of engineering design domains. The object model is complemented with a schema model. The object model and the schema model are justified through several sample cases depicting the mapping from the object model to the schema model.<br/> &copy; Springer-Verlag Berlin Heidelberg 1998.},
key = {Computer aided design},
keywords = {Database systems;E-learning;Expert systems;},
note = {CAD data;Conceptual structures;Engineering design;Learning-based approach;Object model;Virtual representations;},
} 


@inbook{1998124504007 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Facilitating virtual representation of CAD data through a learning based approach to conceptual database evolution employing direct instance sharing},
journal = {Lecture Notes in Computer Science},
author = {Rashid, A. and Sawyer, P.},
volume = {1460},
year = {1998},
pages = {384 - 384},
issn = {03029743},
} 


@inproceedings{20112814136260 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {RDFS Update: From theory to practice},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Gutierrez, Claudio and Hurtado, Carlos and Vaisman, Alejandro},
volume = {6643 LNCS},
number = {PART 2},
year = {2011},
pages = {93 - 107},
issn = {03029743},
abstract = {There is a comprehensive body of theory studying updates and schema evolution of knowledge bases, ontologies, and in particular of RDFS. In this paper we turn these ideas into practice by presenting a feasible and practical procedure for updating RDFS. Along the lines of ontology evolution, we treat schema and instance updates separately, showing that RDFS instance updates are not only feasible, but also deterministic. For RDFS schema update, known to be intractable in the general abstract case, we show that it becomes feasible in real world datasets. We present for both, instance and schema update, simple and feasible algorithms. &copy; 2011 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Ontology},
keywords = {Semantic Web;},
note = {Feasible algorithms;Knowledge basis;Ontology evolution;Practical procedures;Real-world datasets;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-21064-8_7��},
} 


@article{20124415630470 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tracking changes in database schemas},
journal = {Advances in Intelligent Systems and Computing},
author = {Marciniak, Jakub and Pankowski, Tadeusz},
volume = {183 AISC},
year = {2013},
pages = {155 - 165},
issn = {21945357},
abstract = {We discuss the problem of discovering changes in evolving XML schemas. Schema evolution is a natural, unavoidable phenomenon in contemporary data systems, that impacts both data transformation and query rewriting. We propose a rule-based algorithm that determines matched and unmatched schema elements thereby identifying changes in a schema under consideration. Additionally, we develop a method for computing edit distance in terms of some schema operations (insertion, deletion, renaming, and translocation). In result, we are able to obtain a set of operations which transform a given schema into the modified (target) form. The proposed algorithms have been fully implemented.<br/>},
key = {Search engines},
keywords = {Computation theory;Metadata;Query processing;},
note = {Data systems;Data transformation;Database schemas;Edit distance;Query rewritings;Rule based algorithms;Schema evolution;XML schemas;},
URL = {http://dx.doi.org/10.1007/978-3-642-32335-5-15},
} 


@article{20140117160949 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Online, Asynchronous schema change in F1},
journal = {Proceedings of the VLDB Endowment},
author = {Rae, Ian and Rollins, Eric and Shute, Jeff and Sodhi, Sukhdeep and Vingralek, Radek},
volume = {6},
number = {11},
year = {2013},
pages = {1045 - 1056},
issn = {21508097},
abstract = {We introduce a protocol for schema evolution in a globally distributed database management system with shared data, stateless servers, and no global membership. Our protocol is asynchronous-it allows different servers in the database system to transition to a new schema at different times-and online-all servers can access and update all data during a schema change. We provide a formal model for determining the correctness of schema changes under these conditions, and we demonstrate that many common schema changes can cause anomalies and database corruption. We avoid these problems by replacing corruption-causing schema changes with a sequence of schema changes that is guaranteed to avoid corrupting the database so long as all servers are no more than one schema version behind at any time. Finally, we discuss a practical implementation of our protocol in F1, the database management system that stores data for Google AdWords. &copy; 2013 VLDB Endowment.<br/>},
key = {Distributed database systems},
keywords = {Crime;Information management;Management information systems;},
note = {Distributed database;Formal model;Schema changes;Schema evolution;Shared data;},
URL = {http://dx.doi.org/10.14778/2536222.2536230},
} 


@article{20161702305400 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling dynamic relationship types for subsets of entity type instances and across entity types},
journal = {Information Systems},
author = {Krishna, P. Radha and Khandekar, Anushree and Karlapalem, Kamalakar},
volume = {60},
year = {2016},
pages = {114 - 126},
issn = {03064379},
abstract = {In a traditional ER model, once we specify a subclass or superclass relationship, any changes to that relationship are treated as schema evolution. Further, ER models are rigid in the sense that once a relationship type is specified across a set of entity types, an instance of relationship type occur when one instance of all participating entity types are specified. Therefore, it is difficult to introduce in a simplified manner all relationship types across subsets of given set of entity types. In this paper, we provide mechanisms to model in our extended ER model: (i) specification of dynamic relationship types across subsets of instances of entity types, (ii) a simplified specification of relationships across subsets of given set of entity types, and (iii) mapping our extended ER model to relational database schema. We also show through an e-contract example the utility of our extended ER model.<br/> &copy; 2016 Elsevier Ltd. All rights reserved.},
key = {Set theory},
keywords = {Specifications;},
note = {Entity-types;ER-models;Meta model;Model dynamics;Relational database schemata;Relational Model;Schema evolution;Show through;},
URL = {http://dx.doi.org/10.1016/j.is.2016.03.010},
} 


@inproceedings{20110313598932 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CIS: Change identification system},
journal = {KEOD 2010 - Proceedings of the International Conference on Knowledge Engineering and Ontology Development},
author = {Parimala, N. and Gautam, Vinay},
year = {2010},
pages = {347 - 350},
abstract = {Change identification is one of the main challenges for Data Warehouse Schema evolution. Changes to the schema are required, among other situations, when the data warehouse fails to provide information to the decision maker. In this paper we address the issue of identification of changes when such a situation occurs. Towards this, the decision maker is asked to specify the information he/she needs, in business terms, to meet a goal. With the help of ontology and a set of rules we identify whether the information is present in the warehouse or not. The absence of data could be because it is not directly stored or because it is actually absent. In both these cases the changes needed to the data warehouse schema are suggested by the system, called the Change Identification System (CIS).<br/>},
key = {Decision making},
keywords = {Data warehouses;Ontology;},
note = {Change identification;Decision makers;Goal-Information;Multidimensional schemata;Schema evolution;Set of rules;},
} 


@inproceedings{20182305275509 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Uncovering the evolution history of data lakes},
journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
author = {Klettke, Meike and Awolin, Hannes and Storl, Uta and Muller, Daniel and Scherzinger, Stefanie},
volume = {2018-January},
year = {2018},
pages = {2462 - 2471},
address = {Boston, MA, United states},
abstract = {Data accumulating in data lakes can become inaccessible in the long run when its semantics are not available. The heterogeneity of data formats and the sheer volumes of data collections prohibit cleaning and unifying the data manually. Thus, tools for automated data lake analysis are of great interest. In this paper, we target the particular problem of reconstructing the schema evolution history from data lakes. Knowing how the data is structured, and how this structure has evolved over time, enables programmatic access to the lake. By deriving a sequence of schema versions, rather than a single schema, we take into account structural changes over time. Moreover, we address the challenge of detecting inclusion dependencies. This is a prerequisite for mapping between succeeding schema versions, and in particular, detecting nontrivial changes such as a property having been moved or copied. We evaluate our approach for detecting inclusion dependencies using the MovieLens dataset, as well an adaption of a dataset containing botanical descriptions, to cover specific edge cases.<br/> &copy; 2017 IEEE.},
key = {Big data},
keywords = {Lakes;Semantics;},
note = {Automated data;Data collection;Evolution history;Evolution operation;Inclusion dependencies;Integrity constraints;Nosql database;Schema evolution;},
URL = {http://dx.doi.org/10.1109/BigData.2017.8258204},
} 


@inproceedings{20105013479371 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automated co-evolution of conceptual models, physical databases, and mappings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Terwilliger, James F. and Bernstein, Philip A. and Unnithan, Adi},
volume = {6412 LNCS},
year = {2010},
pages = {146 - 159},
issn = {03029743},
abstract = {Schema evolution is an unavoidable consequence of the application development lifecycle. The two primary schemas in an application, the conceptual model and the persistent database model, must co-evolve or risk quality, stability, and maintainability issues. We study application-driven scenarios, where the conceptual model changes and the database and mapping must evolve in kind. We present a technique that, in most cases, allows those evolutions to progress automatically. We treat the mapping as data, and mine that data for patterns. Then, given an incremental change to the conceptual model, we can derive the proper store and mapping changes without user intervention. We characterize the significant subset of mappings for which automatic evolution is possible, and present our techniques for evolution propagation. &copy; 2010 Springer-Verlag.<br/>},
key = {Mapping},
keywords = {Data mining;Database systems;},
note = {Application development;Automatic evolution;Conceptual model;Database modeling;Incremental changes;Physical database;Schema evolution;User intervention;},
URL = {http://dx.doi.org/10.1007/978-3-642-16373-9_11},
} 


@inproceedings{20173204020685 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Storage structure for handling schema versions in temporal data warehouses},
journal = {Advances in Intelligent Systems and Computing},
author = {Gosain, Anjana and Saroha, Kriti},
volume = {518},
year = {2018},
pages = {501 - 511},
issn = {21945357},
address = {Rourkela, India},
abstract = {The temporal data warehouse model, an extension of multidimensional model of data warehouses, has already been proposed in the literature to handle the updates of dimension data correctly. In order to maintain a complete history of data in temporal data warehouses (TDW), it is required to manage schema evolution with time, thereby maintaining the complete history of evolution of schema with schema versions as well as evolution of data defined under different versions of schema. This paper proposes an approach for managing schema versions in temporal data warehouses to allow for an effectual management of different versions of schema and their data.<br/> &copy; Springer Nature Singapore Pte Ltd. 2018.},
key = {Data warehouses},
keywords = {Computation theory;Digital storage;Intelligent computing;},
note = {Dimension Data;Multi-dimensional model;Schema evolution;Schema versioning;Storage structures;Transaction time;Valid time;},
URL = {http://dx.doi.org/10.1007/978-981-10-3373-5_50},
} 


@inproceedings{20102913089937 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Worry-free database upgrades: Automated model-driven evolution of schemas and complex mappings},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
author = {Terwilliger, James F. and Bernstein, Philip A. and Unnithan, Adi},
year = {2010},
pages = {1191 - 1194},
issn = {07308078},
address = {Indianapolis, IN, United states},
abstract = {Schema evolution is an unavoidable consequence of the application development lifecycle. The two primary schemas in an application, the client conceptual object model and the persistent database model, must co-evolve or risk quality, stability, and maintainability issues. We present MoDEF, an extension to Visual Studio that supports automatic evolution of object-relational mapping artifacts in the Microsoft Entity Framework. When starting with a valid mapping between client and store, MoDEF translates changes made to a client model into incremental changes to the store as an upgrade script, along with a new valid mapping to the new store. MoDEF mines the existing mapping for mapping patterns which MoDEF reuses for new client artifacts. &copy; 2010 ACM.<br/>},
key = {Mapping},
keywords = {Database systems;},
note = {Application development;Automated modeling;Automatic evolution;Incremental changes;Model management;O-r mappings;Object-relational mapping;Schema evolution;},
URL = {http://dx.doi.org/10.1145/1807167.1807316},
} 


@inproceedings{20100312642911 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A dialogue-based model for the query synchronization problem},
journal = {Proceedings - 2009 IEEE 5th International Conference on Intelligent Computer Communication and Processing, ICCP 2009},
author = {Polese, Giuseppe and Vacca, Mario},
year = {2009},
pages = {67 - 70},
address = {Cluj-Napoca, Romania},
abstract = {The synchronization of queries is one of the schema evolution problems and it calls for the redefinition of those queries becoming undefined after a schema change, in order to keep them still working on the new schema. This problem is particularly difficult for changes that upset the schema, because it could not be possible to rewrite the queries exactly. In this paper we show that the dialogue can play an important role in these cases and we propose a model of dialogue for query synchronization based on the Hintikka interrogative logic. &copy; 2009 IEEE.<br/>},
key = {Computer circuits},
keywords = {Synchronization;},
note = {Dialogue;Interrogative logic;Schema changes;Schema evolution;Synchronization problem;},
URL = {http://dx.doi.org/10.1109/ICCP.2009.5284780},
} 


@inproceedings{20154201398473 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Oodbms metamodel supporting configuration management of large applications},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Habela, Piotr and Subieta, Kazimierz},
volume = {2425},
year = {2002},
pages = {40 - 52},
issn = {03029743},
address = {Montpellier, France},
abstract = {Many practical cases of database schema evolution requirean effective support from configuration management. Although DBMS construction and software configuration management (SCM) constitute the well established areas of research, they are usually considered in separation from each other. In this paper different issues of SCM are summarized and their relevance to DBMS is investigated. We suggestto extend the OODBMS metamodel to allow recording certain aspectsof application-database dependencies in a database schema repository.The extended metamodel contains both typical database meta model information as well as software configuration information. Such asolution we consider necessary for solving some of schema evolution problems.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002},
key = {Object-oriented databases},
keywords = {Application programs;Information systems;Information use;Object oriented programming;},
note = {Configuration management;Database dependencies;Database schemas;Meta model;Schema evolution;Software configuration;Software configuration management;},
} 


@inproceedings{20171103439766 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A scheme of dynamic attributes addition for tuple datasets},
journal = {ACM International Conference Proceeding Series},
author = {Chiba, Yohsuke and Kitajima, Shogo and Tsuji, Tatsuo and Higuchi, Ken},
year = {2016},
pages = {288 - 292},
address = {Singapore, Singapore},
abstract = {Nowadays in response to the expansion and diversification of business operations, demands for dynamic change of dataset definitions (schema evolution) such as new attribute additions is increasing. However, in order to handle the tuples after the attribute additions efficiently, reorganization of the dataset is necessitated and high operation cost would be accompanied. History-pattern encoding scheme for dynamic multidimensional datasets is based on the notion of multidimensional extendible array. It can provide fast retrieval capability of the datasets while suppressing storage cost minimally. In this paper, using the history-pattern encoding scheme, we will presents the schemes of dynamic attribute addition for dynamically extending tuple datasets, and evaluate them.<br/> &copy; 2016 ACM.},
key = {Digital storage},
keywords = {Encoding (symbols);Information retrieval;Signal encoding;Web services;Websites;},
note = {Business operation;Data encoding;Dynamic attributes;Dynamic changes;Encoding schemes;Fast retrievals;Multi-dimensional datasets;Schema evolution;},
URL = {http://dx.doi.org/10.1145/3011141.3011164},
} 


@inproceedings{20155001656857 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The COAST project*: Design and implementation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Lautemann, Sven-Eric and Eigner, Patricia and Wohrle, Christian},
volume = {1341},
year = {1997},
pages = {229 - 246},
issn = {03029743},
address = {Montreux, Switzerland},
abstract = {The Complez Object and Schema Transformation (COAST) project focuses on the design and implementation of a schema evolution support system using the versiouing concept both at the instance and at the schema level of object-oriented database management systems (OODBMS). The versioning approach to schema evolution offers a high degree of flexibility. In ordinary systems two elements depend heavily on the database schema, and therefore prevent schema changes in many cases: The database which is stored according to the types specified in the schema and the set of application programs on top of the schema which require certain properties from the instances. The basic idea of the versioning mechanism is to keep the old schema and database state as a version to allow continuous operation of existing application programs. This decouples the schema from existing instances and applications, and allows arbitrary schema updates at any time. The main contributions of this paper include the development and integration of a module called Propagation Managerinto a general OODBMS architecture and the explanation of its data structures and algorithms to allow deferred physical instance propagation.<br/> &copy; Springer-Verlag Berlin Heidelberg 1997.},
key = {Object-oriented databases},
keywords = {Application programs;Object oriented programming;Relational database systems;},
note = {Continuous operation;Database schemas;Degree of flexibility;Design and implementations;Schema changes;Schema evolution;Schema transformation;Support systems;},
} 


@inproceedings{20094912532802 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {PRIMA: Archiving and querying historical data with evolving schemas},
journal = {SIGMOD-PODS'09 - Proceedings of the International Conference on Management of Data and 28th Symposium on Principles of Database Systems},
author = {Moon, Hyun J. and Curinoy, Carlo A. and Ham, Myungwon and Zaniolo, Carlo},
year = {2009},
pages = {1019 - 1021},
address = {Providence, RI, United states},
abstract = {Schema evolution poses serious challenges in historical data management. Traditionally, historical data have been archived either by (i) migrating them into the current schema version that is well-understood by users but compromising archival quality, or (ii) by maintaining them under the original schema version in which the data was originally created, leading to perfect archival quality, but forcing users to formulate queries against complex histories of evolving schemas. In the PRIMA system, we achieve the best of both approaches, by (i) archiving historical data under the schema version under which they were originally created, and (ii) letting users express temporal queries using the current schema version. Thus, in PRIMA, the system rewrites the queries to the (potentially many) pertinent versions of the evolving schema. Moreover, the system offers automatic documentation of the schema history, and allows the users to pose temporal queries over the metadata history itself. The proposed demonstration highlights the system features exploiting both a synthetic-educational running example and the real-life evolution histories (schemas and data), which include hundreds of schema versions from Wikipedia and Ensembl. The demonstration offers a thorough walk-through of the system features and a hands-on system testing phase, where the audiences are invited to directly interact with the advanced query interface of PRIMA.<br/>},
key = {Information management},
keywords = {Data privacy;Network security;Query processing;},
note = {Automatic documentation;Historical data;Life evolution;Query interfaces;Schema evolution;System features;System testing;Temporal queries;},
URL = {http://dx.doi.org/10.1145/1559845.1559970},
} 


@inproceedings{1996323215688 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards supporting hard schema changes in TSE},
journal = {International Conference on Information and Knowledge Management, Proceedings},
author = {Ra, Young-Gook and Rundensteiner, Elke A.},
year = {1995},
pages = {290 - 295},
address = {Baltimore, MD, USA},
abstract = {Simulating schema evolution using views offers many advantages over the direct modification of the schema. However, this view approach towards transparent schema evolution has not yet been implemented. One reason for this may be that views can't simulate capacity-augmenting schema changes due to the inherent limitation of view mechanisms that do not augment the underlying schema. One potential solution for this problem is to develop capacity-augmenting view support. However, existing OO view mechanisms do currently not support this capacity-augmenting feature. This paper thus proposes an alternative, more practical, solution to this problem. This solution is neither confined to object-preserving schema changes nor requires capacity-augmenting views. Note that our proposed solution is the first to also support hard changes such as converting values into an object, splitting two classes vertically, etc., in a transparent fashion. We demonstrate the feasibility of our solution by presenting general algorithms for hard schema changes as well as primitive ones.},
key = {Database systems},
keywords = {Algorithms;Computational linguistics;Computer simulation;Computer software;Distributed database systems;Object oriented programming;User interfaces;},
note = {Capacity augmented views;Semantics;Transparent schema evolution;},
} 


@inproceedings{20140817348984 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Putting context into schema matching},
journal = {VLDB 2006 - Proceedings of the 32nd International Conference on Very Large Data Bases},
author = {Bohannon, Philip and Elnahrawy, Eiman and Fan, Wenfei and Flaster, Michael},
year = {2006},
pages = {307 - 318},
address = {Seoul, Korea, Republic of},
abstract = {Attribute-level schema matching has proven to be an important first step in developing mappings for data exchange, integration, restructuring and schema evolution. In this paper we investigate contextual schema matching, in which selection conditions are associated with matches by the schema matching process in order to improve overall match quality. We define a general space of matching techniques, and within this framework we identify a variety of novel, concrete algorithms for contextual schema matching. Furthermore, we show how common schema mapping techniques can be generalized to take more effective advantage of contextual matches, enabling automatic construction of mappings across certain forms of schema heterogeneity. An experimental study examines a wide variety of quality and performance issues. In addition, it demonstrates that contextual schema matching is an effective and practical technique to further automate the definition of complex data transformations. Copyright 2006 VLDB Endowment, ACM.<br/>},
key = {Mapping},
keywords = {Electronic data interchange;},
note = {Attribute levels;Automatic construction;Complex data;Matching techniques;Performance issues;Schema evolution;Schema mappings;Schema matching;},
} 


@inproceedings{20173404067959 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Integrating evolving MDM and EDW systems by data vault based system catalog},
journal = {2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2017 - Proceedings},
author = {Jaksic, D. and Jovanovic, V. and Poscic, P.},
year = {2017},
pages = {1401 - 1406},
address = {Opatija, Croatia},
abstract = {The paper presents results of a research on integration of enterprise data warehouse (EDW) and a master data management (MDM) system. The primary goal was solving a schema evolution problem, and the corner stone of our approach was utilization of a data vault modeling of an integrated meta-model of EDW and MDM as an expansion of a traditional relational database system catalog. The main contributions of this paper are: a) common integration architecture, b) new system catalog based on a meta-model for DW and MDM integration, and c) research prototype used for empirical validation of the effectiveness of the proposed solution.<br/> &copy; 2017 Croatian Society MIPRO.},
key = {Information management},
keywords = {Data integration;Data warehouses;Integration;Microelectronics;Relational database systems;},
note = {Data vault;Empirical validation;Enterprise data warehouse;Integration architecture;Master data management;Research prototype;Schema evolution;System catalogs;},
URL = {http://dx.doi.org/10.23919/MIPRO.2017.7973641},
} 


@article{1996510382404 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Characterization of the effects of schema change},
journal = {Information Sciences},
author = {Ewald, C.A. and Orlowska, M.E.},
volume = {94},
number = {1-4},
year = {1996},
pages = {23 - 39},
issn = {00200255},
abstract = {Schema evolution occurs frequently in practice. In this paper, we show that the entire schema does not need to be redesigned every time a change is made. We characterize the subschema affected by basic schema operations, which allows us to limit the scope of checking required and have a guarantee of the overall design being correct (safe changes). In addition, we characterize the effects on the actual relations of the addition and removal of constraints. In some cases, relations may have to be added to or removed from the database in order to preserve correctness.},
key = {Database systems},
keywords = {Data handling;Information retrieval systems;},
note = {Schema evolution;},
} 


@inproceedings{20161102105450 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Maintenance of queries under database changes: A unified logic based approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Ravve, Elena V.},
volume = {9616},
year = {2016},
pages = {191 - 208},
issn = {03029743},
address = {Linz, Austria},
abstract = {This contribution deals with one single theme, the exploitation of logical reduction techniques in database theory. Two kinds of changes may be applied to databases: structural changes, known also as restructuring or schema evolution, and data changes. We present both of them in the terms of syntactically defined translation schemes. At the same time, we have application programs, computing different queries on the database, which are oriented on some specific generation of the database. Systematically using the technique of translation scheme, we introduce the notion of &Phi;-sums and show how queries, expressible in extensions of First Order Logic (FOL) may be handled over different generations of the &Phi;-sums. Moreover, using the technique of translation scheme, we introduce the notions of an incremental view recomputations. We prove when queries expressible in extensions of FOL allow incremental view recomputations. Our approach covers uniformly the cases we have encountered in the literature and can be applied to all existing query languages.<br/> &copy; Springer International Publishing Switzerland 2016.},
key = {Query languages},
keywords = {Application programs;Computer circuits;Formal logic;Query processing;},
note = {Data base theory;Data change;Extensions of first-order logic;Logic-based approach;Reduction techniques;Schema evolution;Translation schemes;},
URL = {http://dx.doi.org/10.1007/978-3-319-30024-5_11},
} 


@inbook{20190906577131 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting structural evolution of data in web-based systems via schema versioning in the txschema framework},
journal = {Handbook of Research on Contemporary Perspectives on Web-Based Systems},
author = {Brahmia, Zouhaier and Grandi, Fabio and Oliboni, Barbara and Bouaziz, Rafik},
year = {2018},
pages = {271 - 307},
abstract = {&tau;XSchema is a framework for creating and validating temporal XML documents, while using a temporal schema that consists of three components: a conventional XML schema document annotated with a set of temporal logical and physical annotations. Each one of these components can evolve over time to reflect changes in the real world. In addition, schema versioning has been long advocated to be the most efficient way to keep track of both data and schema evolution. Hence, in this chapter the authors complete &tau;XSchema, which is predisposed from the origin to support schema versioning, by defining the operations that are necessary to exploit such a feature and make schema versioning functionalities available to end users. Precisely, the authors' approach provides a complete and sound set of change primitives and a set of high-level change operations, for the maintenance of each component of a &tau;XSchema schema, and defines their operational semantics. Furthermore, they propose a new technique for schema versioning in &tau;XSchema, allowing a complete, integrated, and safe management of schema changes.<br/> &copy; 2018, IGI Global.},
key = {Human computer interaction},
keywords = {Semantics;XML;},
note = {Operational semantics;Physical annotations;Safe managements;Schema evolution;Schema versioning;Structural evolution;Three component;Web-based system;},
URL = {http://dx.doi.org/10.4018/978-1-5225-5384-7.ch013},
} 


@article{20124115549640 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relational support for flexible schema scenarios},
journal = {Proceedings of the VLDB Endowment},
author = {Acharya, Srini and Carlin, Peter and GalindoLegaria, Cesar and Kozielczyk, Krzysztof and Terlecki, Pawel and Zabback, Peter},
volume = {1},
number = {2},
year = {2008},
pages = {1289 - 1300},
issn = {21508097},
abstract = {Efficient support for applications that deal with data heterogeneity, hierarchies and schema evolution is an important challenge for relational engines. In this paper we show how this flexibility can be handled in Microsoft SQL Server. For this purpose, the engine has been equipped in an integrated package of relational extensions. The package includes sparse storage, column set operations, filtered indices, filtered statistics and hierarchy querying with OrdPath labeling. In addition, economical loading of metadata allow us to answer queries independently of the number of columns in a table and drastically improve scaling capabilities. The design of a prototypical content and collaboration application based on a wide table is described, along with experiments validating its performance. &copy; 2008 VLDB Endowment.<br/>},
key = {Digital storage},
keywords = {Engines;Query processing;Windows operating system;},
note = {Collaboration applications;Data heterogeneity;Integrated packages;Microsoft SQL Server;Scaling capability;Schema evolution;Set operation;Wide table;},
URL = {http://dx.doi.org/10.14778/1454159.1454169},
} 


@inproceedings{20164803056934 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automata-based static analysis of XML document adaptations},
journal = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
author = {Solimando, Alessandro and Delzanno, Giorgio and Guerrini, Giovanna},
volume = {96},
year = {2012},
pages = {85 - 98},
issn = {20752180},
address = {Napoli, Italy},
abstract = {The structure of an XML document can be optionally specified by means of XML Schema, thus enabling the exploitation of structural information for efficient document handling. Upon schema evolution, or when exchanging documents among different collections exploiting related but not identical schemas, the need may arise of adapting a document, known to be valid for a given schema S, to a target schema S&prime;. The adaptation may require knowledge of the element semantics and cannot always be automatically derived. In this paper, we present an automata-based method for the static analysis of user-defined XML document adaptations, expressed as sequences of XQuery Update update primitives. The key feature of the method is the use of an automatic inference method for extracting the type, expressed as a Hedge Automaton, of a sequence of document updates. The type is computed starting from the original schema S and from rewriting rules that formally define the operational semantics of a sequence of document updates. Type inclusion can then be used as conformance test w.r.t. the type extracted from the target schema S&prime;.<br/> &copy; 2012, Open Publishing Association. All rights reserved.},
key = {Formal verification},
keywords = {Automata theory;Semantics;Static analysis;XML;},
note = {Automatic inference;Document handling;Key feature;Operational semantics;Rewriting rules;Schema evolution;Structural information;XML schemas;},
URL = {http://dx.doi.org/10.4204/EPTCS.96.7},
} 


@inproceedings{20164002877322 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A case study of the aggregation query model in read-mostly NoSQL document stores},
journal = {ACM International Conference Proceeding Series},
author = {Pasqualin, Diego and Souza, Giovanni and Buratti, Eduardo Luis and De Almeida, Eduardo Cunha and Del Fabro, Marcos Didonet and Weingaertner, Daniel},
volume = {11-13-July-2016},
year = {2016},
pages = {224 - 229},
address = {Montreal, QC, Canada},
abstract = {In this paper we focus on the aggregate query model implemented over NoSQL document-stores for read-mostly data bases. We discuss that the aggregate query model can be a good fit for read-mostly databases if the following design requirements are met: on-line time range queries, aggregates with predefined filters, frequent schema evolution and no ad-hoc. In our model, we present a composite object schema implementation over NoSQL document-stores, in which data associations are nested in a document under the same search key. We present the design choices to obtain a model adapted to our needs. Our schema is inspired by the star schema of Data Warehouses to reduce accessing data associations in many different documents and computing aggregates within the same composite. We present performance results of our empirical study over a 300 million records database that serves in production for the Ministry of Communications of Brazil. Results show the performance gains and penalties of our star composite schema when compared to the traditional multidimensional schema.<br/> &copy; ACM 2016.},
key = {Query processing},
keywords = {Data warehouses;Stars;},
note = {Aggregate queries;Aggregation queries;Composite objects;Data association;Empirical studies;Multidimensional schemata;Performance Gain;Schema evolution;},
URL = {http://dx.doi.org/10.1145/2938503.2938546},
} 


@inproceedings{20184406006011 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dependency-Based Query/View Synchronization upon Schema Evolutions},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Caruccio, Loredana and Polese, Giuseppe and Tortora, Genoveffa},
volume = {11158 LNCS},
year = {2018},
pages = {91 - 105},
issn = {03029743},
address = {Xi'an, China},
abstract = {Query/view synchronization upon the evolution of a database schema is a critical problem that has drawn the attention of many researchers in the database community. It entails rewriting queries and views to make them continue work on the new schema version. Although several techniques have been proposed for this problem, many issues need yet to be tackled for evolutions concerning the deletion of schema constructs, hence yielding loss of information. In this paper, we propose a new methodology to rewrite queries and views whose definitions are based on information that have been lost during the schema evolution process. The methodology exploits (relaxed) functional dependencies to automatically rewrite queries and views trying to preserve their semantics.<br/> &copy; 2018, Springer Nature Switzerland AG.},
key = {Query processing},
keywords = {Data mining;Semantics;},
note = {Critical problems;Database community;Database schemas;Functional dependency;Query rewritings;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-030-01391-2_17},
} 


@inproceedings{20124315590205 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Circle-based improvement strategy of simulated annealing genetic algorithm},
journal = {Communications in Computer and Information Science},
author = {Han, Bing and Jiang, Junna and Wang, Xinchun},
volume = {308 CCIS},
number = {PART 2},
year = {2012},
pages = {502 - 507},
issn = {18650929},
address = {Chengde, China},
abstract = {Circular regulation is an important law of bionomics. Simulated annealing genetic algorithm is an effective method of improving genetic algorithm. Combining circular strategy with simulated annealing genetic algorithm efficiently, a novel simulated annealing genetic algorithm applying circular strategy is proposed. And it is justified according to schema evolution analysis and convergence analysis. It can not only assure the capability of global convergence, but also accelerate the evolution of colony and acquire the satisfactory global optimal solution. &copy; 2012 Springer-Verlag.<br/>},
key = {Simulated annealing},
keywords = {Genetic algorithms;},
note = {Circular strategies;Convergence analysis;Global conver-gence;Global optimal solutions;Golden section;Improvement strategies;Schema evolution;Simulated annealing-genetic algorithms;},
URL = {http://dx.doi.org/10.1007/978-3-642-34041-3_70},
} 


@article{1991120292847 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Prolog-based object oriented engineering DBMS},
journal = {Computers and Structures},
author = {Watson, A.S. and Chan, S.H.},
volume = {40},
number = {1},
year = {1991},
pages = {11 - 21},
issn = {00457949},
address = {London, Engl},
abstract = {In this paper we present the primary concepts of PBASE, a prototype object oriented database system. PBASE is intended to support the needs of engineering applications with specific reference to structural engineering. To address the engineering requirements the object oriented data model used in PBASE incorporates several enhancements, including Schema Evolution, Composite Objects, Declarative Methods and Version Management. Schema evolution allows dynamic changes to the class definitions and the class lattice. Composite objects support the is-part-of relationship between assemblies and components. Declarative methods introduce semantics into objects while version management supports the tracking of objects' versions and alternatives as they evolve during the design process.},
key = {Engineering},
keywords = {Composite Structures--Assembly;Data Processing;Management;Structural Design;},
note = {Composite Objects;Declarative Methods;Prototype Database System;Schema Evolution;Version Management;},
URL = {http://dx.doi.org/10.1016/0045-7949(91)90451-Q},
} 


@article{20130515969406 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Challenges of knowledge evolution in practice},
journal = {AI Communications},
author = {Falkner, Andreas and Haselbock, Alois},
volume = {26},
number = {1},
year = {2013},
pages = {3 - 14},
issn = {09217126},
abstract = {As knowledge changes over time, its representation in knowledge-based systems along with the existing instances (e.g., configured products, schedules, plans, documents, web sites, etc.) must be changed, too. This paper enumerates some of the most important challenges which arise in practice when changing a knowledge base: redesign of the knowledge base, schema evolution of the data bases, upgrade of configuration instances, adaptation of solver, UI, I/O and test suites. Partially, there are research theories for some of these challenges, but only few of them are already available in tools and frameworks. We cannot provide solutions here, but we want to stimulate research in knowledge evolution with a representative set of industrial challenges. Product configuration is a prominent case where the use of knowledge-based AI technologies has been well established over the last years. We use a self-contained real-world example from the field of configuration to describe the challenges. &copy; 2013 - IOS Press and the authors. All rights reserved.<br/>},
key = {Knowledge based systems},
keywords = {Industrial research;},
note = {AI Technologies;configuration;Industrial challenges;Knowledge based;Knowledge change;Knowledge evolution;Product configuration;Schema evolution;},
URL = {http://dx.doi.org/10.3233/AIC-120542},
} 


@inproceedings{20140617284160 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Notes on view synchronization using default logic (extended abstract)},
journal = {17th Italian Symposium on Advanced Database Systems, SEBD 2009},
author = {Polese, Giuseppe and Vacca, Mario},
year = {2009},
pages = {253 - 260},
address = {Camogli, Genova, Italy},
abstract = {The synchronization of views is one of the schema evolution problems and it calls for the redefinition of those views becoming undefined after a schema change, in order to keep them still working on the new schema. This problem is particularly difficult for capacity reducing schema changes, when it could be only possible to approximate the existing views. Recently, the use of schema mappings to express schema changes has allowed both to deal with a wide range of schema change operations and to facilitate the view synchronization; but approximating views requires mappings able to describe approximate schema changes. This paper introduces the default schema mappings, a new kind of mappings based on default logic and it provides a preliminary study showing the possibility of using them to realize an approximate view synchronization process.<br/>},
key = {Synchronization},
keywords = {Computer circuits;Database systems;Mapping;Query processing;},
note = {Default logic;Default query;Extended abstracts;Schema changes;Schema evolution;Schema mappings;View synchronizations;},
} 


@inproceedings{20123515370593 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Search-based evolution of xml schemas},
journal = {Computing and Informatics},
author = {Silva, Julio Cesar Teodoro and Pozo, Aurora Trinidad Ramirez and Vergilio, Silvia Regina and Musicante, Martin A.},
volume = {31},
number = {3},
year = {2012},
pages = {573 - 595},
issn = {13359150},
abstract = {The use of sche&acute;mas makes an XML-based application more reliable, since they contribute to avoid failures by defining the specific format for the data that the application manipulates. In practice, when an application evolves, new requirements for the data may be established, raising the need of schema evolution. In some cases the generation of a schema is necessary, if such schema does not exist. To reduce maintenance and reengineering costs, automatic evolution of sche&acute;mas is very desirable. However, there are no algorithms to satisfactorily solve the problem. To help in this task, this paper introduces a search-based approach that explores the correspondence between sche&acute;mas and context-free grammars. The approach is supported by a tool, named EXS. Our tool implements algorithms of grammatical inference based on LL(I) Parsing. If a grammar (that corresponds to a schema) is given and a new word (XML document) is provided, the EXS system infers the new grammar that: i) continues to generate the same words as before and ii) generates the new word, by modifying the original grammar. If no initial grammar is available, EXS is also capable of generating a grammar from scratch from a set of samples.<br/>},
key = {XML},
keywords = {Context free grammars;Inference engines;Reengineering;},
note = {Automatic evolution;Grammatical inferences;LL parsing;Re-engineering costs;Schema evolution;Search-based;XML based application;XML schemas;},
} 


@article{1995382807492 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Survey of schema versioning issues for database systems},
journal = {Information and Software Technology},
author = {Roddick, John F.},
volume = {37},
number = {7},
year = {1995},
pages = {383 - 393},
issn = {09505849},
abstract = {Schema versioning is one of a number of related areas dealing with the same general problem - that of using multiple heterogeneous schemata for various database related tasks. In particular, schema versioning, and its weaker companion, schema evolution, deal with the need to retain current data and software system functionality in the face of changing database structure. Schema versioning and schema evolution offer a solution to the problem by enabling intelligent handling of any temporal mismatch between data and data structure. This survey discusses the modelling, architectural and query language issues relating to the support of evolving schemata in database systems. An indication of the future directions of schema versioning research is also given.},
key = {Database systems},
keywords = {Automation;Computer aided design;Data structures;Object oriented programming;Query languages;Software engineering;},
note = {Multiple heterogeneous schemata;Schema evolution;Schema versioning;},
URL = {http://dx.doi.org/10.1016/0950-5849(95)91494-K},
} 


@inproceedings{20103413179396 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {EXup: An engine for the evolution of XML schemas and associated documents},
journal = {ACM International Conference Proceeding Series},
author = {Cavalieri, Federico},
year = {2010},
address = {Lausanne, Switzerland},
abstract = {XML Schema is employed for describing the type and structure of information contained in XML documents. Schema evolution means that a schema is modified and the effects of the modification on instances are faced. XSUpdate is a language that allows to easily identify parts of an XML Schema, apply a modification primitive on them and define an adaptation for associated documents. Purpose of this paper is to present the engine we developed for the evaluation of XSUp-date statements against XML Schemas and associated documents. The presented engine relies on the translation of XSUpdate statements in XQuery Update expressions. &copy; 2010 ACM.<br/>},
key = {XML},
keywords = {Database systems;Engines;},
note = {Schema evolution;XML schemas;},
URL = {http://dx.doi.org/10.1145/1754239.1754263},
} 


@inproceedings{1990060426238 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Prolog based object oriented engineering DBMS},
journal = {Artificial Intelligence Techniques and Applications for Civil and Structural Engineers},
author = {Watson, A.S. and Chan, S.H.},
year = {1989},
pages = {37 - 48},
address = {Edinburgh, Scotl},
abstract = {In this paper the authors present the primary concepts of PBASE, a prototype Object Oriented Database System. PBASE is intended to support the needs of engineering applications with specific reference to structural engineering. To address the engineering requirements, the object oriented data model used in PBASE incorporates several enhancements including Schema Evolution, Composite Objects, Declarative Methods and Version Management. Schema evolution allows dynamic changes to the class definitions and the class lattice. Composite objects support the is-part-of relationship between assemblies and components. Declarative methods introduce semantics into objects while version management supports the tracking of objects' versions and alternatives as they evolve during the design process.},
key = {Database Systems},
keywords = {Computer Aided Design;Structural Design;Systems Science and Cybernetics--Hierarchical Systems;},
note = {Composite Objects;Declarative Methods;Object Oriented Database System;Schema Evolution;},
} 


@inproceedings{20140917369391 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bidirectional by necessity: Data persistence and adaptability for evolving application development},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Terwilliger, James F.},
volume = {7680 LNCS},
year = {2013},
pages = {219 - 270},
issn = {03029743},
address = {Braga, Portugal},
abstract = {Database-backed applications are ubiquitous. They have common requirements for data access, including a bidirectional requirement that the application and database must have schemas and instances that are synchronized with respect to the mapping between them. That synchronization must hold under both data updates (when an application is used) and schema evolution (when an application is versioned). The application developer treats the collection of structures and constraints on application data - collectively called a virtual database - as indistinguishable from a persistent database. To have such indistinguishability, that virtual database must be mapped to a persistent database by some means. Most application developers resort to constructing such a mapping from custombuilt middleware because available solutions are unable to embody all of the necessary capabilities. This paper returns to first principles of database application development and virtual databases. It introduces a tool called a channel, comprised of incremental atomic transformations with known and provable bidirectional properties, that supports the implementation of virtual databases. It uses channels to illustrate how to provide a singular mapping solution that meets all of the outlined requirements for an example application. &copy; Springer-Verlag Berlin Heidelberg 2013.<br/>},
key = {Database systems},
keywords = {Mapping;Middleware;},
note = {Application data;Application developers;Application development;Atomic transformation;Data persistence;Database applications;Indistinguishability;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-35992-7_6},
} 


@article{20091612036832 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Efficient revalidation of XML documents},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Raghavachari, Mukund and Shmueli, Oded},
volume = {19},
number = {4},
year = {2007},
pages = {554 - 567},
issn = {10414347},
abstract = {We study the problem of schema revalidation where XML data known to conform to one schema must be validated with respect to another schema. Such revalidation algorithms have applications in schema evolution, query processing, XML-based programming languages, and other domains. We describe how knowledge of conformance to an XML Schema may be used to determine conformance to another XML Schema efficiently. We examine both the situation where an XML document is modified before it is revalidated and the situation where it is unmodified &copy; 2007 IEEE.<br/>},
key = {XML},
keywords = {Computational methods;Information systems;},
note = {Revalidation;Schema evolution;Subtypings;Updates;Validation;XML data;XML schemas;XML-based programming language;},
URL = {http://dx.doi.org/10.1109/TKDE.2007.1004},
} 


@inproceedings{20154201401183 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Model Based Materialized View Evolution: A Review},
journal = {Procedia Computer Science},
author = {Gosain, Anjana and Sabharwal, Sangeeta and Gupta, Rolly},
volume = {57},
year = {2015},
pages = {1273 - 1280},
issn = {18770509},
address = {Delhi, India},
abstract = {Materialized views evolve in order to meet the user's requirement in the dynamically changing data warehouse environment. Therefore, materialized view evolution approach focuses on choosing materialized views in the design process of data warehouses or maintaining a materialized view in response to data changes or to data sources changes and sometimes to monitor the DW quality under schema evolution. Although few researchers have addressed materialized view evolution problem for evolving an appropriate set of views. But, none of the surveys provides a classification of materialized view evolution approaches in order to identify their advantages and disadvantages. This survey tries to fill this gap. The present paper provides a review of model based materialized view evolution methods by identifying the three main dimensions namely; (i) Framework, (ii) Architecture and (iii) Model/Design Model, that are the basis in the classification of materialized view evolution methods. The goal of this paper is to provide a comparative study on model based materialized view evolution methods, by identifying respective potentials and limits.<br/> &copy; 2015 The Authors. Published by Elsevier B.V.},
key = {Data warehouses},
keywords = {Models;Surveys;},
note = {Comparative studies;Data-sources;Design process;Evolution problem;Materialized view;Model-based OPC;Schema evolution;View Maintenanc;},
URL = {http://dx.doi.org/10.1016/j.procs.2015.07.432},
} 


@inproceedings{1995292724587 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transparent object-oriented schema change approach using view evolution},
journal = {Proceedings - International Conference on Data Engineering},
author = {Ra, Young-Gook and Rundensteiner, Elke A.},
year = {1995},
pages = {165 - 172},
address = {Taipei, Taiwan},
abstract = {When a database is shared by many users, updates to the database schema are almost always prohibited because there is a risk of making existing application programs obsolete when they run against the modified schema. This paper addresses the problem by integrating schema evolution with view facilities. When new requirements necessitate schema updates for a particular user, the user specifies schema changes to the personal view rather than to the shared base schema. Our view evolution approach then computes a new view schema that reflects the semantics of the desired schema change, and replaces the old view with the new one. We present algorithms that implement the set of schema evolution operations typically supported by OODB systems as view definitions. This approach provides the means for schema change without affecting other views (and thus without affecting existing application programs). The persistent data is shared by different views of the schema, i.e., both old as well as newly developed applications can continue to interoperate. In this paper, we present examples that demonstrate our approach.},
key = {Database systems},
keywords = {Algorithms;Computational linguistics;Computer simulation;Object oriented programming;Query languages;Specifications;},
note = {Global schema;Object oriented schema;Schema evolution;View evolution approach;},
} 


@inbook{20162602548524 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The iBench integration metadata generator},
journal = {Proceedings of the VLDB Endowment},
author = {Arocena, Patricia C. and Glavic, Boris and Ciucanu, Radu and Miller, Renee J.},
volume = {9},
number = {3},
year = {2016},
pages = {108 - 119},
issn = {21508097},
address = {Delhi, India},
abstract = {Given the maturity of the data integration field it is surprising that rigorous empirical evaluations of research ideas are so scarce. We identify a major roadblock for empirical work - the lack of comprehensive metadata generators that can be used to create benchmarks for different integration tasks. This makes it difficult to compare integration solutions, understand their generality, and understand their performance. We present iBench, the first metadata generator that can be used to evaluate a wide-range of integration tasks (data exchange, mapping creation, mapping composition, schema evolution, among many others). iBench permits control over the size and characteristics of the metadata it generates (schemas, constraints, and mappings). Our evaluation demonstrates that iBench can efficiently generate very large, complex, yet realistic scenarios with different characteristics. We also present an evaluation of three mapping creation systems using iBench and show that the intricate control that iBench provides over metadata scenarios can reveal new and important empirical insights. iBench is an open-source, extensible tool that we are providing to the community. We believe it will raise the bar for empirical evaluation and comparison of data integration systems. &copy; 2015 VLDB Endowment 21508097/15/11.<br/>},
key = {Data integration},
keywords = {Electronic data interchange;Mapping;Metadata;},
note = {Data integration system;Empirical evaluations;Open sources;Raise the bars;Realistic scenario;Schema evolution;},
} 


@inproceedings{20161702289789 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Facilitating workflow evolution in an advanced object environment},
journal = {Proceedings - 7th International Conference on Database Systems for Advanced Applications, DASFAA 2001},
author = {Chiu, Dickson K.W. and Li, Qing and Karlapalem, Kamalakar},
year = {2001},
pages = {148 - 149},
address = {Hong Kong, China},
abstract = {Workflow is automation of a business process. A Workflow Management Systems (WFMS) is a system that assists in defining, managing and executing workflows. To support flexible enactment and adapive features, such as on-line workflow evolutionand exception handling, a WFMS requires advanced modeling functionality. As workflow evolution requires the modification of workflow definitions or adding ECA rules to the system during work in progress, an advanced schema evolution capability is required at run-time. It should be noted that the resolutions based on schema evolution are general-purpose ones, which can help reduce the occurrence of additional exceptions. As such, we have developed ADOME-WFMS based on Advanced Object Modeling Evnironment (ADMOE [4] - - an active OODBMS with role and synamic schema suppport), with a novel exception centric apporach. The contribution and objectives of our research with respect to workflow evolution are as follows.<br/> &copy; 2001 IEEE.},
key = {Computer aided manufacturing},
keywords = {Object-oriented databases;Work simplification;},
note = {Advanced modeling;Business Process;Exception handling;Object model;Schema evolution;Work in progress;Work-flows;Workflow management systems;},
URL = {http://dx.doi.org/10.1109/DASFAA.2001.6044750},
} 


@inproceedings{1993010618953 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An extensible object-oriented database testbed},
journal = {Proceedings - International Conference on Data Engineering},
author = {Morsi, Magdi M. and Navathe, Shamkant B. and Kim, Hyoung-Joo},
year = {1992},
pages = {150 - 157},
address = {Tempe, AZ, USA},
abstract = {The authors describe the object-oriented design and implementation of an extensible schema manager for object-oriented databases. The open class hierarchy approach has been adopted to achieve the extensibility of the implementation. In this approach, the system meta information is implemented as objects of system classes. A graphical interface for an object-oriented database scheme environment, GOOSE, has been developed. GOOSE supports several advanced features which include schema evolution, schema versioning, and DAG (direct acyclic graph) rearrangement view of a class hierarchy. Schema evolution is the ability to make a variety of changes to a database scheme without reorganization. Schema versioning is the ability to define multiple scheme versions and to keep track of schema changes. A novel type of view for object-oriented databases, the DAG rearrangement view of a class hierarchy, is also supported.},
key = {Database systems},
keywords = {Computer graphics;Graph theory;Interfaces (computer);},
note = {Class hierarchies;DAGS;Direct acyclic graphs;Extensible schema manager;GOOSE OODB schema environment;Object oriented databases;Schema evolution;Schema versioning;},
} 


@inproceedings{20172603848438 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Extraction of embedded queries via static analysis of host code},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Manousis, Petros and Zarras, Apostolos and Vassiliadis, Panos and Papastefanatos, George},
volume = {10253 LNCS},
year = {2017},
pages = {511 - 526},
issn = {03029743},
address = {Essen, Germany},
abstract = {Correctly identifying the embedded queries within the source code of an information system is a significant aid to developers and administrators, as it can facilitate the visualization of a map of the information system, the identification of areas affected by schema evolution, code migration, and the planning of the joint maintenance of code and data. In this paper, we provide a solution to the problem of identifying the location and semantics of embedded queries with a generic, language-independent method that identifies the embedded queries of a data-intensive ecosystem, regardless of the programming style and the host language, and represents them in a universal, also language-independent manner that facilitates the aforementioned maintenance, evolution and migration tasks with minimal user effort and significant effectiveness.<br/> &copy; Springer International Publishing AG 2017.},
key = {Static analysis},
keywords = {Codes (symbols);Data visualization;Extraction;Information systems;Information use;Query languages;Query processing;Reverse engineering;Semantics;Systems engineering;},
note = {Code migration;Data intensive;Database queries;Embedded queries;Language independents;Programming styles;Query extractions;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-319-59536-8_32},
} 


@inproceedings{20154801624292 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Implementing NRDR using OO database management system (OODBMS)},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Beydoun, Ghassan and Al-Jadir, Lina},
volume = {2417},
year = {2002},
pages = {602 - 603},
issn = {03029743},
address = {Tokyo, Japan},
abstract = {Motives for using a database management system (DBMS) to build a knowledge base system (KBS), include KBS&rsquo;s lack of ability to manage large sets of rules, to control concurrent access and to manage multiple knowledge bases simultaneously. In work, we build a KBS using a database management system DBMS for its schema evolution ability. We use this ability of an Object Oriented DBMS (OODBMS) to manage the consistency of an incrementally built knowledge base (KB). The significance of this work is two folds: first, it provides an efficient mechanism maintaining consistency of an evolving classification hierarchy, using built-in schema evolution features of an OODBMS. Second, it enhances the interface of an OODBMS, to allow intelligent classification queries over stored objects.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Object-oriented databases},
keywords = {Access control;Artificial intelligence;Concurrency control;Knowledge based systems;Management information systems;},
note = {Classification hierarchies;Concurrent access;Intelligent classification;Knowledge base;Knowledge base system;Knowledge basis;Object oriented;Schema evolution;},
} 


@inproceedings{20170603321104 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Model management and schema mappings: Theory and practice},
journal = {33rd International Conference on Very Large Data Bases, VLDB 2007 - Conference Proceedings},
author = {Bernstein, Philip A. and Ho, Howard},
year = {2007},
pages = {1439 - 1440},
address = {Vienna, Austria},
abstract = {We present an overview of a tutorial on model management - an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.<br/> Copyright 2007 VLDB Endowment, ACM.},
key = {Data integration},
keywords = {Data warehouses;Information management;Inverse problems;Mapping;},
note = {Enterprise information integration;Generic implementation;Integration problems;Model management;Relational mapping;Schema evolution;Schema mappings;Theory and practice;},
} 


@article{20100612701390 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Pivoted table index for querying product- property-value information},
journal = {Journal of Software},
author = {Lee, Hyunja and Shim, Junho},
volume = {5},
number = {2},
year = {2010},
pages = {160 - 167},
issn = {1796217X},
abstract = {The query for triple information on product- attribute (property)-value is one of the most frequent queries in e-commerce. In storing the triple (product-attribute- value) information, a vertical schema is effective for avoiding sparse data and schema evolution, while a conventional horizontal schema often shows better query performance, since the properties are queried as groups clustered by each product. Therefore, we propose two storage schemas: a vertical schema as a primary table structure for the triple information in RDBMS and a pivoted table index created from the basic vertical table as an additional index structure for accelerating query processing. The pivoted table index is beneficial to improving the performance of the frequent pattern query on the group properties associated with each product class. &copy; 2010 Academy Publisher.<br/>},
key = {Electronic commerce},
keywords = {Digital storage;Ontology;},
note = {Frequent pattern queries;Index;Pivoted table;Product attributes;Query performance;RDBMS;Schema evolution;Vertical schema;},
URL = {http://dx.doi.org/10.4304/jsw.5.2.160-167},
} 


@inproceedings{2003097373381 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Visualizing impacts of database schema changes - A controlled experiment},
journal = {2001 IEEE Symposium on Human-Centric Computing},
author = {Karahasanovic, Amela and Sjoberg, Dag I.K.},
year = {2001},
pages = {358 - 365},
address = {Stresa, Italy},
abstract = {Research in schema evolution has been driven by the need for more effective software development and maintenance. Finding impacts of schema changes on the applications and presenting them in an appropriate way are particularly challenging. We have developed a tool that finds impacts of schema changes on applications in object-oriented systems. This tool displays components (packages, classes, interfaces, methods and fields) of a database application system as a graph. Components potentially affected by a change are indicated by changing the shape of the boxes representing those components. Two versions of the tool are available. One version identifies affected parts of applications at the granularity of packages, classes, and interfaces, whereas the other version identifies affected parts at the finer granularity of fields and methods. This paper presents the design and results of a controlled student experiment testing these two granularity levels with respect to productivity and user satisfaction. There are indications that identifying impacts at the finer granularity can reduce the time needed to conduct schema changes and reduce the number of errors. Our results also show that the subjects of the experiment appreciated the idea of visualizing the impacts of schema changes.},
key = {Database systems},
keywords = {Interfaces (computer);Java programming language;Software engineering;Visualization;},
note = {Schema evolution management tool (SEMT);},
URL = {http://dx.doi.org/10.1109/HCC.2001.995292},
} 


@article{20121915003107 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Institute for the management of information systems athena research center},
journal = {SIGMOD Record},
volume = {41},
number = {1},
year = {2012},
pages = {61 - 66},
issn = {01635808},
abstract = {The Institute for the Management of Information Systems (IMIS) was established in 2007 as part of the Athena Research Center. IMIS was directed by Timos Sellis, a Professor at the School of Electrical and Computer Engineering in the National Technical University of Athens. Its goal was to conduct research in the area of data management and large-scale information systems. It was concerned with the management of schema evolution in data-centric ecosystems. These systems comprising of a large number of applications and data stores, were highly vulnerable to schema changes. IMIS adopted a lifecycle approach to the representation of curated information objects to address the challenge of digital curation research, as these evolved in interaction with changing designated communities.<br/>},
key = {Information management},
keywords = {Information systems;Information use;},
note = {Digital curation;Electrical and computer engineering;Information object;Large-scale information systems;Management of information systems;Research center;Schema evolution;Technical universities;},
URL = {http://dx.doi.org/10.1145/2206869.2206881},
} 


@inproceedings{20174104251547 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The design of an integrity consistency checker (ICC) for an object oriented database system},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Delcourt, Christine and Zicari, Roberto},
volume = {512 LNCS},
year = {1991},
pages = {97 - 117},
issn = {03029743},
address = {Geneva, Switzerland},
abstract = {Schema evolution is an important facility in object-oriented databases. However, updates should not result in inconsistencies either in the schema or in the database. We show a tool called ICC, which ensures the structural consistency when updating an object-oriented database system.<br/> &copy; Springer-Verlag Berlin Heidelberg 1991.},
key = {Object oriented programming},
keywords = {Object-oriented databases;Relational database systems;},
note = {Schema evolution;},
URL = {http://dx.doi.org/10.1007/BFb0057017},
} 


@inproceedings{20160801978018 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transposed storage of an object database to reduce the cost of schema changes},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Al-Jadir, Lina and Leonard, Michel},
volume = {1727},
year = {1999},
pages = {48 - 61},
issn = {03029743},
address = {Paris, France},
abstract = {Modifying the schema of a populated database is an expensive operation. We propose to use the non-classical transposed storage of an object database. The transposed storage avoids database reorganization and reduces the number of input/output operations in the context of schema evolution. Thus schema changes are not anymore costly operations. Consequendy immediate and physical propagation of schema changes can be supported. We extend the 007 benchmark with schema evolution operations and submit our F2 DBMS to this benchmark. The obtained resuhs demonstrate the feasibility and performance of our approach.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Information management},
keywords = {Data mining;Digital storage;Information systems;Information use;Object-oriented databases;Reverse engineering;World Wide Web;},
note = {Input/output operations;Schema changes;Schema evolution;},
URL = {http://dx.doi.org/10.1007/3-540-48054-4_5},
} 


@inproceedings{20171803630351 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The dark side of event sourcing: Managing data conversion},
journal = {SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
author = {Overeem, Michiel and Spoor, Marten and Jansen, Slinger},
year = {2017},
pages = {193 - 204},
address = {Klagenfurt, Austria},
abstract = {Evolving software systems includes data schema changes, and because of those schema changes data has to be converted. Converting data between two different schemas while continuing the operation of the system is a challenge when that system is expected to be available always. Data conversion in event sourced systems introduces new challenges, because of the relative novelty of the event sourcing architectural pattern, because of the lack of standardized tools for data conversion, and because of the large amount of data that is stored in typical event stores. This paper addresses the challenge of schema evolution and the resulting data conversion for event sourced systems. First of all a set of event store upgrade operations is proposed that can be used to convert data between two versions of a data schema. Second, a set of techniques and strategies that execute the data conversion while continuing the operation of the system is discussed. The final contribution is an event store upgrade framework that identifies which techniques and strategies can be combined to execute the event store upgrade operations while continuing operation of the system. Two utilizations of the framework are given, the first being as decision support in upfront design of an upgrade system for event sourced systems. The framework can also be utilized as the description of an automated upgrade system that can be used for continuous deployment. The event store upgrade framework is evaluated in interviews with three renowned experts in the domain and has been found to be a comprehensive overview that can be utilized in the design and implementation of an upgrade system. The automated upgrade system has been implemented partially and applied in experiments.<br/> &copy; 2017 IEEE.},
key = {Data handling},
keywords = {Decision support systems;Metadata;Reengineering;},
note = {CQRS;Data conversion;Data transformation;Deployment strategy;Event Sourcing;Event-driven architectures;Schema evolution;Schema versioning;Software Evolution;},
URL = {http://dx.doi.org/10.1109/SANER.2017.7884621},
} 


@article{20094512424290 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML: Some papers in a haystack},
journal = {SIGMOD Record},
author = {Moro, Mirella M. and Braganholo, Vanessa and Dorneles, Carina F. and Duarte, Denio and Galante, Renata and Mello, Ronaldo S.},
volume = {38},
number = {2},
year = {2009},
pages = {29 - 34},
issn = {01635808},
abstract = {XML has been explored by both research and industry communities. More than 5500 papers were published on different aspects of XML. With so many publications, it is hard for someone to decide where to start. Hence, this paper presents some of the research topics on XML, namely: XML on relational databases, query processing, views, data matching, and schema evolution. It then summarizes some (some!) of the most relevant or traditional papers on those subjects.<br/>},
key = {XML},
keywords = {Industrial research;},
note = {Data matching;Relational Database;Research topics;Schema evolution;},
URL = {http://dx.doi.org/10.1145/1815918.1815924},
} 


@inproceedings{20173404061626 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Efficient storage structures for temporal object-oriented databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Li, Chih-Kuang and Wang, Shiwei},
volume = {856 LNCS},
year = {1994},
pages = {246 - 258},
issn = {03029743},
address = {Athens, Greece},
abstract = {Automatically managing the evolutionary histories of both schema and data is an attractive feature for temporal object-oriented database management systems (OODBMS). It can make the temporal OODBMSs more suitable for many new applications such as CAD, CAM, GIS, etc. However, this feature complicates the implementation of temporal OODBMSs. In this paper, we examined the problems raised by handling the schema evolution and the related issues about the storage structure in temporal OODBMSs. Then, we proposed a storage structure and its accessing algorithms for temporal OODBMSs which can handle both schema evolution and data history simultaneously. We also simulated several possible implementations of the storage structure for temporal data maintenance and compared their performances extensively. This work can be used as the basis of implementations of temporal OODBMSs.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Object-oriented databases},
keywords = {Associative storage;Expert systems;Information management;Object oriented programming;Relational database systems;},
note = {Evolutionary history;New applications;Schema evolution;Storage structures;Temporal Data;Temporal Database;Temporal object-oriented database;Time index;},
} 


@inproceedings{20084911770140 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relaxed compliance notions in adaptive process management systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Rinderle-Ma, Stefanie and Reichert, Manfred and Weber, Barbara},
volume = {5231 LNCS},
year = {2008},
pages = {232 - 247},
issn = {03029743},
address = {Barcelona, Spain},
abstract = {The capability to dynamically evolve process models over time and to migrate process instances to a modified model version are fundamental requirements for any process-aware information system. This has been recognized for a long time and different approaches for process schema evolution have emerged. Basically, the challenge is to correctly and efficiently migrate running instances to a modified process model. In addition, no process instance should be needlessly excluded from being migrated. While there has been significant research on correctness notions, existing approaches are still too restrictive regarding the set of migratable instances. This paper discusses fundamental requirements emerging in this context. We revisit the well-established compliance criterion for reasoning about the correct applicability of dynamic process changes, relax this criterion in different respects, and discuss the impact these relaxations have in practice. Furthermore, we investigate how to cope with non-compliant process instances to further increase the number of migratable ones. Respective considerations are fundamental for further maturation of adaptive process management technology. &copy; 2008 Springer Berlin Heidelberg.<br/>},
key = {Data mining},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Adaptive process;Adaptive process management systems;Compliance criterion;Dynamic process;Process instances;Process Modeling;Process-aware information systems;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-540-87877-3-18},
} 


@inproceedings{20122115034953 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {User profile integration made easy - Model-driven extraction and transformation of social network schemas},
journal = {WWW'12 - Proceedings of the 21st Annual Conference on World Wide Web Companion},
author = {Kapsammer, Elisabeth and Kusel, Angelika and Mitsch, Stefan and Proll, Birgit and Retschitzegger, Werner and Schwinger, Wieland and Schonbock, Johannes and Wimmer, Manuel and Wischenbart, Martin and Lechner, Stephan},
year = {2012},
pages = {939 - 948},
address = {Lyon, France},
abstract = {User profile integration from multiple social networks is indispensable for gaining a comprehensive view on users. Although current social networks provide access to user profile data via dedicated apis, they fail to provide accurate schema information, which aggravates the integration of user profiles, and not least the adaptation of applications in the face of schema evolution. To alleviate these problems, this paper presents, firstly, a semi-automatic approach to extract schema information from instance data. Secondly, transformations of the derived schemas to different technical spaces are utilized, thereby allowing, amongst other benefits, the application of established integration tools and methods. Finally, as a case study, schemas are derived for Facebook, Google+, and LinkedIn. The resulting schemas are analyzed (i) for completeness and correctness according to the documentation, and (ii) for semantic overlaps and heterogeneities amongst each other, building the basis for future user profile integration. Copyright is held by the International World Wide Web Conference Committee (IW3C2).<br/>},
key = {Data integration},
keywords = {Extraction;Metadata;Semantics;Social networking (online);},
note = {Integration tools;JSON Schema;Model driven approach;Model transformation;Schema evolution;Schema information;Semi-automatics;User profile;},
URL = {http://dx.doi.org/10.1145/2187980.2188227},
} 


@inproceedings{20174104251546 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema modifications in the LISPO2persistent object-oriented language},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Barbedette, Gilles},
volume = {512 LNCS},
year = {1991},
pages = {77 - 96},
issn = {03029743},
address = {Geneva, Switzerland},
abstract = {This paper addresses the issue of schema evolution in LISPO<inf>2</inf>, a persistent object-oriented language. It introduces the schema modifications supported by the LISPO<inf>2</inf>programming environment and presents the potential inconsistencies resulting from these modifications at the schema, method and object levels. Furthermore, it describes how the environment efficiently detects such inconsistencies using a database representing the schema definition. Moreover for correct modifications, it presents how this database is used to update the schema, to trigger method recompilations and to restructure objects using a semi-lazy evolution policy.<br/> &copy; Springer-Verlag Berlin Heidelberg 1991.},
key = {Object oriented programming},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Persistent objects;Programming environment;Schema evolution;},
URL = {http://dx.doi.org/10.1007/BFb0057016},
} 


@inproceedings{20110113540966 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {From static methods to role-driven service invocation - A metamodel for active content in object databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Leone, Stefania and Norrie, Moira C. and Signer, Beat and De Spindler, Alexandre},
volume = {5829 LNCS},
year = {2009},
pages = {444 - 457},
issn = {03029743},
abstract = {Existing object databases define the behaviour of an object in terms of methods declared by types. Usually, the type of an object is fixed and therefore changes to its behaviour involves schema evolution. Consequently, dynamic configurations of object behaviour are generally not supported. We define the notion of role-based object behaviour and show how we integrated it into an existing object database extended with a notion of collections to support object classification and role modelling. We present a metamodel that enables specific services to be associated with objects based on collection membership and show how such a model supports flexible runtime configuration of loosely coupled services. &copy; Springer-Verlag 2009.<br/>},
key = {Classification (of information)},
keywords = {Data mining;Object-oriented databases;},
note = {Active content;Dynamic configuration;Loosely coupled;Object classification;Objects-based;Run-time configuration;Schema evolution;Service invocation;},
URL = {http://dx.doi.org/10.1007/978-3-642-04840-1_33},
} 


@inproceedings{20154801607432 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {First steps to a formal framework for multilevel database modifications},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Buddrus, Frank and Gartner, Heino and Lautemann, Sven-Eric},
volume = {1308},
year = {1997},
pages = {240 - 251},
issn = {03029743},
address = {Toulouse, France},
abstract = {We propose a formal basis for operations which can be understood as implicitly used in many kinds of schema modifications. Approaches for view definition, schema evolution, and schema versioning all rely on operations which work either on instance, on schema, or on both levels. This paper discusses a basic set of these operations called modification primitives and describes their semantics on the basis of the Extended Entity Relationship (EER) Model in a Hoare-style notation. We focus on the structural part of the schema definition and outline our ideas for arbitrary manipulations of instances.<br/> &copy; Springer-Verlag Berlin Heidelberg 1997.},
key = {Database systems},
keywords = {Expert systems;Semantics;},
note = {Extended entity relationship model;Formal framework;Multilevel database;Schema evolution;Schema versioning;Structural parts;},
} 


@inproceedings{20174604398079 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Scaffolding relational schemas and APIs from content in web mockups},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Murolo, Alfonso and Ehrensberger, Sybil and Asani, Zera and Norrie, Moira C.},
volume = {10650 LNCS},
year = {2017},
pages = {149 - 163},
issn = {03029743},
address = {Valencia, Spain},
abstract = {Web developers often use an interface-driven design process where mockups are gradually refined before being implemented using a platform or framework. We propose a tool, DataMockups, that supports the creation of digital mockups and then generates a relational schema automatically based on sample content and some assumptions on its structure. This aims at reducing development effort, and the database knowledge required by developers. Sample content may be entered manually or automatically using data extracted from similar existing websites. A relational schema is inferred from the data content, and then translated to an SQL database definition before generating a server-side API. To support schema evolution, the generated API provides schema abstractions that offer robustness to future schema modifications. We report on a case study for the schema inference and a performance evaluation of the data detection algorithm.<br/> &copy; Springer International Publishing AG 2017.},
key = {Mockups},
keywords = {Data mining;Inference engines;Scaffolds;},
note = {Data detection;Digital Mock-up;Performance evaluations;Relational schemas;Scaffolding;Schema evolution;Schema generation;Schema inference;},
URL = {http://dx.doi.org/10.1007/978-3-319-69904-2_12},
} 


@inproceedings{20154901647557 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using the F2 OODBMS to support incremental knowledge acquisition},
journal = {Proceedings of the International Database Engineering and Applications Symposium, IDEAS},
author = {Ai-Jadir, L. and Beydoun, G.},
volume = {2002-January},
year = {2002},
pages = {266 - 275},
issn = {10988068},
address = {Edmonton, AB, Canada},
abstract = {Ripple down rules (RDR) is an incremental knowledge acquisition (KA) methodology, where a knowledge base (KB) is constructed as a collection of rules with exceptions. Nested ripple down rules (NRDR) is an extension of this methodology which allows the expert to enter her/his own domain concepts and later refine these concepts hierarchically. In this paper we show similarities between incremental knowledge acquisition and database schema evolution, and propose to use the F2 object-oriented database management system (OODBMS) to implement an NRDR knowledge based system. We use the existing non-standard features of F2 and show how multiple instantiation and object migration (known as multiobjects feature in F2), and schema evolution capabilities in F2 easily accommodate all the update mechanisms required to incrementally build an NRDR KB. We illustrate our approach with a KA session.<br/> &copy; 2002 IEEE.},
key = {Object-oriented databases},
keywords = {Database systems;Knowledge acquisition;Knowledge based systems;Knowledge engineering;Object oriented programming;Relational database systems;},
note = {Data engineering;Database schemas;Incremental knowledge acquisition;Object migration;Ripple Down Rules;Schema evolution;Spatial database;Update mechanisms;},
URL = {http://dx.doi.org/10.1109/IDEAS.2002.1029679},
} 


@inproceedings{20153001064060 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The viewpoint abstraction in object-oriented modeling and the UML},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Motschnig-Pitrik, Renate},
volume = {1920},
year = {2000},
pages = {543 - 557},
issn = {03029743},
address = {Salt Lake City, UT, United states},
abstract = {In object-oriented (OO) development the viewpoint abstraction has attracted by far less attention than classical abstraction mechanisms, such as classification, generalization, and aggregation. In OO databases, however, recent research has produced powerful view concepts supporting customization, schema evolution, and updates of base objects through views. This paper discusses features of the viewpoint abstraction in the context of OO modeling and specifies extensions to the UML to support the modeling of views. We suggest employing an explicit notion of a view based on research on contexts and on OO databases in order to facilitate the customization of OO models through views. Further, the role of views to support an incremental development process will be discussed.<br/> &copy; Springer-Verlag Berlin Heidelberg 2000.},
key = {Abstracting},
keywords = {Data mining;},
note = {Abstraction mechanism;Incremental development;Object oriented;Object oriented model;Recent researches;Schema evolution;View-based;},
} 


@inproceedings{20174804452265 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Types for data-oriented languages},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Cardelli, Luca},
volume = {303 LNCS},
year = {1988},
pages = {1 - 15},
issn = {03029743},
address = {Venice, Italy},
abstract = {Data-oriented languages may benefit from a rich kind structure. We have shown that kinds can provides a framework for relational and database-wide operations, for subtype relations, for schema computations, and perhaps even for schema evolution.<br/> &copy; 1988, Springer-Verlag.},
key = {Database systems},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Schema evolution;},
URL = {http://dx.doi.org/10.1007/3-540-19074-0_44},
} 


@inproceedings{20161702289797 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Encapsulating classification in an OODBMS for data mining applications},
journal = {Proceedings - 7th International Conference on Database Systems for Advanced Applications, DASFAA 2001},
author = {Al-Jadir, L.},
year = {2001},
pages = {100 - 106},
address = {Hong Kong, China},
abstract = {Classification is an important task in data mining. Encapsulating classification in an object-oriented database system requires additional features: we propose multiobjects and schema evolution. Our approach allows us to store classification functions, and to store instances of each group in order to retrieve them later. Since the database is operational, it allows us also to perform dynamic classification, i.e. add/remove instances to/from groups over time. Moreover it allows us to update classification functions (if we choose another population sample or apply another classifier) and have the instances of groups consequently reclassified. We illustrate our approach with a target mailing application.<br/> &copy; 2001 IEEE.},
key = {Classification (of information)},
keywords = {Data encapsulation;Data mining;Object-oriented databases;},
note = {Classification functions;Data mining applications;Dynamic classification;Multi-objects;Schema evolution;},
URL = {http://dx.doi.org/10.1109/DASFAA.2001.916370},
} 


@inproceedings{20154901650216 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Problems in the maintenance of a federated database schema},
journal = {Proceedings - International Conference of the Chilean Computer Science Society, SCCC},
author = {Motz, R.},
volume = {2002-January},
year = {2002},
pages = {124 - 132},
issn = {15224902},
address = {Copiapo, Atacama, Chile},
abstract = {We characterize the problem of maintenance of a federated schema to cope with local schema evolution in a tightly coupled federation. By means of an example, we present the problems that local schema changes could cause on the federated schema and show proposed solutions.<br/> &copy; 2002 IEEE.},
key = {Object-oriented databases},
keywords = {Computer science;Computers;Database systems;Information technology;Investments;Productivity;},
note = {Federated Databases;Object oriented model;Schema changes;Schema evolution;Societies;Tightly-coupled;},
URL = {http://dx.doi.org/10.1109/SCCC.2002.1173183},
} 


@article{20123615397635 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transforming XML documents as schemas evolve},
journal = {Proceedings of the VLDB Endowment},
author = {Kwietniewski, Marcin and Gryz, Jarek and Hazlewood, Stephanie and Run, Paul Van},
volume = {3},
number = {2},
year = {2010},
pages = {1577 - 1580},
issn = {21508097},
abstract = {Database systems often use XML schema to describe the format of valid XML documents. Usually, this format is determined when the system is designed. Sometimes, in an already functioning system, a need arises to change the XML schemas. In such a situation, the system has to transform the old XML documents so that they conform to the new format and that as little information as possible is lost in the process. This process is called schema evolution. We have implemented an XML schema transformation toolkit within IBM Master Data Management Server (MDM). MDM uses XML documents to describe products that an enterprise may be offering to its clients. In this work we focus on evolving schemas rather than on integrating separate or heterogeneous data sources. Our solution includes an extendible schema matching algorithm that was designed with evolving XML schemas in mind and takes advantage of hierarchical structure of XML. It also includes a data transformation and migration method appropriate for environments where migration is performed in an abstraction layer above the DBMS. Finally, we describe a novel way of extending an XSLT editor with an XSLT visualization feature to allow the user's input and evaluation of the transformation. &copy; 2010 VLDB Endowment.<br/>},
key = {XML},
keywords = {Abstracting;Metadata;},
note = {Abstraction layer;Data transformation;Heterogeneous data sources;Hierarchical structures;Master data management;New formats;Schema evolution;Schema matching;},
} 


@inproceedings{20154701562481 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Organic databases},
journal = {International Journal of Computational Science and Engineering},
author = {Jagadish, H.V. and Qian, Li and Nandi, Arnab},
volume = {11},
number = {3},
year = {2015},
pages = {270 - 283},
issn = {17427185},
abstract = {Databases today are carefully engineered: there is an expensive and deliberate design process, after which a database schema is defined; during this design process, various possible instance examples and use cases are hypothesised and carefully analysed; finally, the schema is ready and then can be populated with data. All of this effort is a major barrier to database adoption. In this paper, we explore the possibility of organic database creation instead of the traditional engineered approach. The idea is to let the user start storing data in a database with a schema that is just enough to cove the instances at hand. We then support efficient schema evolution as new data instances arrive. By designing the database to evolve, we can sidestep the expensive front-end cost of carefully engineering the design of the database. Indeed, the deliberate design model complicates not only database creation, but also database transformation (i.e., schema mapping) because traditional schema mapping tasks are carefully engineered with declarative specification hidden beneath complex user interface. In this paper, we also study the issue of organic database transformation, which automatically induces schema mappings from sample target database instances.<br/> &copy; 2015 Inderscience Enterprises Ltd.},
key = {Database systems},
keywords = {Cost engineering;User interfaces;XML;},
note = {Database creation;Database schemas;Database transformation;Information integration;Schema design;Schema evolution;Schema mappings;Target database;},
URL = {http://dx.doi.org/10.1504/IJCSE.2015.072651},
} 


@inproceedings{20152200893751 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {LEESA: Embedding strategic and XPath-like object structure traversals in C++},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Tambe, Sumant and Gokhale, Aniruddha},
volume = {5658},
year = {2009},
pages = {100 - 124},
issn = {03029743},
address = {Oxford, United kingdom},
abstract = {Traversals of heterogeneous object structures are the most common operations in schema-first applications where the three key issues are (1) separation of traversal specifications from type-specific actions, (2) expressiveness and reusability of traversal specifications, and (3) supporting structure-shy traversal specifications that require minimal adaptation in the face of schema evolution. This paper presents Language for Embedded quEry and traverSAl (LEESA), which provides a generative programming approach to address the above issues. LEESA is an object structure traversal language embedded in C++. Using C++ templates, LEESA combines the expressiveness of XPath&rsquo;s axes-oriented traversal notation with the genericity and programmability of Strategic Programming. LEESA uses the object structure meta-information to statically optimize the traversals and check their compatibility against the schema. Moreover, a key usability issue of domain-specific error reporting in embedded DSL languages has been addressed in LEESA through a novel application of Concepts, which is an upcoming C++ standard (C++0x) feature. We present a quantitative evaluation of LEESA illustrating how it can significantly reduce the development efforts of schema-first applications.<br/> &copy; IFIP International Federation for Information Processing 2009.},
key = {C++ (programming language)},
keywords = {Digital subscriber lines;Problem oriented languages;Reusability;Specifications;},
note = {Common operations;Generative programming;Heterogeneous object;Novel applications;Quantitative evaluation;Schema evolution;Strategic programming;Supporting structure;},
} 


@inproceedings{20093512278194 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {LEESA: Embedding strategic and xpath-like object structure traversals in C++},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Tambe, Sumant and Gokhale, Aniruddha},
volume = {5658 LNCS},
year = {2009},
pages = {100 - 124},
issn = {03029743},
address = {Oxford, United kingdom},
abstract = {Traversals of heterogeneous object structures are the most common operations in schema-first applications where the three key issues are (1) separation of traversal specifications from type-specific actions, (2) expressiveness and reusability of traversal specifications, and (3) supporting structure-shy traversal specifications that require minimal adaptation in the face of schema evolution. This paper presents Language for Embedded quEry and traverSAl (LEESA), which provides a generative programming approach to address the above issues. LEESA is an object structure traversal language embedded in C++. Using C++ templates, LEESA combines the expressiveness of XPath's axes-oriented traversal notation with the genericity and programmability of Strategic Programming. LEESA uses the object structure meta-information to statically optimize the traversals and check their compatibility against the schema. Moreover, a key usability issue of domain-specific error reporting in embedded DSL languages has been addressed in LEESA through a novel application of Concepts, which is an upcoming C++ standard (C++0x) feature. We present a quantitative evaluation of LEESA illustrating how it can significantly reduce the development efforts of schema-first applications. &copy; IFIP International Federation for Information Processing 2009.<br/>},
key = {C++ (programming language)},
keywords = {Digital subscriber lines;Problem oriented languages;Reusability;Specifications;},
note = {Common operations;Generative programming;Heterogeneous object;Novel applications;Quantitative evaluation;Schema evolution;Strategic programming;Supporting structure;},
URL = {http://dx.doi.org/10.1007/978-3-642-03034-5_6},
} 


@inproceedings{20173904212175 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Product abstraction evolution by active process facilitators},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Silvestri, Mark J.},
volume = {492 LNCS},
year = {1991},
pages = {109 - 125},
issn = {03029743},
address = {Cambridge, MA, United states},
abstract = {This paper provides a framework for concurrent abstract refinement activity for a product throughout its life cycle. Two distinct abstraction activities are discussed: interdomain and intradomain life cycle representation. Product process facilitation by active computer agents provides the vehicle for schema evolution via specialization.<br/> &copy; Springer-Verlag Berlin Heidelberg 1991.},
key = {Life cycle},
keywords = {Abstracting;Grid computing;Product development;},
note = {Active process;Computer agents;Inter-domain;Intra-domain;Product process;Schema evolution;},
URL = {http://dx.doi.org/10.1007/BFb0014275},
} 


@inproceedings{20114414477565 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema versioning in xSchema-based multitemporal XML repositories},
journal = {Proceedings - International Conference on Research Challenges in Information Science},
author = {Brahmia, Zouhaier and Bouaziz, Rafik and Grandi, Fabio and Oliboni, Barbara},
year = {2011},
pages = {IEEE France Section; University of French West Indies and Guiana - },
issn = {21511349},
address = {Gosier, Guadeloupe},
abstract = {&tau;XSchema [7] is a framework (a language and a suite of tools) for the creation and validation of time-varying XML documents. A &tau;XSchema schema is composed of a conventional XML Schema document annotated with physical and logical annotations. All components of a &tau;XSchema schema (i.e., conventional schema, logical annotations, and physical annotations) can change over time to reflect changes in user requirements or in reference world of the database. Since many applications need to keep track of both data and schema evolution, schema versioning has been long advocated to be the best solution to do this. In this paper, we deal with schema versioning in the &tau;XSchema framework. More precisely, we propose a set of schema change primitives for the maintenance of logical and physical annotations and define their operational semantics. &copy; 2011 IEEE.<br/>},
key = {XML},
keywords = {Semantics;},
note = {Operational semantics;Physical annotations;Schema evolution;Schema versioning;Temporal Database;User requirements;XML repositories;XML schemas;},
URL = {http://dx.doi.org/10.1109/RCIS.2011.6006845},
} 


@inproceedings{20160801995327 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mapping publishing and mapping adaptation in the middleware of railway information grid system},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {You, Ganmei and Liao, Huaming and Sun, Yuzhong},
volume = {3222},
year = {2004},
pages = {110 - 117},
issn = {03029743},
address = {Wuhan, China},
abstract = {When adopting the mediator architecture to integrate distributed, autonomous, relational model based database sources, mappings from the source schema to the global schema may become inconsistent when the rela-tional source schema or the global schema evolves. Without mapping adapta-tion, users may access no data or wrong data. In the paper, we propose a novel approach the global attribute as view with constraints (GAAVC) to publish mappings, which is adaptive for the schema evolution. Also published map-pings satisfy both source schema constraints and global schema constraints, which enable users to get valid data. We also put forward the GAAVC based mapping publishing algorithm and mapping adaptation algorithms. When we compare our approach with others in functionality, it outperforms. Finally the mapping adaptation tool GMPMA is introduced, which has been implemented in the middleware of railway information grid system.<br/> &copy; IFIP International Federation for Information Processing 2004.},
key = {Mapping},
keywords = {Middleware;Railroads;},
note = {Adaptation algorithms;Global schemas;Information Grid;Relational Model;Schema constraints;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-540-30141-7_17},
} 


@inproceedings{20113314232929 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Collaboratively sharing scientific data},
journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering},
author = {Wang, Fusheng and Vergara-Niedermayr, Cristobal},
volume = {10 LNICST},
year = {2009},
pages = {805 - 823},
issn = {18678211},
abstract = {Scientific research becomes increasingly reliant on multi-disciplinary, multi-institutional collaboration through sharing experimental data. Indeed, data sharing is mandatory by government research agencies such as NIH. The major hurdles for data sharing come from: i) the lack of data sharing infrastructure to make data sharing convenient for users; ii) users' fear of losing control of their data; iii) difficulty on sharing schemas and incompatible data from sharing partners; and iv) inconsistent data under schema evolution. In this paper, we develop a collaborative data sharing system SciPort, to support consistency preserved data sharing among multiple distributed organizations. The system first provides Central Server based lightweight data integration architecture, so data and schemas can be conveniently shared across multiple organizations. Through distributed schema management, schema sharing and evolution is made possible, while data consistency is maintained and data compatibility is enforced. With this data sharing system, distributed sites can now consistently share their research data and their associated schemas with much convenience and flexibility. Sci- Port has been successfully used for data sharing in biomedical research, clinical trials and large scale research collaboration. &copy; 2009 ICST Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering.<br/>},
key = {Information management},
keywords = {Clinical research;Computer supported cooperative work;Data integration;},
note = {Biomedical data;Schema evolution;Schema sharing;Scientific data integration;Scientific data sharing;},
URL = {http://dx.doi.org/10.1007/978-3-642-03354-4_58},
} 


@inproceedings{20154201412374 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Storage and querying of E-commerce data},
journal = {VLDB 2001 - Proceedings of 27th International Conference on Very Large Data Bases},
author = {Agrawal, Rakesh and Somani, Amit and Xu, Yirong},
year = {2001},
pages = {149 - 158},
address = {Roma, Italy},
abstract = {New generation of e-commerce applications require data schemas that are constantly evolving and sparsely populated. The conventional horizontal row representation fails to meet these requirements. We represent objects in a vertical format storing an object as a set of tuples. Each tuple consists of an object identifier and attribute name-value pair. Schema evolution is now easy. However, writing queries against this format becomes cumbersome. We create a logical horizontal view of the vertical representation and transform queries on this view to the vertical table. We present alternative implementations and performance results that show the effectiveness of the vertical representation for sparse data. We also identify additional facilities needed in database systems to support these applications well.<br/>},
key = {Digital storage},
keywords = {Electronic commerce;Query processing;},
note = {E-Commerce applications;Object identifier;Schema evolution;Sparse data;},
} 


@inproceedings{20124615675349 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Five-level multi-application schema evolution},
journal = {CEUR Workshop Proceedings},
author = {Necasky, Martin and Mlynkova, Irena},
volume = {471},
year = {2009},
pages = {90 - 104},
issn = {16130073},
address = {Spindleruv Mlyn, Czech republic},
abstract = {Schema evolution has recently gained much interest in both research and practice. However, most of the existing works deal with separate aspects of the problem such as evolution of XML schemas or evolution of conceptual schemas. In addition, all of them view the problem only from the perspective of a single application. In this paper we show that schema evolution has several different levels at which it can be performed and that are highly related. Secondly, we show that schema evolution is not the problem of a single application, but multiple applications having the same problem domain can influence each other as well. In particular we deal with five levels - extensional, operational, logical, platform-specific and platform-independent. We describe the particular levels, how they can be modified and the respective propagation of the modifications to other levels and applications. We also show which of the situations have already been discussed and solved in the existing works as well as which of them still remain open.<br/>},
key = {Specifications},
note = {Conceptual schemas;Five levels;Multi-application;Multiple applications;Platform independent;Problem domain;Schema evolution;XML schemas;},
} 


@article{20104113284376 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML materialized views and schema evolution in VIREX},
journal = {Information Sciences},
author = {Lo, Anthony and Ozyer, Tansel and Tahboob, Radwan and Kianmehr, Keivan and Jida, Jamal and Alhajj, Reda},
volume = {180},
number = {24},
year = {2010},
pages = {4940 - 4957},
issn = {00200255},
abstract = {Web-based databases are gaining increased popularity. This has positively influenced the availability of structured and semi-structured databases for access by a variety of users ranging from professionals to naive users. The number of users accessing online databases will continue to increase if the visual tools connected to web-based databases are flexible and user-friendly enough to meet the expectations of naive users and professionals. Further, XML is accepted as the standard for platform independent data exchange. This motivated for the development of the conversion tools between structured databases and XML. Realizing that such a need has not been well handled by the available tools, including Clio from IBM, we developed VIREX as a visual tool for converting relational databases into XML, and since then has been empowered with further capabilities to manipulate the produced XML schema including the maintenance of materialized views and schema evolution functions. VIREX provides an interactive approach for querying and integrating relational databases to produce XML documents and the corresponding XML schema(s). VIREX supports VRXQuery as a visual naive users-oriented query language that allows users to specify queries and define views directly on the interactive diagram as a sequence of mouse clicks with minimum keyboard input. As the query result, VIREX displays on the screen the XML schema that satisfies the specified characteristics and generates colored (easy to read) XML document(s). The main contribution described in this paper is the novel approach for turning query results into materialized views which are maintained to remain consistent with the underlying database. VIREX supports deferred update of XML views by keeping an ordered summary of the necessary and sufficient information required for the process. Each view has a corresponding marker in the ordered summary to indicate the start of the information to be reflected onto the view when it is accessed. When a view is accessed, its marker moves to the head of the list to mark for the next update. In addition, VIREX supports some basic schema evolution functions include renaming, adding and dropping of elements and attributes, among others. The supported schema evolution functions add flexibility to the view maintenance and materialization process. &copy; 2010 Elsevier Inc. All rights reserved.<br/>},
key = {XML},
keywords = {Electronic data interchange;Query languages;Query processing;Visual languages;Websites;},
note = {Data conversion;Deferred update;Materialized view;Schema evolution;Structured database;Visual query languages;},
URL = {http://dx.doi.org/10.1016/j.ins.2010.08.025},
} 


@inproceedings{20130716007276 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Deductive approach to semistructured schema evolution},
journal = {CEUR Workshop Proceedings},
author = {Luciv, D.},
volume = {154},
year = {2005},
issn = {16130073},
address = {Saint Petersburg, Russia},
abstract = {Schema evolution for different methodologies is an interesting area of research for now and future. Some results of research of semistructured database schema evolution are presented here. Semistructured database schema model based on [11, 4] and its evolution techniques are introduced. Extensions to data model are also described. Some refactoring problems are described. Declarative approach to schema evolution and refactoring is presented.<br/>},
key = {Database systems},
keywords = {Information systems;Information use;},
note = {Model-based OPC;Refactorings;Schema evolution;Semi-structured;Semistructured database;},
} 


@inproceedings{1995292724586 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Axiomatization of dynamic schema evolution in objectbases},
journal = {Proceedings - International Conference on Data Engineering},
author = {Peters, Randal J. and Ozsu, M.Tamer},
year = {1995},
pages = {156 - 164},
address = {Taipei, Taiwan},
abstract = {The schema of a system consists of the constructs that model its entities. Schema evolution is the timely change and management of the schema. Dynamic schema evolution is the management of schema changes while the system is in operation. We propose a sound and complete axiomatic model for dynamic schema evolution in object-base management systems (OBMSs) that support subtyping and property inheritance. The model is formal, which distinguishes it from the traditional approach of informally defining a number of invariants and rules to enforce them. By reducing systems to the axiomatic model, their functionality with respect to dynamic schema evolution can be compared within a common framework.},
key = {Database systems},
keywords = {Computational linguistics;Computer simulation;Invariance;Mathematical models;Object oriented programming;},
note = {Axiomatization;Dynamic schema evolution;Functionality;Metainformation;},
} 


@inproceedings{20160902023006 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Optimizing performance of schema evolution sequences},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Claypool, Kajal T. and Natarajan, Chandrakant and Rundensteiner, Elke A.},
volume = {1944},
year = {2001},
pages = {114 - 127},
issn = {03029743},
address = {Sophia Antipolis, France},
abstract = {More than ever before schema transformation is a prevalent problem that needs to be addressed to accomplish for example the mi- gration of legacy systems to the newer OODB systems, the generation of structured web pages from data in database systems, or the integration of systems with different native data models. Such schema transformations are typically composed of a sequence of schema evolution operations. The execution of such sequences can be very time-intensive, possibly requi- ring many hours or even days and thus effectively making the database unavailable for unacceptable time spans. While researchers have looked at the deferred execution approach for schema evolution in an effort to improve availability of the system, to the best of our knowledge ours is the ffrst effort to provide a direct optimization strategy for a sequence of changes. In this paper, we propose heuristics for the iterative elimination and cancellation of schema evolution primitives as well as for the merging of database modiffcations of primitives such that they can be performed in one effcient transformation pass over the database. In addition we show the correctness of our optimization approach, thus guaranteeing that the initial input and the optimized output schema evolution se- quence produce the same final schema and data state. We provide proof of the algorithm's optimality by establishing the confluence property of our problem search space, i.e., we show that the iterative application of our heuristics always terminates and converges to a unique minimal sequence. Moreover, we have conducted experimental studies that de- monstrate the performance gains achieved by our proposed optimization technique over previous solutions.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Object oriented programming},
keywords = {Iterative methods;Legacy systems;Metadata;Object-oriented databases;Optimization;Websites;},
note = {Direct optimization;Iterative Applications;Optimization approach;Optimization techniques;Optimizing performance;Schema evolution;Schema transformation;Structured web page;},
URL = {http://dx.doi.org/10.1007/3-540-44677-X_8},
} 


@inproceedings{20160401837960 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Models@run.time for object-relational mapping supporting schema evolution},
journal = {CEUR Workshop Proceedings},
author = {Gotz, Sebastian and Kuhn, Thomas},
volume = {1474},
year = {2015},
pages = {41 - 50},
issn = {16130073},
address = {Ottawa, ON, Canada},
abstract = {Persistence of applications written in an object-oriented language using a relational storage system has been investigated for a long time [4]. In this paper, two problems of current approaches to objectrelation mapping are addressed. First, their high configuration effort, and second, their lacking support for continuous development. To address these problems, we introduce a novel object-relational mapping approach, that uses a runtime model of the system. The runtime model is utilized in two ways. First, to derive mapping information from the runtime state of the application, that usually has to be provided by developers. Second, to allow for lossless application schema evolution. That is, we present an approach, that reasons about design time and runtime information to relieve developers from configuration details of the objectrelational mapping and show how to utilize the same information to allow for continuous schema evolution of applications.<br/>},
key = {Mapping},
keywords = {Object oriented programming;},
note = {Continuous development;Mapping information;Object-relational mapping;Object-roles;Relational storage;Run-time information;Runtime models;Schema evolution;},
} 


@inproceedings{20111313852961 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The predicate formulae for object-oriented database schema evolution},
journal = {Proceedings - 2010 2nd WRI World Congress on Software Engineering, WCSE 2010},
author = {Lin, Jie and Yu, Jiankun and Zeng, Zhiyong},
volume = {2},
year = {2010},
pages = {346 - 349},
abstract = {Object-oriented database system is a superior model than relational database system, with schema evolution capacity. Although since the emergence of object-oriented database systems many schema evolution methods have been proposed, a complete formal description haven't been formed. Therefore, it is necessary to use predicate logic to describe the operation of object-oriented database. This paper introduces predicate logic to illustrate schema evolution of object-oriented database with the predicate formulae. Atomic formulae and formulae are presented and the class evolution invariants and rules are also formalized. &copy; 2010 IEEE.<br/>},
key = {Object-oriented databases},
keywords = {Computer circuits;Object oriented programming;Relational database systems;Software engineering;},
note = {Atomic formulae;Database schemas;Formal Description;Object oriented;Predicate;Predicate logic;Schema evolution;},
URL = {http://dx.doi.org/10.1109/WCSE.2010.13},
} 


@inproceedings{20083511484824 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Requirements ontology and multi-representation strategy for database schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bounif, Hassina and Spaccapietra, Stefano and Pottinger, Rachel},
volume = {4623 LNCS},
year = {2007},
pages = {68 - 84},
issn = {03029743},
address = {Seoul, Korea, Republic of},
abstract = {With the emergence of enterprise-wide information systems, ontologies have become by definition a valuable aid for efficient database schema modeling and integration, in addition to their use in other disciplines such as the semantic web and natural language processing. This paper presents another important utilization of ontologies in database schemas: schema evolution. Specifically, our research concentrates on a new three-layered approach for schema evolution. These three layers are 1) a schema repository, 2) a domain ontology called a requirements ontology, and 3) a multi-representation strategy to enable powerful change management. This a priori approach for schema evolution, in contrast with existing a posteriori solutions, can be employed for any data model and for both 1) design from scratch and evolution and 2) redesign and evolution of the database. The paper focuses on the two main foundations of this approach, the requirements ontology and the multi-representation strategy which is based on a stamping mechanism. &copy; 2007 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Ontology},
keywords = {Database systems;Information systems;Information use;Modeling languages;Natural language processing systems;},
note = {Change management;Database schemas;Domain ontologies;Enterprise-wide information systems;Layered approaches;Multi-representation strategy;Requirements ontology;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-540-75474-9_5},
} 


@inproceedings{20104613394929 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Revisiting schema evolution in object databases in support of agile development},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zaschke, Tilmann and Norrie, Moira C.},
volume = {6348 LNCS},
year = {2010},
pages = {10 - 24},
issn = {03029743},
abstract = {Based on a real-world case study in agile development, we examine issues of schema evolution in state-of-the-art object databases. In particular, we show how traditional problems and solutions discussed in the research literature do not match the requirements of modern agile development practices. To highlight these discrepancies, we present the approach to agile schema evolution taken in the case study and then focus on the aspects of backward/forward compatibility and object structures. In each case, we discuss the impact on managing software evolution and present approaches to dealing with these in practice. &copy; 2010 Springer-Verlag.<br/>},
key = {Object-oriented databases},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Agile development;Object structure;Problems and Solutions;Real-world;Schema evolution;Software Evolution;State of the art;},
URL = {http://dx.doi.org/10.1007/978-3-642-16092-9_5},
} 


@article{20123215313992 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Graceful database schema evolution: The PRISM workbench},
journal = {Proceedings of the VLDB Endowment},
author = {Curino, Carlo A. and Moon, Hyun J. and Zaniolo, Carlo},
volume = {1},
number = {1},
year = {2008},
pages = {761 - 772},
issn = {21508097},
abstract = {Supporting graceful schema evolution represents an unsolved problem for traditional information systems that is further exacerbated in web information systems, such as Wikipedia and public scientific databases: in these projects based on multiparty cooperation the frequency of database schema changes has increased while tolerance for downtimes has nearly disappeared. As of today, schema evolution remains an error-prone and time-consuming undertaking, because the DB Administrator (DBA) lacks the methods and tools needed to manage and automate this endeavor by (i) predicting and evaluating the effects of the proposed schema changes, (ii) rewriting queries and applications to operate on the new schema, and (iii) migrating the database. Our PRISM system takes a big first step toward addressing this pressing need by providing: (i) a language of Schema Modification Operators to express concisely complex schema changes, (ii) tools that allow the DBA to evaluate the effects of such changes, (iii) optimized translation of old queries to work on the new schema version, (iv) automatic data migration, and (v) full documentation of intervened changes as needed to support data provenance, database flash back, and historical queries. PRISM solves these problems by integrating recent theoretical advances on mapping composition and invertibility, into a design that also achieves usability and scalability. Wikipedia and its 170+ schema versions provided an invaluable testbed for validating PRISM tools and their ability to support legacy queries. &copy; 2008 VLDB Endowment.<br/>},
key = {Query languages},
keywords = {Information systems;Information use;Prisms;},
note = {Data provenance;Database schemas;Historical queries;Multi-party cooperation;Schema evolution;Scientific database;Unsolved problems;Web information systems;},
URL = {http://dx.doi.org/10.14778/1453856.1453939},
} 


@inproceedings{1998134045882 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in object databases by catalogs},
journal = {Proceedings of the International Database Engineering &amp; Applications Symposium, IDEAS},
author = {Pons, Anne and Keller, Rudolf K.},
year = {1997},
pages = {368 - 376},
address = {Montreal, Can},
abstract = {We are concerned by schema evolution in Object Oriented Databases (OODB) that is processed by a modification on the classes of the schema. We present a new categorization of the different modifications in three categories: primitive, composite and complex modifications. On this basis, we propose a method by decomposition for addressing conceptual schema evolution: `real-life', complex schema modifications are solved by decomposition into simpler, well known and controlled modifications, called composites, which in turn may be reduced to so-called primitives. A key step in making this approach practical, is the provision of two catalogs: one for the primitives based on a sound object model and one for the composites built on top of the primitive catalog. Such catalogs raise the level of abstraction, further reuse and are prerequisites for effective tool support. In this paper, we define the three types of schema modifications, describe these catalogs, put them into the context of our decomposition approach and provide a process for schema evolution.},
key = {Database systems},
keywords = {Data recording;File organization;Object oriented programming;},
note = {Object oriented databases (OODB);Schema evolution;},
} 


@inproceedings{20094612440169 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing the history of metadata in support for DB archiving and schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Curino, Carlo A. and Moon, Hyun J. and Zaniolo, Carlo},
volume = {5232 LNCS},
year = {2008},
pages = {78 - 88},
issn = {03029743},
address = {Barcelona, Spain},
abstract = {Modern information systems, and web information systems in particular, are faced with frequent database schema changes, which generate the necessity to manage them and preserve the schema evolution history. In this paper, we describe the Panta Rhei Framework designed to provide powerful tools that: (i) facilitate schema evolution and guide the Database Administrator in planning and evaluating changes, (ii) support automatic rewriting of legacy queries against the current schema version, (iii) enable efficient archiving of the histories of data and metadata, and (iv) support complex temporal queries over such histories. We then introduce the Historical Metadata Manager (HMM), a tool designed to facilitate the process of documenting and querying the schema evolution itself. We use the schema history of the Wikipedia database as a telling example of the many uses and benefits of HMM. &copy; 2008 Springer Berlin Heidelberg.<br/>},
key = {Information management},
keywords = {Data mining;Data warehouses;Information systems;Information use;Metadata;Query languages;Query processing;},
note = {Data and metadata;Database administrators;Database schemas;Schema evolution;Temporal queries;Web information systems;Wikipedia;},
URL = {http://dx.doi.org/10.1007/978-3-540-87991-6-11},
} 


@inproceedings{2004488478314 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An approach for schema evolution in ODMG databases},
journal = {ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems},
author = {Delgado, Cecilia and Samos, Jose and Torres, Manuel},
year = {2004},
pages = {136 - 141},
address = {Porto, Portugal},
abstract = {Schema evolution is the process of applying changes to a schema in a consistent way and propagating these changes to the instances while the database is in operation. However, when a database is shared by many users, updates to the database schema are always difficult. To overcome this problem, in this paper we propose a version mechanism for schema evolution in ODMG databases that preserves old schemas for continued support of existing programs running on the shared database when schema changes are produced. Our approach uses external schema definition techniques and is based on the fact that if a schema change is requested on an external schema, rather than modifying the schema, a new schema, which reflects the semantics of the schema change, is defined.},
key = {Database systems},
keywords = {Computer simulation;Information analysis;Mathematical models;Object oriented programming;Problem solving;Semantics;User interfaces;},
note = {External schemas;Object-oriented databases;Schema changes;Schema evolution;},
} 


@inproceedings{20074310881604 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Preserving XML queries during schema evolution},
journal = {16th International World Wide Web Conference, WWW2007},
author = {Moro, Mirella M. and Malaika, Susan and Lim, Lipyeow},
year = {2007},
pages = {1341 - 1342},
address = {Banff, AB, Canada},
abstract = {In XML databases, new schema versions may be released as frequently as once every two weeks. This poster describes a taxonomy of changes for XML schema evolution. It examines the impact of those changes on schema validation and query evaluation. Based on that study, it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas.},
key = {XML},
keywords = {Database systems;Query processing;Taxonomies;Websites;},
note = {Query evaluation;Schema evolution;Schema validation;XML queries;},
URL = {http://dx.doi.org/10.1145/1242572.1242841},
} 


@inproceedings{20174104257596 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and gravitation to rigidity: A tale of calmness in the lives of structured data},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vassiliadis, Panos},
volume = {10563 LNCS},
year = {2017},
pages = {18 - 23},
issn = {03029743},
address = {Barcelona, Spain},
abstract = {Evolving dependency magnets, i.e., software modules upon which a large number of other modules depend, is always a hard task. As Robert C. Martin has nicely summarized it (see http://www.oodesign.com/design-principles.html ), fundamental problems of bad design that hinder evolution include immobility, i.e., difficulty in reuse, rigidity, i.e., the tendency for software to be difficult to change and fragility, i.e., the tendency of the software to break in many places every time it is changed. In such cases, developers are reluctant to evolve the software to avoid facing the impact of change. How are these fundamentals related to schema evolution? We know that changes in the schema of a database affect a large (and not necessarily traced) number of surrounding applications, without explicit identification of the impact. These affected applications can then suffer from syntactic and semantic inconsistencies &ndash; with syntactic inconsistency leading to application crashes and semantic inconsistency leading to the retrieval of data other than the ones originally intended. Thus, the puzzle of gracefully facilitating the evolution of data-intensive information systems is evident, and the desideratum of coming up with engineering methods that allow us to design information systems with a view to minimizing the impact of evolution, a noble goal for the research community.<br/> &copy; 2017, Springer International Publishing AG.},
key = {Computer software reusability},
keywords = {Information systems;Information use;Rigidity;Semantics;Syntactics;},
note = {Data intensive;Engineering methods;Impact of changes;Research communities;Schema evolution;Semantic inconsistencies;Software modules;Structured data;},
URL = {http://dx.doi.org/10.1007/978-3-319-66854-3_2},
} 


@inproceedings{20174604398114 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and foreign keys: Birth, eviction, change and absence},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vassiliadis, Panos and Kolozoff, Michail-Romanos and Zerva, Maria and Zarras, Apostolos V.},
volume = {10650 LNCS},
year = {2017},
pages = {106 - 119},
issn = {03029743},
address = {Valencia, Spain},
abstract = {In this paper, we focus on the study of the evolution of foreign keys in the broader context of schema evolution for relational databases. Specifically, we study the schema histories of a six free, open-source databases that contained foreign keys. Our findings concerning the growth of tables verify previous results that schemata grow in the long run in terms of tables. Moreover, we have come to several surprising, new findings in terms of foreign keys. Foreign keys appear to be fairly scarce in the projects that we have studied and they do not necessarily grow in sync with table growth. In fact, we have observed different cultures for the handling of foreign keys, ranging from treating foreign keys as an indispensable part of the schema, in full sync with the growth of tables, to the unexpected extreme of treating foreign keys as an optional add-on that twice resulted in their full removal from the schema of the database.<br/> &copy; Springer International Publishing AG 2017.},
key = {Data mining},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Foreign keys;Open source database;Patterns of change;Relational Database;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-319-69904-2_9},
} 


@inproceedings{2004488478330 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {F2/XML: Managing XML document schema evolution},
journal = {ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems},
author = {Al-Jadir, Lina and El-Moukaddem, Fatme},
year = {2004},
pages = {251 - 258},
address = {Porto, Portugal},
abstract = {XML has become an emerging standard for data representation and data exchange on the Web. Although XML data is self-describing, most application domains tend to use document schemas. Over a period of time, these schemas need to be modified to reflect a change in the real-world, a change in the user's requirements, mistakes or missing information in the initial design. Most of the current XML management systems do not support schema changes. In this paper, we propose the F2/XML method to manage XML document schema evolution. We consider XML documents associated with DTDs, Our method consists in three steps. First, the DTD and XML documents are stored as a database schema and a database instance respectively. Second, DTD changes are applied as schema changes on the database. Third, the updated DTD and XML documents are retrieved from the database. Our method supports a complete set of DTD changes. The semantics of each DTD change is defined by preconditions and postactions, such that the new DTD is valid, existing XML documents conform to the new DTD, and data is not lost if possible. We implemented our method in the F2 object-oriented database system. XML, schema evolution, object-oriented database system.},
key = {XML},
keywords = {Data reduction;Database systems;Information analysis;Object oriented programming;World Wide Web;},
note = {Data representations;Database schema;Object-oriented database systems;Schema evolution;},
} 


@inproceedings{20171803630173 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting schema evolution in schema-less NoSQL data stores},
journal = {SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
author = {Meurice, Loup and Cleve, Anthony},
year = {2017},
pages = {457 - 461},
address = {Klagenfurt, Austria},
abstract = {NoSQL data stores are becoming popular due to their schema-less nature. They offer a high level of flexibility, since they do not require to declare a global schema. Thus, the data model is maintained within the application source code. However, due to this flexibility, developers have to struggle with a growing data structure entropy and to manage legacy data. Moreover, support to schema evolution is lacking, which may lead to runtime errors or irretrievable data loss, if not properly handled. This paper presents an approach to support the evolution of a schema-less NoSQL data store by analyzing the application source code and its history. We motivate this approach on a subject system and explain how useful it is to understand the present database structure and facilitate future developments.<br/> &copy; 2017 IEEE.},
key = {Information management},
keywords = {Reengineering;},
note = {Data store;Database structures;Global schemas;Legacy data;Run-time errors;Schema evolution;Source codes;Structure entropy;},
URL = {http://dx.doi.org/10.1109/SANER.2017.7884653},
} 


@inproceedings{20102012929273 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An IDE-based, integrated solution to schema evolution of object-oriented software},
journal = {ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering},
author = {Piccioni, Marco and Orioly, Manuel and Meyer, Bertrand and Schneider, Teseo},
year = {2009},
pages = {650 - 654},
address = {Auckland, New zealand},
abstract = {With the wide support for serialization in object-oriented programming languages, persistent objects have become common place. Retrieving previously "persisted" objects from classes whose schema changed is however difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses this issues through an IDE-based approach that handles schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of any corrupt objects. This article describes the principles behind invariant-safe schema evolution, and the design and implementation of the ESCHER system. &copy; 2009 IEEE.<br/>},
key = {Object oriented programming},
keywords = {Integrodifferential equations;Software engineering;},
note = {Persistence;Refactorings;Schema evolution;Serialization;Versioning;},
URL = {http://dx.doi.org/10.1109/ASE.2009.100},
} 


@inproceedings{20105213521384 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Toward formal semantics for data and schema evolution in Data Stream Management Systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Fernandez-Moctezuma, Rafael J. and Terwilliger, James F. and Delcambre, Lois M.L. and Maier, David},
volume = {5833 LNCS},
year = {2009},
pages = {85 - 94},
issn = {03029743},
abstract = {Data Stream Management Systems (DSMSs) do not statically respond to issued queries - rather, they continuously produce result streams to standing queries, and often operate in a context where any interruption can lead to data loss. Support for schema evolution in continuous query processing is currently unaddressed. In this work we address evolution in DSMSs by proposing semantics for three evolution primitives: Add Attribute and Drop Attribute (schema evolution), and Alter Data (data evolution). We characterize how a subset of commonly used query operators in a DSMS act on and propagate these primitives. &copy; Springer-Verlag Berlin Heidelberg 2009.<br/>},
key = {Information management},
keywords = {Database systems;Formal methods;Query processing;Semantics;},
note = {Continuous query processing;Data evolution;Data loss;Data stream management systems;Formal Semantics;Query operators;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-04947-7_11},
} 


@article{20123215314002 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing and querying transactiontime databases under schema evolution},
journal = {Proceedings of the VLDB Endowment},
author = {Moon, Hyun J. and Curino, Carlo A. and Deutsch, Alin and Hou, ChienYi and Zaniolo, Carlo},
volume = {1},
number = {1},
year = {2008},
pages = {882 - 895},
issn = {21508097},
abstract = {The old problem of managing the history of database information is now made more urgent and complex by fast spreading web information systems, such as Wikipedia. Our PRIMA system addresses this difficult problem by introducing two key pieces of new technology. The first is a method for publishing the history of a relational database in XML, whereby the evolution of the schema and its underlying database are given a unified representation. This temporally grouped representation makes it easy to formulate sophisticated historical queries on any given schema version using standard XQuery. The second key piece of technology is that schema evolution is transparent to the user: she writes queries against the current schema while retrieving the data from one or more schema versions. The system then performs the labor-intensive and error-prone task of rewriting such queries into equivalent ones for the appropriate versions of the schema. This feature is particularly important for historical queries spanning over potentially hundreds of different schema versions and it is realized in PRIMA by (i) introducing Schema Modification Operators (SMOs) to represent the mappings between successive schema versions and (ii) an XML integrity constraint language (XIC) to efficiently rewrite the queries using the constraints established by the SMOs. The scalability of the approach has been tested against both synthetic data and real-world data from the Wikipedia DB schema evolution history. &copy; 2008 VLDB Endowment.<br/>},
key = {Query processing},
keywords = {Data privacy;Network security;XML;},
note = {Database information;Error prone tasks;Historical queries;Integrity constraints;Relational Database;Schema evolution;Transaction-time database;Web information systems;},
URL = {http://dx.doi.org/10.14778/1453856.1453952},
} 


@inproceedings{20154801607106 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Extending a view mechanism to support schema evolution in federated database systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bellahsene, Zohra},
volume = {1308},
year = {1997},
pages = {573 - 582},
issn = {03029743},
address = {Toulouse, France},
abstract = {This paper discusses the impact of autonomy requirement on schema design in tightly coupled federated database systems. In a federated database system, an important feature of a schema evolution strategy should be how the conceptual autonomy can be preserved. The conceptual autonomy requirement states that evolution of a local schema should not affect the remote schemas, in this paper we propose a view mechanism enhanced with import/export facilities to support schema evolution enforcing the autonomy requirement. More precisely, some schema evolution operations having ability to entail incompatibility of existing programs with regard to the new schema will not actually performed but simulated by creating specific views. Our approach is concerned with preserving both remote schema from schema changes arising on a local schema and maintaining compatibility for local and remote existing programs.<br/> &copy; Springer-Verlag Berlin Heidelberg 1997.},
key = {Database systems},
keywords = {Expert systems;},
note = {Conceptual autonomy;Federated database system;Important features;Schema changes;Schema design;Schema evolution;Tightly-coupled;Virtual class;},
} 


@inproceedings{20151000601335 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The envision project: Towards a visual tool to support schema evolution in distributed databases},
journal = {Proceedings:  DMS 2009 - 15th International Conference on Distributed Multimedia Systems},
author = {Polese, Giuseppe and Vacca, Mario},
year = {2009},
pages = {174 - 179},
address = {Boston, MA, United states},
abstract = {Changes to the schema of databases naturally and frequently occur during the life cycle of information systems; supporting their management, in the context of distributed databases, requires tools to perform changes easily and to propagate them efficiently to the database instances. In this paper we illustrate ENVISION, a project aiming to develop a Visual Tool for Schema Evolution in Distributed Databases to support the database administrator during the schema evolution process. The first stage of this project concerned the design of an instance update language, allowing to perform schema changes in a parallel way [14]; in this paper we deal with further steps toward the complete realization of the project: the choice of a declarative schema update language and the realization of the mechanism for the automatic generation of instance update routines. The architecture of the system, which has been implementing, is also designed.<br/> &copy; 2009 by Knowledge Systems Institute Graduate School.},
key = {Distributed database systems},
keywords = {Data warehouses;Information management;Life cycle;Multimedia systems;},
note = {Automatic Generation;Database administrators;Distributed database;Schema changes;Schema evolution;Update languages;Visual tools;},
} 


@inproceedings{20184305978186 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Datalution: A tool for continuous schema evolution in NoSQL-Backed web applications},
journal = {QUDOS 2016 - Proceedings of the 2nd International Workshop on Quality-Aware DevOps, co-located with ISSTA 2016},
author = {Scherzinger, Stefanie and Sombach, Stephanie and Wiech, Katharina and Klettke, Meike and Storl, Uta},
year = {2016},
pages = {38 - 39},
address = {Saarbrucken, Germany},
abstract = {When an incremental release of a web application is deployed, the structure of data already persisted in the production database may no longer match what the application code expects. Traditionally, eager schema migration is called for, where all legacy data is migrated in one go. With the growing popularity of schema-exible NoSQL data stores, lazy forms of data migration have emerged: Legacy entities are migrated on-the-y, one at-a-time, when they are loaded by the application. In this demo, we present Datalution, a tool demonstrating the merits of lazy data migration. Datalution can apply chains of pending schema changes, due to its Datalog-based internal representation. The Datalution approach thus ensures that schema evolution, as part of continuous deployment, is carried out correctly.<br/> &copy; 2016 ACM.},
key = {Software testing},
note = {Application codes;Data migration;Data store;Internal representation;Schema changes;Schema evolution;Software Evolution;WEB application;},
URL = {http://dx.doi.org/10.1145/2945408.2945416},
} 


@inproceedings{20170503308199 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database Design Debts through Examining Schema Evolution},
journal = {Proceedings - 2016 IEEE 8th International Workshop on Managing Technical Debt, MTD 2016},
author = {Al-Barak, Mashel and Bahsoon, Rami},
year = {2016},
pages = {17 - 23},
address = {Raleigh, NC, United states},
abstract = {Causes of the database debt can stem from ill-conceptual, logical, and/or physical database design decisions, violations to key design databases principles, use of anti-patterns etc. In this paper, we explore the problem of relational database design debt and define the problem. We develop a taxonomy, which classifies various types of debts that can relate to conceptual, logical and physical design of a database. We define the concept of Database Design Debt, discuss their origin, causes and preventive mechanisms. We draw on MediaWiki case study and examine its database schema evolution to support our work. The contribution hopes to make database designers and application developers aware of these debts so they can minimize/avoid their consequences on a given system.<br/> &copy; 2016 IEEE.},
note = {Application developers;Database design;Database schemas;Design database;Physical database;Relational Database;Schema evolution;Technical debts;},
URL = {http://dx.doi.org/10.1109/MTD.2016.9},
} 


@article{1994041233830 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and integration},
journal = {Distributed and Parallel Databases},
author = {Clamen, Stewart M.},
volume = {2},
number = {1},
year = {1994},
pages = {101 - 126},
issn = {09268782},
abstract = {Providing support for schema evolution allows existing databases to be adjusted for varying roles over time. This paper reflects on existing evolution support schemes and introduces a more general and functional mechanism to support schema evolution and instance adaptation for centralized and distributed object-oriented database systems. Our evolution support scheme is distinguished from previous mechanisms in that it is primarily concerned with preserving existing database objects and maintaining compatibility for old applications, while permitting a wider range of evolution operations. It achieves this by supporting schema versioning, allowing multiple representations of instances to persist simultaneously, and providing for programmer specification of how to adapt existing instances. The mechanism is general enough to provide much of the support necessarily for heterogeneous schema integration, as well as incorporating much of the features of object migration and replication.},
key = {Distributed database systems},
keywords = {Computer networks;Data handling;Data reduction;Data structures;Hierarchical systems;Interfaces (computer);Object oriented programming;Storage allocation (computer);},
note = {Heterogeneous schema integration;Instance adaptation;Object oriented database systems;Schema evolution;},
} 


@inproceedings{20154201400253 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in heterogeneous database architectures, A schema transformation approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {McBrien, Peter and Poulovassilis, Alexandra},
volume = {2348},
year = {2002},
pages = {484 - 499},
issn = {03029743},
address = {Toronto, ON, Canada},
abstract = {This paper presents a new approach to schema evolution, which combines the activities of schema integration and schema evolution into one framework. In previous work we have developed a general framework to support schema transformation and integration in heterogeneous database architectures. Here we show how this framework also readily supports evolution of source schemas, allowing the global schema and the query translation pathways to be easily repaired, as opposed to having to be regenerated, after changes to source schemas.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Query processing},
keywords = {Information systems;Information use;Systems engineering;},
note = {Global schemas;Heterogeneous database;New approaches;Query translations;Schema evolution;Schema integration;Schema transformation;},
} 


@inproceedings{20173504105498 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution for object-based accounting database systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Chen, Jia-Lin and McLeod, Dennis and OLeary, Daniel},
volume = {858 LNCS},
year = {1994},
pages = {40 - 52},
issn = {03029743},
address = {Palermo, Italy},
abstract = {When an (accounting) database schema does not meet the requirements of a firm, the schema must be changed. Such schema evolution can be considered as realizable via a sequence of operators. This research proceeds in the following three steps. First, we define a set of basic evolution schema operators and employ the evolution heuristics to guide the evolution process. Second, we explore how domain-specific knowledge can be used to guide the use of evolution operators to complete the evolution task. A well-known accounting data model is used here to guide the schema evolution process. Third, we discuss a tool built to implement the evolution operators, using the evolution heuristics and domain-specific knowledge.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Artificial intelligence},
keywords = {Computer science;Computers;},
note = {Database schemas;Domain-specific knowledge;Evolution operator;Evolution process;Object based;Schema evolution;Schema operators;},
} 


@inproceedings{20160902009435 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and versioning: A logical and computational characterisation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Franconi, Enrico and Grandi, Fabio and Mandreoli, Federica},
volume = {2065},
year = {2001},
pages = {85 - 99},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
abstract = {In this paper we study the logical and computational prop-erties of schema evolution and versioning support in object-oriented databases. To this end, we present the formalisation of a general model for an object base with evolving schemata and define the semantics of the provided schema change operations. We will then sketch how the encoding of such a framework in a suitable Description Logic will al-low the introduction and solution of interesting reasoning tasks at global database and single schema version levels.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Object-oriented databases},
keywords = {Computation theory;Data description;Modeling languages;Semantics;},
note = {Description logic;Formalisation;General model;Global database;Reasoning tasks;Schema changes;Schema evolution;Versioning;},
URL = {http://dx.doi.org/10.1007/3-540-48196-6_5},
} 


@inproceedings{20155001657628 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing schema evolution using a temporal object model},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Goralwalla, Iqbal A. and Szafron, Duane and Ozsu, M. Tamer and Peters, Randal J.},
volume = {1331},
year = {1997},
pages = {71 - 84},
issn = {03029743},
address = {Los Angeles, CA, United states},
abstract = {The issues of schema evolution and temporal object models are generally considered to be orthogonal and are handled independently. This is unrealistic because to properly model applications that need incremental design and experimentation (such as CAD, software design process), the evolutionary histories of the schema objects should be traceable. In this paper we propose a method for managing schema changes by exploiting the functionality of a temporal object model. The result is a uniform treatment of schema evolution and temporal support for many object database management systems applications that require both.<br/> &copy; 1997, Springer Verlag. All rights reserved.},
key = {Computer aided design},
keywords = {Application programs;Data mining;Object-oriented databases;Software design;},
note = {Database management;Evolutionary history;Incremental designs;Model application;Schema evolution;Software design process;Temporal objects;Temporal support;},
} 


@inproceedings{20160801964032 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On schema evolution in multidimensional databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Blaschka, Markus and Sapia, Carsten and Hofling, Gabriele},
volume = {1676},
year = {1999},
pages = {153 - 164},
issn = {03029743},
address = {Florence, Italy},
abstract = {Database systems offering a multidimensional schema on a logical level (e.g. OLAP systems) are often used in data warehouse environments. The user requirements in these dynamic application areas are subject to frequent changes. This implies frequent structural changes of the database schema. In this paper, we present a formal framework to describe evolutions of multidimensional schemas and their effects on the schema and on the instances. The framework is based on a formal conceptual description of a multidimensional schema and a corresponding schema evolution algebra. Thus, the approach is independent of the actual implementation (e.g. MOLAP or ROLAP). We also describe how the algebra enables a tool supported environment for schema evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Data warehouses},
keywords = {Algebra;},
note = {Database schemas;Dynamic applications;Formal framework;Logical levels;Multidimensional database;Multidimensional schemata;Schema evolution;User requirements;},
URL = {http://dx.doi.org/10.1007/3-540-48298-9_17},
} 


@inproceedings{20173204018238 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On behavioral schema evolution in object-oriented databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Morsi, Magdi M. A. and Navathe, Shamkant B. and Shilling, John},
volume = {779 LNCS},
year = {1994},
pages = {173 - 186},
issn = {03029743},
address = {Cambridge, United kingdom},
abstract = {This paper describes the effect of schema evolution operations on the implementation of methods. The effect of these operations is captured as temporary inconsistencies. Extensions for handling these temporary inconsistencies in our Graphical Object-Oriented Schema Environment prototype, called GOOSE are described. In GOOSE, the schema information is maintained as a set of system objects of system defined classes. In order to maintain these temporary inconsistencies, the system classes have been augmented with two classes, namely Uses and Stubs, as specialization of the system class Class. Furthermore, schema evolution operations for explicitly resolving these inconsistencies are defined.<br/> &copy; 1994, Springer Verlag. All rights reserved.},
key = {Object-oriented databases},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Graphical objects;Schema evolution;Schema information;},
} 


@inproceedings{20160701938089 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multiobjects to ease schema evolution in an OODBMS},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Al-Jadir, Lina and Leonard, Michel},
volume = {1507},
year = {1998},
pages = {316 - 333},
issn = {03029743},
address = {Singapore, Singapore},
abstract = {The multiobject mechanism is a pertinent way to implement specialization in an object database and differs from the classical mechanism used in most object-oriented database systems. It supports multiple instantiation, automatic classification and object migration. Consequently it is well suited to take into account schema evolution. It makes schema changes more pertinent, easier to implement, and less expensive than with the classical implementation of specialization indeed. The multiobject mechanism is implemented in the F2 database system which supports schema evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 1998.},
key = {Object-oriented databases},
keywords = {Data mining;Object oriented programming;},
note = {Automatic classification;Classical mechanism;It supports;Multi-objects;Multiobject;Object migration;Schema changes;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-540-49524-6_25},
} 


@inproceedings{20160801978582 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Primitive operations for schema evolution in ODMG databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Delgado, Cecilia and Samos, Jose and Torres, Manuel},
volume = {2817},
year = {2003},
pages = {226 - 237},
issn = {03029743},
address = {Geneve, Switzerland},
abstract = {Schema evolution is the process of applying changes to a schema in a consistent way and propagating these changes to the instances while the database is in operation. In this process there are two problems to consider: semantics of change and change propagation. In this paper, we study the problem of the semantics of change for the schema evolution defined with the ODMG object model. In this context, we provide a formal definition of this object model, we establish a set of axioms to ensure the consistency of a schema when it is modified, and we define a set of primitive operations that allow basic changes to be carried out on an ODMG schema. Other operations, which allow any kind of modification to be carried out, can be derived from these primitives.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
key = {Object-oriented databases},
keywords = {Semantics;},
note = {Change propagation;Formal definition;Object model;Primitive operations;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-540-45242-3_21},
} 


@article{1998494408835 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Temporal approach to managing schema evolution in object database systems},
journal = {Data and Knowledge Engineering},
author = {Goralwalla, Iqbal A. and Szafron, Duane and Ozsu, M.Tamer and Peters, Randal J.},
volume = {28},
number = {1},
year = {1998},
pages = {73 - 105},
issn = {0169023X},
abstract = {The issues of schema evolution and temporal object models are generally considered to be orthogonal and are handled independently. However, to properly model applications that need incremental design and experimentation, the evolutionary histories of the schema objects should be traceable rather than corrective so that historical queries can be supported. In this paper we propose a method for managing schema changes, and propagating these changes to object instances by exploiting the functionality of a temporal object model. The result is a uniform treatment of schema evolution and temporal support for many object database management systems applications that require both.},
key = {Database systems},
keywords = {Computer systems programming;Data structures;Object oriented programming;},
note = {Schema evolution;Temporal object models;},
URL = {http://dx.doi.org/10.1016/S0169-023X(98)00014-7},
} 


@article{20130616003983 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Class schema evolution for persistent object-oriented software: Model, empirical study, and automated support},
journal = {IEEE Transactions on Software Engineering},
author = {Piccioni, Marco and Oriol, Manuel and Meyer, Bertrand},
volume = {39},
number = {2},
year = {2013},
pages = {184 - 196},
issn = {00985589},
abstract = {With the wide support for object serialization in object-oriented programming languages, persistent objects have become commonplace and most large object-oriented software systems rely on extensive amounts of persistent data. Such systems also evolve over time. Retrieving previously persisted objects from classes whose schema has changed is, however, difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses these issues through an IDE-integrated approach that handles class schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of potentially corrupt objects. This paper describes a model for class attribute changes, a measure for class evolution robustness, four empirical studies, and the design and implementation of the ESCHER system. &copy; 1976-2012 IEEE.<br/>},
key = {Object oriented programming},
keywords = {Computer systems programming;Integrodifferential equations;},
note = {Design and implementations;Object serialization;Object-oriented software systems;persistence;Schema evolution;serialization;Transformation functions;Versioning;},
URL = {http://dx.doi.org/10.1109/TSE.2011.123},
} 


@inproceedings{1996363245248 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Consistent view removal in transparent schema evolution systems},
journal = {Proceedings of the IEEE International Workshop on Research Issues in Data Engineering},
author = {Crestana-Taube, Viviane and Rundensteiner, Elke A.},
year = {1996},
pages = {138 - 147},
address = {New Orleans, LA, USA},
abstract = {We have developed the Transparent Schema Evolution (TSE) system that, simulating schema evolution using object-oriented views, allows for the interoperability of applications with diverse and even changing requirements. TSE relieves users of the risk of making existing application programs obsolete when run against the modified schema, because the old view schema is maintained while a new view schema is generated to capture the changes desired by the user. However, TSE may be generating a large number of schema versions (object-oriented view schemata) over time, resulting in an excessive build-up of classes and underlying object instances - some of which may potentially no longer be in use. In this paper, we propose to solve this problem by developing techniques for effective and consistent schema removal. First, we characterize four potential problems of schema consistency that could be caused by removal of a single virtual class; and then outline our solution approach for each of these problems. Second, we demonstrate that view schema removal is sensitive to the order in which individual classes are processed. Our solution to this problem is the development of a dependency graph model for capturing the class relationships, used as a foundation for selecting among removal sequences. Designed to optimize the performance of the TSE system by effective schema version removal, the proposed techniques will enable more effective interoperability among evolving software applications.},
key = {Query languages},
keywords = {Computer operating systems;Computer simulation;Computer software;Data structures;Graph theory;Object oriented programming;Optimization;User interfaces;},
note = {Interoperability;Object oriented view;Single virtual class;Transparent schema evolution system;View schema removal;},
} 


@inproceedings{20171803617145 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Cleager: Eager schema evolution in NoSQL document stores},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
author = {Scherzinger, Stefanie and Klettke, Meike and Storl, Uta},
volume = {241},
year = {2015},
pages = {659 - 662},
issn = {16175468},
address = {Hamburg, Germany},
abstract = {Schema-less NoSQL data stores offer great flexibility in application development, particularly in the early stages of software design. Yet over time, software engineers struggle with the heavy burden of dealing with increasingly heterogeneous data. In this demo we present Cleager, a framework for eagerly managing schema evolution in schema-less NoSQL document stores. Cleager executes declarative schema modification operations as MapReduce jobs on the Google Cloud Platform. We present different scenarios that require data migration, such as adding, removing, or renaming properties of persisted objects, as well as copying and moving them between objects. Our audience can declare the required schema migration operations in the Cleager console, and then verify the results in real time.<br/>},
key = {Software design},
keywords = {Application programs;},
note = {Application development;Cloud platforms;Data migration;Data store;Heterogeneous data;Map-reduce;Real time;Schema evolution;},
} 


@inproceedings{20163302710922 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolutionary database design: Enhancing data abstraction through database modularization to achieve graceful schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Guedes, Gustavo Bartz and Baioco, Gisele Busichia and de Oliveira Moraes, Regina Lucia},
volume = {9827 LNCS},
year = {2016},
pages = {355 - 369},
issn = {03029743},
address = {Porto, Portugal},
abstract = {Software systems are not immutable through time, especially in modern development methods such as agile ones. Therefore, a software system is constantly evolving. Besides coding, the database schema design also plays a major role. Changes in requirements will probably affect the database schema, which will have to be modified to accommodate them. In a software system, changes to the database schema are costly, due to application&rsquo;s perspective, where data semantics needs to be maintained. This paper presents a process to conduct database schema evolution by extending the database modularization to work in an evolutionary manner. The evolutionary database modularization process is executed during conceptual design, improving the abstraction capacity of generated data schema and results in loosely coupled database elements, organized in database modules. Finally, we present the process execution in an agile project.<br/> &copy; Springer International Publishing Switzerland 2016.},
key = {Database systems},
keywords = {Abstracting;Agile manufacturing systems;Application programs;Conceptual design;Expert systems;Modular construction;Semantics;},
note = {Agile methods;Data abstraction;Database schemas;Evolutionary database design;Modern development;Process execution;Schema evolution;Software systems;},
URL = {http://dx.doi.org/10.1007/978-3-319-44403-1_22},
} 


@inproceedings{20183705813389 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dealing with version pertinence to design an efficient schema evolution framework},
journal = {Proceedings - IDEAS 1998: International Database Engineering and Applications Symposium},
author = {Benatallah, B. and Tari, Z.},
year = {1998},
pages = {24 - 33},
address = {Cardiff, Wales, United kingdom},
abstract = {The paper addresses the design of a schema evolution framework enabling an efficient management of object versions. This framework is based on the adaptation and extension of two main schema evolution approaches, that is the approaches based on schema modification and those based on schema versioning. The framework provides an integrated environment to support different levels of adaptation (such as, modification and versioning at the schema level, conversion, object versioning, and emulation at the instance level). In addition, the authors introduce the concept of class/schema version pertinence enabling the database administrator to judge the pertinence of versions with regard the application programs. Finally, they provide operations for immediate refreshing of a database to enable an efficient manipulation of versions by a large number of application programs.<br/> &copy; 1998 IEEE.},
key = {Application programs},
keywords = {Database systems;},
note = {Database administrators;Efficient managements;Integrated environment;Object versioning;Schema evolution;Schema versioning;Versioning;},
URL = {http://dx.doi.org/10.1109/IDEAS.1998.694348},
} 


@inproceedings{20143618128297 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A toolbox for conservative XML schema evolution and document adaptation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Amavi, Joshua and Chabin, Jacques and Halfeld-Ferrari, Mirian and Rety, Pierre},
volume = {8644 LNCS},
number = {PART 1},
year = {2014},
pages = {299 - 307},
issn = {03029743},
address = {Munich, Germany},
abstract = {We propose an algorithm that computes a mapping to obtain a conservative extension of original local schemas. This mapping ensures schema evolution and guides the construction of a document translator. &copy; 2014 Springer International Publishing Switzerland.<br/>},
key = {Expert systems},
keywords = {Mapping;},
note = {Conservative extensions;Schema evolution;XML schema evolutions;},
URL = {http://dx.doi.org/10.1007/978-3-319-10073-9_24},
} 


@inproceedings{20152801024387 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution for XML: A consistency-preserving approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bouchou, Beatrice and Duarte, Denio and Halfeld Ferrari Alves, Mirian and Laurent, Dominique and Musicante, Martin A.},
volume = {3153},
year = {2004},
pages = {876 - 888},
issn = {03029743},
address = {Prague, Czech republic},
abstract = {This paper deals with updates of XML documents that satisfy a given schema, e.g., a DTD. In this context, when a given update violates the schema, it might be the case that this update is accepted, thus implying to change the schema. Our method is intended to be used by a data administrator who is an expert in the domain of application of the database, but who is not required to be a computer science expert. Our approach consists in proposing different schema options that are derived from the original one. The method is consistency-preserving: documents valid with respect to the original schema remain valid. The schema evolution is implemented by an algorithm (called GREC) that performs changes on the graph of a finite state automaton and that generates regular expressions for the modified graphs. Each regular expression proposed by GREC is a choice of schema given to the administrator.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.},
key = {XML},
keywords = {Pattern matching;},
note = {Data administrators;Regular expressions;Schema evolution;},
} 


@inproceedings{20154801616501 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in data warehousing environments-A schema transformation-based approach},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Fan, Hao and Poulovassilis, Alexandra},
volume = {3288},
year = {2004},
pages = {639 - 653},
issn = {03029743},
address = {Shanghai, China},
abstract = {In heterogeneous data warehousing environments, autonomous data sources are integrated into a materialised integrated database. The schemas of the data sources and the integrated database may be expressed in different modelling languages. It is possible for either the data source schemas or the warehouse schema to evolve. This evolution may include evolution of the schema, or evolution of the modelling language in which the schema is expressed, or both. In such scenarios, it is important for the integration framework to be evolvable, so that the previous integration effort can be reused as much as possible. This paper describes how the AutoMed heterogeneous data integration toolkit can be used to handle the problem of schema evolution in heterogeneous data warehousing environments. This problem has been addressed before for specific data models, but AutoMed has the ability to cater for multiple data models, and for changes to the data model.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
key = {Data integration},
keywords = {Data warehouses;Metadata;Modeling languages;Warehouses;},
note = {Autonomous data sources;Heterogeneous data;Heterogeneous data integrations;Integrated database;Integration frameworks;Multiple data;Schema evolution;Schema transformation;},
} 


@inproceedings{20114514497967 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Context schema evolution in context-aware data management},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Quintarelli, Elisa and Rabosio, Emanuele and Tanca, Letizia},
volume = {6998 LNCS},
year = {2011},
pages = {290 - 303},
issn = {03029743},
address = {Brussels, Belgium},
abstract = {Pervasive access - often by means of mobile devices - to the massive amount of available (Web) data suggests to deliver, anywhere at any time, exactly the data that are needed in the current specific situation. The literature has introduced the possibility to describe the context in which the user is involved, and to tailor the available data on its basis. In this paper, after having formally defined the context schema - a representation for the contexts which are to be expected in a given application scenario - a strategy to manage context schema evolution is developed, by introducing a sound and complete set of operators. &copy; 2011 Springer-Verlag.<br/>},
key = {Data mining},
keywords = {Information management;},
note = {Application scenario;In contexts;Pervasive access;Schema evolution;Sound and complete;},
URL = {http://dx.doi.org/10.1007/978-3-642-24606-7_22},
} 


@inproceedings{20154801616735 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Lossless conditional schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Jensen, Ole G. and Bohlen, Michael H.},
volume = {3288},
year = {2004},
pages = {610 - 623},
issn = {03029743},
address = {Shanghai, China},
abstract = {Conditional schema changes change the schema of the tuples that satisfy the change condition. When the schema of a relation changes some tuples may no longer fit the current schema. Handling the mismatch between the intended schema of tuples and the recorded schema of tuples is at the core of a DBMS that supports schema evolution. We propose to keep track of schema mismatches at the level of individual tuples, and prove that evolving schemas with conditional schema changes, in contrast to database systems relying on data migration, are lossless when the schema evolves. The lossless property is a precondition for a flexible semantics that allows to correctly answer general queries over evolving schemas. The key challenge is to handle attribute mismatches between the intended and recorded schema in a consistent way. We provide a parametric approach to resolve mismatches according to the needs of the application. We introduce the mismatch extended completed schema (MECS) which records attributes along with their mismatches, and we prove that relations with MECS are lossless.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
key = {Database systems},
keywords = {Query processing;Semantics;},
note = {Change conditions;Data migration;Keep track of;Lossless;Parametric approach;Schema changes;Schema evolution;},
} 


@inproceedings{20154201410052 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multitemporal conditional schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Jensen, Ole G. and Bohlen, Michael H.},
volume = {3289},
year = {2004},
pages = {441 - 454},
issn = {03029743},
address = {Shanghai, China},
abstract = {Schema evolution is the ability of the database to respond to changes in the real world by allowing the schema to evolve. The multidimensional conditionally evolving schema(MD-CES) is a conceptual model for conditional schema changes, which modify the schema of those tuples that satisfy the change condition. The MD-CES is lossless and preserves schemas, but has an exponential space complexity. In this paper we restrict conditional schema changes to timestamp attributes. Specifically, we develop 1D-CES for schema versioning over one time dimension, and 2D-CES for schema versioning over two time dimensions. We show that the space complexity of these new evolution models is linear or polynomial. 1D-CES and 2D-CES are compared to temporal schema versioning, and we show that, unlike valid time versioning, they are lossless and achieve the same space complexity as temporal versioning if the schema changes are ordered.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
key = {Artificial intelligence},
keywords = {Computer science;Computers;},
note = {Change conditions;Conceptual model;Evolution models;Schema evolution;Schema versioning;Space complexity;Temporal versioning;Time dimension;},
} 


@inproceedings{20102212963866 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {HECATAEUS: Regulating schema evolution},
journal = {Proceedings - International Conference on Data Engineering},
author = {Papastefanatos, George and Vassiliadis, Panos and Simitsis, Alkis and Vassiliou, Yannis},
year = {2010},
pages = {1181 - 1184},
issn = {10844627},
address = {Long Beach, CA, United states},
abstract = {HECATAEUS is an open-source software tool for enabling impact prediction, what-if analysis, and regulation of relational database schema evolution. We follow a graph theoretic approach and represent database schemas and database constructs, like queries and views, as graphs. Our tool enables the user to create hypothetical evolution events and examine their impact over the overall graph before these are actually enforced on it. It also allows definition of rules for regulating the impact of evolution via (a) default values for all the nodes of the graph and (b) simple annotations for nodes deviating from the default behavior. Finally, HECATAEUS includes a metric suite for evaluating the impact of evolution events and detecting crucial and vulnerable parts of the system. &copy; 2010 IEEE.<br/>},
key = {Open source software},
keywords = {Graph theory;Open systems;Query languages;Query processing;},
note = {Database schemas;Default behavior;Default values;Graph theoretic approach;Metric suites;Relational database schemata;Schema evolution;What-if Analysis;},
URL = {http://dx.doi.org/10.1109/ICDE.2010.5447778},
} 


@inproceedings{20140617270369 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Beyond database schema evolution},
journal = {IMETI 2008 - International Multi-Conference on Engineering and Technological Innovation, Proceedings},
author = {Yannakoudakis, E.J. and Diamantis, I.K.},
volume = {1},
year = {2008},
pages = {245 - 250},
address = {Orlando, FL, United states},
abstract = {This paper presents the definition of a new schema model called FDB (Framework DataBase). In modem database environments, there is a need for continuous evolution of database schemas, in order to reflect the changes that happen in the real world. The relational model permits this only through schema modification, which usually leads to time-consuming reorganization of data. On the other hand, schema evolution is supported by the object-oriented model but the commercial systems based on it present certain limitations, especially when a database is already "occupied" with data. The proposed model aims to cover the needs of dynamically evolving database environments and presents an important advantage, compared to the traditional database models: the use of FDB structures give rise to a universal logical schema, which can host any conceptual schema. In practice, this means dynamic definition and modification of entities and their corresponding attributes, without any changes in the schema structure. Copyright &copy; 2008 by the International Institute of Informatics and Systemics.<br/>},
key = {Object-oriented databases},
keywords = {Data structures;Engineering;Industrial engineering;},
note = {Commercial systems;Conceptual schemas;Data basis;Dynamic data;Evolving database;Object oriented model;Schema evolution;Schema Modification;},
} 


@inproceedings{20110113541097 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ontology and schema evolution in data integration: Review and assessment},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Kondylakis, Haridimos and Flouris, Giorgos and Plexousakis, Dimitris},
volume = {5871 LNCS},
number = {PART 2},
year = {2009},
pages = {932 - 947},
issn = {03029743},
abstract = {The development of new techniques and the emergence of new high-throughput tools have led to a new information revolution. The amount and the diversity of the information that need to be stored and processed have led to the adoption of data integration systems in order to deal with information extraction from disparate sources. The mediation between traditional databases and ontologies has been recognized as a cornerstone issue in bringing in legacy data with formal semantic meaning. However, our knowledge evolves due to the rapid scientific development, so ontologies and schemata need to change in order to capture and accommodate such an evolution. When ontologies change, these changes should somehow be rendered and used by the pre-existing data integration systems, a problem that most of the integration systems seem to ignore. In this paper, we review existing approaches for ontology/schema evolution and examine their applicability in a state-of-the-art, ontology-based data integration setting. Then, we show that changes in schemata differ significantly from changes in ontologies. This strengthens our position that current state of the art systems are not adequate for ontology-based data integration. So, we give the requirements for an ideal data integration system that will enable and exploit ontology evolution. &copy; Springer-Verlag 2009.<br/>},
key = {Data integration},
keywords = {Data mining;Mapping;Ontology;Semantics;},
note = {Data integration system;Formal Semantics;Information revolution;Integration systems;Ontology evolution;Schema evolution;Scientific development;State-of-the-art system;},
URL = {http://dx.doi.org/10.1007/978-3-642-05151-7_14},
} 


@inproceedings{20102713049571 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Open and dynamic schema evolution in content-intensive web applications},
journal = {WEBIST 2006 - 2nd International Conference on Web Information Systems and Technologies, Proceedings},
author = {Bossung, Sebastian and Sehring, Hans-Werner and Hupe, Patrick and Schmidt, Joachim W.},
volume = {IT/WIA},
year = {2006},
pages = {109 - 116},
address = {Setubal, Portugal},
abstract = {Modern information systems development is a complex task for it must fulfill a large variety of application-and architecture-oriented requirements. Furthermore, such requirements often are a moving target for the developer, not only because the system has to stay open to a constantly changing application domain, but also because new requirements are added during the extremely long lifetime of such information systems. To make things worse, modern information systems are operated in a 24x7-modus which generates the pressure of highly dynamic, almost online system evolution. A main source of problems such development projects struggle with originates from the lack of a systematic subdivision of large software systems into manageable modules. As a consequence developers are traditionally involved in a complex patchwork of manual efforts to keep the various parts of the system in sync with each other and with the system's requirements. In this paper we outline our approach to information system development which is based on a model for Conceptual Content Management (CCM). Our CCM approach profits from the dynamic, model-driven generation of smaller modules, which can be combined automatically into the full system. The generation process uses a CCM model of the application domain(s) from which our compiler framework dynamically generates the schema-dependent parts of the system. Due to the dynamic nature of this generation process, we are able to provide adequate support for both schema evolution and personalization of such a system. We have successfully employed the CCM approach to the development of complex web information systems. We give a brief account of CCM development and present an application example. &copy; 2010.<br/>},
key = {Information management},
keywords = {Information systems;Information use;Memory architecture;Online systems;},
note = {Application examples;Information system development;Information systems development;Large software systems;Module-based;Schema evolution;Software generation;Web information systems;},
} 


@inproceedings{20131816282760 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of XML schema evolution on valid documents},
journal = {Proceedings of the Interntational Workshop on Web Information and Data Management WIDM},
author = {Guerrini, Giovanna and Mesiti, Marco and Rossi, Daniele},
year = {2005},
pages = {39 - 44},
address = {Bremen, Germany},
abstract = {In this paper we investigate the problem of XML Schema evolution. We first discuss the different kinds of changes that may be needed on an XML Schema. Then, we investigate how to minimize document revalidation, that is, detecting the document parts potentially invalidated by the schema changes that should be revalidated. Copyright 2005 ACM.<br/>},
key = {XML},
keywords = {Knowledge management;},
note = {Revalidation;Schema changes;Schema evolution;XML schema evolutions;XML schemas;},
URL = {http://dx.doi.org/10.1145/1097047.1097056},
} 


@article{20070810436650 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An online bibliography on schema evolution},
journal = {SIGMOD Record},
author = {Rahm, Erhard and Bernstein, Philip A.},
volume = {35},
number = {4},
year = {2006},
pages = {30 - 31},
issn = {01635808},
abstract = {We briefly motivate and present a new online bibliography on schema evolution, an area which has recently gained much interest in both research and practice.},
key = {Online systems},
keywords = {Bibliographic retrieval systems;Research and development management;},
note = {Practice;Schema evolution;},
URL = {http://dx.doi.org/10.1145/1228268.1228273},
} 


@inproceedings{20131716230552 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Enabling migration of enterprise applications in SaaS via progressive schema evolution},
journal = {Lecture Notes in Business Information Processing},
author = {Yan, Jianfeng and Zhang, Bo},
volume = {74 LNBIP},
year = {2011},
pages = {229 - 256},
issn = {18651348},
abstract = {Update of applications in SaaS is expected to be a continuous efforts and cannot be done overnight or over the weekend. In such migration efforts, users are trained and shifted from one existed version to another new version successively. There is a long period of time when both versions of applications coexist. Maintenance of two systems with both existed and new version at the same time is not a cost efficient option and such two systems may suffer from slow response time due to continuous synchronization with each other. In this paper, we focus on how to enable the migration of enterprise applications in SaaS via progressive evolved schema. Instead of maintenance with two systems, our solution is to build a multi-version applications supported system by designing an series of intermediate schemas which are optimized for mixed workloads from both existed and emerging users. With an application migration schedule, an genetic algorithm is used to find out the more effective intermediated schema as well as migration paths and schedule. A key advantage of our approach is optimum performance during the long migration period while maintaining the same level of data movement required by the migration. We evaluated the proposed progressive migration approach on a TPCW benchmark and experimental results validated its effectiveness of across a variety of scenarios. They demonstrate that the schema evolution based application migration could bring about 200% performance gain comparing to the systems with either existed old version or targeted new version. &copy; Springer-Verlag Berlin Heidelberg 2011.<br/>},
key = {Benchmarking},
keywords = {Application programs;Continuous time systems;Genetic algorithms;Software as a service (SaaS);},
note = {Enterprise applications;Intermediate schema;Multi-version;Progressive migration;Schema evolution;TPC-W benchmark;},
URL = {http://dx.doi.org/10.1007/978-3-642-19294-4_10},
} 


@article{1992090371958 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in object-oriented database systems},
journal = {Data and Knowledge Engineering},
author = {Nguyen, G.T. and Rieu, D.},
volume = {4},
number = {1},
year = {1989},
pages = {43 - 67},
issn = {0169023X},
abstract = {This paper gives an overview of current research efforts directed towards evolving data definitions in object-oriented database systems. The emphasis is on their ability to support two complementary aspects: supporting evolving schemas, and propagating the changes on the object instances. Several projects are analyzed: Cadb, Encore, GemStone, Orion, and Sherpa. Current results indicate that if most of them provide schema evolution facilities, they seldom support automatic propagation mechanisms. A proposal is described that enables Sherpa to fully support the propagation of changes and the dynamic classification of the instances whose class definitions are modified. This approach is an extension of techniques used in artificial intelligence for knowledge representation. It extends previous classification mechanisms with a dynamic capability which adequately supports evolving class definitions and instances.},
key = {Database Systems},
keywords = {Artificial Intelligence;},
note = {Data Definitions;Inheritance Propagation;Knowledge Representation;Object Oriented Database Systems;Schema Evolution;},
URL = {http://dx.doi.org/10.1016/0169-023X(89)90004-9},
} 


@inproceedings{20155101694766 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic schema evolution management using version in temporal object-oriented databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {de Matos Galante, Renata and da Silva Roma, Adriana Bueno and Jantsch, Anelise and Edelweiss, Nina and dos Santos, Clesio Saraiva},
volume = {2453},
year = {2002},
pages = {524 - 533},
issn = {03029743},
address = {Aix-en-Provence, France},
abstract = {In this paper, an analysis of the schema evolution process in object oriented databases is made using an object oriented data model that supports temporal features and versions definition - the Temporal Versions Model. A meta schema structure is defined to store information concerning to evolutionary schema states, as well as their classes, attributes and relationships. An implementation proposal is presented, combining specification and manipulation mechanisms including version and time concepts. Two alternatives are defined for the database extension management: multi-pool for schema versioning and single-pool for class versioning. Concerning the physical representation, both approaches can be used in the same application.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Object-oriented databases},
keywords = {Expert systems;},
note = {Class versioning;Information concerning;Multi-pool;Object oriented data;Schema evolution;Schema versioning;Temporal features;Temporal object-oriented database;},
} 


@inproceedings{20154801605343 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Content schema evolution in the coremedia content application platform CAP},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wienberg, Axel and Ernst, Matthias and Gawecki, Andreas and Kummer, Olaf and Wienberg, Frank and Schmidt, Joachim W.},
volume = {2287},
year = {2002},
pages = {712 - 721},
issn = {03029743},
address = {Prague, Czech republic},
abstract = {Based on experience gathered with several releases of the CoreMedia Content Application Platform (CAP), we argue that a modern, generalized Content Management System should, as database systems do, support explicit content schemata. To control the inevitable evolution of the content schema, the schema should be subject to configuration management together with the actual content. We propose a two-layered approach to content schema evolution consisting of &ndash; a system level responsible for bookkeeping and integrity issue detection, and &ndash; a semi-automatic application level responsible for resolving schemarelated issues. A prototype using the proposed approach has been successfully implemented at CoreMedia.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
key = {Database systems},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Application platforms;Configuration management;Content management system;Layered approaches;Schema evolution;Semi-automatic applications;System levels;},
} 


@inproceedings{20132816475635 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database schema evolution using EVER diagrams},
journal = {Proceedings of the Workshop on Advanced Visual Interfaces AVI},
author = {Liu, Chien-Tsai and Chang, Shi-Kuo and Chrysanthis, Panos K.},
year = {1994},
pages = {123 - 132},
address = {Bari, Italy},
abstract = {We present an approach to schema evolution through changes to the ER diagram representing the schema of a database. In order to facilitate changes to the ER schema we enhance the graphical constructs used in ER diagrams, and develop EVER, an Evolutionary ER diagram for specifying the derivation relationships between schema versions, relationships among attributes, and the conditions for maintaining consistent views of programs. In this paper, we demonstrate the mapping of the EVER diagram into an underlying database and the construction of database views for schema versions. Through the reconstruction of views after database reorganization, changes to an ER diagram can be made transparent to the application programs while all objects in the database remain accessible to the application programs. The EVER system can serve as a front-end for object-oriented databases. &copy; 1994 ACM.<br/>},
key = {Object-oriented databases},
keywords = {Application programs;},
note = {Database schemas;Database views;ER diagrams;Front end;Schema evolution;},
URL = {http://dx.doi.org/10.1145/192309.192338},
} 


@inproceedings{20173504082374 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database schema evolution through the specification and maintenance of changes on entities and relationships},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Liu, Chien-Tsai and Chrysanthis, Panos K. and Chang, Shi-Kuo},
volume = {881 LNCS},
year = {1994},
pages = {132 - 151},
issn = {03029743},
address = {Manchester, United kingdom},
abstract = {A flexible database system needs to support changes to its schema in order to facilitate the requirements of new applications and to support interoperability within a multidatabase system. In this paper, we present an approach to schema evolution through changes to the Entity-Relationship (ER) schema of a database. We enhance the graphical constructs used in ER diagrams, and develop EVER, an EVolutionary ER diagram for specifying the derivation relationships between schema versions, relationships among attributes, and the conditions for maintaining consistent views of programs. Algorithms are presented for mapping the EVER diagram into the underlying database and constructing database views for schema versions. Through the reconstruction of views after database reorganization, changes to an ER diagram can be made transparent to the application programs while all objects in the database remain accessible to the application programs.<br/> &copy; Springer-Verlag Berlin Heidelberg 1994.},
key = {Database systems},
keywords = {Application programs;Data mining;Interoperability;Professional aspects;Systems engineering;},
note = {Database schemas;Database views;Entity-relationship;ER diagrams;Multi-database systems;New applications;Schema evolution;},
} 


@inproceedings{20133816756262 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Content schema evolution in the CoreMedia Content Application Platform CAP},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wienberg, Axel and Ernst, Matthias and Gawecki, Andreas and Kummer, Olaf and Wienberg, Frank and Schmidt, Joachim W.},
volume = {2287 LNCS},
year = {2002},
pages = {712 - 721},
issn = {03029743},
address = {Prague, Czech republic},
abstract = {Based on experience gathered with several releases of the CoreMedia Content Application Platform (CAP), we argue that a modern, generalized Content Management System should, as database systems do, support explicit content schemata. To control the inevitable evolution of the content schema, the schema should be subject to configuration management together with the actual content. We propose a two-layered approach to content schema evolution consisting of - a system level responsible for bookkeeping and integrity issue detection, and - a semi-automatic application level responsible for resolving schema-related issues. A prototype using the proposed approach has been successfully implemented at CoreMedia. &copy; Springer-Verlag 2002.<br/>},
key = {Database systems},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Application platforms;Configuration management;Content management system;Layered approaches;Schema evolution;Semi-automatic applications;System levels;},
} 


@inproceedings{20180204625441 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic workflow schema evolution based on workflow type versioning and workflow migration},
journal = {Proceedings - 1999 IFCIS International Conference on Cooperative Information Systems, CoopIS 1999},
author = {Kradolfer, Markus and Geppert, Andreas},
year = {1999},
pages = {104 - 114},
address = {Edinburgh, United kingdom},
abstract = {An important yet open problem in workflow management is the evolution of workflow schemas, i.e., the creation, deletion and modification of workflow types in such a way that the schema remains correct. This problem is aggravated when instances of modified workflow types are active at the time of modification, because any workflow instance has to conform to the definition of its type. This paper presents a framework for dynamic workflow schema evolution that is based on workflow type versioning and workflow migration. Workflow types can be versioned, and a new version can be derived from an existing one by applying modification operations. Workflow type versions allow to handle active instances in an elegant way whenever a schema is modified. If possible, an affected workflow instance is migrated to the new version of its type. Otherwise, it continues to execute under its old type. We introduce correctness criteria that must be met by workflow schemas and workflow schema modification operations. We also define under which conditions the migration of workflow instances to new workflow type versions is allowed.<br/>},
key = {Multi agent systems},
keywords = {Information systems;Information use;Work simplification;},
note = {Correctness criterion;Dynamic workflow;Schema evolution;Versioning;Workflow instances;Workflow managements;},
URL = {http://dx.doi.org/10.1109/COOPIS.1999.792162},
} 


@inproceedings{1997173560918 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reusability and schema evolution in object-oriented query models},
journal = {American Society of Mechanical Engineers, Petroleum Division (Publication) PD},
author = {Alhajj, Reda and Polat, Faruk},
volume = {74},
number = {2},
year = {1996},
pages = {21 - 29},
address = {Montpellier, Fr},
abstract = {In this paper, we have benefited from having an object algebra maintaining closure that makes it possible to have the output from a query persistent in the hierarchy. Based on this, it is possible to maximize reusability in object-oriented databases. Furthermore, we show how the object algebra is utilized to handle basic schema evolution functions without requiring any special set of built-in functions to serve the purpose. The invariants and the conflict resolving rules are specified. It is also shown how other schema functions are derivable from the basic ones.},
key = {Data structures},
keywords = {Algebra;Data processing;Database systems;Hierarchical systems;Object oriented programming;Query languages;},
note = {Conflict resolution;Object oriented databases;Object oriented query model;Reusability;Schema evolution;},
} 


@inproceedings{20154801629729 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {View mechanism for schema evolution in object-oriented DBMS},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bellahsene, Zohra},
volume = {1094},
year = {1996},
pages = {18 - 35},
issn = {03029743},
address = {Edinburgh, United kingdom},
abstract = {This paper discusses the topic of using view mechanism to simulate schema modifications without database reorganisation in Object Oriented Database Systems. Our approach allows each user to specify the schema modifications to his own virtual schema rather than to the base schema shared by many users. One of the main advantages provided by this approach is the preservation of the independence of existing application programs from the schema evolution. The most important issue concerns the control and sharing of the information introduced by capacity-augmenting views. Furthermore, capacity augmenting schema modifications cannot be unambiguously propagated to the base schema when the related virtual class is derived from several classes. This paper proposes a solution based on the definition of a multi-level schema architecture, emphasising: (i) The integration of this information into a federated schema. (ii) Improvement of the sharing and re-use of information between views Furthermore, we argue that view mechanism capabilities must be enhanced in order to be used as a uniform framework to manipulate both the schema and the database, thereby providing full data independence.<br/> &copy; Springer-Verlag Berlin Heidelberg 1996.},
key = {Object-oriented databases},
keywords = {Application programs;Information use;},
note = {Capacity-augmenting views;Data independence;Object oriented;Object views;Reorganisation;Schema evolution;Uniform framework;Virtual schema;},
} 


@article{1997273646528 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Axiomatic model of dynamic schema evolution in objectbase systems},
journal = {ACM Transactions on Database Systems},
author = {Peters, Randal J. and Ozsu, M.Tamer},
volume = {22},
number = {1},
year = {1997},
pages = {75 - 114},
issn = {03625915},
abstract = {A sound and complete axiomatic model is proposed for dynamic schema evolution (DSE) in objectbase systems (OBSs) that supports the fundamental concepts of object-oriented computing such as subtyping and property inheritance. The model can infer all schema relationships from two identified input sets associated with each type called the essential supertypes and essential properties. These sets are typically specified by schema designers but can be automatically supplied within an OBS. The inference mechanism performed by the model has a proven termination.},
key = {Database systems},
keywords = {Algorithms;Object oriented programming;},
note = {Axiomatic model;Dynamic schema evolution (DSE);Objectbase system (OBS);},
URL = {http://dx.doi.org/10.1145/244810.244813},
} 


@inproceedings{20132616442760 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A framework for customizable schema evolution in object-oriented databases},
journal = {Proceedings of the International Database Engineering and Applications Symposium, IDEAS},
author = {Rashid, Awais},
year = {2003},
pages = {342 - 346},
issn = {10988068},
address = {Hong Kong, China},
abstract = {This paper describes an evolution framework supporting customization of the schema evolution and instance adaptation approaches in an object database management system. The framework is implemented as an integral part of an interpreter for a language with a versioned type system and employs concepts from object-oriented frameworks and aspect-oriented programming to support flexible changes. Some example customizations currently implemented with the framework are also described. &copy; 2003 IEEE.<br/>},
key = {Object oriented programming},
keywords = {Aspect oriented programming;Object-oriented databases;Program interpreters;},
note = {Customizable;Integral part;Object-oriented frameworks;Schema evolution;Type systems;},
URL = {http://dx.doi.org/10.1109/IDEAS.2003.1214951},
} 


@inproceedings{1999054498905 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Case study in supporting schema evolution of complex engineering information systems},
journal = {Proceedings - IEEE Computer Society's International Computer Software and Applications Conference},
author = {Jahnke, J.-H. and Nickel, U.A. and Wagenblabt, D.},
year = {1998},
pages = {513 - 520},
issn = {07303157},
address = {Vienna, Austria},
abstract = {Information systems have to evolve continually in order to keep up with emerging requirements. Various problems arise with each such evolution step, e.g. the modification of the application's conceptual data structure, the migration of existing data, the adaption of application code, and the modification of technical documentation. Most database systems provide only limited support for schema evolution while problems like data migration and application migration are tackled manually by the programmers. This evolution process is unsatisfactory for a number of novel complex evolutionary information systems (CEIS) in the area of business and engineering applications. This paper describes our experiences with a case study in developing a CEIS in the domain of analysis and design of mixed signal printed circuit boards. We show that a meta schema approach combined with a well-defined set of schema transformations is a practical way to cope with evolution. Based on this case study, we distinguish application specific from reusable architectural components and propose a systematic approach of building CEIS.},
key = {Information technology},
keywords = {Computer software reusability;Data structures;Database systems;Graph theory;Large scale systems;Printed circuit boards;Software engineering;},
note = {Complex evolutionary information systems;Engineering information system;Schema evolution;},
} 


@inproceedings{20175304593875 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using persistence technology to control schema evolution},
author = {Connor, R.C.H. and Cutts, Q.I. and Kirby, G.N.C. and Morrison, R.},
volume = {Part F129433},
year = {1994},
pages = {441 - 446},
address = {Phoenix, AZ, United states},
abstract = {Traditional database technology may be extended by taking advantage of the facilities of an integrated persisted programming environment. This paper focuses on how such an environment may be used to provide new solutions to a long standing problem in traditional databases, that of schema evolution. A general mechanism is first described, followed by a description of a specific schema editing tool. The persistent environment provides an underlying technology which allows the schema editor to locate and change, either manually or automatically, all affected program and data. The advantages of the mechanism are that it provides understandable semantics for evolution by controlling when the changes are made and by ensuring that changes to schema, program and data are consistent and made in lock step, it is shown how these changes together may be grouped as a transaction within a live system; furthermore, the accommodation of lazy data changes allows minimum loss of availability.<br/> &copy; 1994 ACM.},
key = {Semantics},
keywords = {Computation theory;},
note = {Hyperprogramming;Integrated environment;Persistence;Referential integrity;Schema evolution;},
URL = {http://dx.doi.org/10.1145/326619.326805},
} 


@article{1997193570208 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Role of polymorphic reuse mechanisms in schema evolution in an object-oriented database},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Liu, Ling and Zicari, Roberto and Hursch, Walter and Lieberherr, Karl J.},
volume = {9},
number = {1},
year = {1997},
pages = {50 - 67},
issn = {10414347},
abstract = {A seamless approach to the incremental design and reuse of object-oriented methods and query specifications is presented. We argue for avoiding or minimizing the effort required for manually reprogramming methods and queries due to schema modifications, and demonstrate how the role of polymorphic reuse mechanisms is exploited for enhancing the adaptiveness of database programs against schema evolution in an object-oriented database. The salient features of our approach are the use of propagation patterns and a mechanism for propagation pattern refinement. Propagation patterns are employed as an interesting specification formalism for modeling operational requirements. They encourage the reuse of operational specifications against the structural modification of an object-oriented schema. Propagation pattern refinement is suited for the specification of reusable operational modules. It promotes the reusability of propagation patterns toward the operational requirement changes. This approach has a formal basis and emphasizes structural derivation of specifications. The main innovations are in raising the level of abstraction for behavioral schema design, and for making possible the derivation of operational semantics from structural specifications. As a result, both the modularity and reusability of object-oriented schemas are increased.},
key = {Database systems},
keywords = {Computer simulation;Knowledge engineering;Object oriented programming;Query languages;Software engineering;},
note = {Adaptive software specification;Polymorphic reuse mechanisms;Propagation patterns;Schema evolution;Software reuse;},
URL = {http://dx.doi.org/10.1109/69.567047},
} 


@article{20143600039463 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Understanding database schema evolution: A case study},
journal = {Science of Computer Programming},
author = {Cleve, Anthony and Gobert, Maxime and Meurice, Loup and Maes, Jerome and Weber, Jens},
volume = {97},
number = {P1},
year = {2015},
pages = {113 - 121},
issn = {01676423},
abstract = {Database reverse engineering (DRE) has traditionally been carried out by considering three main information sources: (1) the database schema, (2) the stored data, and (3) the application programs. Not all of these information sources are always available, or of sufficient quality to inform the DRE process. For example, getting access to real-world data is often extremely problematic for information systems that maintain private data. In recent years, the analysis of the evolution history of software programs have gained an increasing role in reverse engineering in general, but comparatively little such research has been carried out in the context of database reverse engineering. The goal of this paper is to contribute to narrowing this gap and exploring the use of the database evolution history as an additional information source to aid database schema reverse engineering. We present a tool-supported method for analyzing the evolution history of legacy databases, and we report on a large-scale case study of reverse engineering a complex information system and curate it as a benchmark for future research efforts within the community.<br/> &copy; 2013 Elsevier B.V.},
key = {Database systems},
keywords = {Application programs;Information systems;Information use;Reverse engineering;},
note = {Complex information systems;Database schemas;Evolution history;Information sources;Research efforts;Schema evolution;Software program;Software repository mining;},
URL = {http://dx.doi.org/10.1016/j.scico.2013.11.025},
} 


@inproceedings{20172603848426 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Survival in schema evolution: Putting the lives of survivor and dead tables in counterpoint},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vassiliadis, Panos and Zarras, Apostolos V.},
volume = {10253 LNCS},
year = {2017},
pages = {333 - 347},
issn = {03029743},
address = {Essen, Germany},
abstract = {How can we plan development over an evolving schema? In this paper, we study the history of the schema of eight open source software projects that include relational databases and extract patterns related to the survival or death of their tables. Our findings are mostly summarized by a pattern, which we call &ldquo;electrolysis pattern&rdquo; due to its diagrammatic representation, stating that dead and survivor tables live quite different lives: tables typically die shortly after birth, with short durations and mostly no updates, whereas survivors mostly live quiet lives with few updates &ndash; except for a small group of tables with high update ratios that are characterized by high durations and survival. Based on our findings, we recommend that development over newborn tables should be restrained, and wherever possible, encapsulated by views to buffer both infant mortality and high update rate of hyperactive tables. Once a table matures, developers can rely on a typical pattern of gravitation to rigidity, providing less disturbances due to evolution to the surrounding code.<br/> &copy; Springer International Publishing AG 2017.},
key = {Open systems},
keywords = {Information systems;Information use;Open source software;Systems engineering;},
note = {Diagrammatic representations;Evolution patterns;Infant mortality;Open source software projects;Relational Database;Schema evolution;Table survival;Typical patterns;},
URL = {http://dx.doi.org/10.1007/978-3-319-59536-8_21},
} 


@inproceedings{20084811737035 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparative of data base evolution in rule association algorithms in incremental and conventional way},
journal = {Proceedings of the International Joint Conference on Neural Networks},
author = {Farias Jr., Euclides Peres and Nievola, Julio Cesar},
year = {2008},
pages = {3131 - 3137},
address = {Hong Kong, China},
abstract = {Many results in the literature indicate that the incremental approach to association mining leads to gain regarding the time needed to obtain the rules, but there is no evaluation about their quality, compared to non-incremental algorithms. This paper presents the comparison of usage of two typical algorithms representing each approach: APriori and ZigZag. Execution time clearly shows the advantage of incremental approaches, but when someone needs accurate results concerning the association rules obtained, the matter should be taken with more caution, because the rules obtained are not necessarily in a relation one-to-one, according to the results obtained. &copy; 2008 IEEE.<br/>},
key = {Neural networks},
note = {Apriori;Association algorithms;Association mining;Execution time;Incremental algorithm;Incremental approach;},
URL = {http://dx.doi.org/10.1109/IJCNN.2008.4634241},
} 


@inproceedings{20145200355943 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {To wards Schema Evolution in Object-aware Process Management Systems},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
author = {Chiao, Carolina Ming and Kunzle, Vera and Reichert, Manfred},
volume = {P-234},
year = {2014},
pages = {101 - 115},
issn = {16175468},
address = {Luxembourg, Luxembourg},
abstract = {Enterprises want to improve the lifecycle support for their businesses processes by modeling, enacting and monitoring them based on process management systems (PrMS). Since business processes tend to change over time, process evolution support is needed. While process evolution is well understood in traditional activity-centric PrMS, it has been neglected in object-aware PrMS so far. Due to the tight integration of processes and data, in particular, changes of the data and process schemes must be handled in an integrated way; i.e., the evolution of the data schema might affect the process schema and vice versa. This paper presents our overall vision on the controlled evolution of object-aware processes. Further, it discusses fundamental requirements for enabling the evolution of object-aware process schemas in PHILharmonicFlows, a framework targeting at comprehensive support of object-aware processes.<br/>},
key = {Management information systems},
keywords = {Information systems;Information use;},
note = {Business Process;Object-aware process managements;Overall vision;Process evolution;Process management systems;Process schemes;Schema evolution;Tight integrations;},
} 


@inproceedings{20124515650291 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting database provenance under schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Gao, Shi and Zaniolo, Carlo},
volume = {7518 LNCS},
year = {2012},
pages = {67 - 77},
issn = {03029743},
address = {Florence, Italy},
abstract = {Database schema upgrades are common in modern information systems, where the provenance of the schema is of much interest, and actually required to explain the provenance of contents generated by the database conversion that is part of such upgrades. Thus, an integrated management for data and metadata is needed, and the Archived Metadata and Provenance Manager (AM&amp;PM) system is the first to address this requirement by building on recent advances in schema mappings and database upgrade automation. Therefore AM&amp;PM (i) extends the Information Schema with the capability of archiving the provenance of the schema and other metadata, (ii) provides a timestamp based representation for the provenance of the actual data, and (iii) supports powerful queries on the provenance of the data and on the history of the metadata. In this paper, we present the design and main features of AM&amp;PM, and the results of various experiments to evaluate its performance. &copy; 2012 Springer-Verlag.<br/>},
key = {Information management},
keywords = {Data mining;Metadata;},
note = {Data and metadata;Database provenances;Database schemas;Information schema;Integrated management;Schema evolution;Schema mappings;Time-stamp;},
URL = {http://dx.doi.org/10.1007/978-3-642-33999-8_9},
} 


@inproceedings{20134717011456 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Change operators for managing schema evolution in geographical databases},
journal = {ICEIS 2013 - Proceedings of the 15th International Conference on Enterprise Information Systems},
author = {Zghal, Sonda},
volume = {3},
year = {2013},
pages = {187 - 193},
address = {Angers, France},
abstract = {In this paper, we propose a set of change operators for schema versioning management in Geographical DataBases (GDB). We deal with a significant number of changes that we believe they are the most common. These changes are: class adjunction, class suppression and class structure modification translated into: Attribute adjunction, attribute suppression and attribute type modification, method adjunction, method suppression and method modification. In fact, these changes take the schema of the GDB from a state to another. In order to keep track of all these changes, we follow a temporal object oriented versioning approach. We give, in this paper, an overview of this approach in order to better assimilating the description of each change operator. To facilitate the transformation of these change operators into any query language, we formalize them using Hoare logic. An illustrative example is given showing their feasibility.<br/>},
key = {Information systems},
keywords = {Information use;Object-oriented databases;Query languages;Query processing;},
note = {Change operators;Class structures;Geographical database;Keep track of;Object oriented approach;Schema evolution;Schema versioning;Temporal objects;},
} 


@inproceedings{20151500740070 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Markov chain model of schema evolution and its application to stationary distribution},
journal = {2014 10th International Conference on Natural Computation, ICNC 2014},
author = {Zhang, Yu-An and Ma, Qinglian and Furutani, Hiroshi},
year = {2014},
pages = {225 - 229},
address = {Xiamen, China},
abstract = {Markov chain is a powerful tool for analyzing the evolutionary process of a stochastic system. To select GA parameters such as mutation rate and population size are important in practical application. The value of this parameter has a big effect on the viewpoint of Markov chain. In this paper, we consider properties of stationary distribution with mutation in GAs. We used Markov chain to calculate distribution. If the population is in linkage equilibrium, we used Wright-Fisher model to get the distribution of first order schema. We define the mixing time is the time to arrive stationary distribution. We adopt Hunter's mixing time to estimate the mixing time of the first order schema.<br/> &copy; 2014 IEEE.},
key = {Markov processes},
keywords = {Chains;Genetic algorithms;Mixing;Population statistics;Stochastic systems;},
note = {Evolutionary process;ITS applications;Markov chain models;Mixing time;Population sizes;Schema evolution;Stationary distribution;Wright-Fisher models;},
URL = {http://dx.doi.org/10.1109/ICNC.2014.6975839},
} 


@inproceedings{20160902040510 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Detecting XSLT rules affected by schema evolution},
journal = {DocEng 2015 - Proceedings of the 2015 ACM Symposium on Document Engineering},
author = {Wu, Yang and Suzuki, Nobutaka},
year = {2015},
pages = {143 - 146},
address = {Lausanne, Switzerland},
abstract = {In general, schemas of XML documents are continuously updated according to changes in the real world. If a schema is updated, then XSLT stylesheets are also affected by the schema update. To maintain the consistencies of XSLT stylesheets with updated schemas, we have to detect the XSLT rules affected by schema updates. However, detecting such XSLT rules manually is a difficult and time-consuming task, since recent DTDs and XSLT stylesheets are becoming more complex and users do not always fully understand the dependencies between XSLT stylesheets and DTDs. In this paper, we consider three subclasses based on unranked tree transducer, and consider an algorithm for detecting XSLT rules affected by a DTD update for the classes.<br/> &copy; 2015 ACM.},
key = {XML},
keywords = {Trees (mathematics);},
note = {Real-world;Schema evolution;Time-consuming tasks;Unranked tree transducers;XSLT;},
URL = {http://dx.doi.org/10.1145/2682571.2797086},
} 


@article{20145200381369 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A principled approach to context schema evolution in a data management perspective},
journal = {Information Systems},
author = {Quintarelli, Elisa and Rabosio, Emanuele and Tanca, Letizia},
volume = {49},
year = {2015},
pages = {65 - 101},
issn = {03064379},
abstract = {Context-aware data tailoring studies the means for the system to furnish the users, at any moment, only with the set of data which is relevant for their current context. These data may be from traditional databases, sensor readings, environmental information, close-by people, points of interest, etc. To implement context-awareness, we use a formal representation of a conceptual context model, used to design the context schema, which intensionally represents all the contexts in which the user may be involved in the considered application scenario. Following this line of thought, in this paper we develop a formal approach and the corresponding strategy to manage the evolution of the context schema of a given context-aware application, when the context perspectives initially envisaged by the system designer are not applicable any more and unexpected contexts are to be activated. Accordingly, when the context schema evolves also the evolution of the corresponding context-aware data portions must be taken care of. The aim of this paper is thus to provide the necessary conceptual and formal notions to manage the evolution of a context schema in the perspective of data tailoring: after introducing a set of operators to manage evolution and proving their soundness and completeness, we analyze the impact that context evolution has on the context-based data tailoring process. We then study how sequences of operator applications can be optimized and finally present a prototype validating the feasibility of the approach.<br/> &copy; 2014 Elsevier Ltd.},
key = {Information management},
keywords = {Hardware;Information systems;},
note = {Context aware applications;Context- awareness;Environmental information;Evolution operator;Formal representations;Schema evolution;Sequences of operators;Soundness and completeness;},
URL = {http://dx.doi.org/10.1016/j.is.2014.11.008},
} 


@inproceedings{20151100637334 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An algorithm for transforming xpath expressions according to schema evolution},
journal = {CEUR Workshop Proceedings},
author = {Hasegawa, Kazuma and Ikeda, Kosetsu and Suzuki, Nobutaka},
volume = {1008},
year = {2013},
issn = {16130073},
address = {Florence, Italy},
abstract = {XML is a de-fact standard format on the Web. In general, schemas of XML documents are continuously updated ac- cording to changes in real world. If a schema is updated, then query expressions have to be transformed so that they are "valid" under the updated schema, since the expressions are no longer valid under the updated schema due to the schema update. However, this is not an easy task since many of recent schemas are large and complex and thus it is becoming difficult to know how to update the query expres- sions correctly. In this paper, we propose an algorithm for transforming XPath expressions according to schema evolu- tion. For an XPath expression p and a schema S, our algo- rithm treats both p and S as tree automata TAp and TAS, respectively. Our algorithm first takes the product automa- ton TAR of TAp and TAS, then analyze TAR to find the correspondence between the states of TAp and TAS. Based on this correspondence, the algorithm transforms TAp ac- cording to an update operation applied to TAS. We also show some preliminary experimental results.<br/>},
key = {XML},
keywords = {Forestry;Tar;Technology transfer;Visualization;},
note = {Query expression;Real-world;Schema evolution;Standard format;Tree automata;XPath;XPath expressions;},
} 


@inproceedings{2004488478352 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution for stars and snowflakes},
journal = {ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems},
author = {Kaas, Christian E. and Pedersen, Torben Bach and Rasmussen, Bjorn D.},
year = {2004},
pages = {425 - 433},
address = {Porto, Portugal},
abstract = {The most common implementation platform for multidimensional data warehouses is RDBMSs storing data in relational star and snowflake schemas. DW schemas evolve over time, which may invalidate existing analysis queries used for reporting purposes. However, the evolution properties of star and snowflake schemas have not previously been investigated systematically. This paper systematically investigates the evolution properties y of star and snowflake schemas. Eight evolution operations are considered, covering insertion and deletion of dimensions, levels, dimension attributes, and measure attributes. For each operation, the formal semantics of the changes for star and snowflake schemas are given, and instance adaption and impact on existing queries are described. Finally, we compare the evolution properties of star and snowflake schemas, concluding that the star schema is considerably more robust towards schema changes than the snowflake schema.},
key = {Data warehouses},
keywords = {Computer programming;Embedded systems;Online systems;Relational database systems;Robustness (control systems);Schematic diagrams;Semantics;},
note = {Evolution properties;Schema evolution;Snowflake schemas;Star schemas;},
} 


@inproceedings{20102613038067 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multidimensional schema evolution: Integrating new OLAP requirements},
journal = {ICEIS 2006 - 8th International Conference on Enterprise Information Systems, Proceedings},
author = {Neji, Mohamed and Nabli, Ahlem and Feki, Jamel and Gargouri, Faiez},
volume = {DISI},
year = {2006},
pages = {331 - 334},
address = {Paphos, Cyprus},
abstract = {Multidimensional databases are an effective support for OLAP processes. They improve the enterprise decision-making. These databases evolve with the decision maker requirements evolution and, are sensitive to data source changes. In this paper, we are interested in the evolution of the data mart schema due to the raise of new OLAP needs. Our approach determines first, what functional data marts will be able to cover a new requirement, if any, and secondly, decides on a strategy of integration. This leads either to the alteration of an existing data mart schema or, to the creation of a new schema suitable for the new requirement.<br/>},
key = {Information systems},
keywords = {Data warehouses;Decision making;Information use;},
note = {Data mart;Enterprise decision-making;Functional datas;Multidimensional database;Multidimensional schemata;OLAP requirement;Requirements evolution;Schema evolution;},
} 


@inproceedings{20092812179050 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Support multi-version applications in SaaS via progressive schema evolution},
journal = {Proceedings - International Conference on Data Engineering},
author = {Jianfeng, Yan and Bo, Zhang},
year = {2009},
pages = {1717 - 1724},
issn = {10844627},
address = {Shanghai, China},
abstract = {Update of applications in SaaS is expected to be a continuous efforts and cannot be done overnight or over the weekend. In such migration efforts, users are trained and shifted from a existing version to a new version successively. There is a long period of time when both versions of applications co-exist. Supporting two systems at the same time is not a cost efficient option and two systems may suffer from slow response time due to continuous synchronization between two systems. In this paper, we focus on how to enable progressive migration of multi-version applications in SaaS via evolving schema. Instead of maintain two systems, our solution is to maintain an intermediate schema that is optimized for mixed workloads for new and old applications. With a application migration schedule, an genetic algorithm is used to find out the more effective intermediated schema as well as migration paths and schedule. A key advantage of our approach is optimum performance during the long migration period while maintaining the same level of data movement required by the migration.We evaluated the proposed progressive migration approach on a TPCW workload and results validated its effectiveness of across a variety of scenarios; Experimental results demonstrate that our incremental migration proposed in this paper could bring about 200% performance gain as ompared to the existing system. &copy; 2008 IEEE.<br/>},
key = {Continuous time systems},
keywords = {Genetic algorithms;},
note = {Application migrations;Cost-efficient;Data movements;Existing systems;Migration path;Optimum performance;Performance Gain;Schema evolution;},
URL = {http://dx.doi.org/10.1109/ICDE.2009.167},
} 


@article{1991010055770 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and the relational algebra},
journal = {Information Systems},
author = {McKenzie, Edwin and Snodgrass, Richard},
volume = {15},
number = {2},
year = {1990},
pages = {207 - 232},
issn = {03064379},
abstract = {In this paper we discuss extensions to the conventional relational algebra to support both aspects of transaction time, evolution of a database's contents and evolution of a database's schema. We define a relation's schema to be the relation's temporal signature, a function mapping the relation's attribute names onto their value domains, and class, indicating the extent of support for time. We also introduce commands to change a relation, now defined as a triple consisting of a sequence of classes, a sequence of signatures, and a sequence of states. A semantic type of system is required to identify semantically incorrect expressions and to enforce consistency constraints among a relation's class, signature, and state following update. We show that these extensions are applicable, without change, to historical algebras that support valid time, yielding and algebraic language for the query and update of temporal databases. The additions preserve the useful properties of the conventional algebra.},
key = {Database Systems},
keywords = {Mathematical Techniques--Algebra;},
note = {Database Schemes;Relational Algebra;Schema Evolution;},
URL = {http://dx.doi.org/10.1016/0306-4379(90)90036-O},
} 


@article{1994071306491 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Quantifying schema evolution},
journal = {Information and Software Technology},
author = {Sjoberg, D.},
volume = {35},
number = {1},
year = {1993},
pages = {35 - 44},
issn = {09505849},
abstract = {Achieving correct changes is the dominant activity in the application software industry. Modification of database schemata is one kind of change which may have severe consequences for database applications. The paper presents a method for measuring modifications to database schemata and their consequences by using a thesaurus tool. Measurements of the evolution of a large-scale database application currently running in several hospitals in the UK are presented and interpreted. The kind of measurements provided by this in-depth study is useful input to the design of change management tools.},
key = {Relational database systems},
keywords = {Codes (symbols);Data processing;Data structures;Information management;Large scale systems;Modification;Software engineering;Statistics;Systems analysis;User interfaces;Vocabulary control;},
note = {Change management tools;Change statistics;Database schemata;Health management system;Large scale database application;Schema evolution;},
URL = {http://dx.doi.org/10.1016/0950-5849(93)90027-Z},
} 


@inproceedings{20174404344896 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Beyond schema evolution to database reorganization},
journal = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA/ECOOP 1990},
author = {Lerner, Barbara Staudt and Nico Habermann, A.},
year = {1990},
pages = {67 - 76},
address = {Ottawa, ON, Canada},
abstract = {While the c ontents of databases can be easily changed, their organization is typically extremely rigid. Some databases relax the rigidity of database organization somewhat by supporting simple changes to individual schemas. As described in this paper, OTGen supports not only more complex schema changes, but also database reorganization. A database administrator uses a declarative notation to describe mappings between objects created with old versions of schemas and their corresponding representations using new versions. OTGen generates a transformer that applies the mappings to update the database to the new definitions, thus facilitating improvements in performance, functionality, and usability of the database.<br/> &copy; 1990 ACM.},
key = {Object oriented programming},
keywords = {Computer systems programming;Data warehouses;Mapping;},
note = {Database administrators;Database organization;Schema changes;Schema evolution;},
URL = {http://dx.doi.org/10.1145/97945.97956},
} 


@inproceedings{1983030039116 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {DMA DIGITAL DATA BASE EVOLUTION.},
journal = {Technical Papers of the American Congress of Surveying and Mapping},
author = {Mirkay, Francis M.},
year = {1982},
pages = {489 - 498},
address = {Denver, CO, USA},
key = {MAPS AND MAPPING},
note = {APPLICATIONS OF DIGITAL DATA;AUTOMATED MAP AND CHART PRODUCTION;DEFENSE MAPPING AGENCY MC AND G DIGITAL DATA HOLDINGS;MAINTENANCE, STORAGE, RETRIEVAL, COMPARISON AND REPORTING ON THIS DIGITAL DATA;ON-LINE INTERACTIVE, DISTRIBUTED NETWORK DATA BASE SYSTEM;PROGRAMS SUPPORTING ADVANCED WEAPONS SYSTEMS;},
} 


@article{20131816274146 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing source schema evolution in Web warehouses},
journal = {Journal of the Brazilian Computer Society},
author = {Marotta, Adriana and Motz, Regina and Ruggia, Raul},
volume = {8},
number = {2},
year = {2002},
pages = {20 - 31},
issn = {01046500},
abstract = {Web Data Warehouses have been introduced to enable the analysis of integrated Web data. One of the main challenges in these systems is to deal with the volatile and dynamic nature of Web sources. In this work we address the effects of adding/removing/changing Web sources and data items to the Data Warehouse (DW) schema. By managing source evolution we mean the automatic propagation of these changes to the DW. The proposed approach is based on a wrapper/mediator architecture, which reduces the impact of Web source changes on the DW schema. This paper presents this architecture and analyses some selected evolution cases in the context of Web DW.},
key = {Data warehouses},
keywords = {Computer architecture;Data acquisition;Data reduction;Interoperability;Mathematical models;World Wide Web;},
note = {Information extraction;Schema Evolution;Web Warehouses;},
URL = {http://dx.doi.org/10.1590/S0104-65002002000200003},
} 


@inproceedings{1990096100301 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adaptive DB schema evolution via constrained relationships},
author = {Christodoulakis, D. and Soupos, P. and Goutas, S.},
year = {1989},
pages = {393 - 398},
address = {Fairfax, VA, USA},
abstract = {A novel object-oriented data model and a tailored query language that enables the construction of evolving database (DB) schemas according to the information provided by transaction monitoring are presented. The data model and the query language are based on the notion of constrained relationships that enable interconnections between DB objects under constraints specified by rules. These relationships capture frequent transactions with the database and freeze them on the database schema. Thus, the database schema can evolve in terms of changing retrieval requirements. Furthermore, these frequency transactions are mapped at a lower level between object instances, so future transactions may be answered more quickly. The schema adaptation mechanism is based on the identification of commonly used, simple query patterns.},
key = {Database Systems},
keywords = {Information Retrieval Systems;Mathematical Techniques--Graph Theory;Systems Science and Cybernetics--Adaptive Systems;},
note = {Adaptive Database Schema Evolution;Constrained Relationships;Constraint Extraction;Data Retrieval;Deduced Relationships;Object-Oriented Data Model;},
URL = {http://dx.doi.org/10.1109/TAI.1989.65346},
} 


@inproceedings{1988080117515 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {INCREMENTAL MECHANISM FOR SCHEMA EVOLUTION IN ENGINEERING DOMAINS.},
author = {Narayanaswamy, K. and Bapa Rao, K.V.},
year = {1988},
pages = {294 - 301},
address = {Los Angeles, CA, USA},
abstract = {The authors focus on one class of schema revisions necessitated by a very basic phenomenon: a given individual object evolves into a family of objects which are similar to it in many ways. This is commonly called the version problem. In theoretical terms, one can handle the above schema change in the standard, object-oriented database models by the interposition of suitable abstractions into the existing type lattice. There are practical and engineering difficulties with such schema changes. The authors propose an incremental mechanism called instance inheritance which is well suited to handling the schema changes without the attendant practical costs. The authors formally characterize this augmentation to the standard database models, and show examples of its applications.},
key = {DATABASE SYSTEMS},
keywords = {COMPUTER AIDED ENGINEERING;INTEGRATED CIRCUITS, VLSI - Computer Aided Design;},
note = {INCREMENTAL MECHANISM;INSTANCE INHERITANCE;SCHEMA EVOLUTION;SIMPLE GENERIC OBJECT-ORIENTED DATA MODEL;},
URL = {http://dx.doi.org/10.1109/ICDE.1988.105472},
} 


@article{1990045230450 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Role of polymorphism in schema evolution in an object-oriented database},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Osborn, Sylvia L.},
volume = {1},
number = {3},
year = {1989},
pages = {310 - 317},
issn = {10414347},
abstract = {A polymorphic object algebra for an object-oriented database model is introduced. Types of schema modification that follow naturally from this model are described. It is shown to what extent queries return identical or equivalent results when the objects in the database are modified to conform to a modified schema.},
key = {Database Systems},
keywords = {Computer Programming Languages;},
note = {Data Models;Object Oriented Databases;Polymorphis;Query Processing;Schema Evolution;Smalltalk;},
URL = {http://dx.doi.org/10.1109/69.87977},
} 


@inproceedings{1997493857774 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {External schemas in a schema-evolution environment for OODBs},
journal = {International Conference on Database and Expert Systems Applications - DEXA},
author = {Samos, Jose and Saltor, Felix},
year = {1997},
pages = {516 - 522},
address = {Toulouse, Fr},
abstract = {External schemas are derived from the database conceptual schema; they can be used to simulate changes in it. Sometimes the final users' information requirements change: they need new information which cannot be derived from the information in the database. The solution put forward here is the definition of partially derived classes (capacity augmenting classes) which may contain non-derived information in the intension and, moreover, unlike other systems, in the extension. When an external schema with partially derived classes is to be defined, the conceptual schema has to be modified in order to include the non-derived information. In order to avoid unnecessary modifications of the conceptual schema the use of a test environment for the definition of temporal external schemas is also proposed.},
key = {Database systems},
keywords = {Computer simulation;Information retrieval;Object oriented programming;User interfaces;},
note = {Object oriented database systems (OODB);Schema evolution environment;},
} 


@inproceedings{20124415616493 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A semantic approach for schema evolution and versioning in object-oriented databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Franconi, Enrico and Grandi, Fabio and Mandreoli, Federica},
volume = {1861 LNAI},
year = {2000},
pages = {1048 - 1062},
issn = {03029743},
address = {London, United kingdom},
abstract = {In this paper a semantic approach for the specification and the management of databases with evolving schemata is introduced. It is shown how a general object-oriented model for schema versioning and evolution can be formalized; how the semantics of schema change operations can be defined; how interesting reasoning tasks can be supported, based on an encoding in description logics. &copy; Springer-Verlag Berlin Heidelberg 2000.<br/>},
key = {Object-oriented databases},
keywords = {Computation theory;Computer circuits;Data description;Logic programming;Semantics;},
note = {Description logic;Object oriented model;Reasoning tasks;Schema changes;Schema evolution;Schema versioning;Semantic approach;Versioning;},
} 


@inproceedings{20173704139802 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A procedural approach to schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Ewald, Catherine A. and Orlowska, Maria E.},
volume = {685 LNCS},
year = {1993},
pages = {22 - 38},
issn = {03029743},
address = {Paris, France},
abstract = {Like data, conceptual schemata change and need to be updated. If not performed with care, updates can cause problems. In this paper, we present a procedure which safely adds a fact type to a conceptual schema. This procedure, as well as being used for simple schema updates, may be applied to design and integration problems. We also present procedures for the safe removal of a fact type, and the safe elimination of redundant information from the conceptual schema.<br/> &copy; Springer-Verlag Berlin Heidelberg 1993.},
key = {Information systems},
keywords = {Information use;Systems engineering;},
note = {Conceptual schemas;Design and integrations;Schema evolution;},
} 


@inproceedings{20173904208962 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Maintaining behavioral consistency during schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bergstein, Paul L. and Hursch, Walter L.},
volume = {742 LNCS},
year = {1993},
pages = {176 - 193},
issn = {03029743},
address = {Kanazawa, Japan},
abstract = {We examine the problem of how to ensure behavioral consistency of an object-oriented system after its schema has been updated. The problem is viewed from the perspective of both the strongly typed and the untyped language model. Solutions are compared in both models using C++ and CLOS as examples.<br/> &copy; Springer-Verlag Berlin Heidelberg 1993.},
key = {C++ (programming language)},
keywords = {Object oriented programming;Systems analysis;},
note = {Behavioral consistency;Language model;Object-oriented system;Schema evolution;},
} 


@inproceedings{20141617597920 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {DAHLIA: A visual analyzer of database schema evolution},
journal = {2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings},
author = {Meurice, Loup and Cleve, Anthony},
year = {2014},
pages = {464 - 468},
address = {Antwerp, Belgium},
abstract = {In a continuously changing environment, software evolution becomes an unavoidable activity. The mining software repositories (MSR) field studies the valuable data available in software repositories such as source code version-control systems, issue/bug-tracking systems, or communication archives. In recent years, many researchers have used MSR techniques as a way to support software understanding and evolution. While many software systems are data-intensive, i.e., their central artefact is a database, little attention has been devoted to the analysis of this important system component in the context of software evolution. The goal of our work is to reduce this gap by considering the database evolution history as an additional information source to aid software evolution. We present DAHLIA (Database ScHema EvoLutIon Analysis), a visual analyzer of database schema evolution. Our tool mines the database schema evolution history from the software repository and allows its interactive, visual analysis. We describe DAHLIA and present our novel approach supporting data-intensive software evolution. &copy; 2014 IEEE.<br/>},
key = {Computer software maintenance},
keywords = {Database systems;Reengineering;Reverse engineering;},
note = {Changing environment;Evolution history;Information sources;Mining software repository (MSR);Software Evolution;Software repositories;Software understanding;System components;},
URL = {http://dx.doi.org/10.1109/CSMR-WCRE.2014.6747219},
} 


@article{1996092991003 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Domain-knowledge-guided schema evolution for accounting database systems},
journal = {Expert Systems with Applications},
author = {Chen, Jia-Lin and McLeod, Dennis and O'Leary, Daniel},
volume = {9},
number = {4},
year = {1995},
pages = {491 - 501},
issn = {09574174},
abstract = {The static meta-data view of accounting database management is that the schema of a database is designed before the database is populated and remains relatively fixed over the life cycle of the system. However, the need to support accounting database evolution is clear: a static meta-data view of an accounting database cannot support next generation dynamic environment where system migration, organization reengineering, and heterogeneous system interoperation are essential. This paper presents a knowledge-based approach and mechanism to support dynamic accounting database schema does not meet the in an object-based data modeling context. When an accounting database schema does not meet the requirements of a firm, the schema must be changed. Such schema evolution can be realized via a sequence of evolution operators. As a result, this paper considers the question: what heuristics and knowledge are necessary to guide a system to choose a sequence of operators to complete a given evolution task for an accounting database? In particular, we first define a set of basic evolution schema operators, employing heuristics to guide the evolution process. Second, we explore how domain-specific knowledge can be used to guide the use of the operators to complete the evolution task. A well-known accounting data model, REA model, is used here to guide the schema evolution process. Third, we discuss a prototype system, REAtool, to demonstrate and test our approach.},
key = {Knowledge based systems},
keywords = {Computer aided software engineering;Computer simulation;Cost accounting;Database systems;Decision support systems;Heuristic methods;Object oriented programming;},
note = {Accounting database management;Evolution schema operators;Heterogeneous system interoperation;Object-based data modeling;Organization reengineering;Static meta-data view;System migration;},
URL = {http://dx.doi.org/10.1016/0957-4174(95)00019-4},
} 


@inproceedings{20154801623886 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A unified framework for supporting dynamic schema evolution in object databases},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Benatallah, Boualem},
volume = {1728},
year = {1999},
pages = {16 - 30},
issn = {03029743},
address = {Paris, France},
abstract = {This paper addresses the design of an integrated framework for managing schema evolution. This framework is based on the adaptation and extension of two main schema evolution approaches, namely schema modification and schema versioning. The proposed framework provides an integrated environment to support different database evolution techniques (such as, modification and versions at the schema level, conversion, object versioning, and screening at the instance level). We introduce the concept of class/schema version pertinence enabling the database administrator to judge the pertinence of versions with regard to database applications. Finally, we propose a declarative language based on OQL, the ODMG query language, that the user can use to guide objects adaptation process when dealing with complex or application specific schema updates.<br/> &copy; Springer-Verlag Berlin Heidelberg 1999.},
key = {Object-oriented databases},
keywords = {Data mining;Query languages;Query processing;},
note = {Adaptation process;Database administrators;Database applications;Declarative Languages;Integrated environment;Integrated frameworks;Object versioning;Schema versioning;},
} 


@inproceedings{20160902009429 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {9th International Workshop on Foundations of Models and Languages for Data and Objects, Workshop on Database Schema Evolution and Meta-Modeling, FoMLaDO/DEMM 2000},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {2065},
year = {2001},
pages = {1 - 244},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
abstract = {The proceedings contain 13 papers. The special focus in this conference is on Database Schema Evolution and Meta-Modeling. The topics include: Consistency management in runtime evolving concurrent information systems; adaptive specifications of technical information systems; evolving the software of a schema evolution system; schema evolution and versioning; a logical and computational characterisation; temporal branching as a conflict management technique; QFD matrix for incremental construction of a warehouse via data marts; change propagation in an axiomatic model of schema evolution for objectbase management systems; conceptual description of adaptive information systems; extending the object query language for transparent metadata access; a metamodeling approach to evolution and defining metrics for conceptual schema evolution.},
} 


@article{2002477229150 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing schema evolution in a container-based persistent system},
journal = {Software - Practice and Experience},
author = {Baltasar Garcia Perez-Schofield, J. and Rosello, Emilio Garcia and Cooper, Tim B. and Cota, Manuel Perez},
volume = {32},
number = {14},
year = {2002},
pages = {1395 - 1410},
issn = {00380644},
abstract = {Managing schema evolution is a problem every persistent system has to cope with to be useful in practice. Schema evolution consists basically of supporting class modification and dealing with data objects created and stored under the old class definitions. Several proposals have been made to handle this problem in systems that follow a full orthogonally persistent approach, but, until now, there has not been any proposal to support it in container-based persistent systems. In this paper we describe a schema evolution management system designed for Barbados. Barbados is a complete programming environment which is based on an architecture of containers to provide persistent storage. Barbados does not provide full orthogonal persistence, but, as will be described in this paper, its architecture has several other advantages. Among them is the fact that this model is especially suitable for solving the schema evolution problem. Copyright &copy; 2002 John Wiley &amp; Sons, Ltd.},
key = {Multiprocessing systems},
keywords = {C (programming language);Computer simulation;Data structures;Object oriented programming;Problem solving;},
note = {Container-based persistent systems;},
URL = {http://dx.doi.org/10.1002/spe.489},
} 


@article{20110413607365 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A unit test approach for database schema evolution},
journal = {Information and Software Technology},
author = {Grolinger, Katarina and Capretz, Miriam A.M.},
volume = {53},
number = {2},
year = {2011},
pages = {159 - 170},
issn = {09505849},
abstract = {Context: The constant changes in today's business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution. Objective: This work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database. Method: The approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively simple database schema. Although the database schema in this study was simple, it was nevertheless able to demonstrate the advantages of the proposed approach. Results: After changes in the database schema, the proposed approach found all SELECT statements as well as the majority of other statements requiring modifications in the application code. Due to its efficiency with SELECT statements, the proposed approach is expected to be more successful with database warehouse applications where SELECT statements are dominant. Conclusion: The unit test approach that accesses databases has proven to be successful in evaluating the application code against the evolved database. In particular, the approach is simple and straightforward to implement, which makes it easily adoptable in practice. &copy; 2010 Elsevier B.V. All rights reserved.<br/>},
key = {Database systems},
keywords = {Application programs;Codes (symbols);Computer software selection and evaluation;},
note = {Business requirement;Database schemas;Database testing;Mock objects;Sample applications;Software applications;Unit testing;Warehouse applications;},
URL = {http://dx.doi.org/10.1016/j.infsof.2010.10.002},
} 


@inproceedings{20160902009439 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Change propagation in an axiomatic model of schema evolution for objectbase management systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Peters, Randal J. and Barker, Ken},
volume = {2065},
year = {2001},
pages = {142 - 162},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
abstract = {Schema evolution is an important component of advanced information systems such as objectbase management systems. These sys-tems typically support volatile and complex application domains that in-clude engineering design, CAD/CAM, multimedia, and geo-information systems. The schema of these applications must be able to evolve along with the changing environment. There are two problems to consider in schema evolution: (i) semantics of change and (ii) change propagation. The first deals with the effects of the schema change on the overall type system. For example, the deletion of a property in a type affects the subtypes inheriting that property. Our previous work has introduced a sound and complete axiomatic model to deal with the semantics of change problem. The second problem deals with the techniques for propagating schema changes to the underlying objects. For example, the addition of an attribute to a type requires additional memory to be allocated to the objects so that values for the attribute may be stored. The first step of change propagation is to identify the affected objects. Subsequent steps carry out the actual changes. This paper deals with the first step by extending the axiomatic model with semantics to determine a sound and complete set of objects affected by a schema change. The extended model can be used with any method for carrying out the changes such as the conversion, screening, and filtering approaches proposed in the literature.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Information management},
keywords = {Computer aided design;Information systems;Information use;Management information systems;Modeling languages;Semantics;},
note = {Advanced Information Systems;Axiomatic models;Change propagation;Changing environment;Complex applications;Engineering design;Management systems;Sound and complete;},
URL = {http://dx.doi.org/10.1007/3-540-48196-6_9},
} 


@article{1998144053604 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution of an object-oriented real-time database system for manufacturing automation},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Zhou, Lei and Rundensteiner, Elke A. and Shin, Kang G.},
volume = {9},
number = {6},
year = {1997},
pages = {956 - 976},
issn = {10414347},
abstract = {A conceptual real-time object-oriented data model, called Real-time Object Model with Performance Polymorphism (ROMPP), is developed. Previous (nonreal-time) schema evolution support is evaluated in the context of real-time databases. In addition, the schema-change framework with new constructs necessary for handling the real-time characteristic of ROMPP is expanded. Using manufacturing-control applications, the applicability of ROMPP and the potential benefits of the proposed schema-evolution system are demonstrated.},
key = {Database systems},
keywords = {Computer aided design;Computer aided manufacturing;Management information systems;Mathematical models;Object oriented programming;Real time systems;},
note = {Database management system;Object oriented real time database systems;Performance polymorphism;},
URL = {http://dx.doi.org/10.1109/69.649319},
} 


@article{2002377079029 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Rule-based schema evolution in object-oriented databases},
journal = {Knowledge-Based Systems},
author = {Alhajj, Reda and Polat, Faruk},
volume = {16},
number = {1},
year = {2003},
pages = {47 - 57},
issn = {09507051},
abstract = {In this paper, a rule-based mechanism for schema evolution in object-oriented databases is presented. We have benefited from having an object algebra maintaining closure that makes it possible to have the output from a query persistent in the hierarchy. The actual class hierarchy and the corresponding hierarchy which reflects the relationship between operands and results of queries are utilized. In order to have query results reflected into the class hierarchy and classes reflected into the operands hierarchy, we also define mappings between the two hierarchies. As a result, it is possible to maximize reusability in object-oriented databases. The object algebra is utilized to handle basic schema evolution functions without requiring any special set of built-in functions. The invariants and the conflict resolving rules are specified. It is also shown how other schema functions are derivable from the basic ones. &copy; 2003 Elsevier Science B.V. All rights reserved.},
key = {Knowledge based systems},
keywords = {Algebra;Conformal mapping;Database systems;Functions;Object oriented programming;Query languages;},
note = {Object-oriented databases;},
URL = {http://dx.doi.org/10.1016/S0950-7051(02)00051-5},
} 


@article{2004057966770 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Model for Compound Type Changes Encountered in Schema Evolution},
journal = {ACM Transactions on Database Systems},
author = {Lerner, Barbara Staudt},
volume = {25},
number = {1},
year = {2000},
pages = {83 - 127},
issn = {03625915},
abstract = {Schema evolution is a problem that is faced by long-lived data. When a schema changes, existing persistent data can become inaccessible unless the database system provides mechanisms to access data created with previous versions of the schema. Most existing systems that support schema evolution focus on changes local to individual types within the schema, thereby limiting the changes that the database maintainer can perform. We have developed a model of type changes incorporating changes local to individual types as well as compound changes involving multiple types. The model describes both type changes and their impact on data by defining derivation rules to initialize new data based on the existing data. The derivation rules can describe local and nonlocal changes to types to capture the intent of a large class of type change operations. We have built a system called Tess (Type Evolution Software System) that uses this model to recognize type changes by comparing schemas and then produces a transformer that can update data in a database to correspond to a newer version of the schema. Categories and Subject Descriptors: H.2.m [Database Management]: Miscellaneous; H.2.3 [Database Management]: Languages - Database (persistent) programming languages; D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement-Restructuring, reverse engineering, and reengineering General Terms: Algorithms, Languages.},
URL = {http://dx.doi.org/10.1145/352958.352983},
} 


@inproceedings{20161402187482 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Continuous deployment and schema evolution in SQL databases},
journal = {Proceedings - 3rd International Workshop on Release Engineering, RELENG 2015},
author = {De Jong, Michael and Van Deursen, Arie},
year = {2015},
pages = {16 - 19},
address = {Florence, Italy},
abstract = {Continuous Deployment is an important enabler of rapid delivery of business value and early end user feedback. While frequent code deployment is well understood, the impact of frequent change on persistent data is less understood and supported. SQL schema evolutions in particular can make it expensive to deploy a new version, and may even lead to downtime if schema changes can only be applied by blocking operations. In this paper we study the problem of continuous deployment in the presence of database schema evolution in more detail. We identify a number of shortcomings to existing solutions and tools, mostly related to avoidable downtime and support for foreign keys. We propose a novel approach to address these problems, and provide an open source implementation. Initial evaluation suggests the approach is effective and sufficiently efficient. &copy; 2015 IEEE.},
URL = {http://dx.doi.org/10.1109/RELENG.2015.14},
} 


@inproceedings{20130115861493 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design and implementation of database schema evolution for service continuity of web-based internet applications},
journal = {14th Asia-Pacific Network Operations and Management Symposium: "Management in the Big Data and IoT Era", APNOMS 2012 - Final Program},
author = {Chen, Yi-Chang and Jeng, Jeu-Yih and Huang, Teh-Sheng and Lin, Phone},
year = {2012},
pages = {IEICE ICM; KICS KNOM - },
address = {Seoul, Korea, Republic of},
abstract = {Today, Internet services are utilized as major service platforms to provide business services. Web-based Internet applications that follow the multi-tier architecture are a well-accepted computation model when providing Internet services. Such a model suffers from interruption caused by software failures, hardware errors or software maintenance. The interruption may incur significant financial loss to Internet service providers. Thus, Service Continuity is one of the main challenges for Web-based Internet applications. Although several service continuity technologies have been developed, many of them focus on a single tier of the applications and do not guarantee end-to-end service continuity. In this paper, we describe the design, implementation, and performance evaluation of our proposed service continuity technology. This technology resolves the long time interruption problem caused by database schema evolution, handles software components upgrade, and achieves end-to-end Service Continuity. We propose a mechanism that contains multiple modules running interactively between each module. This approach only requires minimal amount of code rewriting and a small storage space. Comparing with existing approaches, our approach is more efficient with the least overhead. &copy; 2012 IEEE.<br/>},
key = {Web services},
keywords = {Big data;Client server computer systems;Database systems;Digital storage;Information management;Internet of things;Losses;Websites;},
note = {Database schemas;Design and implementations;End-to-end service;Internet application;Multi tier architecture;Performance evaluations;Service continuity;Software component;},
URL = {http://dx.doi.org/10.1109/APNOMS.2012.6356090},
} 


@inproceedings{2004448426505 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The schema evolution and data migration framework of the environmental mass database IMIS},
journal = {Scientific and Statistical Database Management - Proceedings of the International Working Conference},
author = {Draheim, Dirk and Horn, Matthias and Schulz, Ina},
volume = {16},
year = {2004},
pages = {341 - 344},
issn = {10993371},
address = {Santorini Island, Greece},
abstract = {This paper describes a framework that supports the simultaneous evolution of object-oriented data models and relational schemas with respect to a tool-supported object-relational mapping. The proposed framework accounts for non-trivial data migration induced by type evolution from the outset. The support for data migration is offered on the level of transparent data access. The framework consists of the following integrated parts: an automatic model change detection mechanism, a generator for schema evolution code and a generator for data migration APIs. The framework has been concepted in the IMIS project. IMIS is an information system for environmental radioactivity measurements. Though the indicated domain especially demands a solution like the one discussed in this paper, the achievements are of general purpose for multi-tier system architectures with object-relational mapping.},
key = {Database systems},
keywords = {Automation;Client server computer systems;Computer architecture;Java programming language;Mapping;Mathematical models;Object oriented programming;Query languages;Semantics;Specifications;},
note = {Data migration;Database schemas;Mapping generator;Relational schemas;},
} 


@inproceedings{20181805117644 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A case study in supporting schema evolution of complex engineering information systems},
journal = {Proceedings - International Computer Software and Applications Conference},
author = {Jahnke, J.-H. and Nickel, U.A. and Wagenblasst, D.},
year = {1998},
pages = {513 - 520},
issn = {07303157},
address = {Vienna, Austria},
abstract = {Information systems have to evolve continually in order to keep up with emerging requirements. Various problems arise with each such evolution step, e.g. the modification of the application's conceptual data structure, the migration of existing data, the adaption of application code, and the modification of technical documentation. Most database systems provide only limited support for schema evolution while problems like data migration and application migration are tackled manually by the programmers. This evolution process is unsatisfactory for a number of novel complex evolutionary information systems (CEIS) in the area of business and engineering applications. The paper describes our experiences with a case study in developing a CEIS in the domain of analysis and design of mixed signal printed circuit boards. We show that a meta schema approach combined with a well defined set of schema transformations is a practical way to cope with evolution. Based on this case study, we distinguish application specific from reusable architectural components and propose a systematic approach of building CEIS.<br/> &copy; 1998 IEEE.},
key = {Application programs},
keywords = {Computer software reusability;Information systems;Information use;Printed circuit boards;Printed circuit design;},
note = {Application migrations;Application specific;Architectural components;Complex engineering;Engineering applications;Evolutionary information;Schema transformation;Technical documentations;},
URL = {http://dx.doi.org/10.1109/CMPSAC.1998.716710},
} 


@inproceedings{20101212784323 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Language extensions for the automation of database schema evolution},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Papastefanatos, George and Vassiliadis, Panos and Simitsis, Alkis and Aggistalis, Konstantinos and Pechlivani, Fotini and Vassiliou, Yannis},
volume = {DISI},
year = {2008},
pages = {74 - 81},
address = {Barcelona, Spain},
abstract = {The administrators and designers of modern Information Systems face the problem of maintaining their systems in the presence of frequently occurring changes in any counterpart of it. In other words, when a change occurs in any point of the system -e.g., source, schema, view, software construct- they should propagate the change in all the involved parts of the system. Hence, it is imperative that the whole process should be done correctly, i.e., the change should be propagated to all the appropriate points of the system, with a limited overhead imposed on both the system and the humans, who design and maintain it. In this paper, we are dealing with the problem of evolution in the context of databases. First, we present a coherent, graph-based framework for capturing the effect of potential changes in the database software of an Information System. Next, we describe a generic annotation policy for database evolution and we propose a feasible and powerful extension to the SQL language specifically tailored for the management of evolution. Finally, we demonstrate the efficiency and feasibility of our approach through a case study based on a real-world situation occurred in the Greek public sector.<br/>},
key = {Information systems},
keywords = {Database systems;Graphic methods;Information use;},
note = {Database schemas;Database software;Language extensions;Potential change;Public sector;Real world situations;Software constructs;SQL extension;},
} 


@inproceedings{20174404324774 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An Eclipse Plugin for the controlled schema evolution in the NoSQL database system MongoDB},
title = {Ein Eclipse-Plugin zur kontrollierten Schema-Evolution im NoSQL Datenbanksystem MongoDB},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
author = {Schmidt, Dennis},
volume = {P-259},
year = {2016},
pages = {2151 - 2156},
issn = {16175468},
address = {Klagenfurt, Austria},
} 


@inproceedings{20185106251670 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relational database schema evolution: An industrial case study},
journal = {Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author = {Delplanque, Julien and Etien, Anne and Anquetil, Nicolas and Auverlot, Olivier},
year = {2018},
pages = {635 - 644},
address = {Madrid, Spain},
abstract = {Modern relational database management systems provide advanced features allowing, for example, to include behaviour directly inside the database (stored procedures). These features raise new difficulties when a database needs to evolve (e.g. adding a new table). To get a better understanding of these difficulties, we recorded and studied the actions of a database architect during a complex evolution of the database at the core of a software system. From our analysis, problems faced by the database architect are extracted, generalized and explored through the prism of software engineering. Six problems are identified: (1) difficulty in analysing and visualising dependencies between database's entities, (2) difficulty in evaluating the impact of a modification on the database, (3) replicating the evolution of the database schema on other instances of the database, (4) difficulty in testing database's functionalities, (5) lack of synchronization between the IDE's internal model of the database and the database actual state and (6) absence of an integrated tool enabling the architect to search for dependencies between entities, generate a patch or access an up to date PostgreSQL documentation. We suggest that techniques developed by the software engineering community could be adapted to help in the development and evolution of relational databases.<br/> &copy; 2018 IEEE.},
key = {Relational database systems},
keywords = {Computer software maintenance;Software engineering;},
note = {Complex evolutions;Engineering community;Evolution;Industrial case study;Internal modeling;Relational Database;Relational database management systems;Relational database schemata;},
URL = {http://dx.doi.org/10.1109/ICSME.2018.00073},
} 


@inproceedings{20173204028352 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Zero-downtime SQL database schema evolution for continuous deployment},
journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017},
author = {De Jong, Michael and Van Deursen, Arie and Cleve, Anthony},
year = {2017},
pages = {143 - 152},
address = {Buenos Aires, Argentina},
abstract = {When a web service or application evolves, its database schema - tables, constraints, and indices - often need to evolve along with it. Depending on the database, some of these changes require a full table lock, preventing the service from accessing the tables under change. To deal with this, web services are typically taken offline momentarily to modify the database schema. However with the introduction of concepts like Continuous Deployment, web services are deployed into their production environments every time the source code is modified. Having to take the service offline - potentially several times a day - to perform schema changes is undesirable. In this paper we introduce QuantumDB - a tool-supported approach that abstracts this evolution process away from the web service without locking tables. This allows us to redeploy a web service without needing to take it offline even when a database schema change is necessary. In addition QuantumDB puts no restrictions on the method of deployment, supports schema changes to multiple tables using changesets, and does not subvert foreign key constraints during the evolution process. We evaluate QuantumDB by applying 19 synthetic and 95 industrial evolution scenarios to our open source implementation of QuantumDB. These experiments demonstrate that QuantumDB realizes zero downtime migrations at the cost of acceptable overhead, and is applicable in industrial continuous deployment contexts.<br/> &copy; 2017 IEEE.},
key = {Web services},
keywords = {Database systems;Locks (fasteners);Maintenance;Open source software;Websites;},
note = {Database schemas;Evolution process;Industrial evolution;Key constraints;Open source implementation;OR applications;Production environments;Schema changes;},
URL = {http://dx.doi.org/10.1109/ICSE-SEIP.2017.5},
} 


@inproceedings{20160902009431 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in sql-99 and commercial (Object-)relational dbms},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Turker, Can},
volume = {2065},
year = {2001},
pages = {1 - 32},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
URL = {http://dx.doi.org/10.1007/3-540-48196-6_1},
} 


@inproceedings{20164703032397 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Detecting and Preventing Program Inconsistencies under Database Schema Evolution},
journal = {Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016},
author = {Meurice, Loup and Nagy, Csaba and Cleve, Anthony},
year = {2016},
pages = {262 - 273},
address = {Vienna, Austria},
abstract = {Nowadays, data-intensive applications tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting application programs to database schema changes. Failing to correctly adapt programs to an evolving database schema results in program inconsistencies, which in turn may cause program failures. In this paper, we present a tool-supported approach, that allows developers to (1) analyze how the source code and database schema co-evolved in the past and (2) simulate a database schema change and automatically determine the set of source code locations that would be impacted by this change. Developers are then provided with recommendations about what they should modify at those source code locations in order to avoid inconsistencies. The approach has been designed to deal with Java systems that use dynamic data access frameworks such as JDBC, Hibernate and JPA. We motivate and evaluate the proposed approach, based on three real-life systems of different size and nature.<br/> &copy; 2016 IEEE.},
key = {Query languages},
keywords = {Application programs;Codes (symbols);Computer programming languages;Computer software selection and evaluation;Software reliability;},
note = {Co-evolution;Data-intensive application;Database schemas;Dynamic data access;Evolving database;JDBC;Object-relational mapping;what-if approach;},
URL = {http://dx.doi.org/10.1109/QRS.2016.38},
} 


@inproceedings{20160902009443 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Defining metrics for conceptual schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wedemeijer, Lex},
volume = {2065},
year = {2001},
pages = {220 - 244},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
abstract = {It is generally believed that a well-designed Conceptual Schema will remain stable over time. However, current literature rarely addresses how such stability should be observed and measured in the operational business environment with evolving information needs and database structures. This paper sets up a framework for stability of conceptual schemas and proceeds to develop a set of metrics from it. The metrics are based on straightforward measurements of conceptual features. The validity of the set of metrics is argued here from theory, operational validity may be demonstrated by a longitudinal case study into the evolution of conceptual schemas. The main contribution of this paper is the realization that the measurement of conceptual schema stability is an essential step for understanding and improving current theories and best-practices for designing high-quality schemas that will stand the test of time.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Modeling languages},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Best practices;Conceptual schemas;Current theories;Database structures;High quality;Longitudinal case study;Measurements of;Operational business;},
URL = {http://dx.doi.org/10.1007/3-540-48196-6_13},
} 


@inproceedings{20163902842270 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A survey on the use of database management systems and schema evolution},
title = {Um survey sobre evolucao de esquemas e uso de sistemas gerenciadores de bancos de dados},
journal = {CIBSE 2016 - XIX Ibero-American Conference on Software Engineering},
author = {Santos, Tauany Lorene Santana and Colaco, Methanias and Carvalho, Camila Oliveira and Nascimento, Andre Vinicius R.P. and Dosea, Marcos Barbosa},
year = {2016},
pages = {271 - 284},
address = {Quito, Ecuador},
} 


@inproceedings{20154201413124 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Controlled schema evolution management for NoSQL database systems},
title = {Kontrolliertes Schema-Evolutionsmanagement fur NoSQL-Datenbanksysteme},
journal = {CEUR Workshop Proceedings},
author = {Storl, Uta and Klettke, Meike and Scherzinger, Stefanie},
volume = {1458},
year = {2015},
pages = {439 - 443},
issn = {16130073},
address = {Trier, Germany},
} 


@inproceedings{20151200668828 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A conceptual model for the XML Schema evolution: Overview: Storing, base-model-mapping and visualization},
journal = {CEUR Workshop Proceedings},
author = {Nosinger, Thomas and Klettke, Meike and Heuer, Andreas},
volume = {1020},
year = {2013},
pages = {28 - 33},
issn = {16130073},
address = {Ilmenau, Germany},
abstract = {In this article the conceptual model EMX (Entity Model for XML-Schema) for dealing with the evolution of XML Schema (XSD) is introduced. The model is a simplified representation of an XSD, which hides the complexity of XSD and offers a graphical presentation. For this purpose a unique mapping is necessary which is presented as well as further information about the visualization and the logical structure. A small example illustrates the relationships between an XSD and an EMX. Finally, the integration into a developed research prototype for dealing with the coevolution of corresponding XML documents is presented.<br/>},
key = {XML},
keywords = {Visualization;},
note = {Base models;Co-evolution;Conceptual model;Entity modeling;Graphical presentations;Logical structure;Research prototype;XML schema evolutions;},
} 


@inproceedings{20134316897557 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML schema evolution by context free grammar inference},
journal = {19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007},
author = {Da Silva, Julio C. T. and Musicante, Martin A. and Pozo, Aurora T. R. and Vergilio, Silvia R.},
year = {2007},
pages = {444 - 449},
address = {Boston, MA, United states},
abstract = {XML has become one of the most usual technologies for data storage, manipulation and transference. Schema languages for XML allow defining application-specific formats (or schemas) for XML documents. Schemas are easily verifiable. However, as the application life-cycle goes on, schemas need to be changed in accordance to any new requirement of the data they define. For this reason it is useful to create mechanisms to automatically produce these changes. In this work, we explore the correspondence between schemas and context-free grammars. We focus our attention on the automatic induction of context-free grammars which, in turn, will be translated to schemas for XML documents. An algorithm of grammatical inference is proposed. Our work extends the well-known LL Parsing algorithm to perform grammar inference. Copyright &copy; (2007) by Knowledge Systems Institute (KSI).<br/>},
key = {Context free grammars},
keywords = {Digital storage;Formal languages;Inference engines;Knowledge engineering;Life cycle;Software engineering;XML;},
note = {Application life cycles;Application specific;Automatic induction;Grammar inference;Grammatical inferences;Parsing algorithm;Schema language;XML schema evolutions;},
} 


@inproceedings{20083511487748 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XML schema evolution: Incremental validation and efficient document adaptation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Guerrini, Giovanna and Mesiti, Marco and Sorrenti, Matteo Alberto},
volume = {4704 LNCS},
year = {2007},
pages = {92 - 106},
issn = {03029743},
address = {Vienna, Austria},
abstract = {XML Schemas describe the structure of valid documents and can be exploited for improving both the efficiency and effectiveness of queries on valid documents. XML Schemas, however, may need to be updated to adhere to new requirements and to face changes in the application domain. Starting from a set of schema modification primitives, in this paper we devise an incremental validation approach that allows to efficiently validate documents, known to be valid for the original schema, for an updated schema. Then, we enhance the approach to adapt the documents to the new schema. Experiments prove that our approach increases the performance of standard validation algorithms in this setting and that the cost of the adaptation process is limited. &copy; 2007 Springer-Verlag Berlin Heidelberg.<br/>},
key = {XML},
keywords = {Database systems;},
note = {Adaptation process;Incremental validation;Standard validations;XML schema evolutions;XML schemas;},
URL = {http://dx.doi.org/10.1007/978-3-540-75288-2_8},
} 


@inproceedings{20112514080829 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XEvolve: An XML schema evolution framework},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Picalausa, Francois and Servais, Frederic and Zimanyi, Esteban},
year = {2011},
pages = {1645 - 1650},
abstract = {This paper presents XEvolve, a framework that unifies streaming validation of XML documents, and efficient testing of equivalence and inclusion of specifications for various XML schema languages. For these purposes, this framework relies on Visibly Pushdown Automata (VPA) as a unifying model for the various schema languages. Schemas are first translated into VPA; standard algorithms for VPA can be then used to validate documents as well as to test equivalence or inclusion of schemas. In general, inclusion and equivalence are tested in exponential-time. However, when the given specifications are provided as DTD or XSD, these tests have a polynomial-time complexity with respect to the automaton size. Moreover, in this case the memory foot-print of the validation does not depend on the size of the input document but only on its depth. &copy; 2011 ACM.<br/>},
key = {XML},
keywords = {Automata theory;Polynomial approximation;Specifications;},
note = {Exponential time;Polynomial time complexity;schema;Schema language;Standard algorithms;Visibly pushdown automaton;XML schema evolutions;XML schema languages;},
URL = {http://dx.doi.org/10.1145/1982185.1982530},
} 


@inproceedings{20114714531254 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Model-driven approach to XML schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Necasky, Martin and Mlynkova, Irena and Klimek, Jakub},
volume = {7046 LNCS},
year = {2011},
pages = {514 - 523},
issn = {03029743},
address = {Hersonissos, Crete, Greece},
abstract = {Today, XML is a standard meta-language for representation of exchanged messages between information systems. To enable exchange, the structure of the messages must be established in a form of XML schemas. Usually, more than one type of messages is exchanged and, hence, a family of XML schemas needs to be created. An important task for the designer is, therefore, to design the XML schemas and then evolve them continuously as user requirements change. Doing this manually may be very difficult due to the fact that single change in the user requirements may impact many XML schemas. In this paper, we present a novel approach to evolution of families of XML schemas. It is based on modeling XML schemas at two levels - conceptual and XML schema. The designer performs a change only once in the conceptual schema and our introduced mechanism propagates the change to all affected XML schemas. Propagation from the XML schema to the conceptual level is also supported. &copy; 2011 Springer-Verlag.<br/>},
key = {XML},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Conceptual levels;Conceptual schemas;Meta language;Model driven approach;User requirements;XML schema evolutions;XML schemas;},
URL = {http://dx.doi.org/10.1007/978-3-642-25126-9_63},
} 


@inproceedings{2006289986209 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {X-evolution: A system for XML schema evolution and document adaptation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Mesiti, Marco and Celle, Roberto and Sorrenti, Matteo A. and Guerrini, Giovanna},
volume = {3896 LNCS},
year = {2006},
pages = {1143 - 1146},
issn = {03029743},
address = {Munich, Germany},
abstract = {No abstract available},
URL = {http://dx.doi.org/10.1007/11687238_78},
} 


@inproceedings{20090111826394 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {X-Evolution: A comprehensive approach for XML schema evolution},
journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
author = {Guerrini, Giovanna and Mesiti, Marco},
year = {2008},
pages = {251 - 255},
issn = {15294188},
address = {Turin, Italy},
abstract = {In this paper we present X-Evolution, a Web system developed on top of a commercial DBMS that allows the specification of schema modifications both on a graphical representation of an XML Schema and through a specifically tailored declarative language. X-Evolution supports facilities for performing schema revalidation only when strictly needed and only on the minimal parts of documents affected by the modifications. Moreover, it supports the automatic and query-based adaptation of original schema instances to the evolved schema. &copy; 2008 IEEE.<br/>},
key = {XML},
keywords = {Expert systems;Query processing;},
note = {Declarative Languages;Graphical representations;It supports;Revalidation;Web system;XML schema evolutions;XML schemas;},
URL = {http://dx.doi.org/10.1109/DEXA.2008.128},
} 


@inproceedings{20083611507890 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Hecataeus: A what-if analysis tool for database schema evolution},
journal = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author = {Papastefanatos, George and Anagnostou, Fotini and Vassiliou, Yannis and Vassiliadis, Panos},
year = {2008},
pages = {326 - 328},
issn = {15345351},
address = {Athens, Greece},
abstract = {Databases are continuously evolving environments, where design constructs are added, removed or updated rather often. Small changes in the database configurations might impact a large number of applications and data stores around the system: queries and data entry forms can be invalidated, application programs might crash. HECATAEUS is a tool, which represents the database schema along with its dependent workload, mainly queries and views, as a uniform directed graph. The tool enables the user to create hypothetical evolution events and examine their impact over the overall graph as well as to define rules so that both syntactical and semantic correctness of the affected workload is retained. &copy; 2008 IEEE.<br/>},
key = {Query languages},
keywords = {Application programs;Computer software maintenance;Directed graphs;Reengineering;Semantics;},
note = {Data store;Database configuration;Database schemas;What-if Analysis;},
URL = {http://dx.doi.org/10.1109/CSMR.2008.4493341},
} 


@article{20122815235128 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Update rewriting and integrity constraint maintenance in a schema evolution support system: PRISM++},
journal = {Proceedings of the VLDB Endowment},
author = {Curino, Carlo A. and Moon, Hyun Jin and Deutsch, Alin and Zaniolo, Carlo},
volume = {4},
number = {2},
year = {2010},
pages = {117 - 128},
issn = {21508097},
abstract = {Supporting legacy applications when the database schema evolves represents a long-standing challenge of practical and theoretical importance. Recent work has produced algorithms and systems that automate the process of data migration and query adaptation; however, the problems of evolving integrity constraints and supporting legacy updates under schema and integrity constraints evolution are significantly more difficult and have thus far remained unsolved. In this paper, we address this issue by introducing a formal evolution model for the database schema structure and its integrity constraints, and use it to derive update mapping techniques akin to the rewriting techniques used for queries. Thus, we (i) propose a new set of Integrity Constraints Modification Operators (ICMOs), (ii) characterize the impact on integrity constraints of structural schema changes, (iii) devise representations that enable the rewriting of updates, and (iv) develop a unified approach for query and update rewriting under constraints. We then describe the implementation of these techniques provided by our PRISM++ system. The effectiveness of PRISM++ and its enabling technology has been verified on a testbed containing evolution histories of several scientific databases and web information systems, including the Genetic DB Ensembl (410+ schema versions in 9 years), and Wikipedia (240+ schema versions in 6 years). &copy; 2010 VLDB Endowment.<br/>},
key = {Search engines},
keywords = {Legacy systems;Prisms;Query languages;Query processing;},
note = {Enabling technologies;Evolution history;Evolution modeling;Integrity constraints;Legacy applications;Mapping techniques;Scientific database;Web information systems;},
URL = {http://dx.doi.org/10.14778/1921071.1921078},
} 


@article{1995022572115 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution through changes to FR diagrams},
journal = {Journal of Information Science and Engineering},
author = {Liu, C.T. and Chrysanthis, P.K. and Chang, S.K.},
volume = {9},
number = {4},
year = {1993},
pages = {657 - 657},
issn = {10162364},
} 


@inproceedings{1994122471527 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in CASE databases},
journal = {Software Quality Management - International Conference},
author = {Bouneffa, M. and Boudjlida, N.},
volume = {2},
year = {1994},
pages = {663 - 663},
address = {Edinburgh, United kingdom},
} 


@inbook{1994122489581 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution for object-based accounting database systems},
journal = {Lecture Notes in Computer Science},
author = {Chen, Jia-L. and McLeod, D. and O'Leary, D.},
number = {858},
year = {1994},
pages = {40 - 40},
issn = {03029743},
address = {Palermo, Italy},
} 


@inproceedings{1995102891877 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database schema evolution using ever diagrams},
journal = {Proceedings of the Workshop on Advanced Visual Interfaces},
author = {Liu, C.T. and Chang, S.K. and Chrysanthis, P.K.},
year = {1994},
pages = {123 - 123},
address = {Bari, Italy},
URL = {http://dx.doi.org/10.1145/192309.192338},
} 


@inbook{1998024084670 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing schema evolution using a temporal object model},
journal = {Lecture Notes in Computer Science},
author = {Goralwalla, I.A. and Szafron, D. and Ozsu, M.T. and Peters, R.J.},
volume = {1331},
year = {1997},
pages = {71 - 71},
issn = {03029743},
address = {Los Angeles, CA, United states},
} 


@inproceedings{1996093337845 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {View mechanism for schema evolution in object-oriented DBMS},
journal = {Lecture Notes in Computer Science},
author = {Bellahsene, Zohra},
volume = {1094},
year = {1996},
pages = {18 - 18},
address = {Edinburgh, United kingdom},
} 


@inproceedings{1991120274099 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Hybrid relations for database schema evolution},
journal = {Proceedings - IEEE Computer Society's International Computer Software &amp; Applications Conference},
author = {Takahashi, Junichi},
year = {1990},
pages = {465 - 470},
issn = {07306512},
address = {Chicago, IL, USA},
abstract = {The author describes hybrid relations in relational databases that allow existing relations to be altered by the addition of new attributes without reorganization of the database scheme. The values of new attributes with respect to an existing relation are stored separately from the relation as a set of triples of tuple identifier, attribute name, and value. At query time, a hybrid relation, which has only the attributes requested in a query, is derived virtually by combining the relation and this set of triples. A relation can be reorganized by upgrading its attribute values from these triples. The hybrid relation is defined as an algebraic expression, and equivalent expressions of a query on the hybrid relations are shown for efficient query processing.},
key = {Database systems},
keywords = {Computer Interfaces--Human Factors;Computer Metatheory--Programming Theory;},
note = {Database Schema;},
} 


@article{1997223596069 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data schema design as a schema evolution process},
journal = {Data and Knowledge Engineering},
author = {Proper, H.A.},
volume = {22},
number = {2},
year = {1997},
pages = {159 - 189},
issn = {0169023X},
abstract = {In an information system a key role is played by the underlying data schema. This article starts out from the view that the entire modelling process of an information system's data schema can be seen as a schema transformation process. A transformation process that starts out with an initial draft conceptual schema and ends with an internal database schema for some implementation platform. This allows us to describe the transformation process of a database design as an evolution of a schema through a universe of data schemas. Doing so allows a better understanding of the actual design process, countering the problem of 'software development under the lamppost'. Even when the information system design is finalised, the data schema can evolve further due to changes in the requirements on the system. We present a universe of data schemas that allows us to describe the underlying data schemas at all stages of their development. This universe of data schemas is used as a case study on how to describe the complete evolution of a data schema with all its relevant aspects. The theory is general enough to cater for more modelling concepts, or different modelling approaches. To actually model the evolution of a data schema we present a versioning mechanism that allows us to model the evolutions of the elements of data schemas and their interactions, leading to a better understanding of the schema design process as a whole. Finally, we also discuss the relationship between this simple versioning mechanism and general-purpose version-management systems.},
key = {Data structures},
keywords = {Hierarchical systems;Object oriented programming;Relational database systems;Software engineering;},
note = {Conceptual modelling;Data schema;Entity relationship modelling;Object role modelling;Schema transformation process;Version management;},
URL = {http://dx.doi.org/10.1016/S0169-023X(96)00045-6},
} 


@article{1992010377999 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Temporally oriented data definitions. Managing schema evolution in temporally oriented databases},
journal = {Data and Knowledge Engineering},
author = {Ariav, Gad},
volume = {6},
number = {6},
year = {1991},
pages = {451 - 467},
issn = {0169023X},
abstract = {A simplifying - yet unrealistic - assumption widely held throughout the research of Temporally Oriented Data Models (TODM) is that the associated schema never changes. The implications of allowing data structures to evolve over time within a TODM and related databases are examined in this paper, and key issues and concepts are identified. Specifically, Temporally Oriented Data Definition (TODD) raises questions with respect to (1) the evolution of meanings in databases, (2) the nature of the temporal prevalence of database schema, and (3) the general principles that may guide the implementation of a TODM database with TODD.},
key = {Database Systems},
keywords = {Data Processing - Data Structures;},
note = {Data Models;Temporal Databases;Temporally Oriented Data Models (TODM);},
URL = {http://dx.doi.org/10.1016/0169-023X(91)90023-Q},
} 


@inproceedings{1995082822855 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting exceptions to behavioral schema consistency to ease schema evolution in OODBMS},
journal = {Very Large Data Bases, International Conference Proceedings},
author = {Amiel, Eric and Bellosta, Marie-Jo and Dujardin, Eric and Simon, Eric},
year = {1994},
pages = {108 - 108},
address = {Santiago, Chile},
} 


@inproceedings{20141717605907 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a flexible and transparent database evolution},
journal = {Advances in Intelligent Systems and Computing},
author = {Pereira, Rui Humberto and Perez-Schofield, J. Baltasar Garcia},
volume = {276 VOLUME 2},
year = {2014},
pages = {23 - 33},
issn = {21945357},
address = {Madeira, Portugal},
abstract = {Applications refactorings that imply the schema evolution are common activities in programming practices. Although modern object-oriented databases provide transparent schema evolution mechanisms, those refactorings continue to be time consuming tasks for programmers. In this paper we address this problem with a novel approach based on aspect-oriented programming and orthogonal persistence paradigms, as well as our meta-model. An overview of our framework is presented. This framework, a prototype based on that approach, provides applications with aspects of persistence and database evolution. It also provides a newpointcut/advice language that enables the modularization of the instance adaptation crosscutting concern of classes, which were subject to a schema evolution. We also present an application that relies on our framework. This application was developed without any concern regarding persistence and database evolution. However, its data is recovered in each execution, as well as objects, in previous schema versions, remain available, transparently, by means of our framework. &copy; Springer International Publishing Switzerland 2014.<br/>},
key = {Aspect oriented programming},
keywords = {Information systems;Information use;Modular construction;Object oriented programming;Object-oriented databases;},
note = {Cross-cutting concerns;Meta model;Modularizations;Orthogonal persistence;Programming practices;Refactorings;Schema evolution;Time-consuming tasks;},
URL = {http://dx.doi.org/10.1007/978-3-319-05948-8_3},
} 


@article{20162402503311 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modularizing application and database evolution  an aspect-oriented framework for orthogonal persistence},
journal = {Software - Practice and Experience},
author = {Pereira, Rui Humberto R. and Garcia Perez-Schofield, J. Baltasar and Ortin, Francisco},
volume = {47},
number = {2},
year = {2017},
pages = {193 - 221},
issn = {00380644},
abstract = {In the maintenance of software applications, database evolution is one common difficulty. In object-oriented databases, this process comprises schema evolution and instance adaptation. Both tasks usually require significant effort from programmers and database administrators. In this paper, we propose orthogonal persistence and aspect-oriented programming to support semi-transparent database evolution. A default mechanism for instance evolution is defined, but the user may provide modularized solutions using the aspect-oriented paradigm. We present our framework AOF4OOP to test the feasibility of our proposed approach. This prototype allows programmes to transparently access data in other versions of the database schema. We evaluate our framework, comparing it to related approaches using two real applications and measuring the improvement of the productivity of the programmer. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.<br/> Copyright &copy; 2016 John Wiley & Sons, Ltd.},
key = {Aspect oriented programming},
keywords = {Application programs;Copyrights;Object oriented programming;Object-oriented databases;},
note = {Aspect-oriented frameworks;Aspect-oriented paradigms;Database administrators;Database schemas;instance adaptation;Real applications;Schema evolution;Software applications;},
URL = {http://dx.doi.org/10.1002/spe.2415},
} 


@article{2005189074833 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A database evolution taxonomy for object-oriented databases},
journal = {Journal of Software Maintenance and Evolution},
author = {Rashid, Awais and Sawyer, Peter},
volume = {17},
number = {2},
year = {2005},
pages = {93 - 141},
issn = {1532060X},
abstract = {Like any other database application, object database applications are subject to evolution. Evolution, however, is a critical requirement in object-oriented databases as it is a fundamental characteristic of complex applications such as computer-aided design and manufacturing (CAD/CAM) and office information systems. Object-oriented databases are inherently suited to supporting such applications. In this paper we present a database evolution taxonomy for object-oriented databases. We describe a conceptual database model and use it to define the taxonomy. We also present the various invariants and rules governing the various evolution operations. The execution sequence of rules is described. An implementation of the database model and the evolution taxonomy in the Semi-Autonomous Database Evolution System (SADES), is discussed. The implementation employs aspect-oriented programming techniques to provide a flexible means of transforming objects upon evolution, and implementing some application-specific evolution primitives. A case study compares the evolution taxonomy with existing evolution approaches. The comparison demonstrates that the taxonomy and its corresponding implementation in SADES provide improved coverage of the fundamental evolution operations to which an object database might be subjected. At the same time, erosion of the database structure is avoided by maintaining a coherent and comprehensible view of historical changes. Copyright &copy; 2005 John Wiley &amp; Sons, Ltd.},
key = {Database systems},
keywords = {Computer aided design;Computer aided manufacturing;Evolutionary algorithms;Mathematical models;Object oriented programming;Semantics;},
note = {Aspect-oriented programming;Class versioning;Object database evolution;Object versioning;Schema evolution;Separation of concerns;},
URL = {http://dx.doi.org/10.1002/smr.310},
} 


@inproceedings{20140217192374 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Understanding schema evolution as a basis for database reengineering},
journal = {IEEE International Conference on Software Maintenance, ICSM},
author = {Gobert, Maxime and Maes, Jerome and Cleve, Anthony and Weber, Jens},
year = {2013},
pages = {472 - 475},
address = {Eindhoven, Netherlands},
abstract = {Software repositories can provide valuable information for facilitating software reengineering efforts. In recent years, many researchers have started to follow a holistic approach, considering diverse software artifacts and the links existing between them. However, when analyzing data-intensive systems, comparatively little attention has been devoted to the analysis of an important system artifact: the database. Even fewer approaches attempt to uncover facts about the evolution history of database schemas. We have developed a tool-supported method for analyzing and visualizing database schema history. This paper reports early results of applying and validating this method. We discuss our experiences to date and point out several novel research perspectives in this domain. &copy; 2013 IEEE.<br/>},
key = {Computer software maintenance},
keywords = {Database systems;Reengineering;Software engineering;},
note = {Data reengineering;Data-intensive systems;Database reengineering;Mining software repositories;Schema evolution;Software artifacts;Software reengineering;Software repositories;},
URL = {http://dx.doi.org/10.1109/ICSM.2013.75},
} 


@inproceedings{20080311024156 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A logic framework to support database refactoring},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Chang, Shi-Kuo and Deufemia, Vincenzo and Polese, Giuseppe and Vacca, Mario},
volume = {4653 LNCS},
year = {2007},
pages = {509 - 518},
issn = {03029743},
address = {Regensburg, Germany},
abstract = {We propose a formal framework for database refactoring, analyzing both the changes to the database schema, and their impact on queries. The framework defines a logic model of changes, and views the database refactoring process as an agent based one. The agent tries to discover and resolve inconsistencies, and it is modeled as a problem solver capable to perform changes triggered upon the detection of database schema anomalies. The framework can be considered a first step towards the automation of the database refactoring process. &copy; Springer-Verlag Berlin Heidelberg 2007.},
key = {Formal logic},
keywords = {Database systems;Mathematical models;Problem solving;Query processing;},
note = {Database refactoring;Logic frameworks;},
} 


@inproceedings{20183205674757 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database Reengineering Scheme from Object-Oriented Model to Flattened XML Data Model},
journal = {Communications in Computer and Information Science},
author = {Liu, Yue and Wu, Xukun},
volume = {873},
year = {2018},
pages = {259 - 268},
issn = {18650929},
address = {Guangzhou, China},
abstract = {To deal with issues of data model transformation and to make combined use of different data models, this paper proposed a database reengineering scheme which includes schema translation and data conversion from object-oriented data model to flattened XML data model with is-a and cardinality data semantics preservation. In this paper, conceptual schema of object-oriented data model and flattened XML data model are elaborated by UML class diagram and XML Schema Definition(XSD) graph respectively. Logical schema is described by UNISQL class definition and XSD respectively. This paper firstly analyzes class definitions from OODB and sorts them from most independent classes to most dependent classes. Secondly schema translation is processed by mapping class to complexType and then creating element definitions in XSD file. Thirdly data conversion is processed automatically. Performance tests have shown that the proposed database reengineering scheme is reliable and efficient.<br/> &copy; 2018, Springer Nature Singapore Pte Ltd.},
key = {Data handling},
keywords = {Intelligent systems;Metadata;Object-oriented databases;Reengineering;Semantics;XML;},
note = {Data reengineering;Database reengineering;Model transformation;Object oriented data;Object oriented model;Schema translations;XML data models;Xml schema definitions;},
URL = {http://dx.doi.org/10.1007/978-981-13-1648-7_22},
} 


@inproceedings{20082811354013 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating program conversion in database reengineering a wrapper-based approach},
journal = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author = {Cleve, Anthony},
year = {2006},
pages = {323 - 326},
issn = {15345351},
address = {Bari, Italy},
abstract = {Database reengineering consists in deriving a new database from a legacy database and adapting associated software components accordingly. This migration process typically involves three main steps, namely schema conversion, data conversion and program conversion. This paper presents a wrapper-based approach to automating the program conversion step. The proposed approach combines program transformations and code generation, which are derived from schema transformations. &copy; 2006 IEEE.<br/>},
key = {Data handling},
keywords = {Computer software maintenance;Database systems;Reengineering;},
note = {Associated softwares;Database reengineering;Migration process;Program conversion;Program transformations;Schema conversion;Schema transformation;Wrapper-based approach;},
URL = {http://dx.doi.org/10.1109/CSMR.2006.12},
} 


@inproceedings{2002427140749 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Supporting iterations in exploratory database reengineering processes},
journal = {Science of Computer Programming},
author = {Jahnke, Jens H. and Schafer, Wilhelm and Wadsack, Jorg P. and Zundorf, Albert},
volume = {45},
number = {2-3},
year = {2002},
pages = {99 - 136},
issn = {01676423},
address = {Amsterdam, Netherlands},
abstract = {Key technologies like the World Wide Web, object-orientation, and distributed computing enable new applications, e.g., in the area of electronic commerce, management of information systems, and decision support systems. Today, many companies face the problem that they have to reengineer pre-existing information systems to take advantage of these technologies. Various computer-aided reengineering tools have been developed to reduce the complexity of the reengineering task. A major limitation of current approaches, however, is that they impose a strictly phase-oriented, waterfall-type reengineering process, with little support for iterations. Still, such iterations often occur in real-world examples, e.g., when additional knowledge about the legacy system becomes available or when the legacy system is modified during an ongoing migration process. In this paper, we present an approach to incremental consistency management that allows to overcome this limitation in the domain of database systems by integrating reverse and forward engineering activities in an intertwined process. The described mechanism is based on a formalization of conceptual schema translation and redesign transformations by graph rewriting rules and has been implemented and evaluated with the Varlet database reengineering environment. &copy; 2002 Elsevier Science B.V. All rights reserved.},
key = {Database systems},
keywords = {Computer aided engineering;Decision support systems;Distributed computer systems;Iterative methods;Legacy systems;Reengineering;World Wide Web;},
note = {Database reengineering processes;Information systems;},
URL = {http://dx.doi.org/10.1016/S0167-6423(02)00056-4},
} 


@article{20182305284039 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A cybernetic approach to solving the problem of database reengineering},
journal = {Telecommunications and Radio Engineering (English translation of Elektrosvyaz and Radiotekhnika)},
author = {Yesin, V.I.},
volume = {77},
number = {5},
year = {2018},
pages = {399 - 409},
issn = {00402508},
abstract = {The task of reengineering databases of the organizational management information system is formulated and formalized in the form of the structural adaptation task known from the general adaptive systems theory. The method for its solution is proposed.<br/> &copy; 2018 by Begell House, Inc.},
key = {Reengineering},
keywords = {Database systems;Information management;Information systems;Information use;},
note = {Database reengineering;Organizational management;Structural adaptation;},
URL = {http://dx.doi.org/10.1615/TelecomRadEng.v77.i5.40},
} 


@inproceedings{20171203453181 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Toward a database refactoring support tool},
journal = {Proceedings - 2016 4th International Symposium on Computing and Networking, CANDAR 2016},
author = {Hamaji, Kohei and Nakamoto, Yukikazu},
year = {2016},
pages = {443 - 446},
address = {Hiroshima, Japan},
abstract = {Database schema changes in system development cannot be avoidable. We need refactoring for the schema. Refactoring criteria for the schema, however, has not been clear and the refactoring depends on the skill of database designers. To solve the problems, we consider to finding out refactoring target columns and recommending them as refactoring targets by using clustering techniques with features of database schema and data in the table and implement it as a support tool. We describe the method and the support tool using the method.<br/> &copy; 2016 IEEE.},
key = {Database systems},
keywords = {Learning systems;},
note = {Clustering techniques;Database schemas;Refacroting;Refactorings;Support tool;System development;},
URL = {http://dx.doi.org/10.1109/CANDAR.2016.65},
} 


@article{20154601547385 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database Refactoring: Lessons from the Trenches},
journal = {IEEE Software},
author = {Vial, Gregory},
volume = {32},
number = {6},
year = {2015},
pages = {71 - 79},
issn = {07407459},
abstract = {Although database refactoring has been advocated as an important area of database development, little research has studied its implications. A small software development firm refactored a database related to an application that lets clients optimize their logistics processes. This project was based on the design of clear database development conventions and the need to package documentation in the database itself. The experience led to five key lessons learned: refactoring should be automated whenever possible, the database catalog is crucial, refactoring is easier when it's done progressively, refactoring can help optimize an application and streamline its code base, and refactoring related to application development requires a complex skill set and must be applied sensibly. This article is part of a special issue on Refactoring.<br/> &copy; 2015 IEEE.},
key = {Software design},
keywords = {Application programs;Query languages;Software engineering;},
note = {Data description languages;Database design;Database management;Refactorings;Relational Database;Transaction processing;},
URL = {http://dx.doi.org/10.1109/MS.2015.131},
} 


@inproceedings{20125015790465 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Database refactoring and regression testing of android mobile applications},
journal = {2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics, SISY 2012},
author = {Szabo, Csaba and Samuelis, Ladislav and Ivanovic, Mirjana and Fesic, Toma},
year = {2012},
pages = {135 - 139},
address = {Subotica, Serbia},
abstract = {The widespread usage of mobile devices has made rapid progress recently in the society. The gradual development, modification and improvement of mobile applications is a rule. This contribution presents the application of refactoring and regression testing, which supports the software modification in a systematic and controlled manner. The database refactoring and regression testing is presented within the development of an experimental mobile Android-based application. Finally, the contribution evaluates the results of the experiments and verifies their correctness. &copy; 2012 IEEE.<br/>},
key = {Android (operating system)},
keywords = {Application programs;Database systems;Intelligent systems;Mobile computing;Regression analysis;Software testing;},
note = {Improvement of mobile applications;Mobile applications;Refactorings;Regression testing;Software modification;},
URL = {http://dx.doi.org/10.1109/SISY.2012.6339502},
} 


@inproceedings{20174104269083 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SQL antipatterns detection and database refactoring process},
journal = {Proceedings - 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2017},
author = {Khumnin, Poonyanuch and Senivongse, Twittie},
year = {2017},
pages = {199 - 205},
address = {Kanazawa, Japan},
abstract = {SQL antipatterns are frequently-made missteps that are commonly found in the design of relational databases, the use of SQL, and the development of database applications. They are intended to solve certain problems but will eventually lead to other problems. The motivation of this paper is how to assist database administrators in diagnosing SQL antipatterns and suggest refactoring techniques to solve the antipatterns. Specifically, we attempt to automate the detection of logical database design antipatterns by developing a tool that uses Transact-SQL language to query and analyze the database schema. The tool reports on potential antipatterns and gives an instruction on how to refactor the database schema. In an evaluation based on three databases from the industry, the performance of the tool is satisfactory in terms of recall of the antipatterns but the tool detects a number of false positives which affect its precision. It is found that SQL antipatterns detection still largely depends on the semantics of the data and the detection tool should rather be used in a semi-automated manner, i.e it can point out potential problematic locations in the database schema which require further diagnosis by the database administrators. This approach would be useful especially in the context of large databases where manual antipatterns inspection is very difficult.<br/> &copy; 2017 IEEE.},
key = {Query processing},
keywords = {Artificial intelligence;Semantics;Software engineering;},
note = {Anti-patterns;Antipatterns detections;Database administrators;Database applications;Database schemas;Refactorings;Relational Database;Transact-SQL;},
URL = {http://dx.doi.org/10.1109/SNPD.2017.8022723},
} 


@article{20150900575072 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A workflow for database refactoring},
journal = {International Journal of Innovative Computing, Information and Control},
author = {Domingues, Marcia Beatriz Pereira and de Almeida, Jorge Rady and Costa, Wilian Franca and Mauro Saraiva, Antonio},
volume = {10},
number = {6},
year = {2014},
pages = {2209 - 2220},
issn = {13494198},
abstract = {The development and maintenance of a database are significant challenges due to frequent changes and requirements from users. To follow these changes, the database schema suffers structural modifications that, many times, negatively affect its performance and the results of queries, such as unnecessary relationships, primary and foreign keys created and strongly attached to the domain, with obsolete attributes or inadequate types of attributes. The literature on Agile Methods for Software Development suggests the use of refactoring for the evolution of database schema when there are requirement changes. A refactoring is a simple change that improves the design, but it does not alter the semantics of the data model or add new functionalities. This workflow has significant differences in relation to that proposed by Amblers, and its use brings significant advantages in the maintenance of complex databases with large amounts of data. As a case study, a relational database, used by an information system for precision agriculture, was used. This system is web based and needs to archive data, retrieve information, and respond to large geospatial queries.<br/> &copy; 2014, IJICIC Editorial Office. All rights reserved.},
key = {Query languages},
keywords = {Query processing;Semantics;Software design;},
note = {BPMN;Database schemas;Large amounts of data;Query performance;Refactorings;Relational Database;Requirement change;Structural modifications;},
} 


@inproceedings{1997023538760 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Management perspective of database reengineering},
author = {Yau, Chuk},
year = {1996},
pages = {41 - 41},
address = {Kowloon, Hong kong},
} 


@inproceedings{1997023538771 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Pilot survey of database reengineering and interoperability},
author = {Kwan, Irene S.Y.},
year = {1996},
pages = {199 - 199},
address = {Kowloon, Hong kong},
} 


@inproceedings{20063110040450 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A methodology for database reengineering to web services},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {De Guzman, Ignacio Garcia-Rodriguez and Polo, Macario and Piattini, Mario},
volume = {4066 LNCS},
year = {2006},
pages = {226 - 240},
issn = {03029743},
address = {Bilbao, Spain},
abstract = {Databases are one of the most important components of information systems, since they keep all the information of organizations. Although new standards in databases have appeared in the last years, most databases are still based on SQL-92, and are thus true legacy systems. Most of the services offered by information systems are based on the information stored in their databases. In order to allow interoperability, current trends advise exposing some of these services to the Web, making them available for other users and also for the information system itself. Since dealing with old databases and their associated software is difficult, a methodology to discover services from SQL-92 databases and to offer them via Web Services is proposed. This methodology is based on the MDA approach and implements a reengineering process, which starts from an SQL-92 database and obtains a set of services that can be exposed as Web Services. &copy; Springer-Verlag Berlin Heidelberg 2006.},
key = {Reengineering},
keywords = {Computer software;Database systems;Information services;Interoperability;Legacy systems;Reverse engineering;World Wide Web;},
note = {MDA;Metamodel;Patterns;QVT;Web Services;},
} 


@article{2002266989785 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evaluating theories for managing imperfect knowledge in human-centric database reengineering environments},
journal = {International Journal of Software Engineering and Knowledge Engineering},
author = {Jahnke, Jens H. and Walenstein, Andrew},
volume = {12},
number = {1},
year = {2002},
pages = {77 - 102},
issn = {02181940},
abstract = {Modernizing heavily evolved and poorly documented information systems is a central software engineering problem in our current IT industry. It is often necessary to reverse engineer the design documentation of such legacy systems. Several interactive CASE tools have been developed to support this human-intensive process. However, practical experience indicates that their applicability is limited because they do not adequately handle imperfect knowledge about legacy systems. In this paper, we investigate the applicability of several major theories of imperfect knowledge management in the area of soft computing and approximate reasoning. The theories are evaluated with respect to how well they meet requirements for generating effective human-centred reverse engineering environments. The requirements were elicited with help from practical case studies in the area of database reverse engineering. A particular theory called "possibilistic logic" was found to best meet these requirements most comprehensively. This evaluation highlights important challenges to the designers of knowledge management techniques, and should help reverse engineering tool implementers select appropriate technologies.},
key = {Reengineering},
keywords = {Computer aided software engineering;Computer software selection and evaluation;Database systems;Knowledge engineering;Management information systems;Reverse engineering;},
note = {Approximate reasoning;Human centred reverse engineering;Imperfect knowledge;Possibilistic logic;Soft computing;},
URL = {http://dx.doi.org/10.1142/S0218194002000834},
} 


@inproceedings{20191106618246 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {VESEL: Visual exploration of schema evolution using provenance queries},
journal = {CEUR Workshop Proceedings},
author = {Athinaiou, Christos and Kondylakis, Haridimos},
volume = {2322},
year = {2019},
issn = {16130073},
address = {Lisbon, Portugal},
abstract = {Database schemata are not static artifacts but subject to continuous change as new requirements daily occur and the modeling choices of the past should be updated or adapted. To this direction, multiple approaches available already try to keep multiple co-existing schema versions in parallel or model schema evolution through Schema Modification Operations, known as SMOs. However, to the best of our knowledge, in the era of big data, where thousands of SMOs might appear, it is really hard for developers to identify the modeling choices of the past and to explore how a specific column or table has been evolved. In this demo, we present VESEL, the first system enabling the VisuaL Exploration of Schema Evolution using provenance queries. Our approach relies on a state of the art database evolution language, and can efficiently answer queries about when a specific table or column has been introduced, how - with which SMO operation and why - which is the sequence of changes that led to the creation of the specific table/column. In the demonstration we will present the architecture of our system and the various algorithms implemented, enabling end-users to visually explore schema evolution. Then we will allow conference participants to interact directly with the system to test its capabilities.<br/> &copy; 2019 Copyright held by the owner/author(s).},
key = {Query languages},
keywords = {Query processing;Visualization;},
note = {Co-existing;Database schemas;End users;First systems;Model choice;Schema evolution;State of the art;Visual exploration;},
} 


@inproceedings{20163002636319 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution for databases and data warehouses},
journal = {Lecture Notes in Business Information Processing},
author = {Manousis, Petros and Vassiliadis, Panos and Zarras, Apostolos and Papastefanatos, George},
volume = {253},
year = {2016},
pages = {1 - 31},
issn = {18651348},
address = {Barcelona, Spain},
abstract = {Like all software systems, databases are subject to evolution as time passes. The impact of this evolution is tremendous as every change to the schema of a database affects the syntactic correctness and the semantic validity of all the surrounding applications and de facto necessitates their maintenance in order to remove errors from their source code. This survey provides a walk-through on different approaches to the problem of handling database and data warehouse schema evolution. The areas covered include (a) published case studies with statistical information on database evolution, (b) techniques for managing schema and view evolution, (c) techniques pertaining to the area of data warehouses, and, (d) prospects for future research.<br/> &copy; Springer International Publishing Switzerland 2016.},
key = {Data warehouses},
keywords = {Data handling;Information analysis;Semantics;},
note = {Case-studies;Schema evolution;Software systems;Source codes;Statistical information;},
URL = {http://dx.doi.org/10.1007/978-3-319-39243-1_1},
} 


@inproceedings{20135117114630 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema repository for database schema evolution},
journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
author = {Bounif, Hassina and Pottinger, Rachel},
year = {2006},
pages = {647 - 651},
issn = {15294188},
address = {Krakow, Poland},
abstract = {The paper presents a schema repository, an original repository containing different kinds of database schemas. The repository is part of a multidisciplinary approach for schema evolution called the predictive approach for database evolution. The schema repository has a dual role in the approach: (1) during the datamining process, the repository identifies and analyzes trends on collected schemas belonging to the same domain. (2) the repository is used in the building of the requirements ontology - A domain ontology that contributes in the database design and its evolution. This paper presents both the design and a heuristic-based method to populate such a repository. Copyright &copy; 2006 by The Institute of Electrical and Electronics Engineers, Inc.<br/>},
key = {Database systems},
keywords = {Expert systems;Heuristic methods;Ontology;},
note = {Database design;Database schemas;Domain ontologies;Dual role;Multi-disciplinary approach;Requirements ontology;Schema evolution;},
} 


@inproceedings{20101212784358 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in wikipedia - Toward a web Information system benchmark},
journal = {ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems},
author = {Curino, Carlo A. and Moon, Hyun J. and Tanca, Letizia and Zaniolo, Carlo},
volume = {DISI},
year = {2008},
pages = {323 - 332},
address = {Barcelona, Spain},
abstract = {Evolving the database that is at the core of an Information System represents a difficult maintenance problem that has only been studied in the framework of traditional information systems. However, the problem is likely to be even more severe in web information systems, where open-source software is often developed through the contributions and collaboration of many groups and individuals. Therefore, in this paper, we present an indepth analysis of the evolution history of the Wikipedia database and its schema; Wikipedia is the best-known example of a large family of web information systems built using the open-source software MediaWiki. Our study is based on: (i) a set of Schema Modification Operators that provide a simple conceptual representation for complex schema changes, and (ii) simple software tools to automate the analysis. This framework allowed us to dissect and analyze the 4.5 years of Wikipedia history, which was short in time, but intense in terms of growth and evolution. Beyond confirming the initial hunch about the severity of the problem, our analysis suggests the need for developing better methods and tools to support graceful schema evolution. Therefore, we briefly discuss documentation and automation support systems for database evolution, and suggest that the Wikipedia case study can provide the kernel of a benchmark for testing and improving such systems.<br/>},
key = {Information use},
keywords = {Benchmarking;Database systems;Information systems;Open source software;Open systems;},
note = {Evolution history;In-depth analysis;Maintenance Problem;Schema changes;Schema evolution;Support systems;Web information systems;Wikipedia;},
} 


@inproceedings{20171203457038 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {NoSQL schema evolution and big data migration at scale},
journal = {Proceedings - 2016 IEEE International Conference on Big Data, Big Data 2016},
author = {Klettke, Meike and Storl, Uta and Shenavai, Manuel and Scherzinger, Stefanie},
year = {2016},
pages = {2764 - 2774},
address = {Washington, DC, United states},
abstract = {This paper explores scalable implementation strategies for carrying out lazy schema evolution in NoSQL data stores. For decades, schema evolution has been an evergreen in database research. Yet new challenges arise in the context of cloud-hosted data backends: With all database reads and writes charged by the provider, migrating the entire data instance eagerly into a new schema can be prohibitively expensive. Thus, lazy migration may be more cost-efficient, as legacy entities are only migrated in case they are actually accessed by the application. Related work has shown that the overhead of migrating data lazily is affordable when a single evolutionary change is carried out, such as adding a new property. In this paper, we focus on long-term schema evolution, where chains of pending schema evolution operations may have to be applied. Chains occur when legacy entities written several application releases back are finally accessed by the application. We discuss strategies for dealing with chains of evolution operations, in particular, the composition into a single, equivalent composite migration that performs the required version jump. Our experiments with MongoDB focus on scalable implementation strategies. Our lineup further compares the number of write operations, and thus, the operational costs of different data migration strategies.<br/> &copy; 2016 IEEE.},
key = {Big data},
keywords = {Chains;Database systems;},
note = {Data migration;Incremental Migration;Lazy Composite Migration;Lazy Migration;Nosql database;Predictive Migration;Schema evolution;},
URL = {http://dx.doi.org/10.1109/BigData.2016.7840924},
} 


@inproceedings{20151300683254 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A taxonomy of changes in schema evolution},
journal = {2014 International Conference on Computational Science and Technology, ICCST 2014},
author = {Masood, Nayyer and Andleeb, Naira},
year = {2014},
pages = {Center of Excellence in Semantic Agents; IEEE Malaysia Section C Chapter; Universiti Malaysia Sabah (UMS) - },
address = {Kota Kinabalu, Sabah, Malaysia},
abstract = {Schema evolution is the ability of a database system to respond to changes in the real world by allowing the schema to evolve. Two main categories of research work on schema evolution exist in literature; the changes-based and the tgd-based (target generating dependencies that represent mappings between elements). The tgd-based work is more recent and is mainly based on the use of composition and inversion operators to manage schema evolution. This body of work has established different combinations of tgds that may be formed as a result of a particular schema change, like LAV to GAV. The change in type of mapping (tgd) is then handled by use of inversion and composition operators. However, these operators have not been tested for different types of schema changes. This requires the merging of two aforementioned categories of work on schema evolution which is the focus of this paper. This work provides a basis for studying the applicability of inversion and composition operators on different schema changes falling in each tgd-based category thus helps to deal schema evolution in a better way.<br/> &copy; 2014 IEEE.},
key = {Mapping},
keywords = {Chemical analysis;},
note = {Composition operators;inversion;Inversion operators;Real-world;Schema changes;Schema evolution;Schema integration;},
URL = {http://dx.doi.org/10.1109/ICCST.2014.7045196},
} 


@inproceedings{20100712713742 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling data warehouse schema evolution over extended hierarchy semantics},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Banerjee, Sandipto and Davis, Karen C.},
volume = {5530 LNCS},
year = {2009},
pages = {72 - 96},
issn = {03029743},
abstract = {Models for conceptual design of data warehouse schemas have been proposed, but few researchers have addressed schema evolution in a formal way and none have presented software tools for enforcing the correctness of multidimensional schema evolution operators. We generalize the core features typically found in data warehouse data models, along with modeling extended hierarchy semantics. The advanced features include multiple hierarchies, non-covering hierarchies, non-onto hierarchies, and non-strict hierarchies. We model the constructs in the Uni-level Description Language (ULD) as well as using a multilevel dictionary definition (MDD) approach. The ULD representation provides a formal foundation to specify transformation rules for the semantics of schema evolution operators. The MDD gives a basis for direct implementation in a relational database system; we define model constraints and then use the constraints to maintain integrity when schema evolution operators are applied. This paper contributes a formalism for representing data warehouse schemas and determining the validity of schema evolution operators applied to a schema. We describe a software tool that allows for visualization of the impact of schema evolution through the use of triggers and stored procedures. &copy; 2009 Springer-Verlag.<br/>},
key = {Data warehouses},
keywords = {Computer software;Conceptual design;Relational database systems;Semantics;},
note = {Conceptual model;Description languages;Dictionary definitions;Formal foundation;Model constraints;Multidimensional schemata;Schema evolution;Transformation rules;},
URL = {http://dx.doi.org/10.1007/978-3-642-03098-7_3},
} 


@inproceedings{20183905873173 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Combining provenance management and schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Auge, Tanja and Heuer, Andreas},
volume = {11017 LNCS},
year = {2018},
pages = {222 - 225},
issn = {03029743},
address = {London, United kingdom},
abstract = {The combination of provenance management and schema evolution using the CHASE algorithm is the focus of our research in the area of research data management. The aim is to combine the construction of a CHASE inverse mapping to calculate the minimal part of the original database &mdash; the minimal sub-database &mdash; with a CHASE-based schema mapping for schema evolution.<br/> &copy; Springer Nature Switzerland AG 2018.},
key = {Information management},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Chase algorithm;CHASE inverse;Data evolution;Data provenance;Schema evolution;Schema mappings;},
URL = {http://dx.doi.org/10.1007/978-3-319-98379-0_24},
} 


@inproceedings{20160902009434 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolving the software of a schema evolution system},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Claypool, Kajal T. and Rundensteiner, Elke A. and Heineman, George T.},
volume = {2065},
year = {2001},
pages = {68 - 84},
issn = {03029743},
address = {Dagstuhl Castle, Germany},
abstract = {The object model represents the core of an OODB system. Any change in the object model such as the addition of an association or aggregation relationship affects many sub-systems including the schema evolution system. Under the current tightly-coupled database architecture, updating the object model is an extremely expensive undertaking for a database vendor both in terms of time and resources. Adding a new construct to the object model impacts the schema evolution system in two ways: (1) the new construct requires a new set of schema evolution primitives to enable its evolution; and (2) existing schema evolution primitives must be modified to assure that they conform to the new constraints of the new object model. One traditional approach to address this is to manually change all affected software, a time consuming task. We present an alternate two-prong solution. We first decouple the constraints from the schema evolution primitives and secondly we provide a mechanism that allows for the declarative defini-tion of both the primitives and the constraints. We show via examples that we can reduce the software evolution cost of the schema evolution component completely for semantic extensions to the object model and can partially reduce the cost for most other new modeling constructs.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
key = {Modeling languages},
keywords = {Closed loop control systems;Database systems;Semantics;},
note = {Database architecture;Loosely coupled architectures;Schema evolution;Semantic extensions;Software Evolution;Software evolution costs;Time-consuming tasks;Traditional approaches;},
URL = {http://dx.doi.org/10.1007/3-540-48196-6_4},
} 


@article{20190506429624 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution and foreign keys: a study on usage, heartbeat of change and relationship of foreign keys to table activity},
journal = {Computing},
author = {Vassiliadis, Panos and Kolozoff, Michail-Romanos and Zerva, Maria and Zarras, Apostolos V.},
year = {2019},
issn = {0010485X},
abstract = {In this paper, we study the evolution of foreign keys in the context of schema evolution for relational databases. Specifically, we study the schema histories of a six free, open-source databases that contain foreign keys. Our findings verify previous results that schemata grow in the long run in terms of tables. To our surprise, we discovered that foreign keys appear to be fairly scarce in the projects that we have studied and they do not necessarily grow in sync with table growth. In fact, we have observed different &ldquo;cultures&rdquo; for the handling of foreign keys, ranging from treating foreign keys as an indispensable part of the schema, in full sync with the growth of tables, to the unexpected extreme of treating foreign keys as an optional add-on that twice resulted in their full removal from the schema of the database. Apart from the behavior of entire schemata, we have also studied the behavior of individual tables. We model the schema of any version of the history as a graph, with tables being nodes and foreign keys being edges. The union of these graphs is called the diachronic graph of the schema and contains all the tables and foreign keys that ever appeared in the schema history. The study of the total degree of tables at the diachronic graph, reveals several patterns. The population of tables with total degree in the range of [0&ndash;2] includes almost all the tables that were eventually removed from the schema, as well as the vast majority of survivor tables. These low-degree tables (especially the dead ones) tend to be mostly with zero or very few internal updates in their entire history. At the same time, the few tables with degree higher than 2 are typically born very early in the life of the schema, overwhelmingly survivors, and, unusually active, typically undergoing medium or high update activity.<br/> &copy; 2019, Springer-Verlag GmbH Austria, part of Springer Nature.},
key = {Computer science},
keywords = {Computer programming;Database systems;},
note = {Foreign keys;Low degree;Open source database;Relational Database;Schema evolution;},
URL = {http://dx.doi.org/10.1007/s00607-019-00702-x},
} 


@inproceedings{20112314044602 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution analysis for embedded databases},
journal = {Proceedings - International Conference on Data Engineering},
author = {Wu, Shengfeng and Neamtiu, Iulian},
year = {2011},
pages = {151 - 156},
issn = {10844627},
abstract = {Dynamic software updating research efforts have mostly been focused on updating application code and in-memory state. As more and more applications use embedded databases for storage, dynamic updating solutions will have to support changes to embedded database schemas. The first step towards supporting dynamic updates to embedded database schemas is understanding how these schemas change - so far, schema evolution studies have focused on large, enterprise-class databases. In this paper we propose an approach for automatically extracting embedded schemas from regular applications, e.g., written in C and C++, and automatically computing how schemas change as applications evolve. To showcase our approach, we perform a long-term schema evolution study on four popular open source programs that use embedded databases: Firefox, Monotone, BiblioteQ and Vienna. Our study spans 18 cumulative years of schema evolution and reveals that change patterns and frequency in embedded databases differ from schema changes in enterprise-class databases that formed the object of prior studies. Our platform can be used for performing long-term, large-scale embedded schema evolution studies that are potentially beneficial to dynamic updating and schema evolution researchers. &copy; 2011 IEEE.<br/>},
key = {Database systems},
keywords = {Application programs;C++ (programming language);Digital storage;Open source software;},
note = {Application codes;Change patterns;Dynamic software updating;Dynamic updating;Embedded database;Open source projects;Research efforts;Schema evolution;},
URL = {http://dx.doi.org/10.1109/ICDEW.2011.5767627},
} 


@article{20163402735369 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Gravitating to rigidity: Patterns of schema evolution  and its absence  in the lives of tables},
journal = {Information Systems},
author = {Vassiliadis, Panos and Zarras, Apostolos V. and Skoulis, Ioannis},
volume = {63},
year = {2017},
pages = {24 - 46},
issn = {03064379},
abstract = {Like all software maintenance, schema evolution is a process that can severely impact the lifecycle of a data-intensive software projects, as schema updates can drive depending applications crushing or delivering incorrect data to end users. In this paper, we study the schema evolution of eight databases that are part of larger open source projects, publicly available through open source repositories. In particular, the focus of our research was the understanding of which tables evolve and how. We report on our observations and patterns on how evolution related properties, like the possibility of deletion, or the amount of updates that a table undergoes, are related to observable table properties like the number of attributes or the time of birth of a table. A study of the update profile of tables, indicates that they are mostly rigid (without any updates to their schema at all) or quiet (with few updates), especially in databases that are more mature and heavily updated. Deletions are significantly outnumbered by table insertions, leading to schema expansion. Delving deeper, we can highlight four patterns of schema evolution. The &Gamma; pattern indicating that tables with large schemata tend to have long durations and avoid removal, the Comet pattern indicating that the tables with most updates are the ones with medium schema size, the Inverse &Gamma; pattern, indicating that tables with medium or small durations produce amounts of updates lower than expected, and, the Empty Triangle pattern indicating that deletions involve mostly early born, quiet tables with short lives, whereas older tables are unlikely to be removed. Overall, we believe that the observed evidence strongly indicates that databases are rigidity-prone rather than evolution-prone. We call the phenomenon gravitation to rigidity and we attribute it to the implied impact to the surrounding code that a modification to the schema of a database has.<br/> &copy; 2016 Elsevier Ltd},
key = {Open source software},
keywords = {Application programs;Computer software maintenance;Database systems;Digital storage;Rigidity;},
note = {Evolution history;Exploratory studies;Open source projects;Open source repositories;Schema evolution;Software project;Software repository mining;Triangle pattern;},
URL = {http://dx.doi.org/10.1016/j.is.2016.06.010},
} 


@inproceedings{20123615405575 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A transparent approach for database schema evolution using view mechanism},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Xue, Jianxin and Shen, Derong and Nie, Tiezheng and Kou, Yue and Yu, Ge},
volume = {7418 LNCS},
year = {2012},
pages = {405 - 418},
issn = {03029743},
address = {Harbin, China},
abstract = {Designing databases that evolve over time is still a major problem today. The database schema is assumed to be stable enough to remain valid even as the modeled environment changes. However, the database administrators are faced with the necessity of changing something in the overall configuration of the database schema. Even though some approaches proposed are provided in current database systems, schema evolution remains an error-prone and time-consuming undertaking. We propose an on-demand transparent solution to overcome the schema evolution, which usually impacts existing applications/queries that have been written against the schema. In order to improve the performance of our approach, we optimize our approach with mapping composition. To this end, we show that our approach has a better potential than traditional schema evolution technique. Our approach, as suggested by experimental evaluation, is much more efficient than the other schema evolution techniques. &copy; 2012 Springer-Verlag.<br/>},
key = {Database systems},
keywords = {Information management;},
note = {Backward compatibility;Database administrators;Database schemas;Environment change;Experimental evaluation;Schema evolution;Schema mappings;Transparent solutions;},
URL = {http://dx.doi.org/10.1007/978-3-642-32281-5_40},
} 


@article{20174704446895 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema Evolution Survival Guide for Tables: Avoid Rigid Childhood and Youre En Route to a Quiet Life},
journal = {Journal on Data Semantics},
author = {Vassiliadis, Panos and Zarras, Apostolos V.},
volume = {6},
number = {4},
year = {2017},
pages = {221 - 241},
issn = {18612032},
abstract = {In this paper, we study the factors that relate to the survival of a table in the context of schema evolution in open-source software. We study the history of the schema of eight open-source software projects that include relational databases and extract patterns related to the survival or death of their tables. Our study shows that the probability of a table with a wide schema (i.e., a large number of attributes) being removed is systematically lower than average. Activity and duration are related to survival too. Rigid tables, without any change to their schema, are more likely to be removed than tables that sustain changes. Durations of dead and survival tables demonstrate a mirror image: dead tables&rsquo; durations are mostly short, whereas survivor tables gravitate toward higher durations. Our findings are mostly summarized by a pattern, which we call electrolysis pattern, due to its diagrammatic representation, stating that dead and survivor tables live quite different lives: tables typically die shortly after birth, with short durations and mostly no updates, whereas survivors mostly live quiet lives with few updates&mdash;except for a small group of tables with high update ratios that are characterized by high durations and survival. Equally important is the evidence that schema evolution suffers from the antagonism of gravitation to rigidity, i.e., the tendency to minimize evolution as much as possible in order to minimize the resulting impact to the surrounding code. Several factors contribute to this observation: the absence of long durations in removed tables, the low percentage of tables whose schema size is scaled up or down, and the low numbers of tables with a high rate of updates, contrasted to the high numbers of tables with zero or few updates. We complement our findings with explanations and recommendations to developers.<br/> &copy; 2017, Springer-Verlag GmbH Germany.},
key = {Open source software},
keywords = {Open systems;},
note = {Diagrammatic representations;High rate;Long duration;Mirror images;Open source software projects;Relational Database;Schema evolution;Short durations;},
URL = {http://dx.doi.org/10.1007/s13740-017-0083-x},
} 


@article{20130415945269 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating the database schema evolution process},
journal = {VLDB Journal},
author = {Curino, Carlo and Moon, Hyun Jin and Deutsch, Alin and Zaniolo, Carlo},
volume = {22},
number = {1},
year = {2013},
pages = {73 - 98},
issn = {10668888},
abstract = {Supporting database schema evolution represents a long-standing challenge of practical and theoretical importance for modern information systems. In this paper, we describe techniques and systems for automating the critical tasks of migrating the database and rewriting the legacy applications. In addition to labor saving, the benefits delivered by these advances are many and include reliable prediction of outcome, minimization of downtime, system-produced documentation, and support for archiving, historical queries, and provenance. The PRISM/PRISM++ system delivers these benefits, by solving the difficult problem of automating the migration of databases and the rewriting of queries and updates. In this paper, we present the PRISM/PRISM++ system and the novel technology that made it possible. In particular, we focus on the difficult and previously unsolved problem of supporting legacy queries and updates under schema and integrity constraints evolution. The PRISM/PRISM++ approach consists in providing the users with a set of SQL-based Schema Modification Operators (SMOs), which describe how the tables in the old schema are modified into those in the new schema. In order to support updates, SMOs are extended with integrity constraints modification operators. By using recent results on schema mapping, the paper (i) characterizes the impact on integrity constraints of structural schema changes, (ii) devises representations that enable the rewriting of updates, and (iii) develop a unified approach for query and update rewriting under constraints. We complement the system with two novel tools: the first automatically collects and provides statistics on schema evolution histories, whereas the second derives equivalent sequences of SMOs from the migration scripts that were used for schema upgrades. These tools were used to produce an extensive testbed containing 15 evolution histories of scientific databases and web information systems, providing over 100 years of aggregate evolution histories and almost 2,000 schema evolution steps. &copy; 2012 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Database systems},
keywords = {Information systems;Information use;Legacy systems;Mapping;Prisms;Query processing;},
note = {Integrity constraints;Relational;Rewriting;Schema evolution;Updates;},
URL = {http://dx.doi.org/10.1007/s00778-012-0302-x},
} 


@inproceedings{20100812721516 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating database schema evolution in information system upgrades},
journal = {Proceedings of the 2nd International Workshop on Hot Topics in Software Upgrades, HotSWUp '09},
author = {Curino, Carlo and Moon, Hyun J. and Zaniolo, Carlo},
year = {2009},
pages = {ACM Special Interest Group on Programming Languages, SIGPLAN - },
address = {Orlando, FL, United states},
abstract = {The complexity, cost, and down-time currently created by the database schema evolution process is the source of incessant problems in the life of information systems and a major stumbling block that prevent graceful upgrades. Furthermore, our studies shows that the serious problems encountered by traditional information systems are now further exacerbated in web information systems and cooperative scientific databases where the frequency of schema changes has increased while tolerance for downtimes has nearly disappeared. The PRISM project seeks to develop the methods and tools that turn this error-prone and time-consuming process into one that is controllable, predictable and avoids down-time. Toward this goal, we have assembled a large testbed of schema evolution histories, and developed a language of Schema Modification Operators (SMO) to express concisely these histories. Using this language, the database administrator can specify new schema changes, and then rely on PRISM to (i) predict the effect of these changes on current applications, (ii) translate old queries and updates to work on the new schema version, (iii) perform data migration, and (iv) generate full documentation of intervened changes. Furthermore, PRISM achieves good usability and scalability by incorporating recent advances on mapping composition and invertibility in the implementation of (ii). The progress in automating schema evolution so achieved provides the enabling technology for other advances, such as light-weight database design methodologies that embrace changes as the regular state of software. While these topics remain largely unexplored, and thus provide rich opportunities for future research, an important area which we have been investigated is that of archival information systems, where PRISM query mapping techniques were used to support flashback and historical queries for database archives under schema evolution. Copyright &copy; 2009 ACM.<br/>},
key = {Search engines},
keywords = {Data warehouses;Information systems;Information use;Mapping;Prisms;Query languages;Query processing;},
note = {Archival information systems;Database administrators;Database design;Enabling technologies;Schema evolution;Scientific database;Software upgrades;Web information systems;},
URL = {http://dx.doi.org/10.1145/1656437.1656444},
} 


@inproceedings{20123715434313 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Querying transaction-time databases under branched schema evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Huo, Wenyu and Tsotras, Vassilis J.},
volume = {7446 LNCS},
number = {PART 1},
year = {2012},
pages = {265 - 280},
issn = {03029743},
address = {Vienna, Austria},
abstract = {Transaction-time databases have been proposed for storing and querying the history of a database. While past work concentrated on managing the data evolution assuming a static schema, recent research has considered data changes under a linearly evolving schema. An ordered sequence of schema versions is maintained and the database can restore/query its data under the appropriate past schema. There are however many applications leading to a branched schema evolution where data can evolve in parallel, under different concurrent schemas. In this work, we consider the issues involved in managing the history of a database that follows a branched schema evolution. To maintain easy access to any past schema, we use an XML-based approach with an optimized sharing strategy. As for accessing the data, we explore branched temporal indexing techniques and present efficient algorithms for evaluating two important queries made possible by our novel branching environment: the vertical historical query and the horizontal historical query. Moreover, we show that our methods can support branched schema evolution which allows version merging. Experimental evaluations show the efficiency of our storing, indexing, and query processing methodologies. &copy; 2012 Springer-Verlag.<br/>},
key = {Query processing},
keywords = {Database systems;Expert systems;Indexing (materials working);Indexing (of information);},
note = {Experimental evaluation;Historical queries;Indexing techniques;Recent researches;Schema evolution;Sharing strategies;Transaction-time database;Version merging;},
URL = {http://dx.doi.org/10.1007/978-3-642-32600-4_20},
} 


@inproceedings{20092812179018 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The PRISM workwench: Database schema evolution without tears},
journal = {Proceedings - International Conference on Data Engineering},
author = {Curino, Carlo A. and Moon, Hyun J. and Ham, Myung Won and Zaniolo, Carlo},
year = {2009},
pages = {1523 - 1526},
issn = {10844627},
address = {Shanghai, China},
abstract = {Information Systems are subject to a perpetual evolution, which is particularly pressing in Web Information Systems, due to their distributed and often collaborative nature. Such continuous adaptation process, comes with a very high cost, because of the intrinsic complexity of the task and the serious ramifications of such changes upon database-centric Information System softwares. Therefore, there is a need to automate and simplify the schema evolution process and to ensure predictability and logical independence upon schema changes. Current relational technology makes it easy to change the database content or to revise the underlaying storage and indexes but does little to support logical schema evolution which nowadays remains poorly supported by commercial tools. The PRISM system demonstrates a major new advance toward automating schema evolution (including query mapping and database conversion), by improving predictability, logical independence, and auditability of the process. In fact, PRISM exploits recent theoretical results on mapping composition, invertibility and query rewriting to provide DB Administrators with an intuitive, operational workbench usable in their everyday activities-thus enabling graceful schema evolution. In this demonstration, we will show (i) the functionality of PRISMand its supportive AJAX interface, (ii) its architecture built upon a simple SQL-inspired language of Schema Modification Operators, and (iii) we will allow conference participants to directly interact with the system to test its capabilities. Finally, some of the most interesting evolution steps of popular Web Information Systems, such as Wikipedia, will be reviewed in a brief "Saga of Famous Schema Evolutions". &copy; 2009 IEEE.<br/>},
key = {Query processing},
keywords = {Database systems;Digital storage;Information systems;Information use;Mapping;Prisms;},
note = {Adaptation process;Commercial tools;Database contents;Database schemas;ITS architecture;Query rewritings;Schema evolution;Web information systems;},
URL = {http://dx.doi.org/10.1109/ICDE.2009.46},
} 


@inproceedings{20114014402784 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data privacy preservation during schema evolution for multi-tenancy applications in cloud computing},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zhang, Kun and Li, Qingzhong and Shi, Yuliang},
volume = {6987 LNCS},
number = {PART 1},
year = {2011},
pages = {376 - 383},
issn = {03029743},
address = {Taiyuan, China},
abstract = {In cloud computing, multi-tenancy applications utilize shared resources to serve multi tenants through "single instance multi-tenancy". Applications and databases are both deployed at the platform of un-trusted service providers. Data privacy has become the biggest challenges in wider adoption of cloud computing. Specifically, data schema evolves due to on-demand customization in cloud computing. How to protect the data privacy during data schema evolution for multi-tenancy application is an interesting and important problem. We proposed the privacy requirements for data schema evolution based on the data combination privacy, and then represented the data privacy-preserving architecture. A data schema evolution graph is constructed, and then the privacy-preserving evolution path is searched based on the principle of privacy requirements consistency. Analysis and experiments demonstrate the corrective and effective of the data privacy preservation approach during data schema evolution in cloud computing. &copy; 2011 Springer-Verlag.<br/>},
key = {Data privacy},
keywords = {Cloud computing;Information systems;Information use;},
note = {Data combination;Data-privacy preserving;Privacy preservation;Privacy preserving;Privacy requirements;Schema evolution;Service provider;Shared resources;},
URL = {http://dx.doi.org/10.1007/978-3-642-23971-7_47},
} 


@article{2005028779011 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Temporal and versioning model for schema evolution in object-oriented databases},
journal = {Data and Knowledge Engineering},
author = {Galante, Renata De Matos and Dos Santos, Clesio Saraiva and Edelweiss, Nina and Moreira, Alvaro Freitas},
volume = {53},
number = {2},
year = {2005},
pages = {99 - 128},
issn = {0169023X},
abstract = {In this paper we define the Temporal and Versioning Model for Schema Evolution (TVSE), a model that uses time and version concepts to manage dynamic schema evolution in object-oriented databases. The proposed model is able to manage the schema evolution process considering: schema versioning, schema modification, change propagation and data manipulation. TVSE differs from other schema evolution models by enabling the homogeneous and simultaneous management of the evolution history concerning both intentional and extensional databases. Besides defining the model, we also propose a language to derive and modify schema versions, and also to update data associated with them, creating either new object versions or just keeping the history of these data modifications. We provide an operational semantics for this language which is an essential step towards for establishing the preservation of complex time integrity constraints. &copy; 2004 Elsevier B.V. All rights reserved.},
key = {Database systems},
keywords = {Computer architecture;Data reduction;Knowledge based systems;Mathematical models;Object oriented programming;Semantics;},
note = {Operational semantics;Schema evolution;Schema versioning;Temporal object-oriented database;},
URL = {http://dx.doi.org/10.1016/j.datak.2004.07.001},
} 


@inproceedings{20130615995904 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution in object and process-aware information systems: Issues and challenges},
journal = {Lecture Notes in Business Information Processing},
author = {Chiao, Carolina Ming and Kunzle, Vera and Reichert, Manfred},
volume = {132 LNBIP},
year = {2013},
pages = {328 - 339},
issn = {18651348},
address = {Tallinn, Estonia},
abstract = {Enabling process flexibility is crucial for any process-aware information system (PAIS). In particular, implemented processes may have to be frequently adapted to accommodate to changing environments and evolving needs. When evolving a PAIS, corresponding process schemas have to be changed in a controlled manner. In the context of object-aware processes, which are characterized by a tight integration of process and data, PAIS evolution not only requires process schema evolution, but the evolution of data and user authorization schemas as well. Since the different schemas of an object-aware PAIS are tightly integrated, modifying one of them usually requires concomitant changes of the other schemas. This paper presents a framework for object-aware process support and discusses major requirements and challenges for enabling schema evolution in object-aware PAIS. &copy; 2013 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Administrative data processing},
keywords = {Enterprise resource management;Information systems;Information use;},
note = {Changing environment;Issues and challenges;Process flexibility;Process support;Process-aware information systems;Schema evolution;Tight integrations;},
URL = {http://dx.doi.org/10.1007/978-3-642-36285-9-37},
} 


@inproceedings{20121214882743 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema evolution via multi-version metadata in SaaS},
journal = {Procedia Engineering},
author = {Wu, Shengqi and Zhang, Shidong and Kong, Lanju},
volume = {29},
year = {2012},
pages = {1107 - 1112},
issn = {18777058},
address = {Harbin, China},
abstract = {The Basic-Table combined with Extension-Table (BT&amp;ET) layout has become the popular data storage architecture for SaaS currently. For the sake of improving the processing efficiency we improve the BT&amp;ET layout, and migrate some tenants' frequently accessed extension fields into the basic table based on the tenants' constantly need on data access. With the development of cloud computing, Multi-Tenant data need to be stored in multiple data nodes. When tenant's data storage schema evolution happens, all data nodes hava to do data migration simultaneously and tenants may perceive the influence. In order to minimize the costs and negative load of schema evolution we propose the Multi-Version metadata technology. Tenant's data on each data node may contain different version metadata, and schema envolution can run asynchronously. We carry out experiments and the results figure out that the workload decreases significantly. &copy; 2011 Published by Elsevier Ltd.<br/>},
key = {Digital storage},
keywords = {Metadata;Software as a service (SaaS);},
note = {Data migration;Data storage;Extension field;Multi tenants;Multi-version metadatas;Multiple data;SAAS;Schema evolution;},
URL = {http://dx.doi.org/10.1016/j.proeng.2012.01.096},
} 


@inbook{20172603832326 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing schema evolution in a federated spatial database system},
journal = {Discovery of Geospatial Resources: Methodologies, Technologies, and Emergent Applications},
author = {Wu, Xiaoying and Xia, Jianhong Cecilia and West, Geoff and Arnold, Lesley and Veenendaal, Bert},
year = {2012},
pages = {56 - 77},
abstract = {A Federated Spatial Database System (FSDBS) is the integration of multiple spatial data sources and the realisation of effective spatial data sharing. These are becoming increasingly popular as more and more spatial and non-spatial datasets are integrated, especially those across a number of independent organisations. However, in a FSDBS environment, database schemas are subject to change and the management of these changes is complex and inefficient. This is because schema changes in one local database will invalidate applications built against this local schema, but also applications built against the global schema. The research is motivated by developments in the Shared Land Information Platform, built by Landgate in Western Australia as a Spatial Data Infrastructure enabler that has been running since 2007. The more than 350 datasets in SLIP are from and controlled by 20 organisations with queries built over the different datasets. Changes in the various databases require schema updating to be streamlined. In this chapter, an Automatic Schema Evolution Framework is explored and developed to more effectively manage schema evolution in a FSDBS. This framework provides a Schema Element Dependency Meta-Model, a set of Schema Change Templates, and incorporates view generation, view rewriting, and query rewriting as solutions. These developed methods ensure applications can accommodate schema changes and hence remain valid. &copy; 2012, IGI Global.<br/>},
key = {Query processing},
keywords = {Query languages;},
note = {Database schemas;Information platform;Schema evolution;Spatial data infrastructure;Spatial data sharing;Spatial database systems;Spatial datasets;Western Australia;},
URL = {http://dx.doi.org/10.4018/978-1-4666-0945-7.ch004},
} 


@inproceedings{20124515650316 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Optimising schema evolution operation sequences in object databases for data evolution},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zaschke, Tilmann and Leone, Stefania and Norrie, Moira C.},
volume = {7532 LNCS},
year = {2012},
pages = {369 - 382},
issn = {03029743},
address = {Florence, Italy},
abstract = {We propose an approach to optimising schema evolution operation sequences in object databases. The approach separates operations that add structures from those that remove structures so that all additions are performed before any removals. This separation ensures that there is always a state during schema evolution where data can be evolved from structures that are to be deleted to structures that are new or already exist. Our approach also reduces and groups the schema operations to simplify implementation of data evolution functions by developers. We present a case study used as a first evaluation of the approach. &copy; 2012 Springer-Verlag.<br/>},
key = {Object-oriented databases},
keywords = {Data mining;},
note = {Data evolution;Schema evolution;},
URL = {http://dx.doi.org/10.1007/978-3-642-34002-4_29},
} 


@inproceedings{20143618132146 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data Warehouse Schema Evolution Perspectives},
journal = {Advances in Intelligent Systems and Computing},
author = {Subotic, Danijela},
volume = {312},
year = {2015},
pages = {333 - 338},
issn = {21945357},
address = {Ohrid, Macedonia},
abstract = {The paper presents a short analysis of research related to the problem of the data warehouse (DW) evolution. The main contributions of the paper are: a) analysis of existing methods and approaches to the DW schema evolution, and b) characterization of the general research idea for the DW schema evolution problem. The general research idea includes a meta-Data Vault (DV) model that will integrate the DW with the master data management (MDM) system. We believe the following issues could be resolved: a) tracking the origin of data, b) tracking the history of changes, c) avoiding loss of data, d) faster and simpler migration and transformation of data, and e) trend projections. Also, due to long-term storage of historical data and tracking the origin of data, the DW could be used as a complete system of records and as the basis for the data governance (MDM integrated with the DW). &copy; Springer International Publishing Switzerland 2015.<br/>},
key = {Information management},
keywords = {Data warehouses;Digital storage;Information systems;Information use;Metadata;},
note = {Data vault;Data warehouse evolutions;Master data management;Schema evolution;Schema versioning;View maintenance;},
URL = {http://dx.doi.org/10.1007/978-3-319-10518-5_26},
} 


@inproceedings{20103013091876 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Managing schema evolution in hybrid XML-relational database systems},
journal = {24th IEEE International Conference on Advanced Information Networking and Applications Workshops, WAINA 2010},
author = {Baqasah, Abdullah and Pardede, Eric},
year = {2010},
pages = {455 - 460},
address = {Perth, Australia},
abstract = {Many applications and data enterprises need to change their data content and structure from time totime. The schema evolution in database systems has been studied well by the database community and several projects have been proposed for supporting schema modifications in XML and relational models individually. However, these studies do not investigate well the evolutionary process in hybrid XML-relational systems. In this paper we will investigate the schema evolution for current hybrid XML-relational systems. We will propose a taxonomy for schema evolution in a way that preserves a schema after data evolves. Finally, we proposed a general model which will be evaluated through a case study. &copy; 2010 IEEE.<br/>},
key = {Relational database systems},
keywords = {XML;},
note = {Data contents;Database community;Evolutionary process;General model;Hybrid XML;Relational Model;Relational systems;Schema evolution;},
URL = {http://dx.doi.org/10.1109/WAINA.2010.127},
} 


@article{20080711098193 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Data schema evolution support in XML-relational database systems},
journal = {Programming and Computer Software},
author = {Simanovsky, A.A.},
volume = {34},
number = {1},
year = {2008},
pages = {16 - 26},
issn = {03617688},
abstract = {Many XML-relational systems, i.e., the systems that use an XML schema as an external schema and a relational schema as an internal schema of the data application representation level, require modifications of the data schemas in the course of time. Schema evolution is one of the ways to support schema modifications for the application at the DBMS level. A number of schema evolution support systems for different data models have been suggested. Schema evolution can be applied to mapping-related evolving schemas (such as schemas of XML-relational systems), the transformation problem for which is also known as schema adaptation. In this paper, a survey of various approaches to solving the outlined problems is given. &copy; 2008 MAIK Nauka.},
key = {XML},
keywords = {Data structures;Problem solving;Relational database systems;},
note = {Data application representation level;Data schema evolution;Schema modifications;},
URL = {http://dx.doi.org/10.1007/s11086-008-1003-5},
} 


@article{20113414261803 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of XML Schema evolution},
journal = {ACM Transactions on Internet Technology},
author = {Geneves, Pierre and Layaida, Nabil and Quint, Vincent},
volume = {11},
number = {1},
year = {2011},
issn = {15335399},
abstract = {We consider the problem of XML Schema evolution. In the ever-changing context of the web, XML schemas continuously change in order to cope with the natural evolution of the entities they describe. Schema changes have important consequences. First, existing documents valid with respect to the original schema are no longer guaranteed to fulfill the constraints described by the evolved schema. Second, the evolution also impacts programs, manipulating documents whose structure is described by the original schema. We propose a unifying framework for determining the effects of XML Schema evolution both on the validity of documents and on queries. The system is very powerful in analyzing various scenarios in which forward/backward compatibility of schemas is broken, and in which the result of a query may no longer be what was expected. Specifically, the system offers a predicate language that allows one to formulate properties related to schema evolution. The system then relies on exact reasoning techniques to perform a fine-grained analysis. This yields either a formal proof of the property or a counter-example that can be used for debugging purposes. The system has been fully implemented and tested with real-world use cases, in particular with the main standard document formats used on the web, as defined by W3C. The system precisely identifies compatibility relations between document formats. In case these relations do not hold, the system can identify queries that must be reformulated in order to produce the expected results across successive schema versions. &copy; 2011 ACM .<br/>},
key = {XML},
keywords = {Printing machinery;},
note = {Compatibility relation;Fine-grained analysis;Queries;Reasoning techniques;Schema evolution;Schemas;Web document;XML schema evolutions;},
URL = {http://dx.doi.org/10.1145/1993083.1993087},
} 


@article{1997503876472 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transparent schema-evolution system based on object-oriented view technology},
journal = {IEEE Transactions on Knowledge and Data Engineering},
author = {Ra, Young-Gook and Rundensteiner, Elke A.},
volume = {9},
number = {4},
year = {1997},
pages = {600 - 624},
issn = {10414347},
abstract = {When a database is shared by many users, updates to the database schema are almost always prohibited because there is a risk of making existing application programs obsolete when they run against the modified schema. This paper addresses the problem by integrating schema evolution with view facilities. When new requirements necessitate schema updates for a particular user, then the user specifies schema changes to his personal view, rather than to the shared-base schema. Our view schema-evolution approach then computes a new view schema that reflects the semantics of the desired schema change, and replaces the old view with the new one. We show that our system provides the means for schema change without affecting other views (and thus without affecting existing application programs). The persistent data is shared by different views of the schema, i.e., both old as well as newly developed applications can continue to interoperate. This paper describes a solution approach of realizing the evolution mechanism as a working system, which as its key feature requires the underlying object-oriented view system to support capacity-augmenting views. In this paper, we present algorithms that implement the complete set of typical schema-evolution operations as view definitions. Lastly, we describe the transparent schema-evolution system (TSE) that we have built on top of GemStone, including our solution for supporting capacity-augmenting view mechanisms.},
key = {Distributed database systems},
keywords = {Algorithms;Computational linguistics;Data acquisition;Data structures;Information retrieval;Object oriented programming;},
note = {Capacity augmenting views;Transparent schema evolution system (TSE);},
URL = {http://dx.doi.org/10.1109/69.617053},
} 


@article{20132216387552 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Version management for business process schema evolution},
journal = {Information Systems},
author = {Zhao, Xiaohui and Liu, Chengfei},
volume = {38},
number = {8},
year = {2013},
pages = {1046 - 1069},
issn = {03064379},
abstract = {The current business environment changes rapidly, dictated by user requirements and market opportunities. Organisations are therefore driven to continuously adapt their business processes to new conditions. Thus, management of business process schema evolution, particularly process version control, is in great demand to capture the dynamics of business process schema changes. This paper aims to facilitate version control for business process schema evolution, with an emphasis on version compatibility, co-existence of multiple versions and dynamic version shifts. A multi-level versioning approach is established to specify dependency between business process schema evolutions, and a novel version preserving graph model is proposed to record business process schema evolutions. A set of business process schema updating operations is devised to support the entire set of process change patterns. By maintaining sufficient and necessary schema and version information, our approach provides comprehensive support for navigating process instance executions of different and changing versions, and deriving the process schema of a certain version. A prototype is also implemented for the proof-of-concept purpose. &copy; 2013 Elsevier Ltd.<br/>},
key = {Process control},
keywords = {Administrative data processing;Air navigation;Enterprise resource management;Information management;},
note = {Business environments;Business process management;Market opportunities;Schema evolution;User requirements;Version control;Version information;Version management;},
URL = {http://dx.doi.org/10.1016/j.is.2013.03.006},
} 


@inproceedings{20153601247754 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ControVol: A framework for controlled schema evolution in NoSQL application development},
journal = {Proceedings - International Conference on Data Engineering},
author = {Scherzinger, Stefanie and Cerqueus, Thomas and De Almeida, Eduardo Cunha},
volume = {2015-May},
year = {2015},
pages = {1464 - 1467},
issn = {10844627},
address = {Seoul, Korea, Republic of},
abstract = {Building scalable web applications on top of NoSQL data stores is becoming common practice. Many of these data stores can easily be accessed programmatically, and do not enforce a schema. Software engineers can design the data model on the go, a flexibility that is crucial in agile software development. The typical tasks of database schema management are now handled within the application code, usually involving object mapper libraries. However, today's Integrated Development Environments (IDEs) lack the proper tool support when it comes to managing the combined evolution of the application code and of the schema. Yet simple refactorings such as renaming an attribute at the source code level can cause irretrievable data loss or runtime errors once the application is serving in production. In this demo, we present ControVol, a framework for controlled schema evolution in application development against NoSQL data stores. ControVol is integrated into the IDE and statically type checks object mapper class declarations against the schema evolution history, as recorded by the code repository. ControVol is capable of warning of common yet risky cases of mismatched data and schema. ControVol is further able to suggest quick fixes by which developers can have these issues automatically resolved.<br/> &copy; 2015 IEEE.},
key = {Software design},
keywords = {Codes (symbols);},
note = {Agile software development;Application codes;Application development;Common practices;Database schemas;Integrated development environment;Run-time errors;Schema evolution;},
URL = {http://dx.doi.org/10.1109/ICDE.2015.7113402},
} 

@InProceedings{Ronnback2010,
  author        = {Ronnback, L. and Regardt, O. and Bergholtz, M. and Johannesson, P. and Wohed, P.},
  title         = {Anchor modeling - Agile information modeling in evolving data environments},
  year          = {2010},
  volume        = {69},
  number        = {12},
  pages         = {1229 - 1253},
  note          = {Agile development;Anchor models;Database modeling;Normalization;Table elimination;Temporal Database;},
  __markedentry = {[Juliana:6]},
  abstract      = {Maintaining and evolving data warehouses is a complex, error prone, and time consuming activity. The main reason for this state of affairs is that the environment of a data warehouse is in constant change, while the warehouse itself needs to provide a stable and consistent interface to information spanning extended periods of time. In this article, we propose an agile information modeling technique, called Anchor Modeling, that offers non-destructive extensibility mechanisms, thereby enabling robust and flexible management of changes. A key benefit of Anchor Modeling is that changes in a data warehouse environment only require extensions, not modifications, to the data warehouse. Such changes, therefore, do not require immediate modifications of existing applications, since all previous versions of the database schema are available as subsets of the current schema. Anchor Modeling decouples the evolution and application of a database, which when building a data warehouse enables shrinking of the initial project scope. While data models were previously made to capture every facet of a domain in a single phase of development, in Anchor Modeling fragments can be iteratively modeled and applied. We provide a formal and technology independent definition of anchor models and show how anchor models can be realized as relational databases together with examples of schema evolution. We also investigate performance through a number of lab experiments, which indicate that under certain conditions anchor databases perform substantially better than databases constructed using traditional modeling techniques. &copy; 2010 Elsevier B.V. All rights reserved.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {0169023X},
  journal       = {Data and Knowledge Engineering},
  key           = {Data warehouses},
  keywords      = {Information theory;Interface states;Iterative methods;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/j.datak.2010.10.002},
}

@Article{Lakshmanan1997,
  author        = {Lakshmanan, Laks V.S. and Sadri, Fereidoon and Subramanian, Iyer N.},
  title         = {Logic and algebraic languages for interoperability in multidatabase systems},
  journal       = {Journal of Logic Programming},
  year          = {1997},
  volume        = {33},
  number        = {2},
  pages         = {101 - 149},
  issn          = {07431066},
  note          = {Multidatabase systems;Relational algebra;Schemalog;},
  __markedentry = {[Juliana:6]},
  abstract      = {Developing a declarative approach to interoperability in the context of multidatabase systems is a major goal of this research. We take a first step toward this goal in this paper, by developing a simple logic called SchemaLog which is syntactically higher-order but has a first-order semantics. SchemaLog can provide for interoperability among multiple relational databases in a federation of database systems. We develop a fixpoint theory for the definite clause fragment of SchemaLog and show its equivalence to the model-theoretic semantics. We also develop a sound and complete proof procedure for all clausal theories. We establish the correspondence between SchemaLog and first-order predicate calculus and provide a reduction of SchemaLog to predicate calculus. We propose an extension to classical relational algebra, capable of retrieving and manipulating data and schema from databases in a multidatabase system, and prove its equivalence to a form of relational calculus inspired by SchemaLog syntax. We illustrate the simplicity and power of SchemaLog with a variety of applications involving database programming (with schema browsing), schema integration, schema evolution, cooperative query answering, and sophisticated forms of aggregation in the spirit of OLAP (On-Line Analytical Processing). We also highlight our implementation of SchemaLog realized on a federation of INGRES databases.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Logic programming},
  keywords      = {Boolean algebra;Computational linguistics;Database systems;Equivalence classes;Information retrieval;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/S0743-1066(96)00146-X},
}

@Article{Urban1996,
  author        = {Urban, Susan D. and Shah, Jami J. and Liu, Hong and Rogers, Mary},
  title         = {Shared design manager: interoperability in engineering design},
  journal       = {Integrated Computer-Aided Engineering},
  year          = {1996},
  volume        = {3},
  number        = {3},
  pages         = {158 - 176},
  issn          = {10692509},
  note          = {Engineering design interoperability;File transfer;Meta data component;Multidatabase environment;Object oriented database systems;Open architecture;Shared design manager;Standard for the exchange of product data;},
  __markedentry = {[Juliana:6]},
  abstract      = {This paper describes a multidatabase environment that supports integration and communication between computer-aided design (CAD) tools in an engineering design environment. Our specific approach uses an object-oriented database system as a shared design manager (SDM). The SDM provides a blackboard for communication among CAD tools that uses the STEP (Standard for the Exchange of Product Data) product model as a global conceptual view of data. A unique aspect of the SDM is the flexibility that it provides in configuration of the design environment and in establishing communication. We have developed a meta data component that not only stores information about the global view of the design, but also serves as an interface description repository. Different aspects of the global view can be dynamically associated with specific interfaces. Information about operations associated with each interface can also be dynamically defined. Since the environment uses dynamic binding techniques for operation calls, interface and operation descriptions can easily be modified, and new descriptions can be added to the environment at any time. The flexibility provided by this approach enhances interoperability. among the tools of the environment and makes it easy to configure different sets of design tools, thus providing a more open architecture for communication in engineering design. This also addresses the use of schema evolution concepts to keep the temporary object storage component of the SDM up-to-date as CAD tool interfaces are modified, added, and deleted.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Computer aided design;Computer aided engineering;Computer architecture;Computer software;Data communication systems;File organization;Interfaces (computer);Object oriented programming;Productivity;},
  language      = {English},
}

@InProceedings{Duwairi1996,
  author        = {Duwairi, R.M. and Fiddian, N.J. and Gray, W.A.},
  title         = {Schema integration meta-knowledge classification and reuse},
  year          = {1996},
  volume        = {1094},
  pages         = {1 - 17},
  address       = {Edinburgh, United kingdom},
  note          = {Database technology;Multi-database systems;Object oriented;Present situation;Processing capability;Recent progress;Schema integration;Semantic heterogeneity;},
  __markedentry = {[Juliana:6]},
  abstract      = {Recent progress in communication and database technologies has drastically changed user data processing capabilities. The present situation is characterised by a growing number of applications that require the ability to access and manipulate data from various pre-existing database sources. A possible solution to deal with multiple databases is to logically integrate the component databases by the provision of one or more tailored global schemas. The end-user therefore is presented with a homogeneous and consistent view of the (possibly heterogeneous) multidatabase. Schema integration is an involved process that requires the supervised detection of similarities and dissimilarities between the component schemas, reconciling the dissimilarities once detected and providing a homogeneous view to the end-user. We are currently investigating the schema integration problem in object-oriented multidatabase systems. This paper describes one aspect of our work, namely, the classification and reuse of the knowledge accruing from the schema integration process. Such knowledge is classified into the knowledge that accrues from detecting and reconciling semantic heterogeneity between local schemas and the knowledge that accrues from generating the global schemas. We argue that reuse in schema integration is a vital issue and proves most valuable in saving the integrator's effort, simplifying the generation of new global schemas and supporting global schema evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 1996.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Data handling;Integration;Semantics;},
  language      = {English},
}

@Article{Abiteboul1995,
  author        = {Abiteboul, Serge and Kanellakis, Paris and Ramaswamy, Sridhar and Waller, Emmanuel},
  title         = {Method schemas},
  journal       = {Journal of Computer and System Sciences},
  year          = {1995},
  volume        = {51},
  number        = {3},
  pages         = {433 - 455},
  issn          = {00220000},
  note          = {Code redefinition;Code reuse;Decidability;Formalism;Method composition;Method schemas;Name overloading;Recursion;},
  __markedentry = {[Juliana:6]},
  abstract      = {A method schema is a simple programming formalism for object-oriented databases with features such as classes, methods, inheritance, name overloading, and late binding. An important problem is to check whether a given method schema can lead to an inconsistency in some interpretation. This consistency question is shown to be undecidable in general. Decidability is obtained for monadic and/or recursion-free method schemas. In particular, consistency of monadic method schemas is shown to be decidable in O(nc<sup>3</sup>) time, when n is the size of the method definitions and c is the size of the class hierarchy; also, it is logspace-complete in PTIME, even for monadic, recursion-free schemas. Method signature covariance is shown to simplify the computational complexity of key decidable cases. For example, one coded method in the context of base methods with covariant signatures can be tested for consistency in O(n + c) time for the monadic case (without covariance this problem is in O(nc<sup>2</sup>) time) and in PTIME for the fixed arity polyadic case (without covariance this problem is NP-complete). Incremental consistency checking of method schemas is a formalization of the database schema evolution problem, for which a sound, but necessarily incomplete, heuristic is proposed.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Formal languages},
  keywords      = {Computational complexity;Computational linguistics;Database systems;Decision theory;Equivalence classes;Heuristic methods;Object oriented programming;Recursive functions;},
  language      = {English},
  url           = {http://dx.doi.org/10.1006/jcss.1995.1080},
}

@Article{Oakley1993,
  author        = {Oakley, John P. and Davis, Darryl N. and Shann, R.T.},
  title         = {Manchester visual query language},
  journal       = {Proceedings of SPIE - The International Society for Optical Engineering},
  year          = {1993},
  volume        = {1908},
  pages         = {104 - 114},
  issn          = {0277786X},
  note          = {Hough-group operator;Image database;Manchester Visual Query Language;Visual retrieval;},
  __markedentry = {[Juliana:6]},
  abstract      = {We report a database language for visual retrieval which allows queries on image feature information which has been computed and stored along with images. The language is novel in that it provides facilities for dealing with feature data which has actually been obtained from image analysis. Each line in the Manchester Visual Query Language (MVQL) takes a set of objects as input and produces another, usually smaller, set as output. The MVQL constructs are mainly based on proven operators from the field of digital image analysis. An example is the Hough-group operator which takes as input a specification for the objects to be grouped, a specification for the relevant Hough space, and a definition of the voting rule. The output is a ranked list of high scoring bins. The query could be directed towards one particular image or an entire image database, in the latter case the bins in the output list would in general be associated with different images. We have implemented MVQL in two layers. The command interpreter is a Lisp program which maps each MVQL line to a sequence of commands which are used to control a specialized database engine. The latter is a hybrid graph/relational system which provides low-level support for inheritance and schema evolution. In the paper we outline the language and provide examples of useful queries. We also describe our solution to the engineering problems associated with the implementation of MVQL.},
  address       = {San Jose, CA, USA},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Digital image storage},
  keywords      = {Associative storage;Database systems;Information retrieval systems;Query languages;},
  language      = {English},
}

@InProceedings{Breche1995,
  author        = {Breche, Philippe and Ferrandina, Fabrizio and Kuklok, Martin},
  title         = {Simulation of schema change using views},
  year          = {1995},
  volume        = {978},
  pages         = {247 - 258},
  address       = {London, United kingdom},
  note          = {Change management;Database modification;Object views;Schema and database evolution;Schema changes;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper we concentrate on the description of change management functionalities that have been added to the commercially available &#1365;&#1399; ODBMS to yield a platform suited for applications such as SDEs. A description of declarative high level schema update primitives is presented as well as mechanisms to propagate schema changes to the database. Particular emphasis is given to a mechanism that allows designers first to simulate schema and database modifications by means of object views, and second to materialize them to be propagated onto the real schema and base.<br/> &copy; Springer-Verlag Berlin Heidelberg 1995.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Object-oriented databases},
  keywords      = {Expert systems;},
  language      = {English},
}

@Article{Engels1992,
  author        = {Engels, Gregor and Gogolla, Martin and Hohenstein, Uwe and Huelsmann, Klaus and Loehr-Richter, Perdita and Saake, Gunter and Ehrich, Hans-Dieter},
  title         = {Conceptual modelling of database applications using an extended ER model},
  journal       = {Data and Knowledge Engineering},
  year          = {1992},
  volume        = {9},
  number        = {2},
  pages         = {157 - 204},
  issn          = {0169023X},
  note          = {Conceptual data model;Conceptual database design;Database evolution;Entity-relationship design;Integrity constraints;Transactions;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper, we motivate and present a data model for conceptual design of structural and behavioural aspects of databases. We follow an object centered design paradigm in the spirit of semantic data models. The specification of structural aspects is divided into modelling of object structures and modelling of data types used for describing object properties. The specification of object structures is based on an Extended Entity-Relationship (EER) model. The specification of behavioural aspects is divided into the modelling of admissible database state evolutions by means of temporal integrity constraints and the formulation of database (trans)actions. The central link for integrating these design components is a descriptive logic-based query language for the EER model. The logic part of this language is the basis for static constraints and descriptive action specifications by means of pre- and postconditions. A temporal extension of this logic is the specification language for temporal integrity constraints. We emphasize that the various aspects of a database application are specified using several appropriate, but yet compatible formalisms, which are integrated by a unifying common semantic.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Applications;Query languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/0169-023X(92)90008-Y},
}

@Article{Yoon1993,
  author        = {Yoon, Jong P. and Kerschberg, Larry},
  title         = {Framework for knowledge discovery and evolution in databases},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  year          = {1993},
  volume        = {5},
  number        = {6},
  pages         = {973 - 979},
  issn          = {10414347},
  note          = {Active database evolution;Database mining;Expertise transfer;Knowledge discovery;Knowledge refinement;},
  __markedentry = {[Juliana:6]},
  abstract      = {Although knowledge discovery is increasingly important in databases, discovered knowledge is not always useful to users. It is mainly because the discovered knowledge does not fit the user's interests, or it may be redundant or inconsistent with a priori knowledge. Knowledge discovery in databases depends critically on how well a database is characterized and how consistently the existing and discovered knowledge is evolved. This paper describes a novel concept for knowledge discovery and evolution in databases. The key issues of this work include: using a database query to discover new rules; using not only positive examples (answer to a query) but also negative examples to discover new rules; harmonizing existing rules with the new rules. The main contribution of this paper is the development of a new tool for 1) characterizing the exceptions in databases and 2) evolving knowledge as a database evolves.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Codes (symbols);Data structures;Information retrieval;Knowledge based systems;Probability;Query languages;Technology transfer;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/69.250080},
}

@InProceedings{Minsky1986,
  author        = {Minsky, N. and Rozenshtein, D. and Chomicki, J.},
  title         = {CONTROLLABLE PROLOG DATABASE SYSTEM.},
  year          = {1986},
  pages         = {618 - 628},
  address       = {Los Angeles, CA, USA},
  note          = {DATABASE CONTROL;DATABASE EVOLUTION;DATABASE USE;PROLOG DATABASE SYSTEM;},
  __markedentry = {[Juliana:6]},
  abstract      = {A model is presented which provides a single comprehensive mechanism to control the use, operation and evolution of database systems. This model unifies several concepts generally considered to be quite distinct. In particular, it minimizes the formal distinction between the users of the database, the programs embedded in it, and even the administrators and the programmers maintaining it. Under this model, the concepts of subschema and of program module are replaced with a single concept of frame, which serves as the locus of power and of activity in the system. The proposed control mechanism is closed, in the sense that the process of establishing controls is itself controllable by the same mechanism. This can be used to formalize and control managerial policies about the use and evolution of database systems.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {DATABASE SYSTEMS},
  keywords      = {COMPUTER PROGRAMMING LANGUAGES;},
  language      = {English},
}

@InProceedings{2016,
  title         = {Proceedings - 2nd International Workshop on BIG Data Software Engineering, BIGDSE 2016},
  year          = {2016},
  pages         = {Association for Computing Machinery, Special Interest Group on Software Engineering (ACM SIGSOFT); IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE) -},
  address       = {Austin, TX, United states},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 13 papers. The topics discussed include: decisions as a service for application streaming software analytics; DataLab: a version data management and analytics system; exploring a framework for identity and attribute linking across heterogeneous data systems; providing big data applications with fault-tolerant data migration across heterogeneous NoSQL databases; data model evolution using object-NoSQL mappers: folklore or state-of-the-art?; toward big data value engineering for innovation; understanding quality requirements in the context of big data systems; a reference architecture for big data systems in the national security domain; a big data framework for cloud monitoring; and predicting and fixing vulnerabilities before they occur: a big data approach.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - 2nd International Workshop on BIG Data Software Engineering, BIGDSE 2016},
  language      = {English},
}

@InProceedings{Tripathi2014,
  author        = {Tripathi, Nishtha and Banerjee, Subhasis},
  title         = {Sarrod: SPARQL analyzer and reordering for runtime optimization on big data},
  year          = {2014},
  volume        = {8883},
  pages         = {189 - 196},
  address       = {New Delhi, India},
  note          = {Big data platforms;RDF query language;Resource description framework;Runtime optimization;Sparql queries;Structured graphs;Triple store;Web resources;},
  __markedentry = {[Juliana:6]},
  abstract      = {Resource Description Framework has been widely adopted for representing web resources and structured graph data model. Evolution of Big Data poses challenges in processing these graphs in terms of scalability as the size of the graphs may become enormously big. Tremendous growth of the data size (and in turn the graph size) will have an impounding effect on the execution times of the queries on the graph. Although big data platforms such as Hadoop can mitigate the problem, the query ordering and data flow between the constraints presents opportunities for further optimization. SPARQL, a widely used RDF query language suffers from the similar bottleneck for large graphs. There is hardly any established method to generate all equivalent reordering for a SPARQL query containing joins, outer joins, and group by aggregations. In this paper, we propose a query reordering algorithm viz., SARROD that leverages the property of the graphs that are simple to compute yet powerful for run time optimization. Experimental results show that SARROD reduces response time for SPARQL queries when executed over SHARD graph-store (triple-store) built on the Hadoop implementation of MapReduce by an order of 12% compared to nonordered sequence.<br/> &copy; Springer International Publishing Switzerland 2014.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Big data},
  keywords      = {Graphic methods;Query languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-319-13820-6},
}

@Article{Genero2008,
  author        = {Genero, Marcela and Poels, Geert and Piattini, Mario},
  title         = {Defining and validating metrics for assessing the understandability of entity-relationship diagrams},
  journal       = {Data and Knowledge Engineering},
  year          = {2008},
  volume        = {64},
  number        = {3},
  pages         = {534 - 557},
  issn          = {0169023X},
  note          = {Conceptual data modeling;Entity-Relationship (ER) diagrams;Experimental validation;},
  __markedentry = {[Juliana:6]},
  abstract      = {Database and data model evolution cause significant problems in the highly dynamic business environment that we experience these days. To support the rapidly changing data requirements of agile companies, conceptual data models, which constitute the foundation of database design, should be sufficiently flexible to be able to incorporate changes easily and smoothly. In order to understand what factors drive the maintainability of conceptual data models and to improve conceptual modelling processes, we need to be able to assess conceptual data model properties and qualities in an objective and cost-efficient manner. The scarcity of early available and thoroughly validated maintainability measurement instruments motivated us to define a set of metrics for Entity-Relationship (ER) diagrams. In this paper we show that these easily calculated and objective metrics, measuring structural properties of ER diagrams, can be used as indicators of the understandability of the diagrams. Understandability is a key factor in determining maintainability as model modifications must be preceded by a thorough understanding of the model. The validation of the metrics as early understandability indicators opens up the way for an in-depth study of how structural properties determine conceptual data model understandability. It also allows building maintenance-related prediction models that can be used in conceptual data modelling practice. &copy; 2007 Elsevier B.V. All rights reserved.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Computer simulation;Data structures;Mathematical models;Measurement theory;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/j.datak.2007.09.011},
}

@InProceedings{AitOubelli2017,
  author        = {Ait Oubelli, Lynda and Ait Ameur, Yamine and Bedouet, Judicael and Chausserie-Lapree, Benoit and Larzul, Beatrice},
  title         = {Automating the evolution of data models for space missions. A model-based approach},
  year          = {2017},
  volume        = {10563 LNCS},
  pages         = {340 - 354},
  address       = {Barcelona, Spain},
  note          = {Data migration;Evolution operator;Model comparison;Model evolution;Model-driven Engineering;Semantic transformation;},
  __markedentry = {[Juliana:6]},
  abstract      = {In space industry, model-driven engineering (MDE) is a key technique to model data exchanges with satellites. During the preparation of a space mission, the associated data models are often revised and need to be compared from one version to another. Thus, due to the undeniably growth of changes, it becomes difficult to track them. New methods and techniques to understand and represent the differences, as well as commonalities, between different model&rsquo;s revisions are highly required. Recent research works address the evolution process between the two layers (M2/M1) of the MDE architecture. In this research work, we have explored the use of the layers (M1/M0) of the same architecture in order to define a set of atomic operators and their composition that encapsulate both data model evolution and data migration. The use of these operators improves the quality of data migration, by ensuring full conservation of the information carried by the data.<br/> &copy; 2017, Springer International Publishing AG.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Metadata},
  keywords      = {Electronic data interchange;Engineering research;Semantics;Space flight;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-319-66854-3_26},
}

@InProceedings{Vermolen2012,
  author        = {Vermolen, Sander D. and Wachsmuth, Guido and Visser, Eelco},
  title         = {Generating database migrations for evolving web applications},
  year          = {2012},
  volume        = {47},
  number        = {3},
  pages         = {83 - 92},
  note          = {Data migration;Database migrations;Domain specific languages;Dynamic web applications;Evolution;Object oriented data;Relational Database;WEB application;},
  __markedentry = {[Juliana:6]},
  abstract      = {WebDSL is a domain-specific language for the implementation of dynamic web applications with a rich data model. It provides developers with object-oriented data modeling concepts but abstracts over implementation details for persisting application data in relational databases. When the underlying data model of an application evolves, persisted application data has to be migrated. While implementing migration at the database level breaks the abstractions provided by WebDSL, an implementation at the data model level requires to intermingle migration with application code. In this paper, we present a domain-specific language for the coupled evolution of data models and application data. It allows to specify data model evolution as a separate concern at the data model level and can be compiled to migration code at the database level. Its linguistic integration with WebDSL enables static checks for evolution validity and correctness. &copy; 2011 ACM.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {15232867},
  journal       = {ACM SIGPLAN Notices},
  key           = {Object-oriented databases},
  keywords      = {Abstracting;Problem oriented languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/2189751.2047876},
}

@InProceedings{Vermolen2011,
  author        = {Vermolen, Sander D. and Wachsmuth, Guido and Visser, Eelco},
  title         = {Generating database migrations for evolving web applications},
  year          = {2011},
  pages         = {83 - 92},
  address       = {Portland, OR, United states},
  note          = {Data migration;Database migrations;Domain specific languages;Dynamic web applications;Evolution;Object oriented data;Relational Database;WEB application;},
  __markedentry = {[Juliana:6]},
  abstract      = {WebDSL is a domain-specific language for the implementation of dynamic web applications with a rich data model. It provides developers with object-oriented data modeling concepts but abstracts over implementation details for persisting application data in relational databases. When the underlying data model of an application evolves, persisted application data has to be migrated. While implementing migration at the database level breaks the abstractions provided by WebDSL, an implementation at the data model level requires to intermingle migration with application code. In this paper, we present a domain-specific language for the coupled evolution of data models and application data. It allows to specify data model evolution as a separate concern at the data model level and can be compiled to migration code at the database level. Its linguistic integration with WebDSL enables static checks for evolution validity and correctness. Copyright &copy; 2011 ACM.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {GPCE'11 - Proceedings of the 10th International Conference on Generative Programming and Component Engineering},
  key           = {Object-oriented databases},
  keywords      = {Abstracting;Object oriented programming;Problem oriented languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/2047862.2047876},
}

@InProceedings{Vermolen2008,
  author        = {Vermolen, Sander and Visser, Eelco},
  title         = {Heterogeneous coupled evolution of software languages},
  year          = {2008},
  volume        = {5301 LNCS},
  pages         = {630 - 644},
  address       = {Toulouse, France},
  note          = {Coupled evolution;Database schemas;Elementary transformation;Homogeneous domain;Level transformation;Software artifacts;Software languages;Transformation languages;},
  __markedentry = {[Juliana:6]},
  abstract      = {As most software artifacts, meta-models can evolve. Their evolution requires conforming models to co-evolve along with them. Coupled evolution supports this. Its applicability is not limited to the modeling domain. Other domains are for example evolving grammars or database schemas. Existing approaches to coupled evolution focus on a single, homogeneous domain. They solve the co-evolution problems locally and repeatedly. In this paper we present a systematic, heterogeneous approach to coupled evolution. It provides an automatically derived domain specific transformation language; a means of executing transformations at the top level; a derivation of the coupled bottom level transformation; and it allows for generic abstractions from elementary transformations. The feasibility of the architecture is evaluated by applying it to data model evolution. &copy; 2008 Springer-Verlag Berlin Heidelberg.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Artificial intelligence},
  keywords      = {Computer science;Computers;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-540-87875-9_44},
}

@InProceedings{Benadjaoud1996,
  author        = {Benadjaoud, G.N. and David, B.T.},
  title         = {DEE: A data exchange environment},
  year          = {1996},
  pages         = {309 - 309},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Missier1996,
  author        = {Missier, Paolo and Rusinkiewicz, Marek and Silberschatz, Avi},
  title         = {Providing multidatabase access: An association approach},
  year          = {1996},
  pages         = {337 - 337},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Banham1996,
  author        = {Banham, Tony},
  title         = {Handling terabyte databases on open systems},
  year          = {1996},
  pages         = {235 - 235},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Garcia-Solaco1996,
  author        = {Garcia-Solaco, M. and Saltor, F. and Castellanos, M.},
  title         = {Extensional issues in schema integration},
  year          = {1996},
  pages         = {261 - 261},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Zhan1996,
  author        = {Zhan, Jibin and Luk, W.S. and Wong, Carlos},
  title         = {Object-oriented approach to query interoperability},
  year          = {1996},
  pages         = {141 - 141},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{1998,
  title         = {Proceedings - 1st International Symposium on Object-Oriented Real-Time Distributed Computing, ISORC 1998},
  year          = {1998},
  volume        = {1998-April},
  pages         = {IEEE Computer Society Technical Committee on Distributed Processing -},
  address       = {Kyoto, Japan},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 59 papers. The topics discussed include: supporting real-time multimedia applications with distributed object controlled networks; an implementation model for time-triggered message-triggered object support mechanisms in CORBA-compliant COTS platforms; exception handling in object-oriented real-time distributed systems; specifying and measuring quality of service in distributed object systems; a reliable mobile agents architecture; principles of constructing a timeliness-guaranteed kernel and time-triggered message-triggered object support mechanisms; concurrency control in real-time object-oriented systems: the affected set priority ceiling protocols; associative prioritized worker model with priority inheritance protocol; pseudo-active replication of objects in heterogeneous clusters; an admission control for video broadcast systems; objects collection management in multidimensional DBMS data model; evolution of a distributed, real-time, object-based operating system; SPLAW: a computable agent-oriented programming language; a purpose-oriented access control model for object-based systems; building middleware for real-time dependable distributed services; BDL, a language of distributed reactive objects; and supporting the analyst when reasoning on requirements specifications for real-time and distributed systems.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - 1st International Symposium on Object-Oriented Real-Time Distributed Computing, ISORC 1998},
  language      = {English},
}

@InProceedings{Muhlberger1996,
  author        = {Muhlberger, Ralf M. and Orlowska, Maria E.},
  title         = {Business process driven multidatabase integration methodology},
  year          = {1996},
  pages         = {283 - 283},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Navathe1996,
  author        = {Navathe, Shamkant B. and Donahoo, Michael J.},
  title         = {Towards intelligent integration of heterogeneous information sources},
  year          = {1996},
  pages         = {275 - 275},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Moffatt1996,
  author        = {Moffatt, Christopher},
  title         = {Designing client-server applications for enterprise database connectivity},
  year          = {1996},
  pages         = {215 - 215},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Yu1996,
  author        = {Yu, W. and Eliassen, F.},
  title         = {Flexible transaction management in an interoperable database environment},
  year          = {1996},
  pages         = {187 - 187},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Beldjilali1996,
  author        = {Beldjilali, Tarik},
  title         = {Generalization without reorganization in a simple object oriented DBMS},
  year          = {1996},
  pages         = {103 - 103},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{James1996,
  author        = {James, Anne E.},
  title         = {Database integration system and an example of its application},
  year          = {1996},
  pages         = {297 - 297},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Kuo1996,
  author        = {Kuo, T.Y. and Cheung, T.Y.},
  title         = {On interoperability verification and testing of object-oriented databases},
  year          = {1996},
  pages         = {125 - 125},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Huang1996,
  author        = {Huang, Shi-Ming and Smith, Peter and Tait, John},
  title         = {Object oriented model for data, knowledge, and system reengineering},
  year          = {1996},
  pages         = {25 - 25},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Faiz1996,
  author        = {Faiz, M. and Zaslavsky, A.},
  title         = {Database replica management strategies in multidatabase systems with mobile hosts},
  year          = {1996},
  pages         = {323 - 323},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Chen1996,
  author        = {Chen, Jian and Huang, Qiming},
  title         = {Eliminating the impedance mismatch between relational and object-oriented systems},
  year          = {1996},
  pages         = {89 - 89},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Aebi1996,
  author        = {Aebi, Daniel and Largo, Reto},
  title         = {Reengineering library data: The long way from ADABAS to UNIMARC},
  year          = {1996},
  pages         = {69 - 69},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{England1996,
  author        = {England, Rob},
  title         = {Reengineering VSAM, IMS, and DL/1 applications into relational database},
  year          = {1996},
  pages         = {55 - 55},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Smith1996,
  author        = {Smith, Peter and Bloor, Chris and Huang, Shi-Ming and Gillies, Alan},
  title         = {Need for reengineering when integrating expert system and database system technology},
  year          = {1996},
  pages         = {15 - 15},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{1995,
  title         = {14th International Conference on Object-Oriented and Entity-Relationship Modeling, OOER 1995},
  year          = {1995},
  volume        = {1021},
  pages         = {1 - 450},
  address       = {Gold Coast, QLD, Australia},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 45 papers. The special focus in this conference is on Object Design, Modeling, Models and Languages. The topics include: Modeling and system maintenance; adaptive schema design and evaluation in an object-oriented information system; assertion of consistency within a complex object database using a relationship construct; a cryptographic mechanism for object-instance-based authorization in object-oriented database systems; unifying modeling and programming through an active, object-oriented, model-equivalent programming language; a declarative query approach to object identification; versatile querying facilities for a dynamic object clustering model; reverse engineering of relational database applications; a rigorous approach to schema restructuring; managing schema changes in object-relationship databases; behavior consistent extension of object life cycles; integrated specification of the dynamics of individual objects; database design with behavior and views using parameterized petri nets; security enhanced entity-relationship model for secure relational databases; neural network technology to support view integration; database schema transformation and optimization; mapping an extended entity-relationship schema into a schema of complex objects; binary representation of ternary relationships in ER conceptual modeling; variable sets and functions framework for conceptual modeling; a logic framework for a semantics of object oriented data modeling; object-oriented meta modeling; a conceptual model for business re-engineering methods and tools; data model evolution as a basis of business process management; re-engineering processes in public administrations; uniqueness conditions for ER representations; integrating and supporting entity relationship and object role models and from object specification towards agent design.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Bolloju1996,
  author        = {Bolloju, Narasimha},
  title         = {Semantic query transformation: An approach to achieve semantic interoperability in homogeneous application domains},
  year          = {1996},
  pages         = {117 - 117},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Siu1996,
  author        = {Siu, Brian and Fong, Joseph},
  title         = {Reverse engineering in a client-server environment: Case studies on relational database design},
  year          = {1996},
  pages         = {81 - 81},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Ayre1996,
  author        = {Ayre, John and McFall, Donald and Hughes, John G. and Delobel, Claude},
  title         = {Method for reengineering existing relational database applications for the satisfaction of multimedia based requirements},
  year          = {1996},
  pages         = {1 - 1},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Suzuki1996,
  author        = {Suzuki, Gengo and Yamamuro, Masashi},
  title         = {Schema integration methodology including structural conflict resolution and checking conceptual similarity: Conceptual graphs approach},
  year          = {1996},
  pages         = {247 - 247},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Chang1996,
  author        = {Chang, Ya-hui and Raschid, Louiqa},
  title         = {Using parameterized canonical representations to resolve conflicts and achieve interoperability between relational and object databases},
  year          = {1996},
  pages         = {155 - 155},
  address       = {Kowloon, Hong kong},
  __markedentry = {[Juliana:6]},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{2005,
  title         = {BNAIC 2005 - Proceedings of the 17th Belgium-Netherlands Conference on Artificial Intelligence},
  year          = {2005},
  pages         = {Belgian-Dutch Association for Artificial Intelligence (BNVKI); Flemish Research Council (FWO); Dutch Res. Sch. Inf. Knowl. Syst. (SIKS); Foundation of Knowledge BasedSystems (SKBS); Royal Flemish Academy of Belgium for Sciences and Arts (VWK) -},
  address       = {Brussels, Belgium},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 90 papers. The topics discussed include: neural networks for discrete tomography; multiagent reinforcement learning with adaptive state focus; temporal plan and resource management; contamination in formal argumentation systems; multi-agent simulations of the evolution of combinatorial phonology; efficient feature detection for sequence classification in a receptor database; evolutionary planning heuristics in production management; distributed Bayesian networks in highly dynamic agent organizations; automatic ontology population by googling; argumentation systems for history-based construction of medical guidelines; efficiency and fairness in air traffic control; speeding up feature selection selection by using an information theoretic bound; a dialectics system in which agents play and arbitrate; purposeful perception by attention-steered robots; and tuning the hyperparameter of an AUC-optimized classifier.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {15687805},
  journal       = {Belgian/Netherlands Artificial Intelligence Conference},
  language      = {English},
}

@Article{Guo2015,
  author        = {Guo, Fei and Ding, Yijie and Li, Zhao and Tang, Jijun},
  title         = {Identification of Protein-Protein Interactions by Detecting Correlated Mutation at the Interface},
  journal       = {Journal of Chemical Information and Modeling},
  year          = {2015},
  volume        = {55},
  number        = {9},
  pages         = {2042 - 2049},
  issn          = {15499596},
  note          = {Correlated mutation analysis;Correlation coefficient;Evolutionary distance;Evolutionary information;Geometric similarity;Identification of proteins;Interface prediction;Protein-protein interactions;},
  __markedentry = {[Juliana:6]},
  abstract      = {Protein-protein interactions play key roles in a multitude of biological processes, such as de novo drug design, immune response, and enzymatic activity. It is of great interest to understand how proteins in a complex interact with each other. Here, we present a novel method for identifying protein-protein interactions, based on typical co-evolutionary information. Correlated mutation analysis can be used to predict interface residues. In this paper, we propose a non-redundant database to detect correlated mutation at the interface. First, we construct structure alignments for one input protein, based on all aligned proteins in the database. Evolutionary distance matrices, one for each input protein, can be calculated through geometric similarity and evolutionary information. Then, we use evolutionary distance matrices to estimate correlation coefficient between each pair of fragments from two input proteins. Finally, we extract interacting residues with high values of correlation coefficient, which can be grouped as interacting patches. Experiments illustrate that our method achieves better results than some existing co-evolution-based methods. Applied to SK/RR interaction between sensor kinase and response regulator proteins, our method has accuracy and coverage values of 53% and 45%, which improves upon accuracy and coverage values of 50% and 30% for DCA method. We evaluate interface prediction on four protein families, and our method has overall accuracy and coverage values of 34% and 30%, which improves upon overall accuracy and coverage values of 27% and 21% for PIFPAM. Our method has overall accuracy and coverage values of 59% and 63% on Benchmark v4.0, and 50% and 49% on CAPRI targets. Comparing to existing methods, our method improves overall accuracy value by at least 2%.<br/> &copy; 2015 American Chemical Society.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Biology},
  keywords      = {Enzyme activity;},
  language      = {English},
  url           = {http://dx.doi.org/10.1021/acs.jcim.5b00320},
}

@Article{Zorman2005,
  author        = {Zorman, Milan and Cernohorski, Branimir and Gorek, Gregor and Milan, Ojsterek},
  title         = {Hybrid evolutionary built decision trees for prediction of perspective cross-country skiers},
  journal       = {WSEAS Transactions on Information Science and Applications},
  year          = {2005},
  volume        = {2},
  number        = {1},
  pages         = {32 - 37},
  issn          = {17900832},
  note          = {Cross-country skiers database systems;Hybrid evolutionary decision trees;Knowledge extraction;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper we present an application of intelligent systems using hybrid evolutionary built decision trees to extract new knowledge from a small 'Cross-country skiers' database. Evolutionary built decision trees approach is a combination of one well known machine learning approach - decision trees and recently very popular evolutionary algorithms. By combining both approaches we were aiming for a method, that would combine the advantages of both approaches and should find optimal solution sooner than classical decision tree method. Such method is very suitable for application in fields like medicine and health care, where we have to combine high accuracy of the classifier and transparent knowledge representation. In the particular case of 'Cross-country skiers' database, our main goal was to develop a classifier, that would have as high prediction accuracy as possible for all three outcomes. On the other hand we were also interested in attributes that are the most important for the prediction of cross-country skier's competition potentials. The results of testing are relatively good. Overall accuracy of the classifier, which was tested on the test set ranged from 62.5% to 87.5%. In some cases we were also able to achieve relatively high average class accuracies, which was not a trivial task on database with three possible decisions and small unbalanced training set.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Learning systems},
  keywords      = {Artificial intelligence;Database systems;Decision theory;Evolutionary algorithms;Knowledge acquisition;},
  language      = {English},
}

@Article{Jianqin1999,
  author        = {Jianqin, Liu},
  title         = {Pattern mining and discovery oriented to artificial life},
  journal       = {Transactions of Nonferrous Metals Society of China},
  year          = {1999},
  volume        = {9},
  number        = {1},
  pages         = {197 - 201},
  issn          = {10036326},
  note          = {Artificial life;Functional genomics;Pattern mining and discovery;},
  __markedentry = {[Juliana:6]},
  abstract      = {The nano-technology requires new methodology to handle difficult problems that involve the information processing, material technology and life phenomena in the nano-world. Concentrating on the synthesis of techniques in scientific frontier fields such as KDD(Knowledge Discovery in Database), evolutionary computation, rough set and logic, a new artificial life model for pattern mining and discovery has been proposed and the corresponding emergent algorithm has been built and implemented. The original contribution of the research work can be summarized in the following two principal respects: (a) pattern mining and discovery for genomic dynamics within the theoretic framework of artificial life; (b) information fusion of multi-paradigm for modeling and building of evolutionary KDD system with rough pattern inference. Through computer experiments the artificial sequence generated by computational processes has matched the evidence convinced by the latest scientific reality. The work is helpful to analyze and build the next generation of bio-nonferrous metal materials in the level of genomics and nano-technology.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Data mining},
  keywords      = {Algorithms;Biological materials;Computational methods;Database systems;Nanotechnology;Nonferrous metals;Object oriented programming;},
  language      = {English},
}

@Article{Filteau1988,
  author        = {Filteau, M.C. and Kassicieh, S.K. and Tripp, R.S.},
  title         = {Evolutionary database design and development in very large scale MIS},
  journal       = {Information and Management},
  year          = {1988},
  volume        = {15},
  number        = {4},
  pages         = {203 - 212},
  issn          = {03787206},
  note          = {Database design;Development methodology;Distributed processing;Evolutionary database design;Evolutionary strategies;Incremental development;Large-scale database;Scale management;},
  __markedentry = {[Juliana:6]},
  abstract      = {Developing flexible databases that can accommodate the changes and enhancements necessary in development of large scale Management Information Systems (MIS) is crucial to the success of the system and ultimately to the survival of the organization. Since the MIS is constantly being modified and enhanced, it is necessary for the database to be designed to adapt to change as it occurs. Numerous articles point to the need for large scale systems decomposition into smaller subsystems for implementation purposes. They also require an incremental development strategy which coincides with the database evolutionary strategy. This paper describes a methodology for large scale database development that assists MIS managers in building databases for large scale systems. It describes how these concepts were used for implementation of the US Air Force's largest distributed processing MIS-"The Requirements Data Bank" (RDB). &copy; 1988.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Evolutionary algorithms;Information management;Large scale systems;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/0378-7206(88)90046-8},
}

@InProceedings{Ordonez2019,
  author        = {Ordonez, Carlos and Bellatreche, Ladjel},
  title         = {Enhancing ER diagrams to view data transformations computed with queries},
  year          = {2019},
  volume        = {2324},
  address       = {Lisbon, Portugal},
  note          = {Complex transformations;Data preprocessing;Data provenance;Data transformation;Database transformation;Relational Database;Relational queries;Relational tables;},
  __markedentry = {[Juliana:6]},
  abstract      = {Transforming relational tables to build a data set takes most of the time in a machine learning (ML) project centered around a relational database. The explanation is simple: a relational database has a collection of tables that are joined and aggregated with complex relational queries, and whose columns are transformed with complex SQL expressions, in order to build the required data set. In general, such data is wide, gathering many ML variables together. Such complicated data pre-processing results in a large set of SQL queries that are independently developed from each other for different ML models. The database grows with important tables and views that are absent in the original ER diagram. More importantly, similar SQL queries tend to be written multiple times, creating problems in database evolution, disk space utilization and software maintenance. In this paper, we go in opposite direction from a physical level (tables) to a logical level (entities) representation, providing a unifying diagram of both levels. Specifically, we propose minimal, but powerful, extensions to an ER diagram in UML notation to represent most common database transformations. Our &ldquo;transformation&rdquo; ER diagram helps analytic users understanding complex transformations, consolidating columns representing analytic variables into fewer tables (i.e. eliminating redundant tables), reusing existing SQL queries (i.e. avoid forking new queries) and explaining data provenance (where data originated from).<br/> &copy; 2019 Copyright held by the author(s).},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {16130073},
  journal       = {CEUR Workshop Proceedings},
  key           = {Data handling},
  keywords      = {Big data;Learning systems;Metadata;Query languages;},
  language      = {English},
}

@InProceedings{Mior2018,
  author        = {Mior, Michael J. and Salem, Kenneth},
  title         = {Renormalization of NoSQL database schemas},
  year          = {2018},
  volume        = {11157 LNCS},
  pages         = {479 - 487},
  address       = {Xi'an, China},
  note          = {Application developers;Application level;Conceptual model;Database design;Denormalization;Inclusion dependencies;NoSQL;Renormalization;},
  __markedentry = {[Juliana:6]},
  abstract      = {NoSQL applications often use denormalized databases in order to meet performance goals, but this introduces complications as the database itself has no understanding of application-level denormalization. In this paper, we describe a procedure for reconstructing a normalized conceptual model from a denormalized NoSQL database. The procedure&rsquo;s input includes functional and inclusion dependencies, which may be mined from the NoSQL database. Exposing a conceptual model provides application developers with information that can be used to guide application and database evolution.<br/> &copy; Springer Nature Switzerland AG 2018.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Data mining;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-030-00847-5_34},
}

@InProceedings{2015,
  title         = {19th East European Conference on Advances in Databases and Information Systems, ADBIS 2015},
  year          = {2015},
  volume        = {9282},
  pages         = {1 - 472},
  address       = {Poitiers, France},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 42 papers. The special focus in this conference is on Database Theory, Access Methods, User Requirements and Database Evolution. The topics include: Improving the pruning ability of dynamic metric access methods with local additional pivots and anticipation of information; two phase user driven schema matching; a relationally complete language for database evolution; a generic data warehouse architecture for analyzing workflow logs; integrating an incremental ETL pipeline with a big data store for real-time analytics; analysis of the blocking behaviour of schema transformations in relational database systems; evidence-based languages for conceptual data modelling profiles; a new seeding-based clustering approach for ontology matching; is estimation of data completeness through time series forecasts feasible; best-match time series subsequence search on the intel many integrated core architecture; feedback based continuous skyline queries over a distributed framework; efficient computation of parsimonious temporal aggregation; a self-tuning framework for cloud storage clusters; optimizing sort in hadoop using replacement selection; distributed sequence pattern detection over multiple data streams; hybrid web service discovery based on fuzzy condorcet aggregation; confidentiality preserving evaluation of open relational queries; a general trust management framework for provider selection in cloud environment and sybil tolerance and probabilistic databases to compute web services trust.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Herrmann2018,
  author        = {Herrmann, Kai and Voigt, Hannes and Pedersen, Torben Bach and Lehner, Wolfgang},
  title         = {Multi-schema-version data management: data independence in the twenty-first century},
  journal       = {VLDB Journal},
  year          = {2018},
  volume        = {27},
  number        = {4},
  pages         = {547 - 571},
  issn          = {10668888},
  note          = {Agile software development;Data independence;Database administrators;Database development;Evolution;Orders of magnitude;Software systems;Write operations;},
  __markedentry = {[Juliana:6]},
  abstract      = {Agile software development allows us to continuously evolve and run a software system. However, this is not possible in databases, as established methods are very expensive, error-prone, and far from agile. We present InVerDa, a multi-schema-version database management system (MSVDB) for agile database development. MSVDBs realize co-existing schema versions within one database, where each schema version behaves like a regular single-schema database and write operations are propagated between schema versions. Developers use a relationally complete and bidirectional database evolution language (BiDEL) to easily evolve existing schema versions to new ones. BiDEL scripts are more robust, orders of magnitude shorter, and cause only a small performance overhead compared to handwritten SQL scripts. We formally guarantee data independence: no matter how the data of the co-existing schema versions is physically materialized, each schema version is guaranteed to behave like a regular database. Since, the chosen physical materialization significantly determines the overall performance, we equip database administrators with an advisor that proposes an optimized materialization for the current workload, which can improve the performance by orders of magnitude compared to na&iuml;ve solutions. To our best knowledge, we are the first to facilitate agile evolution of production databases with full support of co-existing schema versions and formally guaranteed data independence.<br/> &copy; 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Agile manufacturing systems;Information management;Software design;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/s00778-018-0508-7},
}

@InProceedings{Fonseca2015,
  author        = {Fonseca, Adriane M. and Camolesi, Luiz},
  title         = {Refactoring rules for graph databases},
  year          = {2015},
  volume        = {353},
  pages         = {33 - 44},
  address       = {Ponta Delgada, Portugal},
  note          = {AS graph;Data integrity;Database management;Information organization;Refactorings;Relational Database;Relational Model;Technological challenges;},
  __markedentry = {[Juliana:6]},
  abstract      = {The information generated nowadays is growing in volume and complexity, representing a technological challenge which demands more than the relational model for databases can currently offer. This situation stimulates the use of different forms of storage, such as Graph Databases. Current Graph Databases allow automatic database evolution, but do not provide adequate resources for the information organization. This is mostly left under the responsibility of the applications which access the database, compromising the data integrity and reliability. The goal of this work is the definition of refactoring rules to support the management of the evolution of Graph Databases by adapting and extending existent refactoring rules for relational databases to meet the requirements of the Graph Databases features. These refactoring rules can be used by developers of graph database management tools to guarantee the integrity of the operations of database evolution.<br/> &copy; Springer International Publishing Switzerland 2015.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {21945357},
  journal       = {Advances in Intelligent Systems and Computing},
  key           = {Graph Databases},
  keywords      = {Digital storage;Information systems;Information use;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-319-16486-1_4},
}

@InProceedings{2014,
  title         = {2014 World Conference on Information Systems and Technologies, WorldCIST 2014},
  year          = {2014},
  volume        = {276 VOLUME 2},
  address       = {Madeira, Portugal},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 47 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Extending Groovy's reification and closures; the connective scheme between the enterprise architecture and organisational structure; towards a flexible and transparent database evolution; virtual desktop infrastructure (VDI) technology; exploring WebRTC technology for enhanced real-time services; integrating public transportation data; model and application architecture indicators of evaluation the enterprise architecture; software tools for project management - focus on collaborative management; an adaptable infrastructure to generate training datasets for decompilation issues; reengineering of software requirement specification; discrimination of class inheritance hierarchies - a vector approach; transformation of Coloured Petri Nets to UML 2 diagrams; a secure mobile platform integrated with electronic medical records; mobile botnet attacks; evaluation of an integrated mobile payment, ticketing and couponing solution based on NFC; augmented reality mobile tourism application; a data aggregation system for music events; an approach for graphical user interface external bad smells detection; improving high availability and reliability of health interoperability systems; developing a learning objects recommender system based on competences to education; current issues on enterprise architecture implementation methodology; user behavior detection based on statistical traffic analysis for thin client services; using individual interactions to infer group interests and to recommend content for groups in public spaces; a distributed architecture for remote validation of software licenses using USB/IP protocol; bandwidth optimization for real-time online and mobile applications; an evaluation on the usability of E-commerce website using think aloud method; towards a structured electronic patient record for supporting clinical decision-making; architecture for serious games in health rehabilitation; a simple movement classification system for smartphones with accelerometer; a comparative study of cerebral palsy adults with distinct boccia experience; visualization of services availability; information persistence architecture for informal and formal care providers; a web system based on a sports injuries model towards global athletes monitoring; an embryo quality assessment tool; predictive models for hospital bed management using data mining techniques; how to use activity theory contradiction concept to support organization control; pedagogical and organizational concerns for the deployment of interactive public displays at schools; application of virtual and augmented reality technology in education of manufacturing engineers; mobile technologies applied to teaching; mobile games for children; authoring environment for location decision games for decision-making skills development; leveraging web information for E-learning and cross-artefacts for the purpose of education.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {21945357},
  journal       = {Advances in Intelligent Systems and Computing},
  language      = {English},
}

@InProceedings{Herrmann2016,
  author        = {Herrmann, Kai and Voigt, Hannes and Seyschab, Thorsten and Lehner, Wolfgang},
  title         = {InVerDa - Co-existing schema versions made foolproof},
  year          = {2016},
  pages         = {1362 - 1365},
  address       = {Helsinki, Finland},
  note          = {Co-existing;Data access;Error prones;Multiple applications;Single point;Software project;Versioning;},
  __markedentry = {[Juliana:6]},
  abstract      = {In modern software landscapes multiple applications usually share one database as their single point of truth. All these applications will evolve over time by their very nature. Often former versions need to stay available, so database developers find themselves maintaining co-existing schema version of multiple applications in multiple versions. This is highly error-prone and accounts for significant costs in software projects, as developers realize the translation of data accesses between schema versions with hand-written delta code. In this demo, we showcase INVERDA, a tool for integrated, robust, and easy to use database versioning. We rethink the way of specifying the evolution to new schema versions. Using the richer semantics of a descriptive database evolution language, we generate all required artifacts automatically and make database versioning foolproof.<br/> &copy; 2016 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {2016 IEEE 32nd International Conference on Data Engineering, ICDE 2016},
  key           = {Database systems},
  keywords      = {Application programs;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/ICDE.2016.7498345},
}

@InProceedings{Fuller2017,
  author        = {Fuller, Benjamin and Varia, Mayank and Yerukhimovich, Arkady and Shen, Emily and Hamlin, Ariel and Gadepally, Vijay and Shay, Richard and Mitchell, John Darby and Cunningham, Robert K.},
  title         = {SoK: Cryptographically Protected Database Search},
  year          = {2017},
  volume        = {0},
  pages         = {172 - 191},
  address       = {San Jose, CA, United states},
  note          = {Database searches;Private information retrieval;Property-preserving;Random access memory;Symmetric encryption;},
  __markedentry = {[Juliana:6]},
  abstract      = {Protected database search systems cryptographically isolate the roles of reading from, writing to, and administering the database. This separation limits unnecessary administrator access and protects data in the case of system breaches. Since protected search was introduced in 2000, the area has grown rapidly, systems are offered by academia, start-ups, and established companies. However, there is no best protected search system or set of techniques. Design of such systems is a balancing act between security, functionality, performance, and usability. This challenge is made more difficult by ongoing database specialization, as some users will want the functionality of SQL, NoSQL, or NewSQL databases. This database evolution will continue, and the protected search community should be able to quickly provide functionality consistent with newly invented databases. At the same time, the community must accurately and clearly characterize the tradeoffs between different approaches. To address these challenges, we provide the following contributions:1) An identification of the important primitive operations across database paradigms. We find there are a small number of base operations that can be used and combined to support a large number of database paradigms.2) An evaluation of the current state of protected search systems in implementing these base operations. This evaluation describes the main approaches and tradeoffs for each base operation. Furthermore, it puts protected search in the context of unprotected search, identifying key gaps in functionality.3) An analysis of attacks against protected search for different base queries.4) A roadmap and tools for transforming a protected search system into a protected database, including an open-source performance evaluation platform and initial user opinions of protected search.<br/> &copy; 2017 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {10816011},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  key           = {Database systems},
  keywords      = {Commerce;Cryptography;Information retrieval;Open systems;Random access storage;Safety devices;Search engines;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/SP.2017.10},
}

@InProceedings{2015a,
  title         = {DATA 2015 - 4th International Conference on Data Management Technologies and Applications, Proceedings},
  year          = {2015},
  pages         = {Institute for Systems and Technologies of Information, Control and Communication (INSTICC) -},
  address       = {Colmar, Alsace, France},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 42 papers. The topics discussed include: finding maximal quasi-cliques containing a target vertex in a graph; a study on term weighting for text categorization: a novel supervised variant of tf.idf; a unifying polynomial model for efficient discovery of frequent itemsets; exploiting linked data towards the production of added-value business analytics and vice-versa; preserving prediction accuracy on incomplete data streams; extended techniques for flexible modeling and execution of data mashups; database evolution for software product lines; a visual technique to assess the quality of datasets - understanding the structure and detecting errors and missing values in open data CSV files; facts collection and verification efforts; database architectures: current state and development; and data quality assessment of company's maintenance reporting: a case study.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {DATA 2015 - 4th International Conference on Data Management Technologies and Applications, Proceedings},
  language      = {English},
}

@InProceedings{Dominguez2008,
  author        = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
  title         = {Model-driven, view-based evolution of relational databases},
  year          = {2008},
  volume        = {5181 LNCS},
  pages         = {822 - 836},
  address       = {Turin, Italy},
  note          = {Abstraction level;Model-driven;Relational Database;View-based;},
  __markedentry = {[Juliana:6]},
  abstract      = {Among other issues, database evolution includes the necessity of propagating the changes inside and between abstraction levels. There exist several mechanisms in order to carry out propagations from one level to another, that are distinguished on the basis of when and how the changes are performed. The strict mechanism, which implies the immediate realization of modifications, is a time-consuming process. In this paper we propose a solution that is closer to the lazy and logical mechanisms, in which changes are delayed or not finally realized, respectively. This solution makes use of the notion of view. The use of views allows the data not to be changed if it is not necessary and facilitates carrying out changes when required. &copy; 2008 Springer-Verlag Berlin Heidelberg.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Expert systems},
  keywords      = {Artificial intelligence;Computer science;Computers;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-540-85654-2_74},
}

@InProceedings{Pham2015,
  author        = {Pham, Cong Cuong and Matta, Nada and Durupt, Alexandre and Eynard, Benoit and Allanic, Marianne and Ducellier, Guillaume and Joliot, Marc and Boutinaud, Philippe},
  title         = {Sharing knowledge in daily activity: Application in bio-imaging},
  year          = {2015},
  volume        = {3},
  pages         = {242 - 247},
  address       = {Lisbon, Portugal},
  note          = {Bio-imaging;Daily activity;Data relationships;Data resources;Knowledge-sharing;Making process;Product life cycle management;Sharing knowledge;},
  __markedentry = {[Juliana:6]},
  abstract      = {Our approach uses the ontology to facilitate the data querying of users in the domain Bio-Imaging where the data resources are heterogeneous and complex. The dependencies among data and the evolution of data resources challenge users (especially for non-technician users) in querying the right data. Ontology can be used to share the users' understanding about data relationships to all community as well as to trace the database evolution. As consequence, using ontology is a promising solution to facilitate the user's query making process and to enhance the query's results.<br/> &copy; 2015 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {IC3K 2015 - Proceedings of the 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
  key           = {Life cycle},
  keywords      = {Knowledge engineering;Knowledge management;Ontology;Query processing;},
  language      = {English},
}

@InProceedings{2015b,
  title         = {2nd International Conference on Multidisciplinary Social Networks Research, MISNC 2015},
  year          = {2015},
  volume        = {540},
  pages         = {1 - 583},
  address       = {Matsuyama, Japan},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 48 papers. The special focus in this conference is on Special Session on Information Technology and Social Networks Mining. The topics include: Switching motivations on instant messaging; the role of consumption value and product types in repurchase intention of printed and online music products; a novel multi-objective optimization algorithm for social network; individual attachment style, self-disclosure, and how people use social network; an innovative use of multidisciplinary applications between information technology and socially digital media for connecting people; result from a multi-country sample; organization learning from failure through knowledge network; the integration of nature disaster and tourist database; evolution of social networks and body mass index for adolescence; effect of task-individual-social software fit in knowledge creation performance; a framework for visualization of criminal networks; effects of human communication on promoting the use of a web-mediated service; evaluating online peer assessment as an educational tool for promoting self-regulated learning; network-based analysis of comorbidities; how social media enhances product innovation in Japanese firms; corporate usage of social media and social networking services in the USA; the impact of knowledge property and social capital on creation performance; the research on MMR algorithm based on hadoop platform; exploring multidimensional conceptualization of online learned capabilities; multiple days trip recommendation based on check-in data; a study of hybrid and sustainable media using active infrared vision and toward automatic assessment of the categorization structure of open data portals.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {18650929},
  journal       = {Communications in Computer and Information Science},
  language      = {English},
}

@InProceedings{Qiu2013,
  author        = {Qiu, Dong and Li, Bixin and Su, Zhendong},
  title         = {An empirical analysis of the co-evolution of schema and code in database applications},
  year          = {2013},
  pages         = {125 - 135},
  address       = {Saint Petersburg, Russia},
  note          = {Application Lifecycle;Co-evolution;Code-level modifications;Complex software systems;Database applications;Empirical analysis;Open source database;Referential integrity constraints;},
  __markedentry = {[Juliana:6]},
  abstract      = {Modern database applications are among the most widely used and complex software systems. They constantly evolve, responding to changes to data, database schemas, and code. It is challenging to manage these changes and ensure that everything co-evolves consistently. For example, when a database schema is modified, all the code that interacts with the database must be changed accordingly. Although database evolution and software evolution have been extensively studied in isolation, the co-evolution of schema and code has largely been unexplored. This paper presents the first comprehensive empirical analysis of the co-evolution of database schemas and code in ten popular large open-source database applications, totaling over 160K revisions. Our major findings include: 1) Database schemas evolve frequently during the application lifecycle, exhibiting a variety of change types with similar distributions across the studied applications; 2) Overall, schema changes induce significant code-level modifications, while certain change types have more impact on code than others; and 3) Co-change analyses can be viable to automate or assist with database application evolution. We have also observed that: 1) 80% of the schema changes happened in 20-30% of the tables, while nearly 40% of the tables did not change; and 2) Referential integrity constraints and stored procedures are rarely used in our studied subjects. We believe that our study reveals new insights into how database applications evolve and useful guidelines for designing assistive tools to aid their evolution. Copyright 2013 ACM.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
  key           = {Open systems},
  keywords      = {Application programs;Codes (symbols);Data warehouses;Open source software;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/2491411.2491431},
}

@InProceedings{2004,
  title         = {3rd International Conference on Generative Programming and Component Engineering, GPCE 2004},
  year          = {2004},
  volume        = {3286},
  pages         = {1 - 490},
  address       = {Vancouver, BC, Canada},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 27 papers. The special focus in this conference is on Aspect Orientation, Staged Programming, Types of Meta-programming and Model-Driven Approaches. The topics include: A generative approach to aspect-oriented programming; supporting flexible object database evolution with aspects; cross-language aspect-oriented programming; meta-programming with typed object-language representations; a multi-stage, object-oriented programming language; a fresh calculus for name management; a unification of inheritance and automatic program specialization; towards generation of efficient transformations; model-driven configuration and deployment of component middleware publish/subscribe services; model-driven program transformation of a large avionics framework; automatic remodularization and optimized synthesis of product-families; a case study of a product line for versioning systems; a model-driven approach for smart card configuration; on designing a target-independent DSL for safe OS process-scheduling components; a generative approach to the implementation of language bindings for the document object model and assembling applications with patterns, models, frameworks and tools.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Zaschke2015,
  author        = {Zaschke, Tilmann and Leone, Stefania and Gmunder, Tobias and Norrie, Moira C.},
  title         = {Improving conceptual data models through iterative development},
  journal       = {Data and Knowledge Engineering},
  year          = {2015},
  volume        = {98},
  pages         = {54 - 73},
  issn          = {0169023X},
  note          = {Agile development;Conceptual model;Evolvability;Model qualities;Semantic verification;},
  __markedentry = {[Juliana:6]},
  abstract      = {Agile methods promote iterative development with short cycles, where user feedback from the previous iteration is used to refactor and improve the current version. To facilitate agile development of information systems, this paper offers three contributions. First, we introduce the concept of evolvability as a model quality characteristic. Evolvability refers to the expected implications of future model refactorings, both in terms of complexity of the required database evolution algorithm and in terms of the expected volume of data to evolve. Second, we propose extending the agile development cycle by using database profiling information to suggest adaptations to the conceptual model to improve performance. For every software release, the database profiler identifies and analyses navigational access patterns, and proposes model optimisations based on data characteristics, access patterns and a cost-benefit model. Based on an experimental evaluation of the profiler we discuss why the quality of conceptual models can generally benefit from profiling and how performance measurements convey semantic information. Third, we discuss the flow of semantic information when developing and using information systems. Beyond these contributions, we also make a case for using object databases in agile development environments. However, most of the presented concepts are also applicable to other database paradigms.<br/> &copy; 2015 Elsevier B.V. All rights reserved.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Information use},
  keywords      = {Agile manufacturing systems;Cost benefit analysis;Evolutionary algorithms;Information systems;Iterative methods;Object-oriented databases;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/j.datak.2015.07.005},
}

@InProceedings{Dominguez2004,
  author        = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
  title         = {Elementary translations: The seesaws for achieving traceability between database schemata},
  year          = {2004},
  volume        = {3289},
  pages         = {377 - 389},
  address       = {Shanghai, China},
  note          = {Database maintenance;Database schemas;Levels of abstraction;Metadata management;Model transformation;Translation rules;},
  __markedentry = {[Juliana:6]},
  abstract      = {There exist several recent approaches that leverages the use of model transformations during software development. The existence of different kinds of models, at different levels of abstraction, involves the necessity of transferring knowledge from one model to another. This framework can also be applied in the context of metadata management for database evolution, in which transformations are needed both to translate schemata from one level to another and to modify existing schemata. In this paper we introduce the notions of translation rule and elementary translation which are used within a forward database maintenance strategy.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Software design},
  keywords      = {Metadata;},
  language      = {English},
}

@InProceedings{Domingues2011,
  author        = {Domingues, Helves Humberto and Kon, Fabio and Ferreira, Joao Eduardo},
  title         = {Asynchronous replication for evolutionary database development: A design for the experimental assessment of a novel approach},
  year          = {2011},
  volume        = {7045 LNCS},
  number        = {PART 2},
  pages         = {818 - 825},
  address       = {Hersonissos, Crete, Greece},
  note          = {Agile methods;Application requirements;Asynchronous data replication;Asynchronous replication;Evolutionary approach;Experimental assessment;Multiple applications;Performance evaluations;},
  __markedentry = {[Juliana:6]},
  abstract      = {Environments with frequent changes in application requirements demand an evolutionary approach for database modeling. The challenge is greater when the database must support multiple applications simultaneously. An existing solution for database evolution is refactoring with a transition period. During this period, both the old and the new database schemas coexist and data is replicated in a synchronous process. This solution brings several difficulties, such as interference with the operation of applications. To minimize these difficulties, in this paper we present an asynchronous approach to keep these schemas updated. This paper presents the design for an experimental assessment of this novel approach for evolutionary database development. &copy; 2011 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Artificial intelligence;Computer science;Computers;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-642-25106-1_29},
}

@Article{Ordonez2014,
  author        = {Ordonez, Carlos and Maabout, Sofian and Matusevich, David Sergio and Cabrera, Wellington},
  title         = {Extending ER models to capture database transformations to build data sets for data mining},
  journal       = {Data and Knowledge Engineering},
  year          = {2014},
  volume        = {89},
  pages         = {38 - 54},
  issn          = {0169023X},
  note          = {Data mining models;Database transformation;Denormalization;ER-models;Keywords;Public database;Relational Database;Transformation;},
  __markedentry = {[Juliana:6]},
  abstract      = {Abstract In a data mining project developed on a relational database, a significant effort is required to build a data set for analysis. The main reason is that, in general, the database has a collection of normalized tables that must be joined, aggregated and transformed in order to build the required data set. Such scenario results in many complex SQL queries that are written independently from each other, in a disorganized manner. Therefore, the database grows with many tables and views that are not present as entities in the ER model and similar SQL queries are written multiple times, creating problems in database evolution and software maintenance. In this paper, we classify potential database transformations, we extend an ER diagram with entities capturing database transformations and we introduce an algorithm which automates the creation of such extended ER model. We present a case study with a public database illustrating database transformations to build a data set to compute a typical data mining model. &copy; 2013 Elsevier B.V.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Classification (of information)},
  keywords      = {Agglomeration;Data mining;Metadata;Query languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/j.datak.2013.11.002},
}

@Article{Davidor1994,
  author        = {Davidor, Yuval and Manner, Reinhard and Schwefel, Hans-Paul},
  title         = {Preface},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {1994},
  volume        = {866 LNCS},
  pages         = {V - IX},
  issn          = {03029743},
  __markedentry = {[Juliana:6]},
  address       = {Jerusalem, Israel},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
}

@InProceedings{Alagi2004,
  author        = {Alagi, Suad and Briggs, David},
  title         = {Semantics of objectified XML constraints},
  year          = {2004},
  volume        = {2921},
  pages         = {147 - 165},
  address       = {Potsdam, Germany},
  note          = {Database integrity;Developed model;Featherweight Java;Functional object;Integrated paradigms;Model-theoretic;Object oriented;Regular expressions;},
  __markedentry = {[Juliana:6]},
  abstract      = {The core of a model theory for a functional object-oriented data model extended with XML-like types is presented. The object oriented component of this integrated paradigm is based on Featherweight Java and XML is represented by regular expression types. The main contributions are in extending both with general logic-based constraints and establishing results on schema and database evolution by inheritance that respects database integrity requirements. The paper shows that formally defined semantics of this integrated paradigm does indeed exist and in fact may be constructed in a model-theoretic fashion. The generality of the developed model theory and its relative independence of a particular logic basis makes it applicable to a variety of approaches to XML (as well as object-oriented) constraints. A pleasing property of this model theory is that it offers specific requirements for semantically acceptable evolution of these sophisticated schemas and their databases.<br/> &copy; Springer-Verlag Berlin Heidelberg 2004.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Object oriented programming},
  keywords      = {Computer circuits;Database systems;Semantics;XML;},
  language      = {English},
}

@InProceedings{Dominguez2010,
  author        = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
  title         = {Stones falling in water: When and how to restructure a view-based relational database},
  year          = {2010},
  volume        = {6295 LNCS},
  pages         = {559 - 562},
  note          = {Database modification;Database schemas;Database views;Relational Database;Restructuring process;Software engineering process;View-based;},
  __markedentry = {[Juliana:6]},
  abstract      = {Nowadays, one of the most important problems of software engineering continues to be the maintenance of both databases and applications. It is clear that any method that can reduce the impact that database modifications produce on application programs is valuable for software engineering processes. We have proposed such a method, by means of a database evolution architecture (MeDEA) that makes use of database views. By using views, changes in the structure of the database schema can be delayed until absolutely necessary. However, some conditions oblige modifications to be made. In this paper we present an approach to detect when the restructuring process must be realized and how to carry out this restructuring process. &copy; 2010 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Application programs},
  keywords      = {Artificial intelligence;Computer science;Computers;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-642-15576-5_45},
}

@InProceedings{Dominguez2010a,
  author        = {Dominguez, Eladio and Lloret, Jorge and Rubio, Angel L. and Zapata, Maria A.},
  title         = {Stones falling in water: When and how to restructure a view-based relational database (extended version)},
  year          = {2010},
  volume        = {639},
  pages         = {137 - 150},
  address       = {Novi Sad, Serbia},
  note          = {Database modification;Database schemas;Database views;Extended versions;Relational Database;Restructuring process;Software engineering process;View-based;},
  __markedentry = {[Juliana:6]},
  abstract      = {Nowadays, one of the most important problems of software engineering continues to be the maintenance of both databases and applications. It is clear that any method that can reduce the impact that database modifications produce on application programs is valuable for software engineering processes. We have proposed such a method, by means of a database evolution architecture (MeDEA) that makes use of database views. By using views, changes in the structure of the database schema can be delayed until absolutely necessary. However, some conditions oblige modifications to be made. In the present paper we present an approach to detect when the restructuring process must be realized and how to carry out this restructuring process.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {16130073},
  journal       = {CEUR Workshop Proceedings},
  key           = {Application programs},
  keywords      = {Information systems;Information use;},
  language      = {English},
}

@InProceedings{2011,
  title         = {18th World Congress on Intelligent Transport Systems and ITS America Annual Meeting 2011},
  year          = {2011},
  volume        = {7},
  pages         = {TransCore -},
  address       = {New York, NY, United states},
  __markedentry = {[Juliana:6]},
  abstract      = {This proceedings contains 80 papers. This forum serves as the focal point for dialogue and networking on the challenges and opportunities surrounding research and deployment of intelligent transportation systems (ITS). The papers published in this conference address a number of challenges and explore opportunities to achieve deployment of DSRC and related wireless communications-based intelligent transportation systems. Issues examined include technical risks such as ensuring interoperability and security; and institutional risks such as addressing liability and privacy issues, meeting stakeholder needs, and ensuring system sustainability. The key terms of this proceedings include AVI/DSRC interoperability, ITS costs database evolution, inferring its impacts, intelligent transportation systems (ITS), intelligent logistics systems, the future of 511, attention-attracting information, Tehran its strategic plan, XML-based its-protocol, social network analysis. &copy; 2011 by the Intelligent Transportation Society of America.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {18th World Congress on Intelligent Transport Systems and ITS America Annual Meeting 2011},
  language      = {English},
}

@InProceedings{Tran2005,
  author        = {Tran, Khoa Duc},
  title         = {Content-based retrieval using a multi-objective genetic algorithm},
  year          = {2005},
  pages         = {561 - 569},
  address       = {Ft. Lauderdale, United kingdom},
  note          = {Annotation;Image representation;Multimedia databases;Multimedia research;},
  __markedentry = {[Juliana:6]},
  abstract      = {Content-based retrieval from multimedia databases is an important multimedia research area where traditional keyword-based approaches are not adequate. Multimedia data is significantly different from alphanumeric data because multimedia data is generally meaningless to a human and multimedia objects are typically large. Moreover, the traditional keyword-based approaches require an enormous amount of human effort during manual annotation and maintaining the consistency of annotations throughout database evolution. Research on content-based retrieval focus on using low-level features like color and texture for image representation, and a geometric framework of distances in the feature space for similarity. However, systematic retrieval of the best matches in a large multimedia database requires exhaustive and exponential search and does not guarantee worst-case performance. In addition, it has been observed that certain image representation schemes perform better than others under certain query situations, and these schemes should be somehow integrated and adjusted on the fly to facilitate effective and efficient image retrieval. Some work has been done applying simple genetic algorithms for content-based retrieval to provide good, but not necessary optimal solutions. However, these simple genetic algorithms can find only one optimum solution in a single run. This research proposes a new content-based retrieval method based on a Multi-Objective Genetic Algorithm (MOGA), which is capable of finding multiple trade-off solutions in one run and providing a natural way for integrating multiple image representation schemes. This research focuses on structural similarity framework that addresses topological, directional and distance relations of image objects. &copy; 2005 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {07347502},
  journal       = {Conference Proceedings - IEEE SOUTHEASTCON},
  key           = {Content based retrieval},
  keywords      = {Data reduction;Database systems;Genetic algorithms;Image retrieval;Multimedia systems;Optimization;Topology;},
  language      = {English},
}

@Article{Vianu1987,
  author        = {Vianu, Victor},
  title         = {DYNAMIC FUNCTIONAL DEPENDENCIES AND DATABASE AGING.},
  journal       = {Journal of the ACM},
  year          = {1987},
  volume        = {34},
  number        = {1},
  pages         = {28 - 59},
  issn          = {00045411},
  note          = {DATABASE AGING;DYNAMIC CONSTRAINTS;FUNCTIONAL DEPENDENCIES;STATIC CONSTRAINTS;SURVIVABILITY;},
  __markedentry = {[Juliana:6]},
  abstract      = {A simple extension of the relational model is introduced to study the effects of dynamic constraints on database evolution. Both static and dynamic constraints are used in conjunction with the model. The static constraints considered here are functional dependencies (FDs). The dynamic constraints involve global updates and are restricted to certain analogs of FDs, called &Prime;dynamic&Prime; FDs. The results concern the effect of the dynamic constraints on the static constraints satisfied by the database in the course of time. The effect of the past history of the database on the static constraints is investigated using the notions of age and age closure. The connection between the static constraints and the potential future evolution of the database is briefly discussed using the notions of survivability and survivability closure.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {DATABASE SYSTEMS},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/7531.7918},
}

@InProceedings{Nakamura2002,
  author        = {Nakamura, Mutsumi and Elmasri, Ramez},
  title         = {Using Smodels (declarative logic programming) to verify correctness of certain active rules},
  year          = {2002},
  pages         = {270 -},
  address       = {San Jose, CA, United states},
  note          = {Declarative logic programming (DLP);Event-condition-action (ECA) rules;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper we show that the language of declarative logic programming (DLP) with answer sets and its extensions can be used to specify database evolution due to updates and active rules, and to verify correctness of active rules with respect to a specification described using temporal logic and aggregate operators. We classify the specification of active rules into four kind of constraints which can be expressed using a particular extension of DLP called Smodels. Smodels allows us to specify the evolution, to specify the constraints, and to enumerate all possible initial database states and initial updates. Together, these can be used to analyze all possible evolution paths of an active database system to verify if they satisfy a set of given constraints.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - International Conference on Data Engineering},
  key           = {Logic programming},
  keywords      = {Constraint theory;Formal logic;Query languages;Semantics;Software prototyping;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/ICDE.2002.994724},
}

@InProceedings{Gentleman1989,
  author        = {Gentleman, W.Morven and MacKay, Stephen A. and Stewart, Darlene A. and Wein, Marceli},
  title         = {Commercial realtime software needs different configuration management},
  year          = {1989},
  pages         = {152 - 161},
  address       = {Princeton, NJ, USA},
  note          = {Configuration Management;Real Time Software;Software Configuration Management;},
  __markedentry = {[Juliana:6]},
  abstract      = {Arguments are presented as to why integrated, monolithic configuration management is not well suited to commercial realtime systems. An alternative approach to configuration management that over several years we have found to be effective and widely useable is described. This approach, Database and Selectors Cel (DaSC), separates treatment of versions that exist simultaneously from the evolution of those versions over time. Versions that exist simultaneously are represented by selectors from a common database. Evolution is represented by layers, as in the film animators cel.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Computer Software},
  keywords      = {Computer Programming;},
  language      = {English},
}

@InProceedings{Amo2002,
  author        = {de Amo, Sandra and Carnielli, Walter A. and Marcos, Joao},
  title         = {A logical framework for integrating inconsistent information in multiple databases},
  year          = {2002},
  volume        = {2284},
  pages         = {67 - 84},
  address       = {Salzau Castle, Germany},
  note          = {Inconsistent database;Inconsistent information;Integrated database;Integration process;Integrity constraints;Logical frameworks;Paraconsistent logic;Sound and complete;},
  __markedentry = {[Juliana:6]},
  abstract      = {When integrating data coming from multiple different sources we are faced with the possibility of inconsistency in databases. In this paper, we use one of the paraconsistent logics introduced in [9,7] (LFI1) as a logical framework to model possibly inconsistent database instances obtained by integrating different sources.We propose a method based on the sound and complete tableau proof system of LFI1 to treat both the integration process and the evolution of the integrated database submitted to users updates. In order to treat the integrated database evolution, we introduce a kind of generalized database context, the evolutionary databases, which are databases having the capability of storing and manipulating inconsistent information and, at the same time, allowing integrity constraints to change in time. We argue that our approach is sufficiently general and can be applied in most circumstances where inconsistency may arise in databases.<br/> &copy; Springer-Verlag Berlin Heidelberg 2002.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Artificial intelligence;Computer science;Computers;},
  language      = {English},
}

@InProceedings{1985,
  title         = {Proceedings of the 4th Annual International Conference on Systems Documentation, SIGDOC 1985},
  year          = {1985},
  pages         = {ACM Special Interest Group for Design of Communications (SIGDOC) -},
  address       = {Ithaca, NY, United states},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 26 papers. The topics discussed include: separating content from form: a language for formatting on-line documentation and dialog; computer user manuals in print: do they have a future?; ADL - a documentation language; software maintenance documentation; document generation from a PSA database; evolution of program documentation through a long-term project; basic and dos jobstreams in IBM PC software documentation; designing computer documentation that will be used: understanding computer user attitudes; better quality through better indexing; and documentations recognition problem: what can we do about it.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the 4th Annual International Conference on Systems Documentation, SIGDOC 1985},
  language      = {English},
}

@InProceedings{Rashid2001,
  author        = {Rashid, Awais},
  title         = {A hybrid approach to separation of concerns: The story of SADES},
  year          = {2001},
  volume        = {2192},
  pages         = {231 - 249},
  address       = {Kyoto, Japan},
  note          = {Composition filters;Cross-cutting concerns;Extensible objects;Hybrid approach;Separation of concerns;},
  __markedentry = {[Juliana:6]},
  abstract      = {A number of approaches have been proposed to achieve separation of concerns. Although all these approaches form suitable candidates for separating cross-cutting concerns in a system, one approach can be more suitable for implementing certain types of concerns as compared to the others. This paper proposes a hybrid approach to separation of concerns. The approach is based on using the most suitable approach for implementing each cross-cutting concern in a system. The discussion is based on using three different approaches: composition filters, adaptive programming and aspect-oriented programming to implement cross-cutting concerns in SADES, a customisable and extensible object database evolution system.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Aspect oriented programming},
  keywords      = {Object oriented programming;Object-oriented databases;},
  language      = {English},
}

@InProceedings{Baral1994,
  author        = {Baral, Chitta},
  title         = {Rule based updates on simple knowledge bases},
  year          = {1994},
  volume        = {1},
  pages         = {136 - 141},
  address       = {Seattle, WA, USA},
  note          = {Database evolutions;Ground atoms;Logic program;Rule based updates;Situation calculus;Update operators;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper we consider updates that are specified as rules and consider simple knowledge bases consisting of ground atoms. We present a translation of the rule based update specifications to extended logic programs using situation calculus notation so as to compute the updated knowledge base. We show that the updated knowledge base that we compute satisfies the update specifications and yet is minimally different from the original database. We then expand our approach to incomplete knowledge bases. We relate our approach to the standard revision and update operators, the formalization of actions and its effects using situation calculus and the formalization of database evolution using situation calculus.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the National Conference on Artificial Intelligence},
  key           = {Knowledge based systems},
  keywords      = {Artificial intelligence;Computational methods;Computer software;Database systems;Logic programming;},
  language      = {English},
}

@InProceedings{Ahman2002,
  title         = {Robotics and computer-integrated manufacturing},
  year          = {2002},
  editor        = {Ahman, M M;Sullivan, W G;},
  volume        = {18},
  number        = {3-4},
  address       = {Dublin, Ireland},
  note          = {Automatic object tracking;EiRev;Equipment design;Key performance indicators (KPI);Learning vector quantization networks (LVQ);Optical sensing systems;Parallel machine scheduling;Placement time estimator function;Variation propagation analysis;},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contains 19 papers from the conference on Robotics and Computer-Integrated Manufacturing. The topics discussed include: flexible automation and intelligent manufacturing 2001; impact of human error on lumber yield in rough mills; the series-parallel replacement problem; equipment replacement decisions and lean manufacturing; a rule based expert system for rapid prototyping system selection; a feature-based database evolution approach in the design process; a task based 'design for all' support tool; and the impact of sequence changes on product lead time.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {07365845},
  journal       = {Robotics and Computer-Integrated Manufacturing},
  key           = {Computer integrated manufacturing},
  keywords      = {Computer vision;Industrial engineering;Integer programming;Machine design;Multilayer neural networks;Pattern recognition;Performance;Problem solving;Process control;Robotics;Scheduling;Self organizing maps;Signal processing;Topology;Tracking (position);Vector quantization;},
  language      = {English},
}

@InProceedings{1998a,
  title         = {Workshops on Object-Oriented Technology, ECOOP 1998},
  year          = {1998},
  volume        = {1543},
  pages         = {1 - 563},
  address       = {Brussels, Belgium},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 190 papers. The special focus in this conference is on The 8th Workshop for PhD Students in Object-Oriented Systems and Techniques, Tools and Formalisms for Capturing and Assessing the Architectural Quality in Object-Oriented Software. The topics include: Framework design and documentation; reengineering with the CORBA meta object facility; enforcing effective hard real-time constraints in object-oriented control systems; online-monitoring in distributed object-oriented client/server environments; a test bench for software; intermodular slicing of object-oriented programs; validation of real-time object oriented applications; parallel programs implementing abstract data type operations; a dynamic logic model for the formal foundation of object-oriented analysis and design; a refinement approach to object-oriented component reuse; a compositional approach to concurrent object systems; component-based architectures to generate software components from OO conceptual models; adding database functionality to an object-oriented development environment; run-time reusability in object-oriented schematic capture; a semi-autonomous database evolution system; framework design for optimization; object-oriented control systems on standard hardware; design of an object-oriented scientific simulation and visualization system; testing components using protocols; virtual types, propagating and dynamic inheritance, and coarse grained structural equivalence; on polymorphic type systems for imperative programming languages; formal methods for component-based systems; compilation of source code into object-oriented patterns; integration of object-based knowledge representation in a reflexive object-oriented language; implementing layered object-oriented designs; an evaluation of the benefits of object oriented methods in software development processes and process measuring, modeling, and understanding.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{1992,
  title         = {11th International Conference on Entity-Relationship Approach, ER 1992},
  year          = {1992},
  volume        = {645 LNCS},
  pages         = {1 - 439},
  address       = {Karlsruhe, Germany},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 26 papers. The special focus in this conference is on Entity-Relationship Approach. The topics include: ER vs. OO; the next ten years of modeling, methodologies, and tools; fundamentals of cardinality constraints; evaluation of complex cardinality constraints; local referential integrity; entity tree clustering &mdash; a method for simplifying ER designs; a temporal statistical model for entity-relationship schemas; semantic similarity relations in schema integration; classifying and reusing conceptual schemas; embedding data modelling in a general architecture for integrated information systems; the use of a lexicon to interpret ER diagrams; an entity-relationship-based methodology for distributed database design: an integrated approach towards combined logical and distribution designs; a specification-based data model; data dictionary design; nesting quantification in a visual data manipulation language; an EER prototyping environment and its implementation in a datalog language; a theory for entity-relationship view updates; transforming conceptual data models into an object model; meta object management and its application to database evolution; modelling of audio/video data; SUPER &mdash; visual interaction with an object-based ER model; natural language restatement of queries expressed in a graphical language; a temporal query language based on conceptual entities and roles; on the design of object-oriented databases and unified class evolution by object-oriented views.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{2003,
  title         = {21st International Conference on Conceptual Modeling, ER 2002 held in conjunction with 2nd International Workshop on Evolution and Change in Data Management, ECDM 2002, ER/IFIP 8.1 Workshop on Conceptual Modelling Approaches to Mobile Information Systems Development, MobIMod 2002, International Workshop on Conceptual Modeling Quality, IWCMQ 2002 and 3rd International Joint Workshop on Conceptual Modeling Approaches for E-business: a Web Service Perspective, eCOMO 2002},
  year          = {2003},
  volume        = {2784},
  pages         = {1 - 451},
  address       = {Tampere, Finland},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 38 papers. The special focus in this conference is on Management of Time and Changes in Information Systems. The topics include: Change management for a temporal versioned object-oriented database; towards temporal information in workflow systems; preserving and querying histories of XML-published relational databases; a lightweight XML constraint check and update framework; change discovery in ontology-based knowledge management systems; an architecture for managing database evolution; reifying design patterns to facilitate systems evolution; managing configuration with evolving constraints in design databases; characterization and preservation; an active approach to model management for evolving information systems; research challenges on the conceptual and logical level; a cooperation model for personalised and situation dependent services in mobile networks; sequence diagrams for mobility; a pattern-based approach to mobile information systems conceptual architecture; support of integrated wireless web access through media types; domain-specific modelling for cross-platform product families; dynamic database generation for mobile applications; basic user requirements for mobile work support systems; accounting and billing of wireless internet services in the third generation networks and theoretical and practical issues in evaluating the quality of conceptual models.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Wood1984,
  author        = {Wood, Dana C.},
  title         = {DIGIMAP - THE EVOLUTION OF A TOTAL DIGITAL MAPPING SYSTEM.},
  year          = {1984},
  pages         = {259 - 269},
  address       = {San Antonio, TX, USA},
  note          = {AUTOMATIC MAP ENHANCEMENT PROGRAM;DIGITAL MAPPING SYSTEM (DIGIMAP);DIGITAL TERRAIN MODELING;},
  __markedentry = {[Juliana:6]},
  abstract      = {It was apparent to Bohannan-Huston Incorporated (BHI) that conventional mapping techniques were a vicious cycle that kept a lot of people employed but provided a rather untimely product to clients. BHI, an engineering and mapping company founded in 1959, had been developing computer graphics applications for its own use for most of its existence and decided that the next major mapping project going out the door would be produced digitally, by whatever means available. The paper discusses DIGIMAP history, DIGIMAP structure, DIGIMAP database evolution, and DIGIMAP-additional functionality.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {MAPS AND MAPPING},
  keywords      = {COMPUTER GRAPHICS;DATABASE SYSTEMS;},
  language      = {English},
}

@InProceedings{1997,
  title         = {6th International Conference on Database Theory ICDT 1997},
  year          = {1997},
  volume        = {1186},
  pages         = {1 - 476},
  address       = {Delphi, Greece},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain  32 papers. The special focus in this conference is on Invited Papers and Tutorial. The topics include:  Querying semi-structured data; methods and problems in data mining; conjunctive query containment revisited; semantics and containment of queries with internal and external conjunctions; efficient complete local tests for conjunctive query constraints with negation; selection of views to materialize in a data warehouse; total and partial well-founded datalog coincide; fine hierarchies of generic computation; local properties of query languages; expressiveness and complexity of active databases; a model theoretic approach to update rule programs; abstract interpretation of active rules and its use in termination analysis; structural issues in active rule systems; discovering all most specific sentences by randomized algorithms; a formal foundation for distributed workflow execution based on state charts; incorporating user preferences in multimedia queries; queries and computation on the web; the complexity of iterated belief revision; expressive power of unary counters; concurrency control theory for deferred materialized views; serializability of nested transactions in multidatabases; adding structure to unstructured data; correspondence and translation for heterogeneous data; type-consistency problems for queries in object-oriented databases; object-oriented database evolution; performance of nearest neighbor queries in R-trees; optimal allocation of two-dimensional data; efficient indexing for constraint and temporal databases; on topological elementary equivalence of spatial databases; model-theoretic minimal change operators for constraint databases and tractable iteration mechanisms for bag languages.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Benazet1995,
  author        = {Benazet, Emmanuel and Guehl, Herve and Bouzeghoub, Mokrane},
  title         = {VITAL: A visual tool for analysis of rules behaviour in active databases},
  year          = {1995},
  volume        = {985},
  pages         = {182 - 196},
  address       = {Glyfada, ATH, Greece},
  note          = {Active database;Complex problems;Coupling mode;Design process;Graphical interface;Statistical information;Theoretical approach;Visual tools;},
  __markedentry = {[Juliana:6]},
  abstract      = {Although active rules are used more and more in different IS applications, designing a coherent set of active rules is not a trivial task. Thus, it is important to provide tools to help the designer in the definition of a correct set of active rules. In this paper, we propose a toolbox which assists the designer in defining, tracing, debugging and understanding the behaviour of a set of active rules. This set of facilities is packaged in a toolbox in order to be used both during the design process, independently of any rule processor, and after the compiling process, depending on a specific rule processor. The former corresponds to a logical validation while the later corresponds to an effective validation with respect to the features of a specific DBMS (e.g. Coupling modes, event interception). The toolbox includes various tools such as (i) a static analyser for a set of rules, which portrays the activation graph and its possible cycles, (ii) the step by step simulator of rule execution, (iii) a graphical interface with navigation and browsing facilities, (iv) and statistical information on the database evolution. This set of tools can be considered as a pragmatic approach to the complex problem of termination and confluence, to which the theoretical approaches have not yet provided an acceptable solution.<br/> &copy; Springer-Verlag Berlin Heidelberg 1995.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Integrated circuit design;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-60365-4_127},
}

@Article{Kamel1993,
  author        = {Kamel, Nabil},
  title         = {Page-queries as a tool for organizing secondary memory auxiliary databases. II. Optimal selection of SADB contents},
  journal       = {Information sciences},
  year          = {1993},
  volume        = {69},
  number        = {1-2},
  pages         = {127 - 156},
  issn          = {00200255},
  note          = {Page queries;Secondary memory auxiliary databases;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper, a specific aspect of a new approach to organizing secondary memory redundancy in centralized databases is investigated. The approach is based on optimal management of a special type of redundant temporaries, collectively called the auxiliary database. The specific aspect investigated is the problem of optimal selection of the contents of the auxiliary database. The new approach is based on exploiting the irregularities in both the distribution of data values in the database and in its access patterns. In order to exploit the nonuniformity in the distribution of both queries and data, special atomic data constructs, called page-queries are used as the smallest indivisible information units in the auxiliary database. A page-query is roughly defined as the result of one query when it is processed against only one memory page. It is shown that use of these atomic data packets as the only building blocks in secondary memory auxiliary databases allows us to realize optimization potential that could otherwise not be achieved. The problem of maintaining the auxiliary database in response to database evolution is approached in a self-adaptive fashion by periodically reorganizing its contents to suit the common usage and data distribution patterns. A mathematical programming formulation of the problem is given for which a fast solution algorithm is known. The approach taken to handle updates is to keep the main database always up to date and to adaptively select the contents of the temporaries based on their update history. Measurements obtained from a real implementation are presented and provide preliminary verification of the proposed approach.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Query languages},
  keywords      = {Computer programming;Data processing;Data storage equipment;Database systems;Information science;Selection;},
  language      = {English},
}

@InProceedings{1983,
  title         = {PROCEEDINGS OF THE SECOND ACM SIGACT-SIGMOD SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS.},
  year          = {1983},
  pages         = {ACM, Special Interest Group for Automata \& Computability Theo; ACM, Special Interest Group for the Management of Data, New York, -},
  address       = {Atlanta, GA, USA},
  note          = {DATABASE MANAGEMENT SYSTEMS;EIREV;FUNCTIONAL DEPENDENCIES;HIERARCHICAL DATABASE MODEL;INCLUSION DEPENDENCIES;RELATIONAL DATABASE MODEL;},
  __markedentry = {[Juliana:6]},
  abstract      = {This proceedings volume contains 38 papers. Topics presented include distributed consensus, faulty processes, recovery algorithms, last process, failing, optimal termination protocols, network partitioning, view integration, inclusion dependencies, universal instances, inferences, functional dependencies, robustness, storage reduction, binary search trees, interpolation, indexes, storage mappings, multidimensional linear dynamic hashing, extendable hashing, distributed data, text searching, queries, quantifiers, attributed grammars, relational queries, granularity hierarchies, concurrency control, resilient nested transactions, decomposition, distributed algorithms, exclusive locking, log write-ahead protocols, IMS/vs logging, reliability, file allocation, networks, GYO reductions, canonical connections, tree and cyclic schemas, tree projections, schemes, acyclic database hypergraphs, inverting relational expressions, information, relational views, sort sets, intersection anomalies, semantics, updates, path expressions, attributes, disaggregations, dynamic constraints, database evolution, and algebraic aspects of relational database decomposition.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {DATABASE SYSTEMS},
  keywords      = {COMPUTER METATHEORY - Formal Logic;COMPUTER PROGRAMMING - Algorithms;COMPUTER SYSTEMS PROGRAMMING - Multiprocessing Programs;COMPUTER SYSTEMS, DIGITAL - Distributed;DATA PROCESSING - Data Handling;},
  language      = {English},
}

@Article{Balsters2001,
  author        = {Balsters, Herman and De Brock, Bert and Conrad, Stefan},
  title         = {Preface},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2001},
  volume        = {2065},
  pages         = {V -},
  issn          = {03029743},
  __markedentry = {[Juliana:6]},
  address       = {Dagstuhl Castle, Germany},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6},
}

@InProceedings{Gelbard2001,
  author        = {Gelbard, Roy and Gilmour, Asher},
  title         = {Temporal branching as a conflict management technique},
  year          = {2001},
  volume        = {2065},
  pages         = {100 - 114},
  address       = {Dagstuhl Castle, Germany},
  note          = {Conflict management;Data objects;Distributed database;External database;Integrated database;Log file;Temporal approach;Temporal Database;},
  __markedentry = {[Juliana:6]},
  abstract      = {The current research attempts to model a technique for man-aging conflicts within an integrated database. An integrated database being a database whose schema is an integration of several external database schemas, independent of each other. The independence of the systems allows conflicting values to be entered into the same data ob-jects. For example, one system may hold the value of female for a specific data object where as another may hold male. The conflict is only discov-ered when the data objects are brought to the integrated database, and then there is need to resolve the conflict. A technique to manage conflicts is developed based on version manage-ment, used in temporal databases, and the log-file approach used in more conventional technologies. The model combines temporal database tools with distributed database management tools. Thus it obtains greater flexibility than existing replication and log-file techniques and is more economic in record volume than the temporal approach.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Modeling languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_6},
}

@InProceedings{McFadyen2001,
  author        = {McFadyen, Ron and Chan, Fung-Yee},
  title         = {Qfd matrix for incremental construction of a warehouse via data marts},
  year          = {2001},
  volume        = {2065},
  pages         = {133 - 141},
  address       = {Dagstuhl Castle, Germany},
  note          = {Data mart;Data store;Evolutionary process;Incremental construction;Planning stages;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper we consider the construction of a dimensional data warehouse. The warehouse is built beginning with the first data mart and proceeding in an iterative manner constructing one mart at a time. In this way the warehouse is seen to evolve over time. This evolutionary process is necessary due to the complexity of data stores, relationships, transformations, and the processing involved. In this paper we consider the problem of identifying the next data mart to construct and present a tool based on Quality Function Deployment for use in the planning stages.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Modeling languages},
  keywords      = {Data warehouses;Quality function deployment;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_8},
}

@InProceedings{Jensen2001,
  author        = {Jensen, Ole G. and Bohlen, Michael H.},
  title         = {Evolving relations},
  year          = {2001},
  volume        = {2065},
  pages         = {115 - 132},
  address       = {Dagstuhl Castle, Germany},
  note          = {Attribute mappings;Conceptual schemas;Null value;Schema changes;},
  __markedentry = {[Juliana:6]},
  abstract      = {This paper presents a framework for evolving relation schemas that is based on conditional schema changes and tuple ver-sioning. With each tuple a recorded schema and a conceptual schema is associated. This allows for a simple and semantically clean solution to the problem of schema mismatches that arise when the schema of a database is changed and some data no longer fits the schema. Specifically, no data needs to be migrated to the new schema, and no special null values are required. We precisely define evolving schemas in terms of schema segments and corresponding attribute mappings, present an algo-rithm to compute answers to queries over evolving schemas, and prove that the query answers consider the maximal set of schema segments consistent with the evolving schema.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Modeling languages},
  keywords      = {Query processing;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_7},
}

@InProceedings{Su2001,
  author        = {Su, Hong and Claypool, Kajal T. and Rundensteiner, Elke A.},
  title         = {Extending the object query language for transparent metadata access},
  year          = {2001},
  volume        = {2065},
  pages         = {182 - 201},
  address       = {Dagstuhl Castle, Germany},
  note          = {Application developers;Database development;De facto standard;Implementation strategies;Object query languages;Query optimization strategies;Schema management;Translation strategies;},
  __markedentry = {[Juliana:6]},
  abstract      = {Many applications in Object Databases (ODB), for example, schema management tools, CASE tools, database development tools and integration wrappers, need extensive queries over both application data as well as metadata. However queries over metadata via OQL, a de-facto standard for object query languages defined for the ODMG 2.0 Object Model, are tied to low-level implementation details of the under-lying schema repository of the database system. Hence, they are neither portable nor easily usable, requiring the application developer to have detailed knowledge of the proprietary structure of the schema repository. In this paper, we propose an extension of OQL, called MetaOQL, to address this limitation. Our proposition of MetaOQL offers several benefits: (1) it is a natural extension of OQL in terms of both its syntax and semantics; (2) it removes the dependency of metadata queries on the particular schema repository, hence providing uniformity and portability of metadata queries across different ODBs; (3) it supports trans-parent navigation over the metadata thus offering ease of use; (4) unlike OQL, it hides metadata querying details from the users hence the queries can be simplified and more easy to read and understand. We have also investigated implementation strategies for MetaOQL. In particular, we propose a translation strategy from MetaOQL to OQL as a preferable solution compared to development of a special-purpose MetaOQL processor. The translation strategy offers the advantage that the MetaOQL queries can be retargeted to work on top of any existing ODB engine equipped with OQL with minimal effort. Furthermore, all OQL query optimization strategies can thus still be brought to bear in our extended system.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Query languages},
  keywords      = {Information management;Metadata;Modeling languages;Object-oriented databases;Query processing;Semantics;Translation (languages);},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_11},
}

@InProceedings{Terrasse2001,
  author        = {Terrasse, Marie-Noelle},
  title         = {A metamodeling approach to evolution},
  year          = {2001},
  volume        = {2065},
  pages         = {202 - 219},
  address       = {Dagstuhl Castle, Germany},
  note          = {Evolution conflicts;Meta model;Metamodeling;Multi-view models;},
  __markedentry = {[Juliana:6]},
  abstract      = {With the increasing complexity of systems being modeled, analysis &amp; design move towards more and more abstract methodologies. Most of them rely on metamodeling tools that employ multi-view models and the four-layer metamodeling architecture. Our idea is to use the metamodeling approach to classify and to constraint the possible evolutions of an information system with the effect to improve both detection of evolution conflicts and disciplined reuse. Within the domain of UML metamodeling, a refinement of the metamodel-level classification is proposed that includes bases for defining a metric of the evolution (in terms of distance between metamodels).<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Classification (of information)},
  keywords      = {Modeling languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_12},
}

@InProceedings{Aoumeur2001,
  author        = {Aoumeur, Nasreddine and Saake, Gunter},
  title         = {Consistency management in runtime evolving concurrent information systems: A co-nets-based approach},
  year          = {2001},
  volume        = {2065},
  pages         = {33 - 60},
  address       = {Dagstuhl Castle, Germany},
  note          = {Consistency management;Dynamic evolution;Inference rules;Integrity constraints;Meta levels;New forms;Real-world information;Runtimes;},
  __markedentry = {[Juliana:6]},
  abstract      = {For adequately specifying and rapid-prototyping concurrent information systems, we proposed in [AS99] a new form of object ori-ented (OO) Petri nets. Referred to as Co-nets, this approach allows in particular to conceive such systems as complex autonomous yet cooperat-ing components. Moreover, for coping with intrinsic dynamic evolution in such systems, we have straightforwardly extended this proposal by introducing notions of meta-places, non-instantiated transitions and a two-step evaluated inference rule [Aou00]. The purpose of this paper is to tackle with another crucial dimension characterizing real-world information systems, namely static and dy-namic integrity constraints. For this aim, we propose to associate with each component a constraints' class. To enforce such constraints, we propose an appropriate synchronization' inference rule that semantically relates constraints' transitions with intrinsically dependent ones in the associated component. For a more flexible consistency management we enrich this first proposal by an adequate meta-level, where constraints may be dynamically created, modified or deleted. Finally, we show how this proposal covers a large number of constraint subclasses, including life-cycle based constraints and constraints based on complex derived information as view classes.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Information management},
  keywords      = {Information systems;Information use;Life cycle;Modeling languages;Petri nets;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_2},
}

@InProceedings{Balko2001,
  author        = {Balko, Soren},
  title         = {Adaptive specifications of technical information systems},
  year          = {2001},
  volume        = {2065},
  pages         = {61 - 67},
  address       = {Dagstuhl Castle, Germany},
  note          = {Development process;External influences;Formal Description;Formal specification techniques;Post-implementation;Software implementation;Specification technique;Technical information systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {Nowadays formal specification techniques have become pop-ular in the development process of many kinds of software systems. Since many technical scenarios are based on computer control, their software implementations also depend on previously established formal descrip-tions. Additionally, technical information systems such as production control facilities often have a long life span. Due to this fact, dynamic changes become increasingly likely. For example, these changes may be induced by introduction of new laws, altered production goals, human interactions, or any other kind of external influences. However, these changes sometimes require an alteration of the software. To fit the im-plementation, its formal specification (if present) has to be adapted appropriately. Ordinary specification methods do not permit a post-implementation change in the specification itself but rather an afresh specification effort throwing away the current formal description. Since the necessary changes would frequently result in minor adaptations in the specification, this situation is very unsatisfactory. To avoid or reduce this re-specification effort, we are working on extensions of established specification techniques which can cover adaptive specifications. The remainder of this paper is organized as follows. Section 1 introduces our project and presents a simple classification of adaptive specifications. In Section 2 we briefly present the main issues of the case study and mo-tivate adaptive specifications. Subsequently we suggest some syntactical extensions of Troll in Section 3. And finally, Section 4 gives an outlook on future work in this project.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Formal specification},
  keywords      = {Information systems;Information use;Modeling languages;Production control;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_3},
}

@InProceedings{Saake2001,
  author        = {Saake, Gunter and Turker, Can and Conrad, Stefan},
  title         = {Evolving objects: Conceptual description of adaptive information systems},
  year          = {2001},
  volume        = {2065},
  pages         = {163 - 181},
  address       = {Dagstuhl Castle, Germany},
  note          = {Adaptive information systems;Basic building block;Behavior evolution;Conceptual levels;Evolving objects;Large organizations;Object specifications;Specification frameworks;},
  __markedentry = {[Juliana:6]},
  abstract      = {Today, information systems are essential parts of large organizations. Since such kinds of systems have a very long life-span, they have to be adapted to new changing requirements occurring during their lifetime. Evolution must be regarded not only at the object state level, but also at the object behavior level. Especially, the explicit handling of (behavior) evolution on the conceptual level is necessary. For that, we introduce the notion of evolving objects as basic building blocks of information systems. The behavior of such an object is divided into a rigid and an evolving part. The rigid behavior is ideally stable for the whole life-span of the object; the evolving behavior can be changed dynamically at runtime. In this paper, we present an extended specification framework for modeling evolving objects. Particularly, this framework provides the basis to explicitly specify behavior evolution.<br/> &copy; Springer-Verlag Berlin Heidelberg 2001.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Modeling languages},
  keywords      = {Information systems;Information use;Specifications;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-48196-6_10},
}

@Article{Lu2007,
  author        = {Lu, Jianguo and Wang, Ju and Wang, Shengrui},
  title         = {XML schema matching},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2007},
  volume        = {17},
  number        = {5},
  pages         = {575 - 597},
  issn          = {02181940},
  note          = {Data integration;Schema matching;Software component search;Tree matching algorithm;},
  __markedentry = {[Juliana:6]},
  abstract      = {XML Schema matching problem can be formulated as follows: given two XML Schemas, find the best mapping between the elements and attributes of the schemas, and the overall similarity between them. XML Schema matching is an important problem in data integration, schema evolution, and software reuse. This paper describes a matching system that can find accurate matches and scales to large XML Schemas with hundreds of nodes. In our system, XML Schemas are modeled as labeled and unordered trees, and the schema matching problem is turned into a tree matching problem. We proposed Approximate Common Structures in trees, and developed a tree matching algorithm based on this concept. Compared with the traditional tree edit-distance algorithm and other schema matching systems, our algorithm is faster and more suitable for large XML Schema matching. &copy; World Scientific Publishing Company.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {XML},
  keywords      = {Algorithms;Computer software reusability;Data acquisition;Problem solving;Trees (mathematics);},
  language      = {English},
  url           = {http://dx.doi.org/10.1142/S0218194007003446},
}

@InProceedings{2009,
  title         = {Advances in Conceptual Modeling - Challenging Perspectives - ER 2009 Workshops CoMoL, ETheCoM, FP-UML, MOST-ONISW, QoIS, RIGiM, SeCoGIS, Proceedings},
  year          = {2009},
  volume        = {5833 LNCS},
  address       = {Gramado, Brazil},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 34 papers. The topics discussed include: towards a theory of conceptual modelling; assessing modal aspects of OntoUML conceptual models in alloy; first-order types and redundant relations in relational databases; on matrix representations of participation constraints; toward formal semantics for data and schema evolution in data stream management systems; applying AUML and UML 2 in the multi-agent systems project; a collaborative support approach on UML sequence diagrams for aspect-oriented software; applying a UML extension to build use cases diagrams in a secure mobile grid application; XMI2USE: a tool for transforming XMI to USE specifications; analysis procedure for validation of domain class diagrams based on ontological analysis; ontology for imagistic domains: combining textual and pictorial primitives; and using a foundational ontology for reengineering a software enterprise ontology.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Coox2003,
  author        = {Coox, S.V.},
  title         = {Axiomatization of the evolution of XML database schema},
  journal       = {Programming and Computer Software},
  year          = {2003},
  volume        = {29},
  number        = {3},
  pages         = {140 - 146},
  issn          = {03617688},
  note          = {Axiomatic models;Database schemas;Model relationships;},
  __markedentry = {[Juliana:6]},
  abstract      = {Database schema consists of constructs that model relationships between its entities. Changes made to the schema with time are called the schema evolution. An axiomatic model of the XML database schema is suggested that automatically maintains its integrity when basic changes are made to the schema. A classification of changes is described. &copy; 2003 MAIK "Nauka/ Interperiodica".},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {XML},
  keywords      = {Automation;Database systems;Mathematical models;},
  language      = {English},
  url           = {http://dx.doi.org/10.1023/A:1023801022137},
}

@InProceedings{1994,
  title         = {2nd International Symposium on Object-Oriented Methodologies and Systems, ISOOMS 1994},
  year          = {1994},
  volume        = {858 LNCS},
  pages         = {1 - 386},
  address       = {Palermo, Italy},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 25 papers. The special focus in this conference is on Object-Oriented Methodologies and Systems. The topics include: Issues in extending a relational system with object-oriented features; a SQL-like query calculus for object-oriented database systems; schema evolution for object-based accounting database systems; designing and implementing inter-client communication in the O2 object-oriented database management system; rigorous object-oriented analysis; quantitative and qualitative aspects of object-oriented software development; towards a general purpose approach to object-oriented analysis; a seamless model for object-oriented systems development; structural and behavioural views on OMT-classes; combining two approaches to object-oriented analysis; a rewriting technique for implementing active object systems; beyond coupling modes; an ambiguity resolution algorithm; integrating objects with constraint-programming languages; object-oriented representation of shape information; the development of an object-oriented multimedia information system; an intelligent information system for heterogeneous data exploration; effective optimistic concurrency control in multiversion object bases; reusing object oriented design; reuse in object-oriented information systems development; an object-oriented approach to the integration of online services into office automation environments and a linguistic approach to the development of object oriented systems using the NL system LOLITA.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Mason2005,
  author        = {Mason, Terrence and Lawrence, Ramon},
  title         = {Dynamic database integration in a JDBC driver},
  year          = {2005},
  pages         = {326 - 333},
  note          = {Annotation;Conceptual;Embedded;Evolution;JDBC;Schema;},
  __markedentry = {[Juliana:6]},
  abstract      = {Current integration techniques are unsuitable for large-scale integrations involving numerous heterogeneous data sources. Existing methods either require the user to know the semantics of all data sources or they impose a static global view that is not tolerant of schema evolution. These assumptions are not valid in many environments. We present a different approach to integration based on annotation. The contribution is the elimination of the bottleneck of global view construction by moving the complicated task of identifying semantics to local annotators instead of global integrators. This allows the integration to be more automated, scaleable, and rapidly deployable. The algorithms are packaged in an embedded database engine contained in a JDBC driver capable of dynamically integrating data sources. Experimental results demonstrate that the Unity JDBC driver efficiently integrates data located in separate data sources with minimal overhead.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {ICEIS 2005 - Proceedings of the 7th International Conference on Enterprise Information Systems},
  key           = {Data integration},
  keywords      = {Database systems;Dynamics;Information systems;Integration;Metadata;Semantics;},
  language      = {English},
}

@InProceedings{Cunha2006,
  author        = {Cunha, Alcino and Oliveira, Jose Nuno and Visser, Joost},
  title         = {Type-safe two-level data transformation},
  year          = {2006},
  volume        = {4085 LNCS},
  pages         = {284 - 299},
  address       = {Hamilton, Canada},
  note          = {Coupled transformation;Data mappings;Format evolution;Generalized abstract datatypes;Generic programming;Program calculation;Refinement calculus;Strategic term rewriting;Two-level transformation;},
  __markedentry = {[Juliana:6]},
  abstract      = {A two-level data transformation consists of a type-level transformation of a data format coupled with value-level transformations of data instances corresponding to that format. Examples of two-level data transformations include XML schema evolution coupled with document migration, and data mappings used for interoperability and persistence. We provide a formal treatment of two-level data transformations that is typesafe in the sense that the well-formedness of the value-level transformations with respect to the type-level transformation is guarded by a strong type system. We rely on various techniques for generic functional programming to implement the formalization in Haskell. The formalization addresses various two-level transformation scenarios, covering fully automated as well as user-driven transformations, and allowing transformations that are information-preserving or not. In each case, two-level transformations are disciplined by one-step transformation rules and type-level transformations induce value-level transformations. We demonstrate an example hierarchical-relational mapping and subsequent migration of relational data induced by hierarchical format evolution. &copy; Springer-Verlag Berlin Heidelberg 2006.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Data transfer},
  keywords      = {Computer programming;Data reduction;Database systems;Formal logic;Mathematical transformations;XML;},
  language      = {English},
}

@InProceedings{Kim2006,
  author        = {Kim, Dongkwang and Jeong, Karpjoo and Shin, Hyoseop and Hwang, Suntae},
  title         = {An XML schema-based semantic data integration},
  year          = {2006},
  pages         = {522 - 525},
  address       = {Hunan, China},
  note          = {Cyber environments;Data integration;},
  __markedentry = {[Juliana:6]},
  abstract      = {Cyber-infrastructures for scientific and engineering applications require integrating heterogeneous legacy data in different formats and from various domains. Such data integration raises challenging issues: (1) Support for multiple independently-managed schemas, (2) Ease of schema evolution, and (3) Simple schema mappings. In order to address these issues, we propose a novel approach to semantic integration of scientific data which uses XML schemas and RDF-based schema mappings. In this approach, XML schema allows scientists to manage data models intuitively and to use commodity XML DBMS tools. A simple RDF-based ontological representation scheme is used for only structural relations among independently-managed XML schemas from different institutes or domains We present the design and implementation of a prototype system developed for the national cyber-environments for civil engineering research activities in Korea (similar to the NEES project in USA) which is called KOCEDgrid. &copy; 2006 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - Fifth International Conference on Grid and Cooperative Computing, GCC 2006},
  key           = {XML},
  keywords      = {Civil engineering;Data acquisition;Database systems;Engineering research;Semantics;Software prototyping;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/GCC.2006.28},
}

@InProceedings{2007,
  title         = {Database and XML Technologies - 5th International XML Database Symposium, XSym 2007, Proceedings},
  year          = {2007},
  volume        = {4704 LNCS},
  pages         = {ETH Zurich -},
  address       = {Vienna, Austria},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 10 papers. The topics discussed include: normalization theory for XML; dynamic fusion of web data; XPath query satisfiability is in PTIME for real-world DTDs; fast answering of XPath Query workloads on web collections; let a single FLWOR bloom: to improve XQuery plan generation; efficient XQuery evaluation of grouping conditions with duplicate removals; on the effectiveness of flexible querying heuristics for XML data; XML schema evolution: incremental validation and efficient document adaptation; managing branch versioning in versioned/temporal XML; SXDGL: snapshot based concurrency control protocol for XML data; and the generation Y of XML schema matching: panel description.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Yu2005,
  author        = {Yu, Cong and Popa, Lucian},
  title         = {Semantic adaptation of schema mappings when schemas evolve},
  year          = {2005},
  volume        = {3},
  pages         = {1006 - 1017},
  address       = {Trondheim, Norway},
  note          = {Mapping adaptation system;Mapping composition;Schemas;},
  __markedentry = {[Juliana:6]},
  abstract      = {Schemas evolve over time to accommodate the changes in the information they represent. Such evolution causes invalidation of various artifacts depending on the schemas, such as schema mappings. In a heterogenous environment, where cooperation among data sources depends essentially upon them, schema mappings must be adapted to reflect schema evolution. In this study, we explore the mapping composition approach for addressing this mapping adaptation problem. We study the semantics of mapping composition in the context of mapping adaptation and compare our approach with the incremental approach of Velegrakis et al [21]. We show that our method is superior in terms of capturing the semantics of both the original mappings and the evolution. We design and implement a mapping adaptation system based on mapping composition as well as additional mapping pruning techniques that significantly speed up the adaptation. We conduct comprehensive experimental analysis and show that the composition approach is practical in various evolution scenarios. The mapping language that we consider is a nested relational extension of the second-order dependencies of Fagin et al [7]. Our work can also be seen as an implementation of the mapping composition operator of the model management framework.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {VLDB 2005 - Proceedings of 31st International Conference on Very Large Data Bases},
  key           = {Database systems},
  keywords      = {Adaptive systems;Information analysis;Mapping;Semantics;},
  language      = {English},
}

@InProceedings{Haller2010,
  author        = {Haller, Klaus},
  title         = {On the implementation and correctness of information system upgrades},
  year          = {2010},
  note          = {Application logic;Correctness;Database applications;Database schemas;Formal correctness;Holistic approach;Standalone applications;Upgrades;},
  __markedentry = {[Juliana:6]},
  abstract      = {Information systems are applications incorporating a database for storing and processing data. Upgrading information systems requires updating the application logic, modifying the database schema, and adopting the data accordingly. Previous research focuses either on schema evolution or on application logic updates. In this paper, we take a holistic approach by addressing the combination. First, we elaborate the three main upgrade patterns: install &amp; copy, rejuvenation/delta only, and rejuvenation/verified. Second, we introduce our upgrade correctness concept. It is a formal correctness criterion for deciding whether an upgrade succeeded. Third, we discuss implementation patterns. Our insights base on various upgrade projects from stand-alone applications to multi-tenant systems having affected more than one hundred banks. &copy; 2010 IEEE.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {IEEE International Conference on Software Maintenance, ICSM},
  key           = {Information use},
  keywords      = {Computer circuits;Data handling;Database systems;Endocrinology;Information systems;Testing;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/ICSM.2010.5609679},
}

@InProceedings{Bounif2004,
  author        = {Bounif, Hassina and Drutskyy, Oleksandr and Jouanot, Fabrice and Spaccapietra, Stefano},
  title         = {A multimodal database framework for multimedia meeting annotations},
  year          = {2004},
  pages         = {17 - 25},
  address       = {Brisbana, Australia},
  note          = {Multimedia database;Multimedia meeting annotation;Text transcription;},
  __markedentry = {[Juliana:6]},
  abstract      = {The main objective of this paper is to present a flexible annotation management framework for a multimedia database system, applied to meeting recordings. Presented research and development activities are carried out within the scope of the IM2 project in which annotations play an important role in describing raw data from various points of view and in enhancing the query process. We focus on a database system capable of managing annotations (e.g. text transcriptions, dialog acts, speaker space position, etc.) and keeping links with raw data (audio, video, digital documents). This database provides a schema evolution mechanism and a meta-description layer ensuring flexible and incremental annotation definitions. To enhance this database system, some research works are currently in progress: a predictive methodology for schema evolution and a query technique that deals with fuzzy concepts and ontological commitments. We describe our on-going prototype development, in which we focus on data storage and interactive data access.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - 10th International Multimedia Modelling Conference, MMM 2004},
  key           = {Multimedia systems},
  keywords      = {Computer simulation;Data recording;Database systems;Fuzzy control;Mathematical models;Metadata;Project management;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/MULMM.2004.1264962},
}

@Article{Wyss2005,
  author        = {Wyss, Catharine M. and Robertson, Edward L.},
  title         = {Relational languages for metadata integration},
  journal       = {ACM Transactions on Database Systems},
  year          = {2005},
  volume        = {30},
  number        = {2},
  pages         = {624 - 660},
  issn          = {03625915},
  note          = {Data integration;Federated interoperable relational algebra (FIRA);Metadata integration;Relational query languages;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this article, we develop a relational algebra for metadata integration, Federated Interoperable Relational Algebra (FIRA). FIRA has many desirable properties such as compositionality, closure, a deterministic semantics, a modest complexity, support for nested queries, a subalgebra equivalent to canonical Relational Algebra (RA), and robustness under certain classes of schema evolution. Beyond this, FIRA queries are capable of producing fully dynamic output schemas, where the number of relations and/or the number of columns in relations of the output varies dynamically with the input instance. Among existing query languages for relational metadata integration, only FIRA provides generalized dynamic output schemas, where the values in any (fixed) number of input columns can determine output schemas. Further contributions of this article include development of an extended relational model for metadata integration, the Federated Relational Data Model, which is strictly downward compatible with the relational model. Additionally, we define the notion of Transformational Completeness for relational query languages and postulate FIRA as a canonical transformationally complete language. We also give a declarative, SQL-like query language that is equivalent to FIRA, called Federated Interoperable Structured Query Language (FISQL). While our main contributions are conceptual, the federated model, FISQL/FIRA, and the notion of transformational completeness nevertheless have important applications to data integration and OLAP. In addition to summarizing these applications, we illustrate the use of FIRA to optimize FISQL queries using rule-based transformations that directly parallel their canonical relational counterparts. We conclude the article with an extended discussion of related work as well as an indication of current and future work on FISQL/FIRA. &copy; 2005 ACM.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Computer programming languages},
  keywords      = {Algebra;Integration;Interoperability;Metadata;Query languages;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/1071610.1071618},
}

@Article{DeCastro1997,
  author        = {De Castro, Cristina and Grandi, Fabio and Scalas, Maria Rita},
  title         = {Schema versioning for multitemporal relational databases},
  journal       = {Information Systems},
  year          = {1997},
  volume        = {22},
  number        = {5},
  pages         = {249 - 290},
  issn          = {03064379},
  note          = {Multitemporal relational databases;Schema versioning;Temporal languages;},
  __markedentry = {[Juliana:6]},
  abstract      = {In order to follow the evolution of application needs, a database management system is easily expected to undergo changes involving database structure after implementation. Schema evolution concerns the ability of maintaining extant data in response to changes in database structure. Schema versioning enables the use of extensional data through multiple schema interfaces as created by a history of schema changes. However, schema versioning has been considered only to a limited extent in current literature. Also in the field of temporal databases, whereas a great deal of work has been done concerning temporal versioning of extensional data, a thorough investigation of schema versioning potentialities has not yet been made. In this paper we consider schema versioning in a broader perspective and introduce new design options whose distinct semantic properties and functionalities will be discussed. First of all, we consider solutions for schema versioning along transaction time but also along valid time. Moreover, the support of schema versioning implies operations both at intensional and extensional level. Two distinct design solutions (namely single- and multi-pool) are presented for the management of extensional data in a system supporting schema versioning. Finally, a further distinction is introduced to define synchronous and asynchronous management of versioned data and schemata. The proposed solutions differ in their semantics and in the possible operations they support. The mechanisms for the selection of data through a schema version are in many cases strictly related to the particular schema versioning solution adopted, that also affects the data definition and manipulation language at user-interface level. In particular, we show how the temporal language TSQL2, originally designed to support basic functionalities of transaction-time schema versioning, can accordingly be extended.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Relational database systems},
  keywords      = {Computational linguistics;Data structures;Query languages;User interfaces;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/S0306-4379(97)00017-3},
}

@Article{Benchikha2007,
  author        = {Benchikha, Fouzia and Boufaida, Mahmoud},
  title         = {The viewpoint mechanism for Object-oriented databases modelling, distribution and evolution},
  journal       = {Journal of Computing and Information Technology},
  year          = {2007},
  volume        = {15},
  number        = {2},
  pages         = {95 - 110},
  issn          = {13301136},
  note          = {Federated Databases;Integration approach;Object oriented data;Referential schema;Viewpoint approach;Viewpoint schema;},
  __markedentry = {[Juliana:6]},
  abstract      = {Over the past years, most of the research dealing with the object multiple representation and evolution has proposed to enrich the monolithic vision of the classical object approach in which an object belongs to one hierarchy class. In databases, much work has been done towards extending models with advanced tools such as view technology, schema evolution support, multiple classification, role modelling and viewpoints. In particular, the integration of the viewpoint mechanism to the conventional object-oriented data model gives it flexibility and allows one to improve the modelling power of objects. The viewpoint paradigm refers to the multiple description, the distribution, and the evolution of object. Also, it can be an undeniable contribution for a distributed design of complex databases. The motivation of this paper is to define an object data model integrating viewpoints in databases and to present a federated database architecture integrating multiple viewpoint sources following a local-as-extended-view data integration approach.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Object-oriented databases},
  keywords      = {Classification (of information);Data integration;},
  language      = {English},
  url           = {http://dx.doi.org/10.2498/cit.1000692},
}

@InProceedings{Beyer2005,
  author        = {Beyer, Kevin and Ozcan, Fatma and Saiprasad, Sundar and Van Der Linden, Bert},
  title         = {DB2/XML: Designing for evolution},
  year          = {2005},
  pages         = {948 - 952},
  address       = {Baltimore, MD, United states},
  note          = {DB2's schema repositories;Document-level validations;Native XML storage;XML data type;},
  __markedentry = {[Juliana:6]},
  abstract      = {DB2 provides native XML storage, indexing, navigation and query processing through both SQL/XML and XQuery using the XML data type introduced by SQL/XML. In this tutorial we focus on DB2's XML support for schema evolution, especially DB2's schema repository and document-level validation. Copyright 2005 ACM.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {07308078},
  journal       = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
  key           = {Computer aided design},
  keywords      = {Data processing;Data storage equipment;Indexing (of information);Information retrieval systems;Navigation;Schematic diagrams;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/1066157.1066299},
}

@InProceedings{2008,
  title         = {Advances in Conceptual Modeling - Challenges and Opportunities - ER 2008 Workshops CMLSA, ECDM, FP-UML, M2AS, RIGiM, SeCoGIS, WISM, Proceedings},
  year          = {2008},
  volume        = {5232 LNCS},
  address       = {Barcelona, Spain},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 45 papers. The topics discussed include: models of the human metabolism; designing privacy-aware personal health record systems; linking biological databases semantically for knowledge discovery; integration of genomic, proteomic and biomedical information on the semantic web; towards a scientific model management system; modeling transformations between versions of a temporal data warehouse; managing the history of metadata in support for DB archiving and schema evolution; a dynamically extensible, service-based infrastructure for mobile applications; modeling strategic alignment using INSTAL; requirements engineering for distributed development using software agents; administrative units, an ontological perspective; and identifying users stereotypes with semantic web mining.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{2007a,
  title         = {Databases and Information Systems - First and Second VLDB Workshops, ODBIS 2005/2006 Trondheim, Norway, September 2-3, 2005, Seoul, Korea, September 11, 2006, Revised Papers},
  year          = {2007},
  volume        = {4623 LNCS},
  address       = {Seoul, Korea, Republic of},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 9 papers. The topics discussed include: a multi-level matching algorithm for combining similarity measures in ontology integration; class structures and lexical similarities of class names for ontology matching; scalable interoperability through the use of COIN lightweight ontology; domain ontologies evolutions to solve semantic conflicts; requirements ontology and multi-representation strategy for database schema evolution; improving the development of data warehouses by enriching dimension hierarchies with WordNet; management of large spatial ontology bases; knowledge extraction using a conceptual information system (ExCIS); and the semantic desktop: a semantic personal information management system based on RDF and topic maps.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{2005a,
  title         = {WIDM 2005 - Proceedings of the 7th ACM International Workshop on Web Information and Data Management, Co-located with CIKM 2005},
  year          = {2005},
  pages         = {ACM Special Interest Group on Information Retrieval (SIGIR); Gesellschaft fur Informatik e.V. (GI) -},
  address       = {Bremen, Germany},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 12 papers. The topics discussed include: a web of data: new architectures for new technology?; web path recommendations based on page ranking and Markov models; semantic similarity methods in WordNet and their application to information retrieval on the web; DirectoryRank: ordering pages in web directories; exploiting Native XML indexing techniques for XML retrieval in relational database systems; query translation scheme for heterogeneous XML data sources; impact of XML schema evolution on valid documents; a framework for semantic web services discovery; narrative text classification for automatic key phrase extraction in web document corpora; on improving local website search using web server traffic logs: a preliminary report; preventing shilling attacks in online recommender systems; looking at both the present and the past to efficiently update replicas of web content; and a search result clustering method using informatively named entities.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the Interntational Workshop on Web Information and Data Management WIDM},
  language      = {English},
}

@InProceedings{1994a,
  title         = {Proceedings of the Workshop on Advanced Visual Interfaces, AVI 1994},
  year          = {1994},
  pages         = {ACM Spec. Interest Group Comput.-Hum. Interact. (SIGCHI) -},
  address       = {Bari, Italy},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 36 papers. The topics discussed include: integrating computer technology, people technology and application technology: strategies and case studies from Georgia tech's graphics, visualization and usability center; automatic generation of textual, audio, and animated help in UIDE: the user interface design; towards a dynamic strategy for computer-aided visual placement; visual techniques for traditional and multimedia layouts; database schema evolution using EVER diagrams; designing and integrating user interfaces of geographic database applications; using visual programming to extend the power of spreadsheet; towards efficient parsing of diagrammatic languages; interactive training of virtual agents; and an information manipulation environment for monitoring parallel programs.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the Workshop on Advanced Visual Interfaces AVI},
  language      = {English},
}

@InProceedings{Kim2006a,
  author        = {Kim, Dongkwang and Jeong, Karpjoo and Hwang, Suntae and Cho, Kum Won},
  title         = {X-SIGMA: XML based simple data integration system for gathering, managing, and accessing scientific experimental data in grid environments},
  year          = {2006},
  pages         = {University of Amsterdam, Netherlands; IEEE Computer Society -},
  address       = {Amsterdam, Netherlands},
  note          = {Compatibility;Data integration systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {Effective scientific data management is crucial for e-Science applications. Scientific data management raises challenging requirements: (1) support for not only experimental data but also context data, (2) both schema evolution and integration, (3) and compatibility with legacy data management conventions or environments. In this paper, we present a scientific data management system (called X-SIGMA) which is designed to address those issues. X-SIGMA is a Grid-based integrated system for managing, integrating, and accessing scientific experimental data. A prototype system is implemented and has been being used to develop the scientific data management system for the national cyber-infrastructure project called KOCED in Korea. &copy; 2006 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {e-Science 2006 - Second IEEE International Conference on e-Science and Grid Computing},
  key           = {Management information systems},
  keywords      = {Grid computing;Legacy systems;Natural sciences computing;Project management;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/E-SCIENCE.2006.261139},
}

@InProceedings{Davidson1997,
  author        = {Davidson, Susan B. and Kosky, Anthony S.},
  title         = {WOL: A language for database transformations and constraints},
  year          = {1997},
  pages         = {55 - 65},
  address       = {Birmingham, UK},
  note          = {Morphase systems;WOL programming language;},
  __markedentry = {[Juliana:6]},
  abstract      = {The need to transform data between heterogeneous databases arises from a number of critical tasks in data management. These tasks are complicated by schema evolution in the underlying databases, and by the presence of non-standard database constraints. We describe a declarative language, WOL, for specifying such transformations, and its implementation in a system called Morphase. WOL is designed to allow transformations between the complex data structures which arise in object-oriented databases as well as in complex relational databases, and to allow for reasoning about the interactions between database transformations and constraints.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - International Conference on Data Engineering},
  key           = {Database systems},
  keywords      = {Computer programming languages;Constraint theory;Data structures;Information management;Object oriented programming;},
  language      = {English},
}

@InProceedings{Burmeister2006,
  author        = {Burmeister, Birgit and Steiert, Hans-Peter and Bauer, Thomas and Baumgartel, Hartwig},
  title         = {Agile processes through goal- and context-oriented business process modeling},
  year          = {2006},
  volume        = {4103 LNCS},
  pages         = {217 - 228},
  address       = {Vienna, Austria},
  note          = {Agile processes;Business process modeling;Change management process;Graph structures;},
  __markedentry = {[Juliana:6]},
  abstract      = {Today's methods for business process modeling like extended event-process-chains only allow the definition of static graph structures. They are not flexible enough for instance to model the change management process of the Mercedes Car Group (MCG) since it requires dynamic selection of process variants, process schema evolution and their (partial) propagation on running workflows, arbitrary dynamic process jumps and changes, etc. We have developed an approach for modeling agile processes based on goals and context rules, which enables the required flexibility. Additionally it is possible to map such a process model to a run-time infrastructure for process execution. &copy; Springer-Verlag Berlin Heidelberg 2006.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Agile manufacturing systems},
  keywords      = {Administrative data processing;Graph theory;Information management;Mathematical models;Object oriented programming;},
  language      = {English},
}

@InProceedings{Lipyeow2005,
  author        = {Lipyeow, Lim and Min, Wang},
  title         = {Managing e-commerce catalogs in a DBMS with native XML support},
  year          = {2005},
  volume        = {2005},
  pages         = {564 - 571},
  address       = {Beijing, China},
  note          = {E-catalog management;Electronic product catalogs;Product information;Query optimization;},
  __markedentry = {[Juliana:6]},
  abstract      = {Electronic commerce is emerging as a major application area for database systems. A large number of e-commerce stores provide electronic product catalogs that allow customers to search products of interest and store owners to manage various product information. Due to the constant schema evolution and the sparsity of e-commerce data, most commercial e-commerce systems use the so-called vertical schema for data storage. However, query processing for data stored using vertical schema is extremely inefficient because current RDBMSs, especially its cost-based query optimizer, are specifically designed to deal with traditional horizontal schemas. In this paper, we show that e-catalog management can be naturally supported in IBM's System RX, the first DBMS that truly supports both XML and relational data in their native forms. By leveraging on System RX's hybrid nature, we present a novel solution for storing, managing, and querying e-catalog data. In addition to traditional queries, we show that our solution supports semantic queries as well. Our solution does not require a separate query optimization layer, because query optimization is handled within the hybrid DBMS engine itself. &copy; 2005 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - ICEBE 2005: IEEE International Conference on e-Business Engineering},
  key           = {Electronic commerce},
  keywords      = {Data processing;Database systems;Information services;Optimization;Query languages;Search engines;Semantics;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/ICEBE.2005.82},
}

@InProceedings{1999,
  title         = {Proceedings - 1999 IFCIS International Conference on Cooperative Information Systems, CoopIS 1999},
  year          = {1999},
  pages         = {International Federation of Cooperative Information Systems -},
  address       = {Edinburgh, United kingdom},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 31 papers. The topics discussed include: detection of heterogeneities in a multiple text database environment; a unified graph-based framework for deriving nominal interscheme properties, type conflicts and object cluster similarities; access keys warehouse: a new approach to the development of cooperative information systems; event composition in time-dependent distributed systems; providing customized process and situation awareness in the collaboration management infrastructure; dynamic workflow. schema evolution based on workflow type versioning and workflow migration; and generic workflow models: how to handle dynamic change and capture management information?.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - 1999 IFCIS International Conference on Cooperative Information Systems, CoopIS 1999},
  language      = {English},
}

@InProceedings{Aulbach2009,
  author        = {Aulbach, Stefan and Jacobs, Dean and Kemper, Alfons and Seibold, Michael},
  title         = {A comparison of flexible schemas for Software as a Service},
  year          = {2009},
  pages         = {881 - 888},
  address       = {Providence, RI, United states},
  note          = {Evolution;Experimental comparison;Extensibility;Flexible schemas;Generic structure;Microsoft SQL Server;Multi tenancies;Multi-tenant database;},
  __markedentry = {[Juliana:6]},
  abstract      = {A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended for different versions of the application and dynamically modified while the system is on-line. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database "owns" the schema in that its structure is explicitly defined in DDL. Included here is the commonly- used mapping where each tenant is given their own private tables, which we take as the baseline, and a mapping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only limited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application "owns" the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal database for SaaS has not yet been developed and offer some suggestions as to how it should be designed. &copy; 2009 ACM.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {SIGMOD-PODS'09 - Proceedings of the International Conference on Management of Data and 28th Symposium on Principles of Database Systems},
  key           = {Software as a service (SaaS)},
  keywords      = {Application programs;Database systems;Information management;Mapping;Web services;Windows operating system;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/1559845.1559941},
}

@InProceedings{Peng2006,
  author        = {Peng, Zhiyong and Shi, Yuan and Zhai, Boxuan},
  title         = {Realization of biological data management by object deputy database system},
  year          = {2006},
  volume        = {4070 LNBI},
  pages         = {49 - 67},
  address       = {Beijing, China},
  note          = {Biological data management;Data redundancy;},
  __markedentry = {[Juliana:6]},
  abstract      = {Traditional database system is not suitable for biological data management. In this paper, we discuss how to manage a large amount of complex biological data by an object deputy database system which can provide rich semantics and enough flexibility. In our system, the flexible inheritance avoids a lot of data redundancy. Our cross class query mechanism allows users to find the more related data based on complex relationships. In addition, the schema evolution of biological data and their annotation can be easily supported along with biological knowledge accumulation. The experiment shows our approach is feasible and more efficient than the traditional ones. &copy; Springer-Verlag Berlin Heidelberg 2006.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Database systems},
  keywords      = {Information management;Knowledge acquisition;Query processing;Semantics;},
  language      = {English},
}

@InProceedings{Chabin2010,
  author        = {Chabin, Jacques and Halfeld-Ferrari, Mirian and Musicante, Martin A. and Rety, Pierre},
  title         = {Minimal tree language extensions: A keystone of XML type compatibility and evolution},
  year          = {2010},
  volume        = {6255 LNCS},
  pages         = {60 - 75},
  note          = {Regular tree grammars;Sub class;Tree grammars;Tree languages;Web service composition;XML schema evolutions;XML types;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper, we propose algorithms that extend a given regular tree grammar G<inf>0</inf>to a new grammar G respecting the following two properties: (i) G belongs to the sub-class of local or single-type tree grammars and (ii) G is the least grammar (in the sense of language inclusion) that contains the language of G<inf>0</inf>. Our algorithms give rise to important tools in the context of web service composition or XML schema evolution. We are particularly interested in applying them in order to reconcile different XML type messages among services. The algorithms are proven correct and some of their applications are discussed. &copy; 2010 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Forestry},
  keywords      = {Web services;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-642-14808-8_5},
}

@InProceedings{Assoudi2011,
  author        = {Assoudi, H. and Lounis, H.},
  title         = {Self-healing data exchange process under evolving schemas: A new mapping adaptation approach based on self-optimization},
  year          = {2011},
  pages         = {188 - 190},
  address       = {Boca Raton, FL, United states},
  note          = {Autonomic Computing;Dependability;Schema mappings;Schema matching;Self-managed;Sufficient correctness;},
  __markedentry = {[Juliana:6]},
  abstract      = {Today, more than ever, enterprises are relying on highly complex IT solutions to respond flexibly and rapidly to the constant changing business environment. Yet, the increasing complexity of IT solutions presents significant challenges. In this paper, we propose a solution to reduce the human intervention needed to maintain data exchange processes after a schema evolution (changes impacting source or target system schemas participating in a data exchange scenario). Our approach, toward reliable self-healed data exchange processes under evolving schemas, is called DEAM (Data Exchange Autonomic Manager). &copy; 2011 IEEE.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {15302059},
  journal       = {Proceedings of IEEE International Symposium on High Assurance Systems Engineering},
  key           = {Electronic data interchange},
  keywords      = {Fault tolerance;Learning systems;Mapping;Systems engineering;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/HASE.2011.42},
}

@InBook{Bouchou2009,
  pages         = {1 - 21},
  title         = {Extending XML types using updates},
  year          = {2009},
  author        = {Bouchou, Beatrice and Duarte, Denio and Halfeld Ferrari, Mirian and Musicante, Martin A.},
  note          = {Element contents;Network connection;Regular expressions;Service evolutions;Service protocols;XML format;XML messaging;XML schema evolutions;},
  __markedentry = {[Juliana:6]},
  abstract      = {The XML Messaging Protocol, a part of the Web service protocol stack, is responsible for encoding messages in a common XML format (or type), so that they can be understood at either end of a network connection. The evolution of an XML type may be required in order to reflect new communication needs, materialized by slightly different XML messages. For instance, due to a service evolution, it might be interesting to extend a type in order to allow the reception of more information, when it is available, instead of always disregarding it. The authors' proposal consists in a conservative XML schema evolution. The framework is as follows: administrators enter updates performed on a valid XML document in order to specify new documents expected to be valid, and the system computes new types accepting both such documents and previously valid ones. Changing the type is mainly changing regular expressions that define element content models. They present the algorithm that implements this approach, its properties and experimental results. &copy; 2009, IGI Global.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Services and Business Computing Solutions with XML: Applications for Quality Management and Best Processes},
  key           = {XML},
  keywords      = {Web services;},
  language      = {English},
  url           = {http://dx.doi.org/10.4018/978-1-60566-330-2.ch001},
}

@InProceedings{1994b,
  title         = {13th International Conference on the Entity-Relationship Approach, ER 1994},
  year          = {1994},
  volume        = {881 LNCS},
  pages         = {1 - 579},
  address       = {Manchester, United kingdom},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 34 papers. The special focus in this conference is on Entity-Relationship Approach. The topics include: Reflections on the relationship between BPR and software process modelling; specifying business processes over objects; deriving complex structured object types for business process modelling; business process modeling in the workflow management environment Leu; an assisting method for enterprise-wide conceptual data modeling in the bottom-up approach; organisational and information system modelling for information systems requirements determination; database schema evolution through the specification and maintenance of changes on entities and relationships; method restructuring and consistency checking for object-oriented schemas; state-conditioned semantics in databases; modelling constraints with exceptions in object-oriented databases; declarative specification of constraint maintenance; on the representation of objects with polymorphic shape and behaviour; a normal form object-oriented entity relationship diagram; cardinality consistency of derived objects in DOOD systems; conceptual modelling and manipulation of temporal databases; a formal software specification tool using the entity-relationship model; an overview of the Lawrence Berkeley laboratory extended entity-relationship database tools; a generic data model for the support of multiple user interaction facilities; using queries to improve database reverse engineering; reconstruction of ER schema from database applications; extracting an entity relationship schema from a relational database through reverse engineering; leveled entity relationship model; formalised conceptual models as a foundation of information systems development; abstraction levels for entity-relationship schemas and an executable meta model for re-engineering of database schemas.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Cicekli2005,
  author        = {Cicekli, Nihan K. and Cosar, Ahmet and Dogac, Asuman and Polat, Faruk and Senkul, Pinar and Toroslu, I. Hakki and Yazici, Adnan},
  title         = {Data management research at the Middle East Technical University},
  journal       = {SIGMOD Record},
  year          = {2005},
  volume        = {34},
  number        = {3},
  pages         = {81 - 84},
  issn          = {01635808},
  note          = {Data management research;METU;Modeling;Technical Research Council;},
  __markedentry = {[Juliana:6]},
  abstract      = {Data management research conducted in the department of Computer Engineering at the Middle East Technical University is discussed. The major research funding sources include the Scientific and Technical Research Council of Turkey, the European Commission, and the internal research funds of METU. The research on workflow management systems has concentrated on modeling and scheduling under resource allocation systems. Research on object-oriented databases is primarily based on extensibility and dynamic schema evolution.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Data processing},
  keywords      = {Industrial research;Management science;Research and development management;Systems analysis;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/1084805.1084822},
}

@InProceedings{2011a,
  title         = {ICDE Workshops 2011 - 2011 IEEE 27th International Conference on Data Engineering Workshops},
  year          = {2011},
  address       = {Hannover, Germany},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 52 papers. The topics discussed include: a call for order in search space generation process of query optimization; balancing load in stream processing with the cloud; autonomous workload-driven reorganization of column groupings in MMDBS; dealing proactively with data corruption: challenges and opportunities; classification algorithms for relation prediction; data lineage in the MOMIS data fusion system; a framework for semi-automatic identification, disambiguation and storage of protein-related abbreviations in scientific literature; a path algebra for multi-relational graphs; formal reasoning about runtime code update; towards a categorical framework to ensure correct software evolutions; predicting upgrade failures using dependency analysis; schema evolution analysis for embedded databases; and causes for inconsistency-tolerant schema update management.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {10844627},
  journal       = {Proceedings - International Conference on Data Engineering},
  language      = {English},
}

@InProceedings{Klimek2010,
  author        = {Klimek, Jakub and Necasky, Martin},
  title         = {Reverse-engineering of XML schemas: A survey},
  year          = {2010},
  volume        = {567},
  pages         = {96 - 107},
  address       = {Stedronin-Plazy, Czech republic},
  note          = {Conceptual model;Schema;User involvement;XML data;XML data sources;XML schema evolutions;XML schema languages;XML schemas;},
  __markedentry = {[Juliana:6]},
  abstract      = {As approaches to conceptual modeling of XML data become more popular, a need arises to reverse-engineer existing schemas to the conceptual models. They make the management of XML schemas easier as well as provide means for accomplishing integration of various XML data sources. Some methods for reverse-engineering of XML schemas have been proposed and in this paper, they are compared using various criteria such as used XML schema languages, level of user involvement, number of XML schemas that can be covered by the conceptual model or support for consecutive XML schema evolution. They are also evaluated according to their potential to be used as parts of a system for management, evolution and integration of XML as a whole.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {16130073},
  journal       = {CEUR Workshop Proceedings},
  key           = {XML},
  keywords      = {Data mining;Reverse engineering;Specifications;},
  language      = {English},
}

@InProceedings{Reichert2003,
  author        = {Reichert, Manfred and Rinderle, Stefanie and Dadam, Peter},
  title         = {ADEPT workflow management system: Flexible support for enterprise-wide business processes},
  year          = {2003},
  volume        = {2678},
  pages         = {370 - 379},
  address       = {Eindhoven, Netherlands},
  note          = {Adaptive workflow;Flexible supports;Holistic approach;Modeling concepts;Research prototype;Temporal constraints;Workflow dependency;Workflow management systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this tool presentation we give an overview of the ADEPT workflow management system (WfMS), which is one of the few available research prototypes dealingwith enterprise-wide, adaptive workflow (WF) management. ADEPT offers sophisticated modeling concepts and advanced features, like temporal constraint management, ad-hoc WF changes, WF schema evolution, synchronization of inter-workflow dependencies, and scalability. We sketch these features and describe how they have been realized within ADEPT. In addition, we show which tools and interfaces are offered to developers and users in this context. ADEPT follows a holistic approach, i.e., the described concepts have not been implemented in an isolated fashion only, but are treated in conjunction with each other by integrating them within one WfMS.<br/> &copy; Springer-Verlag Berlin Heidelberg 2003.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Work simplification},
  keywords      = {Administrative data processing;Enterprise resource management;Petri nets;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/3-540-44895-0_25},
}

@InProceedings{1990,
  title         = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA/ECOOP 1990},
  year          = {1990},
  pages         = {ACM Special Interest Group on Programming Languages (SIGPLAN) -},
  address       = {Ottawa, ON, Canada},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 33 papers. The topics discussed include: message pattern specifications: a new technique for handling errors in parallel object oriented systems; strong typing of object-oriented languages revisited; a logical theory of concurrent objects; actors as a special case of concurrent constraint programming; LO and behold! concurrent structured processes; viewing objects as patterns of communicating agents; what tracers are made of; beyond schema evolution to database reorganization; garbage collection of actors; structured analysis and object oriented analysis; graphical specification of object oriented systems; an iterative-design model for reusable object-oriented software; the design of the C++ booth components; contracts: specifying behavioral compositions in object-oriented systems; reasoning about object-oriented programs that use subtypes; pclos: stress testing CLOS experiencing the metaobject protocol; when objects collide: experiences with reusing multiple class hierarchies; and a parallel object-oriented language with inheritance and subtyping.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA/ECOOP 1990},
  language      = {English},
}

@InProceedings{1993,
  title         = {1st International Symposium on Object Technologies for Advanced Software, ISOTAS 1993},
  year          = {1993},
  volume        = {742 LNCS},
  pages         = {1 - 543},
  address       = {Kanazawa, Japan},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 31 papers. The special focus in this conference is on Object Technologies for Advanced Software. The topics include: Uniting functional and object-oriented programming; a support for software component cooperation; an object orientation kernel; change management and consistency maintenance in software development environments using object oriented attribute grammars; design of an integrated and extensible C++ programming environment; definition of a reflective kernel for a prototype-based language; kernel structuring for object-oriented operating systems; maintaining behavioral consistency during schema evolution; an object-centered approach for manipulating hierarchically complex objects; towards the unification of views and versions for object databases; abstract view objects for multiple OODB integration; an object-oriented query model supporting views; refactoring and aggregation; transverse activities: abstractions in object-oriented programming; dynamic extensibility in a statically-compiled object-oriented language; managing change in persistent object systems; an object-oriented pattern matching language; a class-based logic language for object-oriented databases; name management and object technology for advanced software; constraints in object-oriented analysis; minimizing dependency on class structures with adaptive programs; first class messages as first class continuations; a typing system for a calculus of objects; a type mechanism based on restricted CCS for distributed active objects; adding implicit invocation to languages; requirements and early experiences in the implementation of the SPADE repository using object-oriented technology; object-oriented formal specification development using VDM.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{2012,
  title         = {14th Asia-Pacific Network Operations and Management Symposium: "Management in the Big Data and IoT Era", APNOMS 2012 - Final Program},
  year          = {2012},
  pages         = {KICS KNOM; IEICE ICM -},
  address       = {Seoul, Korea, Republic of},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 61 papers. The topics discussed include: optimization of energy saving in cellular networks using ant colony algorithm; context management for user-centric context-aware services over pervasive networks; integrated hybrid MAC and topology control scheme for M2M area networks; DDoS attack forecasting system architecture using honey net; a seamless content delivery scheme for flow mobility in content centric network; a load balancing algorithm with QoS support over heterogeneous wireless networks; customer energy management platform in the smart grid; application traffic identification based on remote subnet grouping; data allocation method considering server performance and data access frequency with consistent hashing; problems and feasibility consideration of service focused monitoring OSS over mobile networks; and design and implementation of database schema evolution for service continuity of web-based Internet applications.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {14th Asia-Pacific Network Operations and Management Symposium: "Management in the Big Data and IoT Era", APNOMS 2012 - Final Program},
  language      = {English},
}

@InProceedings{Davis2007,
  author        = {Davis, Karen C. and Banerjee, Sandipto},
  title         = {Teaching and assessing a data warehouse design course},
  year          = {2007},
  pages         = {22 - 31},
  address       = {Glasgow, United kingdom},
  note          = {Data warehouse design;Instructional modes;Physical design research;Software tool;},
  __markedentry = {[Juliana:6]},
  abstract      = {This paper describes a course in data warehouse design that includes conceptual, logical, and physical design research. The instructional modes and assessment techniques are described and illustrated. The scope of the course is given via a discussion of the course bibliography; the bibliography includes a workshop report, "Data Warehousing at the Crossroads," created at a week-long international meeting by 18 co-authors. The paper concludes with discussion of a data warehouse conceptual modeling and schema evolution software tool that can be used to supplement the course materials. The tool is part of a larger research effort and we anticipate that it will have educational value for building student intuition and visualising the effects of schema changes. &copy; 2007 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - 24th British National Conference on Databases, BNCOD 2007},
  key           = {Curricula},
  keywords      = {Bibliographies;Computer aided design;Data warehouses;Students;Teaching;Technical presentations;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/BNCOD.2007.18},
}

@InProceedings{Lakshmanan1993,
  author        = {Lakshmanan, Laks V.S. and Sadri, Fereidoon and Subramanian, Iyer N.},
  title         = {On the logical foundations of schema integration and evolution in heterogeneous database systems},
  year          = {1993},
  volume        = {760 LNCS},
  pages         = {81 - 100},
  address       = {Phoenix, AZ, United states},
  note          = {Cooperative query answering;Heterogeneous database systems;Logical foundations;Logical integration;Model-theoretic semantics;Relational Database;Schema integration;Sound and complete;},
  __markedentry = {[Juliana:6]},
  abstract      = {Developing a declarative approach to schema integration in the Context of heterogeneous database systems is a major goal of this research. We take a first step toward this goal in this paper, by developing a simple logic called SchemaLog which is syntactically higher-order but has a first-order semantics. SchemaLog can provide for a logical integration of multiple relational databases in a federation of database systems. We develop a fixpoint theory as well as a sound and complete proof theory for the definite clause fragment of SchemaLog and show their equivalence to the modeltheoretic semantics. We argue that a uniform framework for schema integration as well as schema evolution is both desirable and possible. We illustrate the simplicity and power of SchemaLog with a variety of applications involving database programming (with schema browsing), schema integration, schema evolution, and cooperative query answering.<br/> &copy; Springer-Verlag Berlin Heidelberg 1993.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Relational database systems},
  keywords      = {Integration;Object oriented programming;Object-oriented databases;Query processing;Semantics;},
  language      = {English},
}

@InProceedings{ZouariTurki2012,
  author        = {Zouari Turki, Ines and Ghozzi Jedidi, Faiza and Bouaziz, Rafik},
  title         = {Constraints to manage consistency in multiversion data warehouse},
  year          = {2012},
  pages         = {1607 - 1612},
  address       = {Opatija, Croatia},
  note          = {Constraint violation;Dependency constraints;Hierarchical structures;Multi-dimensional model;Multiversion data warehouse;Structural constraints;Temporal constraints;Temporal extensions;},
  __markedentry = {[Juliana:6]},
  abstract      = {Defining structural constraints in multidimensional models is certainly a necessity to ensure the consistency of such models, especially the hierarchical structure of dimensions. This necessity increases when considering temporal and multiversion multidimensional models due to their temporal extension. Our proposed model for managing multiversion data warehouses (MV-DW) supports several structural and temporal constraints. In this paper, we enrich these constraints by the temporal fact-dimension dependency constraint which deals with relationship between fact and dimensions in a temporal context. Besides, schema and instance evolution operators can introduce inconsistencies in MV-DW schema and/or data. Such inconsistencies can be avoided by checking the defined MV-DW constraints. To this end, we define two classes of technical issues. The first one deals with algorithms that have to be run following to schema evolution operators to check the consistency of the changed DW schema. The second issue deals with triggers that have to cancel data propagation when detecting any instance constraint violation. &copy; 2012 MIPRO.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {MIPRO 2012 - 35th International Convention on Information and Communication Technology, Electronics and Microelectronics - Proceedings},
  key           = {Data warehouses},
  keywords      = {Microelectronics;},
  language      = {English},
}

@InBook{Arocena2015,
  pages         = {1960 - 1963},
  title         = {Gain control over your integration evaluations},
  year          = {2015},
  author        = {Arocena, Patricia C. and Ciucanu, Radu and Glavic, Boris and Miller, Renee J.},
  volume        = {8},
  number        = {12 12},
  address       = {Seoul, Korea, Republic of},
  note          = {Answering queries;Application scenario;Data integration system;Empirical evaluations;Integration problems;Integration systems;Performance and scalabilities;Real-world scenario;},
  __markedentry = {[Juliana:6]},
  abstract      = {Integration systems are typically evaluated using a few realworld scenarios (e.g., bibliographical or biological datasets) or using synthetic scenarios (e.g., based on star-schemas or other patterns for schemas and constraints). Reusing such evaluations is a cumbersome task because their focus is usually limited to showcasing a specific feature of an approach. This makes it dificult to compare integration solutions, understand their generality, and understand their performance for different application scenarios. Based on this observation, we demonstrate some of the requirements for developing integration benchmarks. We argue that the major abstractions used for integration problems have converged in the last decade which enables the application of robust empirical methods to integration problems (from schema evolution, to data exchange, to answering queries using views and many more). Specifically, we demonstrate that schema mappings are the main abstraction that now drives most integration solutions and show how a metadata generator can be used to create more credible evaluations of the performance and scalability of data integration systems. We will use the demonstration to evangelize for more robust, shared empirical evaluations of data integration systems. &copy; 2015 VLDB Endowment 2150-8097/15/08.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {21508097},
  journal       = {Proceedings of the VLDB Endowment},
  key           = {Data integration},
  keywords      = {Abstracting;Biology;Digital storage;Electronic data interchange;Online systems;},
  language      = {English},
  url           = {http://dx.doi.org/10.14778/2824032.2824111},
}

@Article{Halevy2004,
  author        = {Halevy, Alon Y. and Ives, Zachary G. and Madhavan, Jayant and Mork, Peter and Suciu, Dan and Tatarinov, Igor},
  title         = {The piazza peer data management system},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  year          = {2004},
  volume        = {16},
  number        = {7},
  pages         = {787 - 798},
  issn          = {10414347},
  note          = {Data integration;Logical models;Piazza peer data management systems;Schema mediation;},
  __markedentry = {[Juliana:6]},
  abstract      = {Intuitively, data management and data integration tools should be well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: They typically require a comprehensive schema design before they can be used to store or share information and they are difficult to extend because schema evolution is heavyweight and may break backward compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by non-database-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: We propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peers' schemas. PDMSs represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peers' individual schemas. This paper describes several aspects of the Piazza PDMS, including the schema mediation formalism, query answering and optimization algorithms, and the relevance of PDMSs to the Semantic Web.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Algorithms;Computer architecture;Data acquisition;Data transfer;Information analysis;Information management;Mathematical models;Portals;Query languages;Spreadsheets;World Wide Web;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/TKDE.2004.1318562},
}

@InProceedings{Sadri1989,
  author        = {Sadri, Fereidoon},
  title         = {Object-oriented database systems},
  year          = {1989},
  pages         = {195 - 196},
  address       = {Orlando, FL, USA},
  note          = {Computational Models;Object-Oriented Database Systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {The object model, which is based on the abstract data types concept, provides a natural and more powerful modeling capability. This modeling power, coupled with efficiency of implementation, makes object-oriented database systems suitable for complex applications, such as engineering design applications. The author concentrates on (1) differences between object-oriented databases and object-oriented programming languages, and (2) differences between object-oriented databases and classical (relational) databases. The author argues the need for supporting schema evolution and object versions.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {07306512},
  journal       = {Proceedings - IEEE Computer Society's International Computer Software \&amp; Applications Conference},
  key           = {Database Systems},
  keywords      = {Computer Metatheory--Programming Theory;Computer Software--Applications;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/CMPSAC.1989.65081},
}

@InProceedings{Jeon1994,
  author        = {Jeon, Dae Kyung and Urban, Susan D. and Shah, Jami J.},
  title         = {Process modelling aspects of a design history data model},
  year          = {1994},
  volume        = {59},
  pages         = {209 - 218},
  address       = {New Orleans, LA, USA},
  note          = {Data models;Design history;Process modeling;},
  __markedentry = {[Juliana:6]},
  abstract      = {An engineering design history is a step by step account of the events and the states through which a design artifacts evolves. Database technology has not yet provided adequate mechanisms for capturing and reusing design history information. This paper presents process modelling extensions to an object-oriented database for the purpose of dynamically capturing design histories and modelling the temporal relationships between process steps. Schema evolution is an important component of the dynamic capture process. An advantage to the approach described in this paper is that it provides an integrated approach to describing the relationships between structural specifications, process steps, and the rationale for specific design decisions. An example of capturing a design process using the features of the model is presented.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {American Society of Mechanical Engineers, Petroleum Division (Publication) PD},
  key           = {Software engineering},
  keywords      = {Database systems;Object oriented programming;Systems engineering;},
  language      = {English},
}

@Article{Jenson2007,
  author        = {Jenson, Deb},
  title         = {"Viper 2" accelerates it agility},
  journal       = {DB2 Magazine},
  year          = {2007},
  volume        = {12},
  number        = {3},
  pages         = {26 - 30},
  note          = {Hybrid data servers;Linux distributions;XML documents;XML repositories;},
  __markedentry = {[Juliana:6]},
  abstract      = {IBM has created a new hybrid data server with Viper 2, code name for the next DB2 9.5 release, that manages both relational and XML repositories to respond to business demand. DV2 Viper 2 extends the XML capability to accelerate application delivery. DV2 viper 2 has XSLT support built in. The new XSLTRANSFORM function converts XML document that reside in a database into HTML, plain text, or other forms of XML. DB2 helps to update XML schemas in real-time without losing access to existing XML documents. DV2 Viper 2 introduces schema evolution, which provides the ability to validate both existing and new XML documents against an evolved version of a registered schema. DB2 Viper 2 AIX and Linux distributions will include Tivoli System Automation to complement DB2's high-availability functionality. DB2 Viper 2 automatically reduces the amount of storage consumed by backup, load copy, and log files by regularly removing old files.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Servers},
  keywords      = {Competitive intelligence;Data acquisition;Database systems;HTML;Real time systems;XML;},
  language      = {English},
}

@Article{Chikishev2017,
  author        = {Chikishev, D. and Pozhidaeva, E.},
  title         = {Mathematical modeling of steel chemical composition and modes of thermomechanical treatment influence on hot-rolled plate mechanical properties},
  journal       = {International Journal of Advanced Manufacturing Technology},
  year          = {2017},
  volume        = {92},
  number        = {9-12},
  pages         = {3725 - 3738},
  issn          = {02683768},
  note          = {Bridge constructions;Chemical compositions;Micro-alloyed steels;Multi layer perceptron;Multi-layered Perceptron;Optimal chemical compositions;Process parameters;Thermomechanical rolling;},
  __markedentry = {[Juliana:6]},
  abstract      = {Methods of economical alloying of high pipe steels were considered in the paper. Due to the large environmental, energy, and economic issues in the destruction of the pipeline, the quality of steel pipes is very important. The method described in this article is considered to be relevant not only for pipe steels but also for high-strength steels used in shipbuilding, bridge construction, etc. Review mechanisms of strengthening microalloyed steels were carried out in this work. Schema evolution of the microstructure of microalloyed steels was discussed. The influence of alloying elements in steel by means of optimal process parameters of thermomechanical rolling was analyzed. There are two methods of mathematical modeling used in the study: artificial neural networks (ANNs) which are based on the multilayered perceptron to select the optimal chemical composition and finite element analysis to optimize the process parameters. An experimental dataset was used to train multilayer perceptron (MLP) networks to allow for prediction of the yield strength, tensile strength, and elongation of steel. Due to large availability, low cost, and high accuracy of the results, these methods are considered to be the most promising ones. The mathematical model for calculating mechanical properties of a rolling pipe has been developed. Two ways to reduce the cost of a hot-rolled plate made of microalloyed steels were developed. There has been developed the complex of replacement technological impact which can make it possible to replace or reduce the amount of expensive chemical elements (vanadium, nickel, copper, manganese, chromium, and niobium) without loss of quality.<br/> &copy; 2017, Springer-Verlag London.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {High strength steel},
  keywords      = {Alloying elements;Chemical analysis;Finite element method;Mechanical properties;Microalloying;Multilayer neural networks;Multilayers;Neural networks;Rolling;Steel metallography;Steel pipe;Tensile strength;Thermomechanical treatment;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/s00170-017-0435-6},
}

@Article{Bertino1991,
  author        = {Bertino, Elisa and Martino, Lorenzo},
  title         = {Object-oriented database management systems: Concepts and issues},
  journal       = {Computer},
  year          = {1991},
  volume        = {24},
  number        = {4},
  pages         = {33 - 47},
  issn          = {00189162},
  note          = {Computational Models;Data Models;Object-Oriented Systems;Query Processing;},
  __markedentry = {[Juliana:6]},
  abstract      = {Requirements imposed on both the object data model and object management by the support of complex objects are outlined. The basic concepts of an object-oriented data model are discussed. They are objects and object identifiers, aggregation, classes and instantiation mechanisms, metaclasses, and inheritance. Object-oriented models are compared with semantic, relational, and Codasyl models. Object-oriented query languages and query processing are considered. Some operational aspects of data management in object-oriented systems are examined. Schema evolution is discussed.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database Systems},
  keywords      = {Computer Programming--Object Oriented Programming;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/2.76261},
}

@Article{Rezgui1998,
  author        = {Rezgui, Yacine and Brown, Alex and Cooper, Grahame and Brandon, Peter and Betts, Martin},
  title         = {Intelligent models for collaborative construction engineering},
  journal       = {Computer-Aided Civil and Infrastructure Engineering},
  year          = {1998},
  volume        = {13},
  number        = {3},
  pages         = {151 - 161},
  issn          = {10939687},
  note          = {Collaborative construction engineering;Construction project lifecycle;},
  __markedentry = {[Juliana:6]},
  abstract      = {Many critical issues still need to be tackled properly in order to manage concurrent engineering projects effectively. These issues include versioning, notification, and propagation, all of which need to be addressed throughout the construction project lifecycle. To this end, the COMMIT project strives to achieve an improved level of support for the versioning of project information at both the conceptual (schema evolution) and the instance levels. Versioning, notification, and propagation are addressed through the CIMM (a generic and context-independent information management model for supporting collaborative construction projects) and are described in this paper. The main concepts of the construction-oriented COMMIT Canonical Model (CCM), specialized from the CIMM, are also presented, followed by a description of the first prototype implementation of the CIMM tackling information versioning. This research is ongoing and supported by a well-established U.K. steering group.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Computer aided engineering},
  keywords      = {Civil engineering;Computer simulation;Concurrent engineering;Construction;Information management;Project management;},
  language      = {English},
}

@InProceedings{Jeon1994a,
  author        = {Jeon, Dae Kyung and Urban, Susan D. and Shah, Jami J.},
  title         = {Process modelling aspects of a design history data model},
  year          = {1994},
  volume        = {59},
  pages         = {209 - 218},
  address       = {New Orleans, LA, USA},
  note          = {Design history;Process modeling;},
  __markedentry = {[Juliana:6]},
  abstract      = {An engineering design history is a step by step account of the events and the states through which a design artifacts evolves. Database technology has not yet provided adequate mechanisms for capturing and reusing design history information. This paper presents process modelling extensions to an object-oriented database for the purpose of dynamically capturing design histories and modelling the temporal relationships between process steps. Schema evolution is an important component of the dynamic capture process. An advantage to the approach described in this paper is that it provides an integrated approach to describing the relationships between structural specifications, process steps, and the rationale for specific design decisions. An example of capturing a design process using the features of the model is presented.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {American Society of Mechanical Engineers, Petroleum Division (Publication) PD},
  key           = {Software engineering},
  keywords      = {Data models;Database systems;Object oriented programming;Systems engineering;},
  language      = {English},
}

@InProceedings{2009a,
  title         = {DATESO 2009 - Proceedings of the 9th Annual Workshop on Databases, Texts, Specifications and Objects},
  year          = {2009},
  volume        = {471},
  address       = {Spindleruv Mlyn, Czech republic},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 14 papers. The topics discussed include: speeding up shortest path search in public transport networks; from web pages to web communities; benchmarking coding algorithms for the R-tree compression; compression of the stream array data structure; Tequila - a query language for the semantic web; XML-&lambda; type system and data model revealed; efficiency improvement of narrow range query processing in R-tree; using top trees for easy programming of tree algorithms; various aspects of user preference learning and recommender systems; translation of ontology retrieval problem into relational queries; combination of TA- and MD-algorithm for efficient solving of top-K problem according to user's preferences; dimension reduction methods for iris recognition; five-level multi-application schema evolution; and the BPM to UML activity diagram transformation using XSLT.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {16130073},
  journal       = {CEUR Workshop Proceedings},
  language      = {English},
}

@InProceedings{2017,
  title         = {29th International Conference on Advanced Information Systems Engineering, CAiSE 2017},
  year          = {2017},
  volume        = {10253 LNCS},
  pages         = {1 - 649},
  address       = {Essen, Germany},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 39 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Information systems for retail companies; information systems for retail companies; understanding the blockchain using enterprise ontology; accommodating openness requirements in software platforms; development of mobile data collection applications by domain experts; checking process compliance on the basis of uncertain event-to-activity mappings; aligning modeled and observed behavior; multi-party business process resilience by design; identifying domains and concepts in short texts via partial taxonomy and unlabeled data; user interests clustering in business intelligence interactions; analysis of online discussions in support of requirements discovery; discovering causal factors explaining business process performance variation; enriching decision making with data based thresholds of process-related KPIS; characterizing drift from event streams of business processes; massively distributed environments and closed itemset mining; uncovering the runtime enterprise architecture of a large distributed organisation; summarisation and relevance evaluation techniques for big data exploration; instance based process matching using event-log information; analyzing process variants to understand differences in key performance indices; discovering hierarchical consolidated models from process families; survival in schema evolution; on the similarity of process change operations; agile transformation success factors; structural descriptions of process models based on goal-oriented unfolding; aligning textual and graphical descriptions of processes through ILP techniques; use cases for understanding business process models and predictive business process monitoring considering reliability estimates.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Kimura1996,
  author        = {Kimura, Yutaka and Tsuruoka, Kunitoshi},
  title         = {On introducing flexible object structures to a C++-based object-oriented database programming language, PERCIO/C++},
  journal       = {NEC Research \&amp; Development},
  year          = {1996},
  volume        = {37},
  number        = {2},
  pages         = {267 - 274},
  issn          = {00480436},
  note          = {Flexible object structures;Object oriented database;Variable length object;},
  __markedentry = {[Juliana:6]},
  abstract      = {Variable-length objects are urgently required in recent information-intensive and internet-intensive application domains, such as office information system, multimedia document system, and Web software. Furthermore, in these fields, the schema tends to be changed. This paper proposes one approach to integrating a variable-length object concept into C++-based object-oriented database systems. In addition, schema evolution, based on the variable-length object, is proposed. These features are introduced while keeping C++ specification, without affecting applications, and without performance degradation by changing schema. These features have been implemented for an NEC's commercial object-oriented database management system, called PERCIO.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {C (programming language)},
  keywords      = {Database systems;Information retrieval systems;Object oriented programming;},
  language      = {English},
}

@InProceedings{Koch2000,
  author        = {Koch, Christoph and Kovacs, Zsolt and Le Goff, Jean-Marie and McClatchey, Richard and Petta, Paolo and Solomonides, Tony},
  title         = {Explicit modeling of the semantics of large multi-layered object-oriented databases},
  year          = {2000},
  volume        = {1920},
  pages         = {52 - 65},
  address       = {Salt Lake City, UT, United states},
  note          = {Database applications;Database schemas;Design Patterns;Explicit modeling;Framework-based approach;Large amounts of data;Large-scale objects;Reusable softwares;},
  __markedentry = {[Juliana:6]},
  abstract      = {Description-driven systems based on meta-objects are an increasingly popular way to handle complexity in large-scale object- oriented database applications. Such systems facilitate the management of large amounts of data and provide a means to avoid database schema evolution in many settings. Unfortunately, the description-driven approach leads to a loss of simplicity of the schema, and additional software behaviour is required for the management of dependencies, description relationships, and other Design Patterns that recur across the schema. This leads to redundant implementations of software that cannot be handled by using a framework-based approach. This paper presents an approach to address this problem which is based on the concept of an ontology of Design Patterns. Such an ontology allows the convenient separation of the structure and the semantics of database schemata. Through that, reusable software can be produced which separates application behaviour from the database schema.<br/> &copy; Springer-Verlag Berlin Heidelberg 2000.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Object-oriented databases},
  keywords      = {Application programs;Computer software reusability;Data mining;Information management;Object oriented programming;Ontology;Relational database systems;Semantics;},
  language      = {English},
}

@InProceedings{Favre2007,
  author        = {Favre, Cecile and Bentayeb, Fadila and Boussaid, Omar},
  title         = {Evolution of data warehouses' optimization: A workload perspective},
  year          = {2007},
  volume        = {4654 LNCS},
  pages         = {13 - 22},
  address       = {Regensburg, Germany},
  note          = {Optimization strategy;Query rewriting;Workload;},
  __markedentry = {[Juliana:6]},
  abstract      = {Data warehouse (DW) evolution usually means evolution of its model. However, a decision support system is composed of the DW and of several other components, such as optimization structures like indices or materialized views. Thus, dealing with the DW evolution also implies dealing with the maintenance of these structures. However, propagating evolution to these structures thereby maintaining the coherence with the evolutions on the DW is not always enough. In some cases propagation is not sufficient and redeployment of optimization strategies may be required. Selection of optimization strategies is mainly based on workload, corresponding to user queries. In this paper, we propose to make the workload evolve in response to DW schema evolution. The objective is to avoid waiting for a new workload from the updated DW model. We propose to maintain existing queries coherent and create new queries to deal with probable future analysis needs. &copy; Springer-Verlag Berlin Heidelberg 2007.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Data warehouses},
  keywords      = {Data structures;Optimization;Query processing;},
  language      = {English},
}

@InProceedings{Halevy2003,
  author        = {Halevy, Alon Y. and Ives, Zachary G. and Suciu, Dan and Tatarinov, Igor},
  title         = {Schema mediation in peer data management systems},
  year          = {2003},
  pages         = {505 - 516},
  address       = {Bangalore, India},
  note          = {Data integration;Peer data management system;Schema mediation;},
  __markedentry = {[Juliana:6]},
  abstract      = {Intuitively, data management and data integration tools should be well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: they typically require a comprehensive schema design before they can be used to store or share information, and they are difficult to extend because schema evolution is heavyweight and may break backwards compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by non-database-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: we propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peers' schemas. PDMS represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peers' individual schemas. This paper considers the problem of schema mediation in a PDMS. Our first contribution is a flexible language for mediating between peer schemas, which extends known data integration formalisms to our more complex architecture. We precisely characterize the complexity of query answering for our language. Next, we describe a reformulation algorithm for our language that generalizes both global-as-view and local-as-view query answering algorithms. Finally, we describe several methods for optimizing the reformulation algorithm, and an initial set of experiments studying its performance.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - International Conference on Data Engineering},
  key           = {Distributed database systems},
  keywords      = {Algorithms;Computational complexity;Electronic document exchange;HTML;Optimization;Query languages;Semantics;World Wide Web;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/ICDE.2003.1260817},
}

@InProceedings{Berdaguer2007,
  author        = {Berdaguer, Pablo and Cunha, Alcino and Pacheco, Hugo and Visser, Joost},
  title         = {Coupled schema transformation and data conversion for XML and SQL},
  year          = {2007},
  volume        = {4354 LNCS},
  pages         = {290 - 304},
  address       = {Nice, France},
  note          = {Data transformation;Functional languages;Haskell;Level transformation;Schema transformation;Structural information;Transformation;XML schema evolutions;},
  __markedentry = {[Juliana:6]},
  abstract      = {A two-level data transformation consists of a type-level transformation of a data format coupled with value-level transformations of data instances corresponding to that format. We have implemented a system for performing two-level transformations on XML schemas and their corresponding documents, and on SQL schemas and the databases that they describe. The core of the system consists of a combinator library for composing type-changing rewrite rules that preserve structural information and referential constraints. We discuss the implementation of the system's core library, and of its SQL and XML front-ends in the functional language Haskell. We show how the system can be used to tackle various two-level transformation scenarios, such as XML schema evolution coupled with document migration, and hierarchical-relational data mappings that convert between XML documents and SQL databases. &copy; Springer-Verlag Berlin Heidelberg 2007.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Metadata},
  keywords      = {Data handling;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-540-69611-7-19},
}

@Article{Vincent2000,
  author        = {Vincent, Millist W. and Levene, Mark},
  title         = {Restructuring partitioned normal form relations without information loss},
  journal       = {SIAM Journal on Computing},
  year          = {2000},
  volume        = {29},
  number        = {5},
  pages         = {1550 - 1567},
  issn          = {00975397},
  note          = {Nest;},
  __markedentry = {[Juliana:6]},
  abstract      = {Nested relations in partitioned normal form (PNF) are an important subclass of nested relations that are useful in many applications. In this paper we address the question of determining when every PNF relation stored under one nested relation scheme can be transformed into another PNF relation stored under a different nested relation scheme without loss of information, referred to as the two schemes being data equivalent. This issue is important in many database application areas such as view processing, schema integration, and schema evolution. The main result of the paper provides two characterizations of data equivalence for nested schemes. The first is that two schemes are data equivalent if and only if the two sets of multivalued dependencies induced by the two corresponding scheme trees are equivalent. The second is that the schemes are equivalent if and only if the corresponding scheme trees can be transformed into the other by a sequence of applications of a local restructuring operator and its inverse.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Relational database systems},
  keywords      = {Optimization;Query languages;User interfaces;},
  language      = {English},
  url           = {http://dx.doi.org/10.1137/S0097539797326319},
}

@InProceedings{Zamboulis2006,
  author        = {Zamboulis, Lucas and Fan, Hao and Belhajjame, Khalid and Siepen, Jennifer and Jones, Andrew and Martin, Nigel and Poulovassilis, Alexandra and Hubbard, Simon and Embury, Suzanne M. and Paton, Norman W.},
  title         = {Data access and integration in the ISPIDER proteomics Grid},
  year          = {2006},
  volume        = {4075 LNBI},
  pages         = {3 - 18},
  address       = {Hinxton, United kingdom},
  note          = {Data integration;Grid data access;Grid environment;Proteomics data resources;},
  __markedentry = {[Juliana:6]},
  abstract      = {Grid computing has great potential for supporting the integration of complex, fast changing biological data repositories to enable distributed data analysis. One scenario where Grid computing has such potential is provided by proteomics resources which are rapidly being developed with the emergence of affordable, reliable methods to study the proteome. The protein identifications arising from these methods derive from multiple repositories which need to be integrated to enable uniform access to them. A number of technologies exist which enable these resources to be accessed in a Grid environment, but the independent development of these resources means that significant data integration challenges, such as heterogeneity and schema evolution, have to be met. This paper presents an architecture which supports the combined use of Grid data access (OGSA-DAI), Grid distributed querying (OGSA-DQP) and data integration (AutoMed) software tools to support distributed data analysis. We discuss the application of this architecture for the integration of several autonomous proteomics data resources. &copy; Springer-Verlag Berlin Heidelberg 2006.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Distributed computer systems},
  keywords      = {Computer architecture;Data reduction;Molecular biology;Proteins;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/11799511_3},
}

@InProceedings{Kuramitsu2000,
  author        = {Kuramitsu, K. and Sakamura, K.},
  title         = {Distributed object-oriented schema for XML-based electronic catalog sharing semantics among businesses},
  year          = {2000},
  volume        = {1},
  pages         = {87 - 96},
  address       = {Hong Kong, China},
  note          = {Distributed objects;Distributed schemas;Electronic catalogs;Internet services;Semantic interoperability;Semantic relationships;Service integration;XML-based languages;},
  __markedentry = {[Juliana:6]},
  abstract      = {Internet commerce is increasing the demands of service integrations by sharing XML-based catalogs. We propose the PCO (Portable Compound Object) data model supporting semantic inheritance to ensure the synonymy of heterogeneous semantics among distributed schemas that different authors define independently. Also, the PCO model makes semantic relationships independent of an initial class hierarchy, and it enables rapid schema evolution across the entire Internet business. This preserves semantic interoperability without changing pre-defined classes. We have also encoded the PCO model into two XML-based languages: the PCO Specification Language (PSL) and the Portable Composite Language (PCL). This paper demonstrates that intermediaries defining service semantics in PSL can automatically integrate multiple suppliers' PCL catalogs for their agent-mediated services.<br/> &copy; 2000 IEEE.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the 1st International Conference on Web Information Systems Engineering, WISE 2000},
  key           = {Semantic Web},
  keywords      = {Electronic commerce;Information systems;Information use;Specification languages;Systems engineering;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/WISE.2000.882379},
}

@Article{Du2001,
  author        = {Du, Timon C. and Wu, Jen-Long},
  title         = {Using object-oriented paradigm to develop an evolutional vehicle routing system},
  journal       = {Computers in Industry},
  year          = {2001},
  volume        = {44},
  number        = {3},
  pages         = {229 - 249},
  issn          = {01663615},
  note          = {Evolutional vehicle routing systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {Since the customer requirements and implementation environments change rapidly, the conventional application models have difficulties in satisfying the needs of users. Fortunately, the object-oriented paradigm represents things in terms of objects, and has the advantages of integrating application functions and data management. This advantage provides the evolutional features. Therefore, this study adopts a component assembly model to develop an evolutional system development process including database schema evolution, application object reuse, and the integration. Since a vehicle routing system is a computer system that uses a database to maintain data and application functions to implement delivering algorithms that focus on how to deliver customer orders under different circumstances, the evolution capability is demonstrated in various vehicle routing problems. &copy; 2001 Elsevier Science B.V.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Intelligent vehicle highway systems},
  keywords      = {Computer systems programming;Information technology;Management information systems;Object oriented programming;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/S0166-3615(01)00073-2},
}

@Article{Dahchour2002,
  author        = {Dahchour, Mohamed and Pirotted, Alain and Zimanyi, Esteban},
  title         = {Materialization and its metaclass implementation},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  year          = {2002},
  volume        = {14},
  number        = {5},
  pages         = {1078 - 1094},
  issn          = {10414347},
  note          = {Conceptual modeling;Metaclass;},
  __markedentry = {[Juliana:6]},
  abstract      = {Materialization is a powerful and ubiquitous abstraction pattern for conceptual modeling that relates a class of categories (e.g., models of cars) and a class of more concrete objects (e.g., individual cars). This paper presents materialization as a generic relationship between two classes of objects and describes an abstract implementation of it. The presentation is abstract in that it is not targeted at a specific object system. The target system is supposed to provide: 1) basic object-modeling facilities, supplemented with an explicit metaclass concept and 2) operations for dynamic schema evolution like creation or deletion of a subclass of a given class and modification of the type of an attribute of a class. The presentation is generic in that the semantics of materialization is implemented in a metaclass, which is a template to be instantiated in applications. Application classes are created as instances of the metaclass and they are thereby endowed with structure and behavior consistent with the generic semantics of materialization.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Metadata},
  keywords      = {Computer software;Mathematical models;Object oriented programming;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/TKDE.2002.1033775},
}

@InProceedings{2015c,
  title         = {18th East European Conference on Advances in Databases and Information Systems and Associated Satellite Events, ADBIS 2014},
  year          = {2015},
  volume        = {312},
  pages         = {ICT-ACT Association, Municipality Ohrid; Ministry of Information Society and Administration; University Ss Cyril and Methodius in Skopje -},
  address       = {Ohrid, Macedonia},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 26 papers. The special focus in this Conference is on Advances in Databases and Information Systems and Associated Satellite Events. The topics include: Benchmarking graph databases on the problem of community detection; efficient processing of streams of frequent item set queries; a content-driven ETL processes for open data; secure data storage and exchange with a private wallet; flexs a logical model for physical data layout; reasoning over spatial orientation relations using rules; integrated representation of temporal intervals and durations for the semantic web; partitioning for scalable complex event processing on data streams; improving high-performance GPU graph traversal with compression; GPU-accelerated quantification filters for analytical queries in multidimensional databases; linked open data for medical institutions and drug availability lists in Macedonia; technologies for databases change management; factors that influence the quality of crowdsourcing; framework for social media big data quality analysis; querying and managing complex data; querying and managing complex data and data warehouse schema evolution perspectives.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {21945357},
  journal       = {Advances in Intelligent Systems and Computing},
  language      = {English},
}

@InProceedings{1994c,
  title         = {4th International Conference on Extending Database Technology, EDBT 1994},
  year          = {1994},
  volume        = {779 LNCS},
  pages         = {1 - 406},
  address       = {Cambridge, United kingdom},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 33 papers. The special focus in this conference is on Extending Database Technology. The topics include: The object management group standardization of object technology; type derivation using the projection operation; subsumption between queries to object-oriented databases; matrix relation for statistical database management; deductive database support for data visualization; subsumption-free bottom-up evaluation of logic programs with partially instantiated data structures; virtual schemas and bases; role-based query processing in multidatabase systems; content routing for distributed information servers; a unified approach to concurrency control and transaction recovery; algorithms for flexible space management in transaction systems supporting fine-granularity locking; indexing alternatives for multiversion locking; a rule-based approach for the design and implementation of information systems; on behavioral schema evolution in object-oriented databases; representing and using performance requirements during the development of information systems; a model-theoretic semantics of the multilevel relational model; correctness of ISA hierarchies in object-oriented database schemas; power efficient filtering of data on air; video information contents and architecture; optimizing storage of objects on mass storage systems with robotic devices; on the estimation of join result sizes; DBJ - a dynamic balancing hash join algorithm in multiprocessor database systems; tabu search optimization of large join queries; the implementation and performance evaluation of the ADMS query optimizer; optimization of nested queries in a complex object model; a multi-threaded architecture for prefetching in object bases and supporting full-text information retrieval with a persistent object store.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Blakeley1994,
  author        = {Blakeley, Jose A.},
  title         = {Open object database management systems},
  year          = {1994},
  volume        = {23},
  number        = {2},
  pages         = {520 -},
  address       = {Minneapolis, MN, USA},
  note          = {Multimedia systems;Versioning models;},
  __markedentry = {[Juliana:6]},
  abstract      = {Object database management systems (OODBs) are providing the data management solution for applications in computer integrated manufacturing, office information systems, and multimedia. This tutorial presents an overview of the concepts and issues involved in the design of open object DBMSs including object-relational DBMSs. Topics to be covered include the motivation for object data management and open architectures; a comparison of three object data management approaches: persistent programming languages, extended relational DBMSs, and extensible toolkits; an overview of concepts and implementation issues including persistence models, object query processing, storage management, versioning models, and schema evolution; a discussion of the challenges in building object-relational DBMSs for each of the four approaches described above; an overview of object services architectures (e.g., a comparison of existing products and research systems; and a look at ODMG-93, SQL3 and OMG CORBA and OSA standards.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
  key           = {Relational database systems},
  keywords      = {Computer integrated manufacturing;Management;Object oriented programming;Query languages;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/191843.191989},
}

@InProceedings{Hakim1994,
  author        = {Hakim, M.Maher and Garrett Jr., James H.},
  title         = {Modeling engineering design information: An object-centered approach},
  year          = {1994},
  number        = {1},
  pages         = {563 - 570},
  address       = {Washington, DC, USA},
  note          = {Description logic;Knowledge representation;},
  __markedentry = {[Juliana:6]},
  abstract      = {The benefits of applying object-oriented modeling techniques to represent engineering design information are now well recognized. Various approaches for modeling engineering design data and knowledge based on those techniques have been recently proposed. We argue that the object-oriented paradigm, which is class-centered, is inadequate for capturing the dynamic nature of engineering design data and knowledge because of its lack of support for schema evolution, object evolution, representing partial and inconsistent information, and representing semantic and user-defined relationships. We propose an object-centered approach for modeling engineering design information based on description logic. Description Logic is a knowledge representation paradigm which uses a `description language' to represent knowledge and data and a `description classifier' to make inferences from these descriptions. Description logic languages support the intentional definitions of classes, relationships, and objects. In this paper, we present the advantages of using description logic for representing engineering data and knowledge. We discuss how a description logic implementation of the object-centered approach can be used in two areas which deals with engineering information: 1) the representation of engineering design product models during design, and 2) the representation of the knowledge contained in engineering design standards and specifications.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Computing in Civil Engineering (New York)},
  key           = {Computer aided engineering},
  keywords      = {Computational linguistics;Computer aided design;Data structures;High level languages;Logic programming;Object oriented programming;Product design;},
  language      = {English},
}

@InProceedings{McClatchey1997,
  author        = {McClatchey, R. and Estrella, F. and Le Goff, J.-M. and Kovacs, Z. and Baker, N.},
  title         = {Object databases in a distributed scientific workflow application},
  year          = {1997},
  pages         = {11 - 21},
  address       = {Biarritz, Fr},
  note          = {Object databases;Scientific workflow management systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {The construction of large scientific experiments (such as the Compact Muon Solenoid experiment (CMS) being undertaken for CERN) necessitates the use of complex production management operations often distributed over many geographically separated research institutes. These workflow operations are often only loosely-defined at the outset of the construction and can evolve rapidly as the results of experiments are analyzed. Existing commercial workflow management systems are largely based on relational database management systems (RDBMSs) and are unable to cope with the requirements of such scientific workflow environments and, in particular, with the dynamic schema evolution which results from the rapidly evolving scientific workflow definitions. This paper reports on the requirements for object repositories in the implementation of a prototype scientific workflow management system entitled CRISTAL and considers issues surrounding data duplication between object repositories and scientific workflow management in CRISTAL.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the Basque International Workshop on Information Technology, BIWIT},
  key           = {Relational database systems},
  keywords      = {Distributed computer systems;Object oriented programming;Production control;},
  language      = {English},
}

@InProceedings{2016a,
  title         = {27th International Conference on Database and Expert Systems Applications, DEXA 2016},
  year          = {2016},
  volume        = {9827 LNCS},
  pages         = {1 - 448},
  address       = {Porto, Portugal},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 27 papers. The special focus in this conference is on Temporal, Spatial, High Dimensional Databases, Data Mining, Authenticity, Privacy, Security, and Trust. The topics include: Target-oriented keyword search over temporal databases; general purpose index-based method for efficient MAXRS query; an efficient method for identifying MAXRS location in mobile ad hoc networks; discovering periodic-frequent patterns in transactional databases using all-confidence and periodic-all-confidence; more efficient algorithms for mining high-utility itemsets with multiple minimum utility thresholds; mining minimal high-utility itemsets; context-based risk-adaptive security model and conflict management; modeling information diffusion via reputation estimation; mining arbitrary shaped clusters and outputting a high quality dendrogram; hierarchically clustered LSH for hierarchical outliers detection; incorporating clustering into set similarity join algorithms; a query processing framework for array-based computations; result of inference and machine learning integration; a reverse nearest neighbor based active semi-supervised learning method for multivariate time series classification; leveraging structural hierarchy for scalable network comparison; incremental stream processing of nested-relational queries; incremental continuous query processing over streams and relations with isolation guarantees; an improved method of keyword search over relational data streams by aggressive candidate network consolidation; enhancing data abstraction through database modularization to achieve graceful schema evolution; querying multigraphs via efficient indexing; re-constructing hidden semantic data models by querying SPARQL endpoints; a new formal approach to semantic parsing of instructions and to file manager design and ontology-based deep restricted Boltzmann machine.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Dadam2009,
  author        = {Dadam, Peter and Reichert, Manfred},
  title         = {The ADEPT project: A decade of research and development for robust and flexible process support : Cllenges and Achievements},
  journal       = {Computer Science - Research and Development},
  year          = {2009},
  volume        = {23},
  number        = {2},
  pages         = {81 - 97},
  issn          = {18652034},
  note          = {Business process management;Correctness by construction;Process change;Process flexibility;Workflow managements;},
  __markedentry = {[Juliana:6]},
  abstract      = {This paper gives insights into the ADEPT project. Its target was to develop a next generation process management technology, which is by orders of magnitudes more powerful and flexible than contemporary process management systems. The ADEPT technology should provide advanced features and properties within one system, which seem to exclude each other, but which are required for the support of a broad spectrum of processes: ease-of-use for end users and system developers, high flexibility through the support of non-trivial ad-hoc deviations at the process instance level, quick implementation of process changes through process schema evolution, and correctness guarantees enabling robust execution of implemented processes. This paper describes the background and the real-world cases which motivated our research. It further explains the technological challenges we faced, describes the solutions we elaborated, and discusses the current status of the ADEPT project. &copy; 2009 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Administrative data processing},
  keywords      = {Enterprise resource management;Human computer interaction;Robustness (control systems);Work simplification;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/s00450-009-0068-6},
}

@Article{Spooner1997,
  author        = {Spooner, David L. and Hardwick, Martin},
  title         = {Using views for product data exchange},
  journal       = {IEEE Computer Graphics and Applications},
  year          = {1997},
  volume        = {17},
  number        = {5},
  pages         = {58 - 65},
  issn          = {02721716},
  note          = {Express V programming language;Mapping schema;Product data exchange;},
  __markedentry = {[Juliana:6]},
  abstract      = {A database view, used in conjunction with data exchange standards, can facilitate the sharing of product model data between software tools in designing and manufacturing computing environments. Views simplify the use of a database and provide an advantage in responding to schema evolution of databases and software application systems. A view mechanism closely related to a data exchange standard enhances the standard by simplifying the software tool development for exploiting the standard. The roles of views range from direct participation in data exchange functions to development of indeces and infrastructures for locating data prior to data exchange.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Computer aided design;Computer aided manufacturing;Computer aided software engineering;Computer programming languages;Computer simulation;Data communication systems;Data structures;Distributed computer systems;Indexing (of information);Information retrieval;Product design;Standards;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/38.610208},
}

@InProceedings{Li2004,
  author        = {Li, Quanzhong and Kim, Michelle Y. and So, Edward and Wood, Steve},
  title         = {XVM: XML virtual machine},
  year          = {2004},
  volume        = {2},
  pages         = {1732 - 1733},
  address       = {Nicosia, Cyprus},
  note          = {Corporate information systems;XML applications;XML virtual machines (XVM);XVM;},
  __markedentry = {[Juliana:6]},
  abstract      = {XML is an emerging standard for data representation and data exchange on the Internet. XML-based web applications have been widely used in e-commerce and enterprise information management. In this paper, we propose an extensible, integrated XML processing architecture, the XML Virtual Machine (XVM). The XVM provides a framework for processing XML data, developing and deploying XML-based applications. By using a component-based technique, the XVM provides a high degree of modularity and reusability. XVM components are dynamically loaded and composed during XML data processing. New components can be easily added to existing applications and new applications can reuse existing components without difficulty. These features enable an XML application to keep up with requirements and schema evolution and to process compound documents. Both client-side and server-side XML applications can be developed and deployed in an integrated way. Also in this paper, we present an XML application container built on top of the XVM, along with several sample applications.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the ACM Symposium on Applied Computing},
  key           = {Electronic document exchange},
  keywords      = {Computer aided design;Computer architecture;Computer programming languages;System program documentation;Virtual storage;XML;},
  language      = {English},
}

@InProceedings{Atzeni2009,
  author        = {Atzeni, Paolo and Bellomarini, Luigi and Bugiotti, Francesca and Gianforme, Giorgio},
  title         = {MISM: A platform for model-independent solutions to model management problems},
  year          = {2009},
  volume        = {5880 LNCS},
  pages         = {133 - 161},
  note          = {Data translations;Database problems;Engineering problems;Forward engineerings;Manipulation operators;Model management;Roundtrip engineerings;Schema management;},
  __markedentry = {[Juliana:6]},
  abstract      = {Model management is a metadata-based approach to database problems aimed at supporting the productivity of developers by providing schema manipulation operators. Here we propose MISM (Model Independent Schema Management), a platform for model management offering a set of operators to manipulate schemas, in a manner that is both model-independent (in the sense that operators are generic and apply to schemas of different data models) and model-aware (in the sense that it is possible to say whether a schema is allowed for a data model). This is the first proposal for model management in this direction. We consider the main operators in model management: merge, diff, and modelgen. These operators play a major role in solving various problems related to schema evolution (such as data integration, data exchange or forward engineering), and we show in detail a solution to a major representative of the class, the round-trip engineering problem. &copy; 2009 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Problem solving},
  keywords      = {Data integration;Electronic data interchange;Engineering education;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-642-10562-3_5},
}

@InProceedings{Wade1994,
  author        = {Wade, Andrew E.},
  title         = {ODBMS role in 64 bit distributed client-server computing},
  year          = {1994},
  pages         = {603 - 608},
  address       = {Boston, MA, USA},
  note          = {Computing environment;Distributed client server computing;Downsizing;Manufacturing information systems;Multiple databases;Schemas;},
  __markedentry = {[Juliana:6]},
  abstract      = {For full distribution, single logical view to objects anywhere, from anywhere, Objectivity/DB is introduced which is based on 64 bits, providing access to millions of tera-objects, each of which may be many gigabytes. Support for production environments includes multiple schemas, which may be shared among databases or private, encrypted schemas, dynamic addition of schemas, and schema evolution. Integration must include legacy data bases, such RDBMSs, and must cooperate with standards such as ODMG-93 and the OMG COBRA. Finally, the logical view must remain valid, and applications must continue to work, as the mapping to the physical environment changes, moving objects and databases to new platforms.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Electro International, Conference Proceedings},
  key           = {Distributed database systems},
  keywords      = {Computer architecture;Computer operating systems;Computer workstations;Data processing;Data storage equipment;Graphical user interfaces;Network protocols;Operations research;Personal computers;Standards;},
  language      = {English},
}

@Article{Guerrini1997,
  author        = {Guerrini, Giovanna and Bertino, Elisa and Catania, Barbara and Garcia-Molina, Jesus},
  title         = {Formal model of views for object-oriented database systems},
  journal       = {Theory and Practice of Object Systems},
  year          = {1997},
  volume        = {3},
  number        = {3},
  pages         = {157 - 183},
  issn          = {10743227},
  note          = {Object oriented database systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {The definition of a view mechanism is an important issue for object-oriented database systems, in order to provide a number of features that are crucial for the development of advanced applications. Due to the complexity of the data model, the object-oriented paradigm introduces new problems in the definition of a view mechanism. Several approaches have been defined, each defining a particular view mechanism tailored to a set of functionalities that the view mechanism should support. In particular, views can be used as shorthand in queries, can support the definition of external schemas, can be used for content-dependent authorization, and, finally, can support some form of schema evolution. In this paper, we formally introduce a view model for object-oriented databases. Our view model is comparable to existing view models for what concerns the supported features; however, our model is the only one for which a formal definition is given. This formal definition of object-oriented view mechanisms is useful both for understanding what views are and as a basis for further investigations on view properties. The paper introduces the model, discussing all the supported features both from a theoretical and practical point of view. A comparison of our model with other models is also presented.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Data structures;Object oriented programming;},
  language      = {English},
  url           = {http://dx.doi.org/10.1002/(SICI)1096-9942(1997)3:3<157::AID-TAPO1>3.0.CO;2-R},
}

@Article{Jones1998,
  author        = {Jones, R. and Mapelli, L. and Ryabov, Yu. and Soloviev, I.},
  title         = {OKS persistent in-memory object manager},
  journal       = {IEEE Transactions on Nuclear Science},
  year          = {1998},
  volume        = {45},
  number        = {4 pt 1},
  pages         = {1958 - 1964},
  issn          = {00189499},
  note          = {Object Kernel support (OKS);Object manager;},
  __markedentry = {[Juliana:6]},
  abstract      = {The OKS (Object Kernel Support) is a library to support a simple, active persistent in-memory object manager. It is suitable for applications which need to create persistent structured information with fast access but do not require full database functionality. It can be used as the frame of configuration databases and real-time object managers for Data Acquisition and Detector Control Systems in such fields as setup, diagnostics and general configuration description. OKS is based on an object model that supports objects, classes, associations, methods, inheritance, polymorphism, object identifiers, composite objects, integrity constraints, schema evolution, data migration and active notification. OKS stores the class definitions and their instances in portable ASCII files. It provides query facilities, including indices support. The OKS has a C++ API (Application Program Interface) and includes Motif based GUI applications to design class schema and to manipulate objects. OKS has been developed on top of the Rogue Wave Tools.h++ C++ class library.},
  address       = {Beaune, Fr},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Object oriented programming},
  keywords      = {C (programming language);Database systems;Graphical user interfaces;Nuclear physics;SCADA systems;},
  language      = {English},
}

@Article{Tsotras1995,
  author        = {Tsotras, Vassilis J. and Gopinath, B. and Hart, George W.},
  title         = {Efficient management of time-evolving databases},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  year          = {1995},
  volume        = {7},
  number        = {4},
  pages         = {591 - 608},
  issn          = {10414347},
  note          = {Evolving systems;Object oriented databases;Temporal databases;Time evolving databases;Versioning;},
  __markedentry = {[Juliana:6]},
  abstract      = {Efficiently managing the history of a time-evolving system is one of the central problems in many database environments, like database systems that incorporate versioning, or object-oriented databases that implicitly or explicitly maintain the history of persistent objects. In this paper we propose algorithms that reconstruct past states of an evolving system for two general cases, i.e., when the system's state is represented by a set or by a hierarchy (a forest of trees). Sets are widely used as a canonical form of representing information in databases or program states. For more complex applications, like schema evolution in object-oriented databases, it becomes necessary to manage the history of data structures that have the form of forests or even graphs. The proposed algorithms use minimal space (proportional to the number of changes occurring in the evolution) and have the advantage of being on-line (in the amortized sense). Any past system state s(t) is reconstructed in time O(|s(t)| + loglogT), where |s(t)| is the size of the answer and T is the maximal evolution time. For all practical cases the loglogT factor is a constant, therefore our algorithms provide almost random access to any past system state. Moreover, we show that the presented algorithms are optimal among all algorithms that use space linear in the number of changes in the system's evolution.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Algorithms;Data structures;Object oriented programming;Query languages;Set theory;Software engineering;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/69.404032},
}

@InProceedings{Schleifer1996,
  author        = {Schleifer, Paul and Sun, Yuan and Patel, Dilip},
  title         = {The implementation of a chronicle collection class in Smalltalk/DB},
  year          = {1996},
  volume        = {Part F128723},
  pages         = {209 - 215},
  address       = {Philadelphia, PA, United states},
  note          = {Chronicle;Single inheritance;Time sequences;Transaction time;Valid time;},
  __markedentry = {[Juliana:6]},
  abstract      = {In this paper, we present the implementation of a compact temporal object model based on time sequences as a general purpose collection class, called the Chronic le, in the Smalltalk/DB language. This provides an object database with the potential to express temporality and so creates a semantically rich environment for implementing applications that model temporally variable objects. We describe the concepts of the Chronicle class and give a detailed description of its components. This implementation supports the time-stamping of objects with transaction times and valid times, and does not depend on multiple inheritance as a mechanism for conferring temporality. Our implementation creates no additional barriers to schema evolution and instance migration, and storage performance is enhanced by the sharing of components that specify temporal semantics. However, because the classes of objects stored in a Chronicle are not known in advance, a comprehensive set of query methods cannot be generated and many queries must be created as low level expressions.<br/> &copy; 1996 ACM.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings of the ACM Symposium on Applied Computing},
  key           = {Stamping},
  keywords      = {Encapsulation;Object-oriented databases;Query processing;Semantics;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/331119.331177},
}

@InProceedings{Schwarz1994,
  author        = {Schwarz, Peter and Shoens, Kurt},
  title         = {Managing change in the Rufus system},
  year          = {1994},
  pages         = {170 - 179},
  address       = {Houston, TX, USA},
  note          = {Conformity based data model;Database management systems;Information system;Melampus data model;Minimum disruption;Rufus system;Schema changes;User data;},
  __markedentry = {[Juliana:6]},
  abstract      = {Rufus is an information system that models user data with objects taken from a class system. Due to the importance of coping with changes to the schema, Rufus has adopted the conformity-based model of Melampus [12]. This model enables Rufus to cope with schema changes more easily than traditional class- and inheritance-based data models. This paper reviews the Melampus data model and describes how we implemented it in the Rufus system. We show how changes to the schema can be accommodated with minimum disruption. We also review design decisions that contributed to streamlined schema evolution and compare our approach with those proposed in the literature.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Proceedings - International Conference on Data Engineering},
  key           = {Data structures},
  keywords      = {Data acquisition;Data storage equipment;Decision theory;Query languages;Storage allocation (computer);Systems analysis;},
  language      = {English},
}

@InProceedings{Zugal2012,
  author        = {Zugal, Stefan and Pinggera, Jakob and Weber, Barbara},
  title         = {Toward enhanced life-cycle support for declarative processes},
  year          = {2012},
  volume        = {24},
  number        = {3},
  pages         = {285 - 302},
  note          = {Business Process;Business process management;Business process model;Process model understandability;Process Modeling;},
  __markedentry = {[Juliana:6]},
  abstract      = {The need for flexible process-aware information systems resulted in a recent interest in declarative approaches, as they promise a high degree of flexibility. However, the potential of current declarative approaches is impeded by deficiencies in understandability and maintainability. This paper proposes an approach toward better understandability and maintainability of declarative processes by adopting wellestablished techniques from the domain of software engineering. More specifically, the ideas of test-driven development and automated acceptance testing are adopted to interweave process specification and process testing. Thereby, during modeling, testcases balance the circumstantial/sequential information mismatch as well as improve understandability by dispensing with hard mental operations and removing hidden dependencies. Because testcases are also understandable to domain experts, they foster communication between domain experts and model builders, providing a common basis for communication. During process execution, testcases, in turn, help to document the reasons for process deviations and ensure that respective deviations can be easily considered during schema evolution. Furthermore, testcases ensure that no undesired behavior is introduced through process adaptations. Copyright &copy; 2011 John Wiley &amp; Sons, Ltd.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {20477481},
  journal       = {Journal of software: Evolution and Process},
  key           = {Acceptance tests},
  keywords      = {Administrative data processing;Enterprise resource management;Life cycle;Maintainability;Software engineering;},
  language      = {English},
  url           = {http://dx.doi.org/10.1002/smr.554},
}

@Article{Yang2014,
  author        = {Yang, Haijun and Qi, Shu and Li, Minqiang},
  title         = {The performance of cohort genetic algorithms on royal road function and multi-modal functions},
  journal       = {Journal of Computational and Theoretical Nanoscience},
  year          = {2014},
  volume        = {11},
  number        = {8},
  pages         = {1817 - 1825},
  issn          = {15461955},
  note          = {Algorithm performance;Genetic algorithm (GAs);Improve performance;Maintaining population diversity;Multi-modal functions;Performance;Population diversity;Pre-mature convergences;},
  __markedentry = {[Juliana:6]},
  abstract      = {The hitchhiking is a well-known phenomenon for global optimization by genetic algorithms (GAs). This is a crucial factor which induces premature convergence in GAs. Cohort genetic algorithm (CGA) is a multipopulation algorithm proposed by Holland, which is used to explore search spaces for building blocks by Hyper-plane Defined Functions. In this paper, the convergence proof of CGA is presented. The parameter control of CGA is the key to improve performance of the algorithm. Moreover, maintaining population diversity is an important strategy to avoid premature convergence and escape from local optimum. So, different values of parameters are checked to measure the algorithm performance. In this paper, on the basis of a novel definition of population diversity, CGA is used to optimize the royal road function (RR) and multi-modal functions. The results show that the performance of CGA on multi-modal functions is better than on RR. Furthermore, an explanation is addressed for this phenomenon in the view of schema evolution. Copyright &copy; 2014 American Scientific Publishers.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Parameter estimation},
  keywords      = {Genetic algorithms;Global optimization;Roads and streets;},
  language      = {English},
  url           = {http://dx.doi.org/10.1166/jctn.2014.3573},
}

@Article{Bhuiyan2009,
  author        = {Bhuiyan, Mohammad Masumuzzaman and Latiful Hoque, Abu Sayed Md},
  title         = {High performance SQL queries on compressed relational database},
  journal       = {Journal of Computers},
  year          = {2009},
  volume        = {4},
  number        = {12},
  pages         = {1263 - 1274},
  issn          = {1796203X},
  note          = {Compressed database;Database applications;High performance query;Microsoft SQL Server;Performance improvements;Query response time;Relational Database;Storage requirements;},
  __markedentry = {[Juliana:6]},
  abstract      = {Loss-less data compression is potentially attractive in database application for storage cost reduction and performance improvement. The existing compression architectures work well for small to large database and provide good performance. But these systems can execute a limited number of queries executed on single table. We have developed a disk-based compression architecture, called DHIBASE, to support large database and at the same time, perform high performance SQL queries on single or multiple tables in compressed form. We have compared our system with widely used Microsoft SQL Server. Our system performs significantly better than SQL Server in terms of storage requirement and query response time. DHIBASE requires 10 to 15 times less space and for some operation it is 18 to 22 times faster. As the system is column oriented, schema evolution is easy. &copy; 2009 ACADEMY PUBLISHER.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Query languages},
  keywords      = {Cost reduction;Digital storage;Query processing;Relational database systems;Windows operating system;},
  language      = {English},
  url           = {http://dx.doi.org/10.4304/jcp.4.12.1263-1274},
}

@Article{Jarke1995,
  author        = {Jarke, Matthias and Gallersdoerfer, Rainer and Jeusfeld, Manfred A. and Staudt, Martin and Eherer, Stefan},
  title         = {ConceptBase - a deductive object base for meta data management},
  journal       = {Journal of Intelligent Information Systems},
  year          = {1995},
  volume        = {4},
  number        = {2},
  pages         = {167 - 192},
  issn          = {09259902},
  note          = {Client server architecture;Conceptual modeling;Deductive databases;Graph oriented;Integrity constraints;Logic oriented;Query classes;Semantics;},
  __markedentry = {[Juliana:6]},
  abstract      = {Deductive object bases attempt to combine the advantages of deductive relational databases with those of object-oriented databases. We review modeling and implementation issues encountered during the development of ConceptBase, a prototype deductive object manager supporting the Telos object model. Significant features include: 1) The symmetric treatment of object-oriented, logic-oriented and graph-oriented perspectives, 2) an infinite metaclass hierarchy as a prerequisite for extensibility and schema evolution, 3) a simple yet powerful formal semantics used as the basis for implementation, 4) a client-server architecture supporting collaborative work in a wide-area setting. Several application experiences demonstrate the value of the approach especially in the field of meta data management.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database systems},
  keywords      = {Computational linguistics;Computer architecture;Computer simulation;Data structures;Equivalence classes;Formal logic;Hierarchical systems;Knowledge representation;Object oriented programming;Query languages;Relational database systems;},
  language      = {English},
}

@InProceedings{Wagner1994,
  author        = {Wagner, Flavio R. and Golendziner, Lia G. and Fornari, Miguel R.},
  title         = {Tightly coupled approach to design and data management},
  year          = {1994},
  pages         = {194 - 199},
  address       = {Grenoble, Fr},
  note          = {Data integrity;STAR EDA framework;},
  __markedentry = {[Juliana:6]},
  abstract      = {This paper describes the tight coupling between design and data modeling and management facilities in the STAR EDA framework. STAR implements an innovative and flexible data model that allows the user to define, for each object type, a schema of the design alternatives and views to be created during the design process. Alternatives and views can be hierarchically related through an inheritance mechanism in order to establish arbitrary data integrity constraints that must be followed when new object descriptions are generated. On-the-fly extensions and modifications to each object schema are supported by a schema evolution mechanism. The evolutionary schemata of alternatives and views and the integrity constraints that are established through the object schemata build the basis for a design methodology management mechanism.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {European Design Automation Conference - Proceedings},
  key           = {Database systems},
  keywords      = {Computational linguistics;Computer aided design;Constraint theory;Data handling;Data structures;Hierarchical systems;Information management;Mathematical models;},
  language      = {English},
}

@InProceedings{Kapros2014,
  author        = {Kapros, Evangelos and McGinnes, Simon},
  title         = {Updating database schemas without breaking the UI: Modeling using cognitive semantic categories},
  year          = {2014},
  pages         = {23 - 31},
  address       = {Rome, Italy},
  note          = {Adaptive information systems;Automatically generated;Cognitive semantics;Data independence;Management functions;Programming skills;Usability testing;Web-based applications;},
  __markedentry = {[Juliana:6]},
  abstract      = {Data management user interfaces are ubiquitous in information systems and web-based applications. From the oldest spreadsheet to the most modern database, end users and administrators alike have interacted with tabular data. Usually, each concept is represented by a table and columns. Change to the structure of each concept requires structural change to the tables and columns, which is costly. Tailor-made database and web applications may overcome this obstacle by designing UIs on top of the data layer, providing some degree of data independence. However, changes in their schemas do not automatically propagate into the user interface, and so their maintenance is expensive. In this paper we present a user interface that lets the end user alter the schema without the need for programming skills, eliminating the need for expensive software maintenance. To this end we propose an automatically generated user interface to include schema and data management functions. We built and evaluated an Adaptive Information System user interface (AIS UI), incorporating schema evolution functionality. In usability testing, firsttime users were able to perform various data management tasks equally fast or faster than users using Microsoft Access, and on average &tilde;43% faster than users using Microsoft Excel. Task completion rates using the AIS significantly exceeded those using Microsoft Access and were comparable (&gt;95%) with those using Microsoft Excel. Copyright &copy; 2014 ACM 978-1-4503-2725-1/14/06.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {EICS 2014 - Proceedings of the 2014 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
  key           = {User interfaces},
  keywords      = {Database systems;Information management;Information systems;Information use;Semantics;Spreadsheets;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/2607023.2607027},
}

@Article{Chung1992,
  author        = {Chung, Yunkung and Fischer, Gary W.},
  title         = {Illustration of object-oriented databases for the structure of a bill of materials},
  journal       = {Computers in Industry},
  year          = {1992},
  volume        = {19},
  number        = {3},
  pages         = {257 - 270},
  issn          = {01663615},
  note          = {Bill of materials;Material requirements planning (MRP);Object oriented database (OODB);},
  __markedentry = {[Juliana:6]},
  abstract      = {In this tutorial paper, the basic concepts of applying an object-oriented database (OODB) system, called ORION, to the management of a bill of materials (BOM) is addressed. The illustration attempts to demonstrate the linkage between the needs of a CAD/CAM environment and the production planning environment with specific reference to the material requirements planning (MRP) system of the future. The structure of a composite object made up of objects is the major consideration of the proposed object-oriented BOM (OOBOM). The concentration is on composite objects in the OOBOM schema evolution, their propagations of changing object instances, and the unique object identity of each manufacturing part. The proposed OOBOM system is rudimentary, and may be embodied by using the Itasca<sup>TM</sup> system, which is a commercial OODB system whose prototype is ORION, or by using the C<sup>++</sup> language. In this paper, both viewpoints, ORION and C<sup>++</sup>, are discussed.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Industrial management},
  keywords      = {Computer aided design;Computer program listings;Computer programming;Database systems;Inventory control;Management;Object oriented programming;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/0166-3615(92)90063-S},
}

@InProceedings{Rinderle2005,
  author        = {Rinderle, Stefanie and Weber, Barbara and Reichert, Manfred and Wild, Werner},
  title         = {Integrating process learning and process evolution - A semantics based approach},
  year          = {2005},
  volume        = {3649},
  pages         = {252 - 267},
  address       = {Nancy, France},
  note          = {Case-based reasoning (CBR);Process evolution;Process learning;Process-aware information systems (PAIS);},
  __markedentry = {[Juliana:6]},
  abstract      = {Companies are developing a growing interest in aligning their information systems in a process-oriented way. However, current process-aware information systems (PAIS) fail to meet process flexibility requirements, which reduces the applicability of such systems. To overcome this limitation PAIS should capture the whole process life cycle and all kinds of changes in an integrated way. In this paper we present such a holistic approach providing full process life cycle support by combining the ADEPT framework for dynamic process changes with the concepts and methods provided by case-based reasoning (CBR) technology. This allows expressing the semantics of process changes, their memorization and their reuse to perform similar changes in the future. If the same or similar process instance changes occur frequently, potential process type changes are suggested to the process engineer. The process engineer can then perform a schema evolution and migrate running instances to the new schema version by using the ADEPT framework. Finally, the case-base related to the old schema version is migrated as well. &copy; Springer-Verlag Berlin Heidelberg 2005.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science},
  key           = {Learning systems},
  keywords      = {Data storage equipment;Information management;Integration;Life cycle;Process engineering;Semantics;},
  language      = {English},
}

@InProceedings{Amirian2014,
  author        = {Amirian, Pouria and Basiri, Anahid and Winstanley, Adam},
  title         = {Evaluation of data management systems for geospatial big data},
  year          = {2014},
  volume        = {8583 LNCS},
  number        = {PART 5},
  pages         = {678 - 690},
  address       = {Guimaraes, Portugal},
  note          = {Column family database;Document database;Geo-spatial;Geo-spatial data;Spatial database;},
  __markedentry = {[Juliana:6]},
  abstract      = {Big Data encompasses collection, management, processing and analysis of the huge amount of data that varies in types and changes with high frequency. Often data component of Big Data has a positional component as an important part of it in various forms, such as postal address, Internet Protocol (IP) address and geographical location. If the positional components in Big Data extensively used in storage, retrieval, analysis, processing, visualization and knowledge discovery (geospatial Big Data) the Big Data systems need certain type of techniques and algorithms for management, analytics and sharing. This paper describes the concept of geospatial Big Data management with focus on using typical and modern database management systems. Then the typical and modern types of databases for management of geospatial Big Data are evaluated based on model for storage, query languages, handling connected data, distribution models and schema evolution. As the results of the evaluations and benchmarks of this paper illustrate there is no single solution for efficient management of geospatial Big Data and in order to utilize unique characteristics of geospatial Big Data (such as topological, directional and distance relationship) a polyglot geospatial data persistence system is needed. &copy; 2014 Springer International Publishing.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Big data},
  keywords      = {Data handling;Data mining;Data visualization;Digital storage;Graph Databases;Information management;Internet protocols;Query languages;Query processing;Search engines;Storage management;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-319-09156-3_47},
}

@InProceedings{1999a,
  title         = {1st International Conference on Data Warehousing and Knowledge Discovery, DaWaK 1999},
  year          = {1999},
  volume        = {1676},
  pages         = {1 - 399},
  address       = {Florence, Italy},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 41 papers. The special focus in this conference is on Data Warehouse Design and On-Line Analytical Processing. The topics include: Dynamic data warehouse design; star/snow-flake schema driven object-relational data warehouse design and query processing strategies.; the design and implementation of modularized wrappers/monitors in a data warehouse; managing meta objects for design of warehouse data; dealing with complex reports in OLAP applications; OLAP-based scalable profiling of customer behavior; compressed datacubes for fast OLAP applications; an approach to efficient implementation for the data warehouse architecture; on the independence of data warehouse from databases in maintaining join views; heuristic algorithms for designing a data warehouse with SPJ views; a framework for optimizing incremental view maintenance at data warehouses; genetic algorithm for materialized view selection in data warehouse environments; optimization of sequences of relational queries in decision-support environments; dynamic data warehousing; set-derivability of multidimensional aggregates; using the real dimension of the data; on schema evolution in multidimensional databases; lazy aggregates for real-time OLAP; incremental refinement of mining queries; a data structure for data mining; a new approach for the discovery of frequent itemsets; k-means clustering algorithm for categorical attributes and considering main memory in mining association rules.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Hamill2008,
  author        = {Hamill, P. and Cranshaw, J. and Malon, D. and Vaniachine, A.},
  title         = {Petaminer: Efficient navigation to petascale data using event-level metadata},
  year          = {2008},
  volume        = {70},
  address       = {Erice, Italy},
  note          = {Analysis frameworks;Data architectures;Feasibility studies;High-rate streaming;Optimal operation;Private companies;Relational Database;Storage and retrievals;},
  __markedentry = {[Juliana:6]},
  abstract      = {MySQL is a standards-compliant, high-performance, relational database with a pluggable storage engine architecture that enables third parties to create low-level interfaces to data stored in different formats, permitting more optimal operation in various use scenarios. The Petaminer is a joint project with a private company whose purpose is to exploit a feature of MySQL to provide a direct "MySQL interface" to ROOT files. We report on the development of a custom MySQL storage engine to directly access the LHC experiment's event TAG metadata stored in ROOT files. ROOT is a structured, extensible analysis framework and data architecture for storage and retrieval of arbitrarily scaled, distributed data. ROOT supports high-rate streaming of data as well as storage distributed across multiple file systems and domains. It was designed to contain physics event data among other uses, and so has numerous features that make it ideal for this purpose, including hierarchical, column-oriented data structures, dynamic schema evolution, and support for binary float numbers. The ability to access column-oriented event data stored in distributed ROOT files using standard MySQL tools offers the potential to improve the efficiency and accessibility of metadata-keyed queries for LHC data analysis. The Petaminer project demonstrated feasibility of using ROOT TTree files as a pluggable storage engine for MySQL. We present first results of our feasibility studies of creating a columnoriented MySQL storage engine that uses the ROOT API to access TAG metadata directly from ROOT files, and uses ROOT-FastBit to map MySQL indexing operations to FastBit indices. The prototype storage engine demonstrates both query functionality and improved performance. As a result, Petaminer enhances MySQL with column-oriented storage capable to support binary float numbers with bitmap indexing. Work is in progress to extend the functionalities of the Petaminer software.<br/> &copy; 2008 Sissa Medialab Srl. All rights reserved.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {18248039},
  journal       = {Proceedings of Science},
  key           = {Engines},
  keywords      = {Indexing (of information);Metadata;Query processing;Virtual storage;},
  language      = {English},
}

@InProceedings{Peng2010,
  author        = {Peng, Zhiyong and Wang, Hui and Peng, Yuwei and Xu, Bo and Huang, Zeqian},
  title         = {A three layer system architecture for web-based unstructured data management},
  year          = {2010},
  pages         = {447 - 450},
  address       = {Busan, Korea, Republic of},
  note          = {Advanced functions;Dynamic classification;Dynamic management;Meta search engines;Personalized service;Semantic relationships;System architectures;Three-layer systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {With the rapid growing of data on Web, we are facing three serious problems. Firstly, there are a huge number of data resources which are heterogeneous and dynamic. Secondly, most of data on Web are unstructured. Thirdly, there are various kinds ofWeb users who have different interests and requirements. In this paper, we proposed a new system architecture for unstructured data management on Web to solve these problems by integrating data spaces, database and meta search engine. The system architecture consists of three layers for data gathering on demand, dynamic management and personalized service, respectively. Data servicing layer allows Web users to create data spaces with advanced functions to manipulate and access Web data, eg, cross media query and automatic recommendation. Data managing layer models both Web data and their semantic relationships using our object deputy database named as TOTEM; it also supports schema evolution and dynamic classification. Data gathering layer extracts user's interest from his or her data space; gathers the related data on Web and further analyzes their semantic relationships. Finally, we implemented a prototype system Tmusic based on this new system architecture to show its availability. &copy; 2010 IEEE.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {Advances in Web Technologies and Applications - Proceedings of the 12th Asia-Pacific Web Conference, APWeb 2010},
  key           = {Information management},
  keywords      = {Classification (of information);Memory architecture;Query processing;Search engines;Semantic Web;Semantics;Websites;},
  language      = {English},
  url           = {http://dx.doi.org/10.1109/APWeb.2010.21},
}

@Article{Li1991,
  author        = {Li, Q.},
  title         = {Extending semantic object model. Towards more unified view of information objects},
  journal       = {Information and Software Technology},
  year          = {1991},
  volume        = {33},
  number        = {2},
  pages         = {106 - 113},
  issn          = {09505849},
  note          = {Diagonal Uniformity;Horizontal Uniformity;Information Management;Object Databases;Semantic Modelling;Vertical Uniformity;},
  __markedentry = {[Juliana:6]},
  abstract      = {Object-based approaches to information management exhibits uniform treatment of all information objects. Two kinds of uniformity supported by existing models and systems can be classified: 'horizontal' and 'vertical'. The former is realised by the single concept of object for modelling all conceptual entities at various level of complexity and abstraction. The latter is embodied by the notion of class hierarchy and inheritance of attributes and instances along the class hierarchy. The paper demonstrates that a third kind of uniformity, termed 'diagonal' uniformity, is also desired by many applications. By allowing inter-mixing and inter-changing among different modelling constructs, the system exhibits a more consistent and uniform view of all information objects, and provides more flexible and powerful modelling capabilities. Such a feature is especially needed in areas such as schema evolution, information sharing, and integration among multiple databases. A framework for supporting such diagonal uniformity is described.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Database Systems},
  keywords      = {Artificial Intelligence;Computer Science;Information Theory;Mathematical Models;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/0950-5849(91)90055-G},
}

@Article{Lakshmanan1997a,
  author        = {Lakshmanan, Laks V.S. and Sadri, Fereidoon and Subramanian, Iyer N.},
  title         = {Logic and Algebraic languages for interoperability in multidatabase systems},
  journal       = {Journal of Logic Programming},
  year          = {1997},
  volume        = {33},
  number        = {2},
  pages         = {X - 149},
  issn          = {07431066},
  __markedentry = {[Juliana:6]},
  abstract      = {Developing a declarative approach to interoperability in the context of multidatabase systems is a major goal of this research. We take a first step toward this goal in this paper, by developing a simple logic called SchemaLog which is syntactically higher-order but has a first-order semantics. SchemaLog can provide for interoperability among multiple relational databases in a federation of database systems. We develop a fixpoint theory for the definite clause fragment of SchemaLog and show its equivalence to the model-theoretic semantics. We also develop a sound and complete proof procedure for all clausal theories. We establish the correspondence between SchemaLog and first-order predicate calculus and provide a reduction of SchemaLog to predicate calculus. We propose an extension to classical relational algebra, capable of retrieving and manipulating data and schema from databases in a multidatabase system, and prove its equivalence to a form of relational calculus inspired by SchemaLog syntax. We illustrate the simplicity and power of SchemaLog with a variety of applications involving database programming (with schema browsing), schema integration, schema evolution, cooperative query answering, and sophisticated forms of aggregation in the spirit of OLAP (On-Line Analytical Processing). We also highlight our implementation of SchemaLog realized on a federation of INGRES databases. &copy; Elsevier Science Inc., 1997.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/S0743-1066(96)00146-X},
}

@InProceedings{Moussouni2007,
  author        = {Moussouni, F. and Berti-Equille, L. and Roze, G. and Loreal, O. and Guerin, E.},
  title         = {QDex: A database profiler for generic bio-data exploration and quality aware integration},
  year          = {2007},
  volume        = {4832 LNCS},
  pages         = {5 - 16},
  address       = {Nancy, France},
  note          = {Bio-data integration;Data quality;Databanks;Database profiling;},
  __markedentry = {[Juliana:6]},
  abstract      = {In human health and life sciences, researchers extensively collaborate with each other, sharing genomic, biomedical and experimental results. This necessitates dynamically integrating different databases into a single repository or a warehouse. The data integrated in these warehouses are extracted from various heterogeneous sources, having different degrees of quality and trust. Most of the time, they are neither rigorously chosen nor carefully controlled for data quality. Data preparation and data quality metadata are recommended but still insufficiently exploited for ensuring quality and validating the results of information retrieval or data mining techniques. In a previous work, we built a data warehouse called GEDAW (Gene Expression Data Warehouse) that stores various information: data on genes expressed in the liver during iron overload and liver diseases, relevant information from public databanks (mostly in XML), DNA-chips home experiments and also medical records. Based on our past experience, this paper reports briefly on the lessons learned from biomedical data integration and data quality issues, and the solutions we propose to the numerous problems of schema evolution of both data sources and warehousing system. In this context, we present QDex, a Quality driven bio-Data Exploration tool, which provides a functional and modular architecture for database profiling and exploration, enabling users to set up query workflows and take advantage of data quality profiling metadata before the complex processes of data integration in the warehouse. An illustration with QDex Tool is shown afterwards. &copy; Springer-Verlag Berlin Heidelberg 2007.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Metadata},
  keywords      = {Bioinformatics;Computer architecture;Data mining;User interfaces;Warehouses;},
  language      = {English},
}

@InProceedings{Tamzalit1999,
  author        = {Tamzalit, Dalila and Oussalah, Chabane},
  title         = {From object evolution to object emergence},
  year          = {1999},
  pages         = {514 - 521},
  address       = {Kansas City, MO, USA},
  note          = {Genetic evolution object model;},
  __markedentry = {[Juliana:6]},
  abstract      = {Database applications which model aspects of the real world should be able to express as accurately as possible the different nuances of reality; that includes the need to evolve internally in response to signals of updates coming from the environment. These updates are not always supplied in an ideal and complete manner and are not always predefined or precisely defined. In practice, requirements for evolution generally occur during the manipulation of objects while running the database. It is frequently necessary to change individual objects, less frequently the database schema. Database systems need to have mechanisms capable, whenever and as well as possible, of assimilating this new information correctly and diagnosing and implementing the changes necessary. This paper concerns the evolution of objects inside databases. Our two main objectives are: to allow objects to evolve their structures dynamically during database maintenance and use, with all necessary impacts on the database schema; to allow, similarly, the creation and display of different plans for evolving the design, like ways of schema evolution, giving in this way a simulation tool for database design and maintenance. So, we propose a Genetic Evolution Object Model developed to have inherent capabilities for auto-adaptation between classes and instances.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  journal       = {International Conference on Information and Knowledge Management, Proceedings},
  key           = {Database systems},
  keywords      = {Computer simulation;Data structures;Mathematical models;},
  language      = {English},
}

@InProceedings{2003a,
  title         = {9th International Conference on Object-Oriented Information Systems, OOIS 2003},
  year          = {2003},
  volume        = {2817},
  pages         = {1 - 425},
  address       = {Geneve, Switzerland},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 42 papers. The special focus in this conference is on Evolution of OOIS and OOIS Frameworks. The topics include: Agent oriented software development; model driven architecture; once upon a time a DTD evolved into another DTD; evolution of collective object behavior in presence of simultaneous client-specific views; evolving derived entity types in conceptual schemas in the UML; object-oriented graceful evolution monitors; stepwise and rigorous development of evolving concurrent information systems; a requirements elicitation approach in the context of system evolution; UML-based metamodeling for information system engineering and evolution; building a wizard for framework instantiation based on a pattern language; event-based software architectures; aided domain frameworks construction and evolution; a contract-based approach of resource management in information systems; representing user-interface patterns in UML; accommodating changing requirements with EJB; a framework for supporting views in component oriented information systems; enabling design evolution in software through pattern oriented approach; extracting domain-specific and domain-neutral patterns using software stability concepts; designing storage structures for management of materialised methods in object-oriented databases; overcoming the complexity of object-oriented DBMS metadata management and primitive operations for schema evolution in ODMG databases.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@Article{Nash2006,
  author        = {Nash, Alan},
  title         = {Foundations of information integration},
  journal       = {ProQuest Dissertations and Theses Global},
  year          = {2006},
  note          = {Composition problem;Core computation;Embedded dependencies;Equality-generating dependency;Exchange problems;Information integration;Tuple-generating dependencies;Universal solutions;},
  __markedentry = {[Juliana:6]},
  abstract      = {We study three fundamental problems in information integration: (1) the data integration query problem, (2) the data exchange core computation problem, and (3) the schema mapping composition problem. The first problem consists of computing the certain answers to a query over a target schema for a source instance under constraints which relate the source and target schemas. We show how to compute certain answers for a larger family of constraints and queries than those previously addressed. One of the main tools is the chase, which we study and extend significantly. The second problem deals with inserting data from one database into another database having a different schema. Fagin, Kolaitis, and Popa have shown that among the universal solutions of a solvable data exchange problem, there exists---up to isomorphism---a most compact one, "the core", and have convincingly argued that this core should be the database to be materialized. We show how to compute the core in the general setting where the mapping between the source and target schemas is given by source-to-target constraints which are arbitrary tuple generating dependencies (TGDs) and target constraints consisting of equality generating dependencies (EGDs) and weakly-acyclic TGDs. The third problem, composition of mappings between schemas, is essential to support schema evolution, data exchange, data integration, and other data management tasks. We study the issues involved in composing schema mappings given by embedded dependencies that need not be source-to-target and we concentrate on obtaining (first-order) embedded dependencies. We provide a composition algorithm and several negative results. In particular, we show that even full dependencies that are not limited to be source-to-target are not closed under composition and that determining whether the composition can be given by these kinds of dependencies is undecidable. These negative results carry over to mappings given by embedded dependencies. ProQuest Subject Headings: Mathematics, Computer science.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Data integration},
  keywords      = {Computer science;Database systems;Electronic data interchange;Information management;Information retrieval;Mapping;Mathematical techniques;Problem solving;Query processing;},
  language      = {English},
}

@Article{Cunha2011,
  author        = {Cunha, Alcino and Visser, Joost},
  title         = {Transformation of structure-shy programs with application to XPath queries and strategic functions},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {6},
  pages         = {516 - 539},
  issn          = {01676423},
  note          = {Program calculation;Program transformations;Type generalization;Type-specialization;XML query language;},
  __markedentry = {[Juliana:6]},
  abstract      = {Various programming languages allow the construction of structure-shy programs. Such programs are defined generically for many different datatypes and only specify specific behavior for a few relevant subtypes. Typical examples are XML query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags. Other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object-oriented programming. In this paper, we present an algebraic approach to transformation of declarative structure-shy programs, in particular for strategic functions and XML queries. We formulate a rich set of algebraic laws, not just for transformation of structure-shy programs, but also for their conversion into structure-sensitive programs and vice versa. We show how subsets of these laws can be used to construct effective rewrite systems for specialization, generalization, and optimization of structure-shy programs. We present a type-safe encoding of these rewrite systems in Haskell which itself uses strategic functional programming techniques. We discuss the application of these rewrite systems for XPath query optimization and for query migration in the context of schema evolution. &copy; 2010 Elsevier B.V. All rights reserved.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Object oriented programming},
  keywords      = {Algebra;Application programs;Functional programming;Query languages;Structural optimization;XML;},
  language      = {English},
  url           = {http://dx.doi.org/10.1016/j.scico.2010.01.003},
}

@InProceedings{Wade1993,
  author        = {Wade, Andrew E.},
  title         = {Single logical view over enterprise-wide distributed databases},
  year          = {1993},
  volume        = {22},
  number        = {2},
  pages         = {441 - 444},
  address       = {Washington, DC, USA},
  note          = {Enterprise wide distributed databases;Object database management system;Objectivity;Schema;Single logical view;},
  __markedentry = {[Juliana:6]},
  abstract      = {Two trends in today's corporate world demand distribution: downsizing from centralized mainframe single database environments; and wider integration, connecting finance, engineering, manufacturing information systems for enterprise-wide modeling and operations optimization. The resulting environment consists of multiple databases, at the group level, department level, and corporate level, with the need to manage dependencies among data in all of them. The solution is full distribution, providing a single logical view to objects anywhere, from anywhere. Users see a logical model of objects connected to objects, with atomic transactions and propagating methods, even if composite objects are split among multiple databases, each under separate administrative control, on multiple, heterogeneous platforms, operating systems, and network protocols. Support for production environments includes multiple schemas, which may be shared among databases, private, or encrypted, dynamic addition of schemas, and schema evolution. Finally, the logical view must remain valid, and applications must continue to work, as the mapping to the physical environment changes, moving objects and databases to new platforms.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {01635808},
  journal       = {SIGMOD Record (ACM Special Interest Group on Management of Data)},
  key           = {Distributed database systems},
  keywords      = {C (programming language);Computer operating systems;Computer simulation;Data structures;Management information systems;Network protocols;Object oriented programming;Optimization;},
  language      = {English},
  url           = {http://dx.doi.org/10.1145/170036.170128},
}

@Article{Bentayeb2008,
  author        = {Bentayeb, Fadila and Favre, Cecile and Boussaid, Omar},
  title         = {A user-driven data warehouse evolution approach for concurrent personalized analysis needs},
  journal       = {Integrated Computer-Aided Engineering},
  year          = {2008},
  volume        = {15},
  number        = {1},
  pages         = {21 - 36},
  issn          = {10692509},
  note          = {Global analysis;Knowledge integration;Rule-based Data Warehouse (R-DW);},
  __markedentry = {[Juliana:6]},
  abstract      = {Data warehouses store aggregated data issued from different sources to meet users' analysis needs for decision support. The nature of the work of users implies that their requirements are often changing and do not reach a final state. Therefore, a data warehouse cannot be designed in one step, usually it evolves over the time. In this paper, we propose a user-driven approach that enables a data warehouse schema update. It consists in integrating the users' knowledge in the data warehouse modeling to allow new analysis possibilities. More precisely, we consider the specific users' knowledge, which defines new aggregated data, under the form of "if-then" rules that we call aggregation rules. These rules are used to dynamically create new granularity levels in dimension hierarchies, following an automatic and concurrent way. Our approach is composed of four phases: (1) users' knowledge acquisition, (2) knowledge integration, (3) data warehouse schema update, and (4) on-line analysis. To support our approach, we define a Rule-based Data Warehouse (R-DW) model composed of two parts: one "fixed" part and one "evolving" part. The fixed part corresponds to the initial data warehouse schema, whose purpose is to provide an answer to global analysis needs. The evolving part is defined by means of aggregation rules, which allow personalized analyses. To validate our approach, we developed a prototype called WEDriK (data Warehouse Evolution Driven by Knowledge), in which the R-DW model is implemented within the Oracle 10g DBMS. We also present how to achieve our approach by proposing a model dedicated to the management of the data warehouse schema evolution and the updates' algorithms. Furthermore, we applied our approach on banking data of the French bank LCL-Le Cre&acute;dit Lyonnais and we illustrate our purpose with the LCL case study. &copy; 2008 - IOS Press and the author(s). All rights reserved.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  key           = {Data warehouses},
  keywords      = {Algorithms;Data storage equipment;Decision support systems;Knowledge based systems;Mathematical models;Software prototyping;},
  language      = {English},
}

@InProceedings{1998b,
  title         = {17th International Conference on Conceptual Modeling, ER 1998},
  year          = {1998},
  volume        = {1507},
  pages         = {1 - 480},
  address       = {Singapore, Singapore},
  __markedentry = {[Juliana:6]},
  abstract      = {The proceedings contain 36 papers. The special focus in this conference is on Conceptual Modeling and Design. The topics include: The rise, fall and return of software industry in japan; conceptual design and development of information services; an EER-based conceptual model and query language for time-series data; a conceptual design framework for temporal entities; lessons to be learned from database schema methodology; formalizing the informational content of database user interfaces; a conceptual-modeling approach to extracting data from the web; information coupling in web databases; structure-based queries over the world wide web; integrated approach for modelling of semantic and pragmatic dependencies of information systems; inference of aggregate relationships through database reverse engineering; on the consistency of int-cardinality constraints; web sites need models and schemes; a process modeling and analysis tool environment; an evaluation of two approaches to exploiting real-world knowledge by intelligent database design tools; metrics for evaluating the quality of entity relationship models; a transformational approach to correct schema re-nements; improving the quality of entity relationship models; the TROLL approach to conceptual modelling; process failure in a rapidly changing high-tech organisation; a real deductive object-oriented database language and multiobjects to ease schema evolution in an OODBMS.},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  language      = {English},
}

@InProceedings{Buchner2010,
  author        = {Buchner, Thomas and Matthes, Florian and Neubert, Christian},
  title         = {Data model driven implementation of web cooperation systems with Tricia},
  year          = {2010},
  volume        = {6348 LNCS},
  pages         = {70 - 84},
  note          = {Application developers;Domain specific languages;Model driven approach;Separation of concerns;System implementation;WEB application;Web frameworks;Web information systems;},
  __markedentry = {[Juliana:6]},
  abstract      = {We present the data modeling concepts of Tricia, an open-source Java platform used to implement enterprise web information systems as well as social software solutions including wikis, blogs, file shares and social networks. Tricia follows a data model driven approach to system implementation where substantial parts of the application semantics are captured by domain-specific models (data model, access control model and interaction model). In this paper we give an overview of the Tricia architecture and development process and present the concepts of its data model: plugins, entities, properties, roles, mixins, validators and change listeners are motivated and described using UML class diagrams and concrete examples from Tricia projects. We highlight the benefits of this data modeling framework for application developers (expressiveness, modularity, reuse, separation of concerns) and show its impact on user-related services (content authoring, integrity checking, link management, queries and search, access control, tagging, versioning, schema evolution and multilingualism). This provides the basis for a comparison with other model based approaches to web information systems. &copy; 2010 Springer-Verlag.<br/>},
  copyright     = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
  issn          = {03029743},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  key           = {Open systems},
  keywords      = {Access control;Application programs;Data structures;Information analysis;Information systems;Information use;Modeling languages;Network architecture;Open source software;Problem oriented languages;Semantics;Social sciences computing;Software architecture;Software engineering;},
  language      = {English},
  url           = {http://dx.doi.org/10.1007/978-3-642-16092-9_9},
}

@Comment{jabref-meta: databaseType:bibtex;}
