@INPROCEEDINGS{6676935, 
author={M. {Gobert} and J. {Maes} and A. {Cleve} and J. {Weber}}, 
booktitle={2013 IEEE International Conference on Software Maintenance}, 
title={Understanding Schema Evolution as a Basis for Database Reengineering}, 
year={2013}, 
volume={}, 
number={}, 
pages={472-475}, 
abstract={Software repositories can provide valuable information for facilitating software reengineering efforts. In recent years, many researchers have started to follow a holistic approach, considering diverse software artifacts and the links existing between them. However, when analyzing data-intensive systems, comparatively little attention has been devoted to the analysis of an important system artifact: the database. Even fewer approaches attempt to uncover facts about the evolution history of database schemas. We have developed a tool-supported method for analyzing and visualizing database schema history. This paper reports early results of applying and validating this method. We discuss our experiences to date and point out several novel research perspectives in this domain.}, 
keywords={configuration management;data analysis;data mining;data visualisation;database management systems;software maintenance;systems re-engineering;Schema evolution;database reengineering;software repositories;software reengineering efforts;diverse software artifacts;data-intensive system analysis;tool-supported method;database schema history analysis;database schema history visualization;evolution history;Databases;Silicon;Software;History;Cloning;Visualization;Tin;data reengineering;mining software repositories;schema evolution}, 
doi={10.1109/ICSM.2013.75}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{1698423, 
author={H. {Bounif} and R. {Pottinger}}, 
booktitle={17th International Workshop on Database and Expert Systems Applications (DEXA'06)}, 
title={Schema Repository for Database Schema Evolution}, 
year={2006}, 
volume={}, 
number={}, 
pages={647-651}, 
abstract={The paper presents a schema repository, an original repository containing different kinds of database schemas. The repository is part of a multidisciplinary approach for schema evolution called the predictive approach for database evolution. The schema repository has a dual role in the approach: (1) During the data-mining process, the repository identifies and analyzes trends on collected schemas belonging to the same domain. (2) The repository is used in the building of the requirements ontology - a domain ontology that contributes in the database design and its evolution. This paper presents both the design and a heuristic-based method to populate such a repository}, 
keywords={data mining;database management systems;ontologies (artificial intelligence);schema repository;database schema evolution;predictive database evolution;data mining;requirements ontology;Databases;Ontologies;Enterprise resource planning;Information systems;Information management;Paper technology;Buildings;Internet;Costs;Humans}, 
doi={10.1109/DEXA.2006.125}, 
ISSN={1529-4188}, 
month={Sep.},}
@INPROCEEDINGS{4493341, 
author={G. {Papastefanatos} and F. {Anagnostou} and Y. {Vassiliou} and P. {Vassiliadis}}, 
booktitle={2008 12th European Conference on Software Maintenance and Reengineering}, 
title={Hecataeus: A What-If Analysis Tool for Database Schema Evolution}, 
year={2008}, 
volume={}, 
number={}, 
pages={326-328}, 
abstract={Databases are continuously evolving environments, where design constructs are added, removed or updated rather often. Small changes in the database configurations might impact a large number of applications and data stores around the system: queries and data entry forms can be invalidated, application programs might crash. HECATAEUS is a tool, which represents the database schema along with its dependent workload, mainly queries and views, as a uniform directed graph. The tool enables the user to create hypothetical evolution events and examine their impact over the overall graph as well as to define rules so that both syntactical and semantic correctness of the affected workload is retained.}, 
keywords={database management systems;directed graphs;program diagnostics;Hecataeus;what-if analysis tool;database schema evolution;database configurations;queries;data entry forms;uniform directed graph;Data analysis;EMP radiation effects;Computer crashes;Spatial databases;Database systems;Computer science;Information systems;Web server;Performance analysis;Predictive models}, 
doi={10.1109/CSMR.2008.4493341}, 
ISSN={1534-5351}, 
month={April},}
@INPROCEEDINGS{4624724, 
author={G. {Guerrini} and M. {Mesiti}}, 
booktitle={2008 19th International Workshop on Database and Expert Systems Applications}, 
title={X-Evolution: A Comprehensive Approach for XML Schema Evolution}, 
year={2008}, 
volume={}, 
number={}, 
pages={251-255}, 
abstract={In this paper we present X-Evolution, a Web system developed on top of a commercial DBMS that allows the specification of schema modifications both on a graphical representation of an XML Schema and through a specifically tailored declarative language. X-Evolution supports facilities for performing schema revalidation only when strictly needed and only on the minimal parts of documents affected by the modifications. Moreover, it supports the automatic and query-based adaptation of original schema instances to the evolved schema.}, 
keywords={Internet;query processing;XML;X-Evolution;XML schema evolution;Web system;query-based adaptation;XML;Evolution (biology);Postal services;Chemical elements;Servers;Navigation;Databases;XML;XML schema evolution;document adaptation}, 
doi={10.1109/DEXA.2008.128}, 
ISSN={1529-4188}, 
month={Sep.},}
@INPROCEEDINGS{777976, 
author={ and R. {Elmasri}}, 
booktitle={Proceedings. Sixth International Workshop on Temporal Representation and Reasoning. TIME-99}, 
title={Study and comparison of schema versioning and database conversion techniques for bi-temporal databases}, 
year={1999}, 
volume={}, 
number={}, 
pages={88-98}, 
abstract={Schema evolution and schema versioning are two techniques used for managing database evolution. Schema evolution keeps only the current version of a schema and database after applying schema changes. Schema versioning creates new schema versions and converts the corresponding data while preserving the old schema versions and data. To provide the most generality, bi-temporal databases can be used to realize schema versioning, since they allow both retroactive and proactive updates to the schema and database. We first study two proposed database conversion approaches for supporting schema evolution and schema versioning: single table version approach and multiple table version approach. We then propose the partial table version approach to solve the problems encountered in these approaches when applied to bi-temporal databases.}, 
keywords={temporal databases;database theory;schema versioning;database conversion;bi-temporal databases;schema evolution;database evolution;retroactive updates;proactive updates;single table version approach;multiple table version approach;Relational databases;Database systems;Application software;Data engineering;Engineering management;Electrical capacitance tomography;Banking}, 
doi={10.1109/TIME.1999.777976}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7884653, 
author={L. {Meurice} and A. {Cleve}}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Supporting schema evolution in schema-less NoSQL data stores}, 
year={2017}, 
volume={}, 
number={}, 
pages={457-461}, 
abstract={NoSQL data stores are becoming popular due to their schema-less nature. They offer a high level of flexibility, since they do not require to declare a global schema. Thus, the data model is maintained within the application source code. However, due to this flexibility, developers have to struggle with a growing data structure entropy and to manage legacy data. Moreover, support to schema evolution is lacking, which may lead to runtime errors or irretrievable data loss, if not properly handled. This paper presents an approach to support the evolution of a schema-less NoSQL data store by analyzing the application source code and its history. We motivate this approach on a subject system and explain how useful it is to understand the present database structure and facilitate future developments.}, 
keywords={database management systems;software maintenance;source code (software);storage management;schema evolution;schema-less NoSQL data stores;global schema;data model;application source code;data structure entropy;legacy data;database structure;Java;Data structures;Entropy;Data mining;Urban areas}, 
doi={10.1109/SANER.2017.7884653}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7733463, 
author={S. {Ristic} and S. {Kordic} and M. {Celikovic} and V. {Dimitrieski} and I. {Lukovic}}, 
booktitle={2016 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={A model-to-model transformation of a generic relational database schema into a form type data model}, 
year={2016}, 
volume={}, 
number={}, 
pages={1577-1580}, 
abstract={An important phase of a data-oriented software system reengineering is a database reengineering process and, in particular, its subprocess - a database reverse engineering process. In this paper we present one of the model-to-model transformations from a chain of transformations aimed at transformation of a generic relational database schema into a form type data model. The transformation is a step of the data structure conceptualization phase of a model-driven database reverse engineering process that is implemented in IIS*Studio development environment.}, 
keywords={relational databases;software engineering;model-to-model transformation;relational database schema;form type data model;data-oriented software system reengineering;database reverse engineering process;data structure conceptualization phase;IIS*Studio development environment;Data models;Relational databases;Reverse engineering;Business;Object oriented modeling;Information systems}, 
doi={}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{7310988, 
author={G. {Vial}}, 
journal={IEEE Software}, 
title={Database Refactoring: Lessons from the Trenches}, 
year={2015}, 
volume={32}, 
number={6}, 
pages={71-79}, 
abstract={Although database refactoring has been advocated as an important area of database development, little research has studied its implications. A small software development firm refactored a database related to an application that lets clients optimize their logistics processes. This project was based on the design of clear database development conventions and the need to package documentation in the database itself. The experience led to five key lessons learned: refactoring should be automated whenever possible, the database catalog is crucial, refactoring is easier when it's done progressively, refactoring can help optimize an application and streamline its code base, and refactoring related to application development requires a complex skill set and must be applied sensibly. This article is part of a special issue on Refactoring.}, 
keywords={database management systems;software maintenance;database refactoring;database development;software development;Software development;Data models;Code refractoring;Servers;Maintenance engineering;database management;database design;data manipulation languages;data description languages;relational databases;transaction processing;database evolution;database refactoring;software engineering;software development;refactoring}, 
doi={10.1109/MS.2015.131}, 
ISSN={0740-7459}, 
month={Nov},}
@INPROCEEDINGS{795208, 
author={V. {Pereira Moreira} and N. {Edelweiss}}, 
booktitle={Proceedings. Tenth International Workshop on Database and Expert Systems Applications. DEXA 99}, 
title={Schema versioning: queries to the generalized temporal database system}, 
year={1999}, 
volume={}, 
number={}, 
pages={458-459}, 
abstract={Raw data and database structures are evolving entities that require adequate support for past, present and even future versions. Temporal databases supporting schema versioning were developed with the aim of satisfying this requirement. This paper considers a generalized temporal database system, which provides support for time at both intensional and extensional levels. The support for schema versioning raises two complex subjects: the storage of the several schema versions and their associate data, and the processing of queries involving more than one schema version. The main goal of this paper is to analyse the second aspect in order to propose a strategy to answer multi-schema queries.}, 
keywords={temporal databases;query processing;schema versioning;generalized temporal database system;database structures;multi-schema queries;schema evolution;Database systems;Iron;Postal services;Electrical capacitance tomography;Transaction databases}, 
doi={10.1109/DEXA.1999.795208}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5480783, 
author={A. {Baqasah} and E. {Pardede}}, 
booktitle={2010 IEEE 24th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Managing Schema Evolution in Hybrid XML-Relational Database Systems}, 
year={2010}, 
volume={}, 
number={}, 
pages={455-460}, 
abstract={Many applications and data enterprises need to change their data content and structure from time to time. The schema evolution in database systems has been studied well by the database community and several projects have been proposed for supporting schema modifications in XML and relational models individually. However, these studies do not investigate well the evolutionary process in hybrid XML-relational systems. In this paper we will investigate the schema evolution for current hybrid XML-relational systems. We will propose a taxonomy for schema evolution in a way that preserves a schema after data evolves. Finally, we proposed a general model which will be evaluated through a case study.}, 
keywords={relational databases;XML;schema evolution;hybrid XML relational database systems;data enterprises;Database systems;XML;Relational databases;Application software;Taxonomy;Industrial relations;Computer network management;Conference management;Content management;Engineering management;XML Schema;Schema Evolution;XML Database}, 
doi={10.1109/WAINA.2010.127}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{5319373, 
author={H. {Wang} and B. {Shen} and C. {Chen}}, 
booktitle={2009 WRI World Congress on Software Engineering}, 
title={Model-Driven Reengineering of Database}, 
year={2009}, 
volume={3}, 
number={}, 
pages={113-117}, 
abstract={A lot of work has been done applying model-driven approach to those business domain concerned software development. These researches mostly show how to transform business domain models to software application with different paradigms, rather than how to transform specific software artifacts generally regarding of business domain factor, such as database, the common infrastructure of nowadays software system. The later kind of work can make more contribution to general software development rather than some specific business domains. In this paper, we present a MDA based approach to perform database reengineering and also build a framework based on current framework (EMF, Operational-QVT).}, 
keywords={database management systems;software architecture;database model-driven reengineering;software development;software artifacts;business domain factor;model driven architecture;Data models;Relational databases;Application software;Data mining;Programming;Software systems;Reverse engineering;Data analysis;Performance analysis;Logic;MDA;Database Reengineering;EMF;Operational-QVT}, 
doi={10.1109/WCSE.2009.384}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7045196, 
author={N. {Masood} and N. {Andleeb}}, 
booktitle={2014 International Conference on Computational Science and Technology (ICCST)}, 
title={A taxonomy of changes in schema evolution}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Schema evolution is the ability of a database system to respond to changes in the real world by allowing the schema to evolve. Two main categories of research work on schema evolution exist in literature; the changes-based and the tgd-based (target generating dependencies that represent mappings between elements). The tgd-based work is more recent and is mainly based on the use of composition and inversion operators to manage schema evolution. This body of work has established different combinations of tgds that may be formed as a result of a particular schema change, like LAV to GAV. The change in type of mapping (tgd) is then handled by use of inversion and composition operators. However, these operators have not been tested for different types of schema changes. This requires the merging of two aforementioned categories of work on schema evolution which is the focus of this paper. This work provides a basis for studying the applicability of inversion and composition operators on different schema changes falling in each tgd-based category thus helps to deal schema evolution in a better way.}, 
keywords={database management systems;database system;schema evolution;changes-based evolution;tgd-based evolution;LAV;GAV;composition operators;inversion operators;Cities and towns;Context;Remuneration;Taxonomy;Computer science;Educational institutions;Database systems;composition;inversion;schema evolution;schema integration;target generating dependencies}, 
doi={10.1109/ICCST.2014.7045196}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{938080, 
author={H. -. {Schek}}, 
booktitle={Proceedings 2001 International Database Engineering and Applications Symposium}, 
title={Evolution of database technology: hyperdatabases}, 
year={2001}, 
volume={}, 
number={}, 
pages={139-141}, 
abstract={Our vision is that hyperdatabases become available that extend and evolve from database technology. Hyperdatabases move up to a higher level, closer to the applications. A hyperdatabase manages distributed objects and software components as well as workflows, in analogy to a database system that manages data and transactions. In short, hyperdatabases will provide "higher order data independence", e.g., immunity of applications against changes in the implementation of components and workload transparency. Such an evolution of database technology should keep its pivotal role as infrastructure for application development for data-intensive, central and distributed application. The hyperdatabase concept abstracts from the host of current infrastructures and middleware technology.}, 
keywords={database management systems;distributed object management;transaction processing;workflow management software;database technology evolution;hyperdatabases;database technology;distributed object management;software components;workflows;higher order data independence;workload transparency;pivotal role;application development;distributed application;hyperdatabase concept;middleware technology;Transaction databases;Spatial databases;Application software;Image databases;Database systems;Relational databases;Management information systems;Distributed databases;Abstracts;Middleware}, 
doi={10.1109/IDEAS.2001.938080}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6340133, 
author={Y. {Song} and X. {Peng} and Z. {Xing} and W. {Zhao}}, 
booktitle={2012 IEEE 36th Annual Computer Software and Applications Conference}, 
title={Automatic Adaptation of Software Applications to Database Evolution by Graph Differencing and AOP-Based Dynamic Patching}, 
year={2012}, 
volume={}, 
number={}, 
pages={111-118}, 
abstract={Modern information systems, such as enterprise applications and e-commerce applications, often consist of databases surrounded by a large variety of software applications depending on the databases. During the evolution and deployment of such information systems, developers have to ensure the global consistency between database schemas and surrounding software applications. However, in such situations as Enterprise Application Integration (EAI), databases are shared by a number of software applications contributed by different independent parties, and the developers of those applications often have little or no control on when and how database schema evolves over time. As a result, databases and software applications may not always remain in sync. Such inconsistency may lead to data loss, program failures, or decreased performance. The fundamental challenge in evolving and deploying such database-centric information systems is the fact that databases and their surrounding software applications are subject to independent, asynchronous, and potentially conflicting evolution processes. In this paper, we present an approach to automatically adapting software applications to the evolution of their underlying databases by graph-based schema differencing and aspect-oriented dynamic patching. Our empirical study shows that our approach can automatically adapt software applications to a number of common types of database schema evolution, which accounts for over 87.5% of all schema evolution in the subject system. Our approach allows database schema maintainer to evolve database schema more freely without being afraid of breaking surrounding software applications; it also allows application developers to catch up database schema evolution more quickly without diverting too much from their main business concerns.}, 
keywords={aspect-oriented programming;business data processing;database management systems;graph theory;software maintenance;software application automatic adaptation;database evolution;graph-based schema differencing;AOP-based dynamic patching;database-centric information systems;global consistency;database schemas;enterprise applications;e-commerce applications;enterprise application integration;EAI;aspect-oriented dynamic patching;Databases;Software;Production;Information systems;Grammar;Runtime;Adaptation models;software evolution;database evolution;software maintenance;application adapter}, 
doi={10.1109/COMPSAC.2012.21}, 
ISSN={0730-3157}, 
month={July},}
@ARTICLE{842266, 
author={V. M. {Crestana-Jensen} and A. J. {Lee} and E. A. {Rundensteiner}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Consistent schema version removal: an optimization technique for object-oriented views}, 
year={2000}, 
volume={12}, 
number={2}, 
pages={261-280}, 
abstract={Powerful solutions enabling interoperability must allow applications to evolve and the requirements of shared databases to change, while minimizing such changes on other integrated applications. Several approaches have been proposed to make interoperability possible by using object-oriented techniques. These approaches may generate a large number of schema versions over time, resulting in an excessive build-up of classes and underlying object instances, not all being necessarily still in use. This results in degradation of system performance due to the view maintenance and the storage overhead costs. In this paper, we address the problem of removing obsolete view schemas. We characterize four potential problems of schema consistency that could be caused by the removal of a single derived class. We demonstrate that schema version removal is sensitive to the order in which individual classes are processed, and present a formal dependency model that captures all dependencies between classes as logic clauses and manipulates them to make decisions on class deletions and non-deletions while guaranteeing the consistency of the schema. We have also developed and proven consistent a dependency graph (DG) representation of the formal model. Lastly, we present a cost model for evaluating alternative removal patterns on a DG to assure selection of the optimal solution. The proposed techniques have been implemented in our Schema View Removal (SVR) tool. Lastly, we report experimental findings for applying our techniques for consistent schema version removal on the MultiView/TSE (Transparent Schema Evolution) system.}, 
keywords={optimisation;configuration management;object-oriented databases;open systems;software performance evaluation;graph theory;database theory;consistent schema version removal;optimization technique;object-oriented views;interoperability;application evolution;shared database requirements;change minimization;integrated applications;object-oriented techniques;object class build-up;object instances;system performance degradation;view maintenance cost;storage overhead cost;obsolete view schemas;schema consistency;derived class removal;class processing order;formal dependency model;class dependencies;logic clauses;class deletions;class nondeletions;dependency graph representation;cost model;Schema View Removal tool;SVR tool;MultiView/TSE system;transparent schema evolution;Application software;Object oriented modeling;Degradation;System performance;Software performance;Material storage;Object oriented databases;Logic;Cost function;Information management}, 
doi={10.1109/69.842266}, 
ISSN={1041-4347}, 
month={March},}
@INPROCEEDINGS{633291, 
author={ and and and J. {Fong}}, 
booktitle={1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation}, 
title={A data dictionary system approach for database schema translation}, 
year={1997}, 
volume={4}, 
number={}, 
pages={3966-3971 vol.4}, 
abstract={Database system reengineering technique has been used to resolve the problem of converting from existing out-of-data database systems to new technique database systems in order to reduce the new system implementation cost. The technique consists of three parts: schema translation, data conversion, and program translation. The schema translation requires the transferring of existing database schema into the new database schema with better semantics. Recapturing the semantics is a complicated and difficult work. During the database design phase, the semantics has been lost from the conceptual data model to the logical data model. It is difficult to recapture it. After recapturing the semantics from the original conceptual database schema, we also need to have a sophisticated knowledge base to store the knowledge. The paper describes a new schema translation system, which can recapture the missing/hidden semantics of a database. The kernel of the system is an extended entity relationship (EER) data dictionary system (DDS), which can store all the semantics of the new database schema.}, 
keywords={database theory;database management systems;systems re-engineering;systems analysis;entity-relationship modelling;relational algebra;data dictionary system approach;database schema translation;database system reengineering technique;out-of-data database systems;system implementation cost;schema translation;data conversion;program translation;database design phase;conceptual data model;logical data model;conceptual database schema;knowledge base;schema translation system;missing/hidden semantics;extended entity relationship data dictionary system;Dictionaries;Database systems;Costs;Data conversion;Data models;Object oriented databases;Relational databases;Computer science;File systems;Data mining}, 
doi={10.1109/ICSMC.1997.633291}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{7884621, 
author={M. {Overeem} and M. {Spoor} and S. {Jansen}}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={The dark side of event sourcing: Managing data conversion}, 
year={2017}, 
volume={}, 
number={}, 
pages={193-204}, 
abstract={Evolving software systems includes data schema changes, and because of those schema changes data has to be converted. Converting data between two different schemas while continuing the operation of the system is a challenge when that system is expected to be available always. Data conversion in event sourced systems introduces new challenges, because of the relative novelty of the event sourcing architectural pattern, because of the lack of standardized tools for data conversion, and because of the large amount of data that is stored in typical event stores. This paper addresses the challenge of schema evolution and the resulting data conversion for event sourced systems. First of all a set of event store upgrade operations is proposed that can be used to convert data between two versions of a data schema. Second, a set of techniques and strategies that execute the data conversion while continuing the operation of the system is discussed. The final contribution is an event store upgrade framework that identifies which techniques and strategies can be combined to execute the event store upgrade operations while continuing operation of the system. Two utilizations of the framework are given, the first being as decision support in upfront design of an upgrade system for event sourced systems. The framework can also be utilized as the description of an automated upgrade system that can be used for continuous deployment. The event store upgrade framework is evaluated in interviews with three renowned experts in the domain and has been found to be a comprehensive overview that can be utilized in the design and implementation of an upgrade system. The automated upgrade system has been implemented partially and applied in experiments.}, 
keywords={software architecture;software maintenance;data conversion management;software system evolution;event sourced systems;event sourcing architectural pattern;schema evolution;automated upgrade system;continuous deployment;event store upgrade framework;Data conversion;Data models;Software;Memory;Transforms;Event Sourcing;CQRS;Event Driven Architecture;Schema Evolution;Software Evolution;Schema Versioning;Deployment Strategy;Data Transformation;Data Conversion}, 
doi={10.1109/SANER.2017.7884621}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{581740, 
author={S. -. {Lautemann}}, 
booktitle={Proceedings 13th International Conference on Data Engineering}, 
title={A propagation mechanism for populated schema versions}, 
year={1997}, 
volume={}, 
number={}, 
pages={67-78}, 
abstract={Object-oriented database systems (OODBMS) offer powerful modeling concepts as required by advanced application domains like CAD/CAM/CAE or office automation. Typical applications have to handle large and complex structured objects which frequently change their value and their structure. As the structure is described in the schema of the database, support for schema evolution is a highly required feature. Therefore, a set of schema update primitives must be provided which can be used to perform the required changes, even in the presence of populated databases and running applications. In this paper, we use the versioning approach to schema evolution to support schema updates as a complex design task. The presented propagation mechanism is based on conversion functions that map objects between different types and can be used to support schema evolution and schema integration.}, 
keywords={object-oriented databases;configuration management;abstract data types;data structures;database theory;populated schema versions;propagation mechanism;object-oriented database systems;modeling concepts;advanced application domains;complex structured objects;database schema evolution;schema update primitives;running applications;versioning approach;complex design task;conversion functions;object type mapping;schema integration;Object oriented databases;Taxonomy;Database systems;Object oriented modeling;Office automation;Spatial databases;Application software;Software engineering;Testing}, 
doi={10.1109/ICDE.1997.581740}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{931854, 
author={ and and }, 
booktitle={ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)}, 
title={Dynamic behavior control of autonomous mobile robots using schema co-evolutionary algorithm}, 
year={2001}, 
volume={1}, 
number={}, 
pages={560-565 vol.1}, 
abstract={The theoretical foundations of genetic algorithms (GA) are the schema theorem and the building block hypothesis. In the meaning of these foundational concepts, simple genetic algorithms (SGA) allocate more trials to the schemata whose average fitness remains above average. Although SGA does well in many applications as an optimization method, still it does not guarantee the convergence of a global optimum. Therefore as an alternative scheme, there is a growing interest in a co-evolutionary system, where two populations constantly interact and co-evolve in contrast with traditional single population evolutionary algorithms. In this paper, we propose a new design method of an optimal fuzzy logic controller using a co-evolutionary concept. In general, it is very difficult to find optimal fuzzy rules by experience when the input and/or output variables are going to increase. So we propose a co-evolutionary method finding optimal fuzzy rules. Our algorithm is that after constructing two population groups made up of rule base and its schema, by co-evolving these two populations, we find the optimal fuzzy logic controller. By applying the proposed method to a path planning problem of autonomous mobile robots when moving objects exist, we show the validity of the proposed method.}, 
keywords={mobile robots;evolutionary computation;fuzzy control;control system synthesis;path planning;dynamic behavior control;autonomous mobile robots;schema co-evolutionary algorithm;schema theorem;building block hypothesis;genetic algorithms;average fitness;optimization method;global optimum convergence;optimal fuzzy logic controller;optimal fuzzy rules;path planning;Mobile robots;Genetic algorithms;Evolution (biology);Evolutionary computation;Genetic programming;Optimization methods;Fuzzy logic;Optimal control;Environmental factors;Arm}, 
doi={10.1109/ISIE.2001.931854}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6747219, 
author={L. {Meurice} and A. {Cleve}}, 
booktitle={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)}, 
title={DAHLIA: A visual analyzer of database schema evolution}, 
year={2014}, 
volume={}, 
number={}, 
pages={464-468}, 
abstract={In a continuously changing environment, software evolution becomes an unavoidable activity. The mining software repositories (MSR) field studies the valuable data available in software repositories such as source code version-control systems, issue/bug-tracking systems, or communication archives. In recent years, many researchers have used MSR techniques as a way to support software understanding and evolution. While many software systems are data-intensive, i.e., their central artefact is a database, little attention has been devoted to the analysis of this important system component in the context of software evolution. The goal of our work is to reduce this gap by considering the database evolution history as an additional information source to aid software evolution. We present DAHLIA (Database ScHema EvoLutIon Analysis), a visual analyzer of database schema evolution. Our tool mines the database schema evolution history from the software repository and allows its interactive, visual analysis. We describe DAHLIA and present our novel approach supporting data-intensive software evolution.}, 
keywords={data mining;data visualisation;database management systems;program debugging;software maintenance;source code (software);mining software repositories field;source code version-control systems;communication archives;MSR techniques;DAHLIA;data-intensive software evolution;software understanding;issue tracking systems;database schema evolution;visual analyzer;bug tracking systems;History;Visualization;Visual databases;Software systems;Indexes}, 
doi={10.1109/CSMR-WCRE.2014.6747219}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7780160, 
author={L. {Meurice} and A. {Cleve}}, 
booktitle={2016 IEEE Working Conference on Software Visualization (VISSOFT)}, 
title={DAHLIA 2.0: A Visual Analyzer of Database Usage in Dynamic and Heterogeneous Systems}, 
year={2016}, 
volume={}, 
number={}, 
pages={76-80}, 
abstract={Understanding the links between application programs and their database is useful in various contexts such as migrating information systems towards a new database platform, evolving the database schema, or assessing the overall system quality. However, data-intensive applications nowadays tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting programs to an evolving database schema. In this paper, we present DAHLIA 2.0, an interactive visualization tool that allows developers to analyze the database usage in order to support data-intensive software evolution and more precisely, program-database co-evolution.},
keywords={data visualisation;database management systems;interactive systems;DAHLIA 2.0;visual analyzer;database usage;heterogeneous systems;interactive visualization tool;data-intensive software evolution;program-database co-evolution;Visual databases;Data visualization;Java;Buildings;Visualization;History;Database usage visualization;database-program co-evolution}, 
doi={10.1109/VISSOFT.2016.15}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8509408, 
author={U. {Störl} and D. {Müller} and A. {Tekleab} and S. {Tolale} and J. {Stenzel} and M. {Klettke} and S. {Scherzinger}}, 
booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)}, 
title={Curating Variational Data in Application Development}, 
year={2018}, 
volume={}, 
number={}, 
pages={1605-1608}, 
abstract={Building applications for processing data lakes is a software engineering challenge. We present Darwin, a middleware for applications that operate on variational data. This concerns data with heterogeneous structure, usually stored within a schema-flexible NoSQL database. Darwin assists application developers in essential data and schema curation tasks: Upon request, Darwin extracts a schema description, discovers the history of schema versions, and proposes mappings between these versions. Users of Darwin may interactively choose which mappings are most realistic. Darwin is further capable of rewriting queries at runtime, to ensure that queries also comply with legacy data. Alternatively, Darwin can migrate legacy data to reduce the structural heterogeneity. Using Darwin, developers may thus evolve their data in sync with their code. In our hands-on demo, we curate synthetic as well as real-life datasets.}, 
keywords={data handling;middleware;NoSQL databases;query processing;rewriting systems;software maintenance;variational data;application development;data lakes;software engineering challenge;schema-flexible NoSQL database;schema curation tasks;schema description;schema versions;legacy data;Darwin;middleware;heterogeneous structure;query rewriting;History;Data mining;Software;Evolution (biology);NoSQL databases;Task analysis;NoSQL databases;schema evolution;schema management;data migration;query rewriting;variational data}, 
doi={10.1109/ICDE.2018.00187}, 
ISSN={2375-026X}, 
month={April},}
@INPROCEEDINGS{5718407, 
author={J. {Lin} and J. {Yu} and Z. {Zeng}}, 
booktitle={2010 Second World Congress on Software Engineering}, 
title={The Predicate Formulae for Object-Oriented Database Schema Evolution}, 
year={2010}, 
volume={2}, 
number={}, 
pages={346-349}, 
abstract={Object-oriented database system is a superior model than relational database system, with schema evolution capacity. Although since the emergence of object-oriented database systems many schema evolution methods have been proposed, a complete formal description haven't been formed. Therefore, it is necessary to use predicate logic to describe the operation of object-oriented database. This paper introduces predicate logic to illustrate schema evolution of object-oriented database with the predicate formulae. Atomic formulae and formulae are presented and the class evolution invariants and rules are also formalized.}, 
keywords={formal logic;object-oriented databases;object-oriented database schema evolution;predicate logic;predicate formulae;atomic formulae;Vehicles;Companies;Computers;Database systems;Object oriented modeling;Data models;object-oriented database;schema evolution;predicate}, 
doi={10.1109/WCSE.2010.13}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5609724, 
author={A. {Cleve}}, 
booktitle={2010 IEEE International Conference on Software Maintenance}, 
title={Program analysis and transformation for data-intensive system evolution}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Data-intensive software systems are generally made of a database and a collection of application programs in strong interaction with the former. They constitute critical assets in most enterprises, since they support business activities in all production and management domains. Data-intensive systems form most of the so-called legacy systems: they typically are one or more decades old, they are very large, heterogeneous and highly complex. Many of them significantly resist modifications and change due to the lack of documentation, to the use of aging technologies and to inflexible architectures. Therefore, the evolution of data-intensive systems clearly calls for automated support. This thesis explores the use of automated program analysis and transformation techniques in support to the evolution of the database component of the system. The program analysis techniques aim to ease the database evolution process, by helping the developers to understand the data structures that are to be changed, despite the lack of precise and up-to-date documentation. The objective of the program transformation techniques is to support the adaptation of the application programs to the new database. This adaptation process is studied in the context of two realistic database evolution scenarios, namely database database schema refactoring and database platform migration.}, 
keywords={data structures;database management systems;program diagnostics;software architecture;software maintenance;data-intensive system evolution;data-intensive software system;application program;legacy system;aging technology;inflexible architectures;automated program analysis;automated program transformation technique;database component evolution;data structures;database schema refactoring;database platform migration;Databases;Reverse engineering;Software systems;Data models;Data structures;Context}, 
doi={10.1109/ICSM.2010.5609724}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{1311227, 
author={D. {Draheim} and M. {Horn} and I. {Schulz}}, 
booktitle={Proceedings. 16th International Conference on Scientific and Statistical Database Management, 2004.}, 
title={The schema evolution and data migration framework of the environmental mass database IMIS}, 
year={2004}, 
volume={}, 
number={}, 
pages={341-344}, 
abstract={This paper describes a framework that supports the simultaneous evolution of object-oriented data models and relational schemas with respect to a tool-supported object-relational mapping. The proposed framework accounts for non-trivial data migration induced by type evolution from the outset. The support for data migration is offered on the level of transparent data access. The framework consists of the following integrated parts: an automatic model change detection mechanism, a generator for schema evolution code and a generator for data migration APIs. The framework has been concepted in the IMIS project. IMIS is an information system for environmental radioactivity measurements. Though the indicated domain especially demands a solution like the one discussed in this paper, the achievements are of general purpose for multi-tier system architectures with object-relational mapping.}, 
keywords={object-oriented databases;relational databases;object-oriented programming;application program interfaces;environmental science computing;radioactivity measurement;evolutionary computation;entity-relationship modelling;schema evolution;environmental mass database;object-oriented data model evolution;relational schemas;tool-supported object-relational mapping;nontrivial data migration;type evolution;transparent data access;automatic model change detection mechanism;data migration API;IMIS project;information system;environmental radioactivity measurements;multitier system architectures;Sea measurements;Information systems;Water storage;Object oriented databases;Data models;Predictive models;Particle measurements;Soil measurements;Manuals;Relational databases}, 
doi={10.1109/SSDM.2004.1311227}, 
ISSN={1099-3371}, 
month={June},}
@INPROCEEDINGS{1428471, 
author={O. Y. {Yuliana} and S. {Chittayasothorn}}, 
booktitle={International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II}, 
title={XML schema re-engineering using a conceptual schema approach}, 
year={2005}, 
volume={1}, 
number={}, 
pages={255-260 Vol. 1}, 
abstract={At present, the Extensible Markup Language (XML) is popular for both data presentation and data transfer activities. XML Documents follow XML Schemas. It is possible to use them as database schemas and databases. An XML Schema can be hierarchical or flat. The hierarchical one is normally used for data presentation and the flat one is for data transfer. This research project emphasizes on the data transfer application of XML. The objective of the project is to develop a technique to reengineer poorly designed XML Schemas into the normalized ones. A software tool is developed. The tool accepts well-formatted XML Schemas and well-formatted and validated XML documents as data sources. They are reengineered into the optimal normal form schemas which guarantee not to have redundancies. The Nijssen's Information Analysis Methodology (NIAM) is used as the conceptual schema model of XML databases.}, 
keywords={relational databases;XML;software tools;data structures;information analysis;document handling;XML schema reengineering;conceptual schema;extensible markup language;data presentation;data transfer;XML documents;database schemas;software tool;optimal normal form schemas;information analysis methodology;XML databases;NIAM;hierarchical XML;flat XML;XML;Software tools;Application software;Internet;Relational databases;Data engineering;Information analysis;Protocols;Costs;Design methodology}, 
doi={10.1109/ITCC.2005.301}, 
ISSN={}, 
month={April},}
@ARTICLE{4163027, 
author={S. W. {Ambler}}, 
journal={IEEE Software}, 
title={Test-Driven Development of Relational Databases}, 
year={2007}, 
volume={24}, 
number={3}, 
pages={37-43}, 
abstract={In test-first development, developers formulate and implement a detailed design iteratively, one test at a time. Test-driven development (also called test-driven design) combines TFD with refactoring, wherein developers make small changes (refactorings) to improve code design without changing the code's semantics. When developers decide to use TDD to implement a new feature, they must first ask whether the current design is the easiest possible design to enable the feature's addition. Implementing test-driven database design involves database refactoring, regression testing, and continuous integration. TDDD is an integrated part of the overall development process, not a standalone activity that data professionals perform in parallel with application TDD. Although from a technical view point, TDDD is relatively straightforward, we must overcome several challenges to its whole sale adoption throughout the IT community}, 
keywords={program testing;relational databases;SQL;test-driven development;relational database;test-driven database design;database refactoring;regression testing;continuous integration testing;Relational databases;Spatial databases;Visual databases;Software testing;Information retrieval;Dynamic programming;System testing;Transaction databases;Cultural differences;Programming profession;test-driven development;TDD;database refactoring;database testing;test-driven database design;TDD;relational database;behavior-driven development;BDD}, 
doi={10.1109/MS.2007.91}, 
ISSN={0740-7459}, 
month={May},}
@ARTICLE{649319, 
author={ and E. A. {Rundensteiner} and K. G. {Shin}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Schema evolution of an object-oriented real-time database system for manufacturing automation}, 
year={1997}, 
volume={9}, 
number={6}, 
pages={956-977}, 
abstract={Database schemata often experience considerable changes during the development and initial use phases of database systems for advanced applications such as manufacturing automation and computer-aided design. An automated schema evolution system can significantly reduce the amount of effort and potential errors related to schema changes. Although schema evolution for nonreal-time databases was the subject of previous research, its impact on real-time database systems remains unexplored. These advanced applications typically utilize object-oriented data models to handle complex data types. However, there exists no agreed-upon real-time object-oriented data model that can be used as a foundation to define a schema-evolution framework. Therefore, the authors first design a conceptual real-time object-oriented data model, called Real-time Object Model with Performance Polymorphism (ROMPP). It captures the key characteristics of real-time applications-namely, timing constraints and performance polymorphism-by utilizing specialization-dimension and letter-class hierarchy constructs, respectively. They then re-evaluate previous (nonreal-time) schema evolution support in the context of real-time databases. This results in modifications to the semantics of schema changes and to the needs of schema change resolution rules and schema invariants. Furthermore, they expand the schema change framework with new constructs-including new schema change operators, new resolution rules, and new invariants-necessary for handling the real-time characteristics of ROMPP.}, 
keywords={manufacturing data processing;object-oriented databases;real-time systems;production control;data structures;object-oriented real-time database system;manufacturing automation;automated schema evolution system;object-oriented data models;complex data type handling;Real-time Object Model with Performance Polymorphism;timing constraints;specialization-dimension constructs;letter-class hierarchy constructs;schema change resolution rules;schema invariants;resolution rules;axiomatic model;semantics;manufacturing control applications;Real time systems;Database systems;Object oriented databases;Data models;Application software;Manufacturing automation;Design automation;Computer errors;Object oriented modeling;Timing}, 
doi={10.1109/69.649319}, 
ISSN={1041-4347}, 
month={Nov},}
@ARTICLE{277766, 
author={ and D. {McLeod}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Conceptual database evolution through learning in object databases}, 
year={1994}, 
volume={6}, 
number={2}, 
pages={205-224}, 
abstract={Changes to the conceptual structure (meta-data) of a database are common in many application environments and are in general inadequately supported by existing database systems. An approach to supporting such meta-data evolution in a simple, extensible, object database environment is presented. Machine learning techniques are the basis for a cooperative user/system database design and evolution methodology. An experimental end-user database evolution tool based on this approach has been designed and implemented.<<ETX>>}, 
keywords={learning (artificial intelligence);object-oriented databases;software tools;data structures;deductive databases;conceptual database evolution;cooperative user/system database design;object databases;conceptual structure;meta-data evolution;machine learning techniques;end-user database evolution tool;Database systems;Object oriented databases;Spatial databases;Machine learning;Object oriented modeling;Artificial intelligence;Relational databases;Computer science;Design methodology;Indexes}, 
doi={10.1109/69.277766}, 
ISSN={1041-4347}, 
month={April},}
@INPROCEEDINGS{5169330, 
author={Y. {Liu} and C. {Liu}}, 
booktitle={2009 International Association of Computer Science and Information Technology - Spring Conference}, 
title={A Schema-Guiding Evolutionary Algorithm for 0-1 Knapsack Problem}, 
year={2009}, 
volume={}, 
number={}, 
pages={160-164}, 
abstract={A Schema-Guiding Evolutionary Algorithm (SGEA) is proposed in this paper. The novel SGEA has many good features. It proposes the schema-modified operator to adjust the distribution of the population. What's more, it constructs an elite-schema space and proposes the cluster-center schema to guide the direction of individual's evolution. And by such two strategies, the diversity of the population and the local and global search power can be greatly improved. The experimental results show that the SGEA proposed in this paper has many better performances, compared with other methods such as simple genetic algorithm, greedy algorithm and so forth.}, 
keywords={evolutionary computation;knapsack problems;schema-guiding evolutionary algorithm;0-1 knapsack problem;schema-modified operator;elite-schema space;genetic algorithm;greedy algorithm;Evolutionary computation;Greedy algorithms;Springs;Space technology;Genetic algorithms;Computer science;Information science;Chaos;Information technology;Resource management;evolutionary algorithm;0-1 knapsack;schema}, 
doi={10.1109/IACSIT-SC.2009.31}, 
ISSN={}, 
month={April},}
@ARTICLE{567047, 
author={ and R. {Zicari} and W. {Hursch} and K. J. {Lieberherr}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={The role of polymorphic reuse mechanisms in schema evolution in an object-oriented database}, 
year={1997}, 
volume={9}, 
number={1}, 
pages={50-67}, 
abstract={A seamless approach to the incremental design and reuse of object oriented methods and query specifications is presented. We argue for avoiding or minimizing the effort required for manually reprogramming methods and queries due to schema modifications, and demonstrate how the role of polymorphic reuse mechanisms is exploited for enhancing the adaptiveness of database programs against schema evolution in an object oriented database. The salient features of our approach are the use of propagation patterns and a mechanism for propagation pattern refinement. Propagation patterns are employed as an interesting specification formalism for modeling operational requirements. They encourage the reuse of operational specifications against the structural modification of an object oriented schema. Propagation pattern refinement is suited for the specification of reusable operational modules. It promotes the reusability of propagation patterns toward the operational requirement changes. This approach has a formal basis and emphasizes structural derivation of specifications. The main innovations are in raising the level of abstraction for behavioral schema design, and for making possible the derivation of operational semantics from structural specifications. As a result, both the modularity and reusability of object oriented schemas are increased.}, 
keywords={object-oriented databases;software reusability;query processing;formal specification;polymorphic reuse mechanisms;schema evolution;object oriented database;seamless approach;incremental design;query specifications;object oriented methods;reprogramming methods;schema modifications;database programs;propagation pattern refinement;specification formalism;operational requirements;operational specifications;structural modification;object oriented schema;reusable operational modules;operational requirement changes;formal basis;structural derivation;abstraction level;behavioral schema design;Object oriented databases;Database systems;Spatial databases;Computer science;Computer Society;Object oriented modeling;Technological innovation;Software systems;Productivity;Performance evaluation}, 
doi={10.1109/69.567047}, 
ISSN={1041-4347}, 
month={Jan},}
@INPROCEEDINGS{694348, 
author={B. {Benatallah} and Z. {Tari}}, 
booktitle={Proceedings. IDEAS'98. International Database Engineering and Applications Symposium (Cat. No.98EX156)}, 
title={Dealing with version pertinence to design an efficient schema evolution framework}, 
year={1998}, 
volume={}, 
number={}, 
pages={24-33}, 
abstract={The paper addresses the design of a schema evolution framework enabling an efficient management of object versions. This framework is based on the adaptation and extension of two main schema evolution approaches, that is the approaches based on schema modification and those based on schema versioning. The framework provides an integrated environment to support different levels of adaptation (such as, modification and versioning at the schema level, conversion, object versioning, and emulation at the instance level). In addition, the authors introduce the concept of class/schema version pertinence enabling the database administrator to judge the pertinence of versions with regard the application programs. Finally, they provide operations for immediate refreshing of a database to enable an efficient manipulation of versions by a large number of application programs.}, 
keywords={configuration management;database theory;object-oriented databases;version pertinence;efficient schema evolution framework design;object version management;schema modification;schema versioning;integrated environment;adaptation;schema version pertinence;database administrator;class pertinence;application programs;immediate refreshing;version manipulation;Databases;Computer science;Emulation;Technology management;Degradation;Costs}, 
doi={10.1109/IDEAS.1998.694348}, 
ISSN={1098-8068}, 
month={July},}
@INPROCEEDINGS{213197, 
author={M. M. {Morsi} and S. B. {Navathe} and }, 
booktitle={[1992] Eighth International Conference on Data Engineering}, 
title={An extensible object-oriented database testbed}, 
year={1992}, 
volume={}, 
number={}, 
pages={150-157}, 
abstract={The authors describe the object-oriented design and implementation of an extensible schema manager for object-oriented databases. The open class hierarchy approach has been adopted to achieve the extensibility of the implementation. In this approach. the system meta information is implemented as objects of system classes. A graphical interface for an object-oriented database scheme environment, GOOSE, has been developed. GOOSE supports several advanced features which include schema evolution, schema versioning, and DAG (direct acyclic graph) rearrangement view of a class hierarchy. Schema evolution is the ability to make a variety of changes to a database scheme without reorganization. Schema versioning is the ability to define multiple scheme versions and to keep track of schema changes. A novel type of view for object-oriented databases, the DAG rearrangement view of a class hierarchy, is also supported.<<ETX>>}, 
keywords={graphical user interfaces;object-oriented databases;object-oriented design;extensible schema manager;open class hierarchy;system meta information;system classes;graphical interface;object-oriented database scheme environment;GOOSE;schema evolution;schema versioning;direct acyclic graph;schema changes;DAG rearrangement view;Object oriented databases;Testing;Data models;Spatial databases;Application software;Database systems;Management information systems;Educational institutions;Technology management;Software engineering}, 
doi={10.1109/ICDE.1992.213197}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{1602388, 
author={A. {Cleve}}, 
booktitle={Conference on Software Maintenance and Reengineering (CSMR'06)}, 
title={Automating program conversion in database reengineering: a wrapper-based approach}, 
year={2006}, 
volume={}, 
number={}, 
pages={4 pp.-326}, 
abstract={Database reengineering consists in deriving a new database from a legacy database and adapting associated software components accordingly. This migration process typically involves three main steps, namely schema conversion, data conversion and program conversion. This paper presents a wrapper-based approach to automating the program conversion step. The proposed approach combines program transformations and code generation, which are derived from schema transformations}, 
keywords={automatic programming;database management systems;electronic data interchange;object-oriented programming;program compilers;software maintenance;systems re-engineering;automating program conversion;database reengineering;wrapper-based approach;legacy database;software components;schema conversion;data conversion;program transformations;code generation;schema transformations;Relational databases;Data structures;Spatial databases;Data engineering;Prototypes;Acoustical engineering;Laboratories;Application software;Computer science;Data conversion}, 
doi={10.1109/CSMR.2006.12}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{4812563, 
author={C. A. {Curino} and H. J. {Moon} and M. {Ham} and C. {Zaniolo}}, 
booktitle={2009 IEEE 25th International Conference on Data Engineering}, 
title={The PRISM Workwench: Database Schema Evolution without Tears}, 
year={2009}, 
volume={}, 
number={}, 
pages={1523-1526}, 
abstract={Information Systems are subject to a perpetual evolution, which is particularly pressing in Web Information Systems, due to their distributed and often collaborative nature. Such continuous adaptation process, comes with a very high cost, because of the intrinsic complexity of the task and the serious ramifications of such changes upon database-centric Information System softwares. Therefore, there is a need to automate and simplify the schema evolution process and to ensure predictability and logical independence upon schema changes. Current relational technology makes it easy to change the database content or to revise the underlaying storage and indexes but does little to support logical schema evolution which nowadays remains poorly supported by commercial tools. The PRISM system demonstrates a major new advance toward automating schema evolution (including query mapping and database conversion), by improving predictability, logical independence, and auditability of the process. In fact, PRISM exploits recent theoretical results on mapping composition, invertibility and query rewriting to provide DB Administrators with an intuitive, operational workbench usable in their everyday activities-thus enabling graceful schema evolution. In this demonstration, we will show (i) the functionality of PRISM and its supportive AJAX interface, (ii) its architecture built upon a simple SQL-inspired language of Schema Modification Operators, and (iii) we will allow conference participants to directly interact with the system to test its capabilities. Finally, some of the most interesting evolution steps of popular Web Information Systems, such as Wikipedia, will be reviewed in a brief "Saga of Famous Schema Evolutions".}, 
keywords={database management systems;groupware;Internet;Java;SQL;XML;PRISM workwench;database schema evolution;Web information systems;collaborative nature;intrinsic complexity;database-centric information system softwares;logical independence;AJAX interface;SQL;Information systems;Relational databases;Pressing;System testing;Management information systems;Usability;Bridges;Prototypes;Data engineering;Distributed databases;schema evolution;query rewriting;schema mapping;schema modification operators;schema versioning}, 
doi={10.1109/ICDE.2009.46}, 
ISSN={1063-6382}, 
month={March},}
@INPROCEEDINGS{4636603, 
author={ and and and }, 
booktitle={2008 IEEE International Conference on Automation and Logistics}, 
title={A new method for solving 0/1 knapsack problem based on evolutionary algorithm with schema replaced}, 
year={2008}, 
volume={}, 
number={}, 
pages={2569-2571}, 
abstract={Knapsack problem is a typical NP complete problem. Knapsack problem correspondent mathematical model is proposed in this paper, and the evolutionary algorithm with schema replaced is raised. In this algorithm, it leads the search direction of the population by collecting the best several individuals to a schema. Because of that, the searching efficiency is improved. At last, the simulation experiment is given and the answer of the knapsack problem which is solved by simple evolutionary algorithm and the evolutionary algorithm with schema replaced two is compared. By this comparison, the advantage which use schema replaced to solve knapsack problem is proved.}, 
keywords={computational complexity;evolutionary computation;knapsack problems;search problems;knapsack problem;evolutionary algorithm;NP complete problem;schema replaced;searching efficiency;Evolutionary computation;Biological cells;Convergence;Genetic algorithms;Lead;Algorithm design and analysis;Greedy algorithms;knapsack problem;schema replaced;evolutionary algorithm}, 
doi={10.1109/ICAL.2008.4636603}, 
ISSN={2161-8151}, 
month={Sep.},}
@INPROCEEDINGS{7589806, 
author={L. {Meurice} and C. {Nagy} and A. {Cleve}}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={Detecting and Preventing Program Inconsistencies under Database Schema Evolution}, 
year={2016}, 
volume={}, 
number={}, 
pages={262-273}, 
abstract={Nowadays, data-intensive applications tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting application programs to database schema changes. Failing to correctly adapt programs to an evolving database schema results in program inconsistencies, which in turn may cause program failures. In this paper, we present a tool-supported approach, that allows developers to (1) analyze how the source code and database schema co-evolved in the past and (2) simulate a database schema change and automatically determine the set of source code locations that would be impacted by this change. Developers are then provided with recommendations about what they should modify at those source code locations in order to avoid inconsistencies. The approach has been designed to deal with Java systems that use dynamic data access frameworks such as JDBC, Hibernate and JPA. We motivate and evaluate the proposed approach, based on three real-life systems of different size and nature.}, 
keywords={database management systems;Java;object-oriented programming;query processing;software fault tolerance;software tools;source code (software);program inconsistencies detection;program inconsistencies prevention;database schema evolution;data-intensive applications;queries;database server;string concatenation;object-relational-mapping;ORM frameworks;application programs;program failures;tool-supported approach;source code locations;Java systems;dynamic data access frameworks;Java;History;Relational databases;Open source software;Libraries;Industries;what-if approach;database schema evolution;ORM;JDBC;program-database co-evolution}, 
doi={10.1109/QRS.2016.38}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{344065, 
author={G. {Moerkotte} and A. {Zachmann}}, 
booktitle={Proceedings of IEEE 9th International Conference on Data Engineering}, 
title={Towards more flexible schema management in object bases}, 
year={1993}, 
volume={}, 
number={}, 
pages={174-181}, 
abstract={An approach to database schema management is presented that allows easy tailoring of schema management, high-level specification of schema consistency, and development of advanced tools supporting the user during schema evolution. The application of this approach to the development of a simple schema manager for the core of the GOM database programming language is described. The flexibility afforded both developers and users by the approach is also discussed.<<ETX>>}, 
keywords={object-oriented databases;formal specification;user support;tools development;flexible schema management;object bases;high-level specification;schema consistency;schema evolution;GOM database programming language;Database systems;Object oriented modeling;Probes;Object oriented databases;Data models}, 
doi={10.1109/ICDE.1993.344065}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4633616, 
author={H. {Kadri-Dahmani} and C. {Bertelle} and G. H. E. {Duchamp} and A. {Osmani}}, 
booktitle={2005 12th IEEE International Conference on Electronics, Circuits and Systems}, 
title={Evolutive complex scheduling in interaction networks for quality improvment in geographical data base updating}, 
year={2005}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={The aim of this paper is to propose a scheduling processes for the mechanism of geographical data bases updating in term of improvement of its quality. Because of the complex interacting network of the components involved in each updating cycle of this mechanism, a traditional analytic solver cannot success in most cases. So we propose to use an evolutionary computation based on a genetic algorithm which is well adapted for the complex interaction network of the involved components. A genetic algorithm may allow to improve the updating quality in increasing the validation rate of a transaction.}, 
keywords={complex networks;genetic algorithms;geographic information systems;scheduling;transaction processing;geographical database updation;evolutive complex scheduling;complex interaction network;database quality improvement;evolutionary computation;genetic algorithm;database transaction}, 
doi={10.1109/ICECS.2005.4633616}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{534131, 
author={ and J. {Harrison}}, 
booktitle={Proceedings of 1996 Australian Software Engineering Conference}, 
title={An integrated database reengineering architecture-a generic approach}, 
year={1996}, 
volume={}, 
number={}, 
pages={146-154}, 
abstract={Methods and tools have been proposed and developed to facilitate database design recovery under the framework of reverse engineering and reengineering. These tools and methods are usually limited to a particular scenario and requirement. They are not adaptive, or general enough, to suit other scenarios. In most cases, new tools and methods will have to be redeveloped to suit these scenarios. This can result in a tremendous waste of effort and cost. In this paper we describe an integrated generic architecture for reengineering legacy databases. The goal of this research is to formalize an integrated generic architecture that is applicable to different database reengineering scenarios and requirements. We examine the conceptual and technical requirement for enabling such an architecture. An overview of the database reengineering process, which serves as a guideline for practitioners and tool developers, is presented. The results of a preliminary feasibility study based on the results of an implementation of relational database reverse engineering tools within the content of the architecture is also provided.}, 
keywords={systems re-engineering;reverse engineering;relational databases;software maintenance;software tools;integrated database reengineering architecture;database design recovery;reverse engineering;integrated generic architecture;legacy databases;preliminary feasibility study;relational database reverse engineering tools;Reverse engineering;Relational databases;Computer architecture;Costs;Software maintenance;Computer science;Guidelines;Software systems;Data engineering;Erbium}, 
doi={10.1109/ASWEC.1996.534131}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6006845, 
author={Z. {Brahmia} and R. {Bouaziz} and F. {Grandi} and B. {Oliboni}}, 
booktitle={2011 FIFTH INTERNATIONAL CONFERENCE ON RESEARCH CHALLENGES IN INFORMATION SCIENCE}, 
title={Schema versioning in tXSchema-based multitemporal XML repositories}, 
year={2011}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={tXSchema [7] is a framework (a language and a suite of tools) for the creation and validation of time-varying XML documents. A tXSchema schema is composed of a conventional XML Schema document annotated with physical and logical annotations. All components of a tXSchema schema (i.e., conventional schema, logical annotations, and physical annotations) can change over time to reflect changes in user requirements or in reference world of the database. Since many applications need to keep track of both data and schema evolution, schema versioning has been long advocated to be the best solution to do this. In this paper, we deal with schema versioning in the tXSchema framework. More precisely, we propose a set of schema change primitives for the maintenance of logical and physical annotations and define their operational semantics.}, 
keywords={temporal databases;XML;schema versioning;tXSchema-based multitemporal XML repositories;time-varying XML documents;XML;Containers;Semantics;Databases;History;Data models;Calendars;tXSchema;Schema versioning;XML;XML Schema;Temporal database}, 
doi={10.1109/RCIS.2011.6006845}, 
ISSN={2151-1357}, 
month={May},}
@INPROCEEDINGS{796485, 
author={ and and }, 
booktitle={Proceedings Technology of Object-Oriented Languages and Systems (Cat. No.PR00393)}, 
title={A equivalent object-oriented schema evolution approach using the path-independence language}, 
year={1999}, 
volume={}, 
number={}, 
pages={212-217}, 
abstract={The software legacy problem caused by schema evolution in an object-oriented database is a method of equivalent schema evolution based on a path independence (PI) language. The PI language which has been adopted in some systems raises the level of abstraction for behavioral schema design and is almost independent of the specific schema digraph. We develop the PI language and advocate an equivalent schema evolution method which not only resolves the software reuse problem in schema evolution but also supports a schema version mechanism which is an advanced feature in database systems.}, 
keywords={object-oriented databases;software reusability;database languages;object-oriented languages;configuration management;equivalent object-oriented schema evolution;path independence language;software legacy;object-oriented database;PI language;abstraction level;behavioral schema design;schema digraph;software reuse;schema version mechanism;Object oriented databases;Optical propagation;Spatial databases;Multimedia databases;Data models;Computer science;Data engineering;Database systems;Application software;Design automation}, 
doi={10.1109/TOOLS.1999.796485}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7325438, 
author={V. {Filatov} and V. {Radchenko}}, 
booktitle={2015 Xth International Scientific and Technical Conference "Computer Sciences and Information Technologies" (CSIT)}, 
title={Reengineering relational database on analysis functional dependent attribute}, 
year={2015}, 
volume={}, 
number={}, 
pages={85-87}, 
abstract={The task of re-engineering information system which is based uses a relational database. An approach to the definition of functionally dependent attributes of the database in step reengineering and modified synthesis algorithm logic of a relational database.}, 
keywords={information systems;relational databases;reengineering relational database;functional dependent attribute analysis;re-engineering information system;modified synthesis algorithm logic;Relational databases;Information systems;Yttrium;Computer science;Information technology;Electronic mail;reengineering;information systems;relational database;relation;the database schema;functional dependencies;normalization}, 
doi={10.1109/STC-CSIT.2015.7325438}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7113402, 
author={S. {Scherzinger} and T. {Cerqueus} and E. C. d. {Almeida}}, 
booktitle={2015 IEEE 31st International Conference on Data Engineering}, 
title={ControVol: A framework for controlled schema evolution in NoSQL application development}, 
year={2015}, 
volume={}, 
number={}, 
pages={1464-1467}, 
abstract={Building scalable web applications on top of NoSQL data stores is becoming common practice. Many of these data stores can easily be accessed programmatically, and do not enforce a schema. Software engineers can design the data model on the go, a flexibility that is crucial in agile software development. The typical tasks of database schema management are now handled within the application code, usually involving object mapper libraries. However, today's Integrated Development Environments (IDEs) lack the proper tool support when it comes to managing the combined evolution of the application code and of the schema. Yet simple refactorings such as renaming an attribute at the source code level can cause irretrievable data loss or runtime errors once the application is serving in production. In this demo, we present ControVol, a framework for controlled schema evolution in application development against NoSQL data stores. ControVol is integrated into the IDE and statically type checks object mapper class declarations against the schema evolution history, as recorded by the code repository. ControVol is capable of warning of common yet risky cases of mismatched data and schema. ControVol is further able to suggest quick fixes by which developers can have these issues automatically resolved.}, 
keywords={data models;database management systems;Internet;software prototyping;source code (software);SQL;ControVol;controlled schema evolution framework;NoSQL application development;scalable Web applications;NoSQL data stores;software engineers;data model;agile software development;database schema management;application code;object mapper libraries;integrated development environments;IDE;source code level;code repository;Databases;Software;Runtime;Production;Google;Java;History}, 
doi={10.1109/ICDE.2015.7113402}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{5964470, 
author={ and }, 
booktitle={2011 International Conference on Remote Sensing, Environment and Transportation Engineering}, 
title={Design of hyperspectral multidimensional database for rock and mineral}, 
year={2011}, 
volume={}, 
number={}, 
pages={1092-1095}, 
abstract={Spectral database plays a very important role in hyperspectral remote sensing research, the development of hyperspectral remote sensing make new demands on spectral database. Multi-dimensional database technology is the latest stage in the evolution of database technology.It has a wide range of applications in access and analysis large volumes of data. Hyperspectral image data have similarity with the cube which is the form of multi-dimensional database organizing data. The feasibility and necessity was discussed for the applications of multi-dimensional database technology in hyperspectral data management and analysis in the first part. In the following part, according to the theory of multi-dimensional database, related concept of hyperspectral remote sensing and the goal of rock and mineral's hyperspectral multi-dimensional database, the basic concepts were defined for hyperspectral multi-dimensional database for rock and mineral. The database was designed based on the concepts.The functional requirements of database were analysed. A multi-dimensional data structure which is nested was proposed for rock and mineral, the fact table and dimension hierarchy was designed for rock and mineral's hyperspectral multi-dimensional database, the snowflake schema was used to constructe multidimensional data model for rock and mineral. All these laid the foundation for the implemention of hyperspectral multi-dimensional database for rock and mineral.}, 
keywords={geophysical techniques;minerals;remote sensing;rocks;spectral database;hyperspectral remote sensing research;multidimensional database technology;hyperspectral image data;multidimensional database organizing data;hyperspectral data management;hyperspectral data analysis;rock hyperspectral multidimensional database;mineral hyperspectral multidimensional database;multidimensional data structure;snowflake schema;multidimensional data model;Databases;Hyperspectral imaging;Data warehouses;Rocks;Minerals;Multi-dimensional Database Design;Hyperspectral Database;Multi-dimensional Data Model}, 
doi={10.1109/RSETE.2011.5964470}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7818652, 
author={K. {Hamaji} and Y. {Nakamoto}}, 
booktitle={2016 Fourth International Symposium on Computing and Networking (CANDAR)}, 
title={Toward a Database Refactoring Support Tool}, 
year={2016}, 
volume={}, 
number={}, 
pages={443-446}, 
abstract={Database schema changes in system development cannot be avoidable. We need refactoring for the schema. Refactoring criteria for the schema, however, has not been clear and the refactoring depends on the skill of database designers. To solve the problems, we consider to finding out refactoring target columns and recommending them as refactoring targets by using clustering techniques with features of database schema and data in the table and implement it as a support tool. We describe the method and the support tool using the method.}, 
keywords={database management systems;pattern clustering;database refactoring support tool;clustering techniques;machine learning;Database;Refacroting;Machine Learning}, 
doi={10.1109/CANDAR.2016.0082}, 
ISSN={2379-1896}, 
month={Nov},}
@INPROCEEDINGS{614052, 
author={E. {Andonoff} and G. {Hubert} and A. {Le Parc} and G. {Zurfluh}}, 
booktitle={Proceedings of the Third Basque International Workshop on Information Technology - BIWIT'97 - Data Management Systems}, 
title={A query algebra for object-oriented databases integrating versions}, 
year={1997}, 
volume={}, 
number={}, 
pages={62-72}, 
abstract={Presents an algebra to query object-oriented databases that integrate versions. This algebra rests on an object-oriented data model which permits, by means of versions, the description of both value and schema evolutions for entities. The specificities of versions are taken into account. So, versions describing an entity may be queried either by considering the derivation hierarchy they belong to (all the entity states are considered) or independently, one from another (only some entity states are considered).}, 
keywords={object-oriented databases;query processing;process algebra;configuration management;database theory;query algebra;object-oriented databases;versions;object-oriented data model;value evolution;schema evolution;entity description;derivation hierarchy;entity states;versionable objects;versionable types;Algebra;Object oriented databases;Relational databases;Object oriented modeling;Data models;Database systems;Database languages;Documentation;Software engineering;Engineering management}, 
doi={10.1109/BIWIT.1997.614052}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7169446, 
author={M. d. {Jong} and A. v. {Deursen}}, 
booktitle={2015 IEEE/ACM 3rd International Workshop on Release Engineering}, 
title={Continuous Deployment and Schema Evolution in SQL Databases}, 
year={2015}, 
volume={}, 
number={}, 
pages={16-19}, 
abstract={Continuous Deployment is an important enabler of rapid delivery of business value and early end user feedback. While frequent code deployment is well understood, the impact of frequent change on persistent data is less understood and supported. SQL schema evolutions in particular can make it expensive to deploy a new version, and may even lead to downtime if schema changes can only be applied by blocking operations. In this paper we study the problem of continuous deployment in the presence of database schema evolution in more detail. We identify a number of shortcomings to existing solutions and tools, mostly related to avoidable downtime and support for foreign keys. We propose a novel approach to address these problems, and provide an open source implementation. Initial evaluation suggests the approach is effective and sufficiently efficient.}, 
keywords={public domain software;SQL;continuous deployment;SQL database schema evolution;open source implementation;Databases;Software;Switches;Synchronization;Production;Hard disks;Vehicles;Continuous Deployment;SQL databases;schema evolution}, 
doi={10.1109/RELENG.2015.14}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5306293, 
author={S. {Strobl} and M. {Bernhart} and T. {Grechenig} and W. {Kleinert}}, 
booktitle={2009 IEEE International Conference on Software Maintenance}, 
title={Digging deep: Software reengineering supported by database reverse engineering of a system with 30+ years of legacy}, 
year={2009}, 
volume={}, 
number={}, 
pages={407-410}, 
abstract={This paper describes the industrial experience in performing database reverse engineering on a large scale software reengineering project. The project in question deals with a highly heterogeneous in-house information system (IS) that has grown and evolved in numerous steps over the past three decades. This IS consists of a large number of loosely coupled single purpose systems with a database driven COBOL application at the centre, which has been adopted and enhanced to expose some functionality over the web. The software reengineering effort that provides the context for this paper deals with unifying these components and completely migrating the IS to an up-to-date and homogeneous platform. A database reverse engineering (DRE) process was tailored to suit the project environment consisting of almost 350 tables and 5600 columns. It aims at providing the developers of the software reengineering project with the necessary information about the more than thirty year old legacy databases to successfully perform the data migration. The application of the DRE process resulted in the development of a high-level categorization of the data model, a wiki based redocumentation structure and the essential data-access statistics.}, 
keywords={COBOL;data models;database management systems;information retrieval;reverse engineering;software maintenance;systems re-engineering;large scale software reengineering;database reverse engineering;heterogeneous in-house information system;single purpose systems;database driven COBOL application;data model;wiki based redocumentation structure;data-access statistics;legacy system;Reverse engineering;Software performance;Relational databases;Collaborative software;Computer industry;Large-scale systems;Information systems;Application software;Software reusability;Information technology}, 
doi={10.1109/ICSM.2009.5306293}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{37412, 
author={K. J. {Lynch} and L. M. {Hoopes}}, 
booktitle={Eighth Annual International Phoenix Conference on Computers and Communications. 1989 Conference Proceedings}, 
title={An interface for rapid prototyping and evolutionary support of database-intensive applications}, 
year={1989}, 
volume={}, 
number={}, 
pages={344-348}, 
abstract={The authors present a programmer interface that enables rapid prototyping and evolutionary support of the database portion of applications. The interface supports flexible, rapid application development, and insulates applications from the underlying data model and database schema changes. The interface comprises a set of routines that allow the programmer to use logical objects and the logical relationships between objects to implement queries against a given database in a nonprocedural fashion. These routines implement a generic query processor that produces syntactically correct queries using only the object relationships existing at run time.<<ETX>>}, 
keywords={database management systems;software tools;rapid prototyping;evolutionary support;database-intensive applications;programmer interface;rapid application development;data model;logical objects;queries;Prototypes;Costs;Programming profession;Qualifications;Data models;Database languages;Information analysis;Information systems;Information retrieval;Research and development}, 
doi={10.1109/PCCC.1989.37412}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7811384, 
author={A. {Ringlstetter} and S. {Scherzinger} and T. F. {Bissyandé}}, 
booktitle={2016 IEEE/ACM 2nd International Workshop on Big Data Software Engineering (BIGDSE)}, 
title={Data Model Evolution Using Object-NoSQL Mappers: Folklore or State-of-the-Art?}, 
year={2016}, 
volume={}, 
number={}, 
pages={33-36}, 
abstract={In big data software engineering, the schema flexibility of NoSQL document stores is a major selling point: When the document store itself does not actively manage a schema, the data model is maintained within the application. Just like object-relational mappers for relational databases, object-NoSQL mappers are part of professional software development with NoSQL document stores. Some mappers go beyond merely loading and storing Java objects: Using dedicated evolution annotations, developers may conveniently add, remove, or rename attributes from stored objects, and also conduct more complex transformations. In this paper, we analyze the dissemination of this technology in Java open source projects. While we find evidence on GitHub that evolution annotations are indeed being used, developers do not employ them so much for evolving the data model, but to solve different tasks instead. Our observations trigger interesting questions for further research.}, 
keywords={Big Data;data models;Java;NoSQL databases;software engineering;data model evolution;object-NoSQL mappers;big data software engineering;NoSQL document stores;software development;Java objects;evolution annotations;Java open source projects;GitHub;Data models;Java;Transient analysis;Software;Loading;Big data;Software engineering;Object-NoSQL mappers;data model evolution}, 
doi={10.1109/BIGDSE.2016.014}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1214951, 
author={A. {Rashid}}, 
booktitle={Seventh International Database Engineering and Applications Symposium, 2003. Proceedings.}, 
title={A framework for customizable schema evolution in object-oriented databases}, 
year={2003}, 
volume={}, 
number={}, 
pages={342-346}, 
abstract={This paper describes an evolution framework supporting customization of the schema evolution and instance adaptation approaches in an object database management system. The framework is implemented as an integral part of an interpreter for a language with a versioned type system and employs concepts from object-oriented frameworks and aspect-oriented programming to support flexible changes. Some example customizations currently implemented with the framework are also described.}, 
keywords={object-oriented databases;object-oriented programming;evolutionary computation;customizable schema evolution;object-oriented database;evolution framework;instance adaptation;object database management system;language interpreter;versioned type system;object-oriented framework;aspect-oriented programming;flexible change support;database schema;schema modification;AOP;Object oriented databases;Object oriented modeling;Database systems;Application software;History;Genetic programming;Object oriented programming;Concrete;Data engineering;Knowledge engineering}, 
doi={10.1109/IDEAS.2003.1214951}, 
ISSN={1098-8068}, 
month={July},}
@INPROCEEDINGS{7461391, 
author={J. {Eder} and J. {Köpke}}, 
booktitle={2009 IEEE International Technology Management Conference (ICE)}, 
title={Towards semantic interoperability in an evolving environment}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={The interoperability between different business parties is a major issue in Enterprise Integration. As the world is not static and all things tend to evolve also enterprises evolve. Unfortunately, interoperable applications make the complex task of information system evolution even more complex. This has the consequence that changes are very expensive or that they are simply not realized. This results in static structures which are not fully suited to deal with the Enterprises needs. Changes happen on the syntactic and the semantic level. A change on the syntactic level can be a simple rename of a field in an exchanged document. A change on the semantic level is typically the case if the domain changes. This means that the meaning of data is changed. There are conflicting constraints for solutions supporting semantic interoperability in evolutionary enterprises: The necessity of expressive languages for formulating the changes and the consequences including the formulation of transformations on the one hand and the requirement for a very high throughput and the computation of a large number of instance documents on the other hand. We therefore introduce an architecture which is intended to combine scalability with complex reasoning. Ontological languages and tools, transformation of XML documents and the techniques for knowledge compilation are the basis for finding a viable and affordable solution.}, 
keywords={business data processing;evolutionary computation;information systems;ontologies (artificial intelligence);XML;semantic interoperability;enterprise integration;information system evolution;static structure;syntactic level;semantic level;evolutionary enterprise;ontological language;XML document;XML;Generators;Information System Evolution;Ontology Evolution;XML Schema Evolution;Instance Migration}, 
doi={10.1109/ITMC.2009.7461391}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8022723, 
author={P. {Khumnin} and T. {Senivongse}}, 
booktitle={2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
title={SQL antipatterns detection and database refactoring process}, 
year={2017}, 
volume={}, 
number={}, 
pages={199-205}, 
abstract={SQL antipatterns are frequently-made missteps that are commonly found in the design of relational databases, the use of SQL, and the development of database applications. They are intended to solve certain problems but will eventually lead to other problems. The motivation of this paper is how to assist database administrators in diagnosing SQL antipatterns and suggest refactoring techniques to solve the antipatterns. Specifically, we attempt to automate the detection of logical database design antipatterns by developing a tool that uses Transact-SQL language to query and analyze the database schema. The tool reports on potential antipatterns and gives an instruction on how to refactor the database schema. In an evaluation based on three databases from the industry, the performance of the tool is satisfactory in terms of recall of the antipatterns but the tool detects a number of false positives which affect its precision. It is found that SQL antipatterns detection still largely depends on the semantics of the data and the detection tool should rather be used in a semi-automated manner, i.e it can point out potential problematic locations in the database schema which require further diagnosis by the database administrators. This approach would be useful especially in the context of large databases where manual antipatterns inspection is very difficult.}, 
keywords={program diagnostics;relational databases;SQL;SQL antipatterns detection;database refactoring process;relational databases design;Transact-SQL language;database schema query;database schema analysis;Tools;Structured Query Language;Cloning;Relational databases;Programming;Semantics;SQL antipattern;database refactoring;Transact-SQL;relational database}, 
doi={10.1109/SNPD.2017.8022723}, 
ISSN={}, 
month={June},}
@ARTICLE{617053, 
author={ and E. A. {Rundensteiner}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A transparent schema-evolution system based on object-oriented view technology}, 
year={1997}, 
volume={9}, 
number={4}, 
pages={600-624}, 
abstract={When a database is shared by many users, updates to the database schema are almost always prohibited because there is a risk of making existing application programs obsolete when they run against the modified schema. The paper addresses the problem by integrating schema evolution with view facilities. When new requirements necessitate schema updates for a particular user, then the user specifies schema changes to his personal view, rather than to the shared base schema. Our view schema evolution approach then computes a new view schema that reflects the semantics of the desired schema change, and replaces the old view with the new one. We show that our system provides the means for schema change without affecting other views (and thus without affecting existing application programs). The persistent data is shared by different views of the schema, i.e., both old as well as newly developed applications can continue to interoperate. The paper describes a solution approach of realizing the evolution mechanism as a working system, which as its key feature requires the underlying object oriented view system to support capacity augmenting views. We present algorithms that implement the complete set of typical schema evolution operations as view definitions. Lastly, we describe the transparent schema evolution system (TSE) that we have built on top of GemStone, including our solution for supporting capacity augmenting view mechanisms.}, 
keywords={object-oriented databases;software maintenance;configuration management;transparent schema evolution system;object oriented view technology;shared database;database schema updates;application programs;modified schema;view facilities;schema updates;view schema evolution approach;semantics;persistent data;evolution mechanism;object oriented view system;capacity augmenting views;schema evolution operations;view definitions;TSE;GemStone;capacity augmenting view mechanisms;Object oriented databases;Multimedia databases;Application software;Computer Society;Data models;Computer aided manufacturing;CADCAM;Multimedia systems;Information systems;Database systems}, 
doi={10.1109/69.617053}, 
ISSN={1041-4347}, 
month={July},}
@INPROCEEDINGS{8632144, 
author={V. {Filatov} and V. {Semenets}}, 
booktitle={2018 International Scientific-Practical Conference Problems of Infocommunications. Science and Technology (PIC S T)}, 
title={Methods for Synthesis of Relational Data Model in Information Systems Reengineering Problems}, 
year={2018}, 
volume={}, 
number={}, 
pages={247-251}, 
abstract={The purpose of the article is to develop a method for reengineering relational databases, taking into account the presence of implicit interrelated functionally dependent data that affect the structure of the logical model. The following results are obtained: the article proposes an approach to detecting previously unknown functional dependences, based on the analysis of a set of relational database. Classes of relational databases reengineering problems are singled out; the stage of forming the target logical scheme, common to the problems of adaptation and refactoring, is investigated. The sub-task of verification of the relational database logical scheme correspondence to the third normal form within the framework of this stage is considered using the synthesis method; it is shown that its solution involves a number of difficulties, in particular, the need to find the set of functional dependences being performed using the current data instance of some relational database. An approach is proposed for finding a set of functional dependencies from the relational structure data instance. As a direction for further research, one can highlight the implementation of empty values support at the stage of detection of functional dependences, as well as issues of data transfer without loss from the original structure of the database to the target one, obtained as a result of applying the reengineering methods.}, 
keywords={data models;database management systems;information systems;relational databases;software maintenance;systems re-engineering;information systems reengineering problems;logical model;unknown functional dependences;target logical scheme;relational database logical scheme correspondence;synthesis method;functional dependencies;relational structure data instance;reengineering methods;relational data model;dependent data;data instance;Data models;Relational databases;Information systems;Software;Maintenance engineering;Finite element analysis;reengineering;relational database;functional dependence;detection of dependencies;universal relation;closure of functional dependences}, 
doi={10.1109/INFOCOMMST.2018.8632144}, 
ISSN={}, 
month={Oct},}
@ARTICLE{87977, 
author={S. L. {Osborn}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={The role of polymorphism in schema evolution in an object-oriented database}, 
year={1989}, 
volume={1}, 
number={3}, 
pages={310-317}, 
abstract={A polymorphic object algebra for an object-oriented database model is introduced. Types of schema modification that follow naturally from this model are described. It is shown to what extent queries return identical or equivalent results when the objects in the database are modified to conform to a modified schema.<<ETX>>}, 
keywords={database management systems;database theory;object-oriented programming;polymorphism;schema evolution;object-oriented database;polymorphic object algebra;model;Object oriented databases;Object oriented modeling;Data models;Database systems;Relational databases;Algebra;Object oriented programming;Aggregates;Indexes;Database languages}, 
doi={10.1109/69.87977}, 
ISSN={1041-4347}, 
month={Sep.},}
@INPROCEEDINGS{315307, 
author={C. T. {Liu} and S. K. {Chang} and P. K. {Chrysanthis}}, 
booktitle={Proceedings of ICCI'93: 5th International Conference on Computing and Information}, 
title={An entity-relationship approach to schema evolution}, 
year={1993}, 
volume={}, 
number={}, 
pages={575-578}, 
abstract={A dynamically evolving information system supports changes to its database schema in order to facilitate the needs of new application programs. This paper presents an approach to schema evolution through changes to the entity-relationship schema of a database. This approach has the advantage of being closer to the designer's perception of data, rather than to the logical database schema which describes how data are stored in the database. The underlying database structure is re-organized, if necessary, to accommodate new data without the changes affecting existing objects. In this way and through the construction of views, modifications of existing programs are avoided while all objects in the database are accessible to all application programs, both new and old. Thus, the invariance of the programs' interface is preserved by the views.<<ETX>>}, 
keywords={data structures;database management systems;entity-relationship modelling;relational databases;schema evolution;entity-relationship approach;database schema;dynamically evolving information system;application programs;Erbium;Object oriented databases;Object oriented modeling;Relational databases;Information systems;Context modeling;Computer science;Application software;Graphics;Data models}, 
doi={10.1109/ICCI.1993.315307}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{819811, 
author={F. {Grandi} and F. {Mandreoli} and M. R. {Scalas}}, 
booktitle={Proceedings 11th Australasian Database Conference. ADC 2000 (Cat. No.PR00528)}, 
title={A generalized modeling framework for schema versioning support}, 
year={2000}, 
volume={}, 
number={}, 
pages={33-40}, 
abstract={Advanced object-oriented applications require the management of schema versions, in order to cope with changes in the structure of the stored data. Two types of versioning have been separately considered so far: branching and temporal. The former arose in application domains like CAD/CAM and software engineering, where different solutions have been proposed to support design schema versions (consolidated versions). The latter concerns temporal databases, where some works have considered temporal schema versioning to fulfil advanced needs of other typical object-oriented applications like GIS and multimedia. In this work, we propose a general model which integrates the two approaches by supporting both design and temporal schema versions. The model is provided with a complete set of schema change primitives for fully-fledged version manipulation whose semantics is described in the paper.}, 
keywords={object-oriented databases;temporal databases;configuration management;data structures;database theory;generalized modeling framework;schema versioning support;object-oriented applications;data structure changes;branching;temporal schema versioning;CAD/CAM;software engineering;design schema versions;consolidated versions;temporal databases;geographic information systems;multimedia;schema change primitives;version manipulation;semantics;schema evolution;Geographic Information Systems;Computer aided manufacturing;CADCAM;Design engineering;Electrical capacitance tomography;History;Relational databases;US Department of Transportation;Postal services;Bibliographies}, 
doi={10.1109/ADC.2000.819811}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6201658, 
author={H. {Liu} and S. {Wu}}, 
booktitle={2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
title={Data storage schema upgrade via metadata evolution in SaaS}, 
year={2012}, 
volume={}, 
number={}, 
pages={3148-3151}, 
abstract={SaaS has become a typical cloud computing delivery model. In sparse table, one of the most popular multi-tenant data storage schemas for SaaS, all tenants' data are stored into sparse table and mapped to tenant's logical view by metadata. During the data storage schema upgrade in SaaS, all tenants' data need to be migrated into the new data schema before it becomes effective to ensure the integrity of the tenants' data. However, the migration is complex and brings overhead workload. Worse still, it may cause the system unusable. In this paper, we propose metadata evolution technology. We can realize the mapping from the old data schema to new data schema smoothly via metadata evolution during schema upgrade. So the schema upgrade can be completed without data migration.}, 
keywords={cloud computing;data integrity;meta data;storage management;SaaS;cloud computing delivery model;multitenant data storage schema upgrade;sparse table;tenant logical view;tenant data integrity;metadata evolution technology;Memory;Engines;Decision support systems;Educational institutions;Databases;Software;Data models;SaaS;schema upgrade;data migration;metadata evolution}, 
doi={10.1109/CECNet.2012.6201658}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6356090, 
author={ and and and }, 
booktitle={2012 14th Asia-Pacific Network Operations and Management Symposium (APNOMS)}, 
title={Design and implementation of database schema evolution for service continuity of web-based internet applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Today, Internet services are utilized as major service platforms to provide business services. Web-based Internet applications that follow the multi-tier architecture are a well-accepted computation model when providing Internet services. Such a model suffers from interruption caused by software failures, hardware errors or software maintenance. The interruption may incur significant financial loss to Internet service providers. Thus, Service Continuity is one of the main challenges for Web-based Internet applications. Although several service continuity technologies have been developed, many of them focus on a single tier of the applications and do not guarantee end-to-end service continuity. In this paper, we describe the design, implementation, and performance evaluation of our proposed service continuity technology. This technology resolves the long time interruption problem caused by database schema evolution, handles software components upgrade, and achieves end-to-end Service Continuity. We propose a mechanism that contains multiple modules running interactively between each module. This approach only requires minimal amount of code rewriting and a small storage space. Comparing with existing approaches, our approach is more efficient with the least overhead.}, 
keywords={business data processing;database management systems;interrupts;software maintenance;Web services;database schema evolution;service continuity technology;Web-based Internet applications;Internet services;business services;multitier architecture;computation model;software failures;hardware errors;software maintenance;long time interruption problem;software components upgrade handling;end-to-end service continuity;code rewriting;storage space;Databases;Software;Internet;Web and internet services;Interrupters;Computer architecture;Java;Web-based Internet Applications;Service Continuity;Database Schema Evolution}, 
doi={10.1109/APNOMS.2012.6356090}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7776448, 
author={M. {Al-Barak} and R. {Bahsoon}}, 
booktitle={2016 IEEE 8th International Workshop on Managing Technical Debt (MTD)}, 
title={Database Design Debts through Examining Schema Evolution}, 
year={2016}, 
volume={}, 
number={}, 
pages={17-23}, 
abstract={Causes of the database debt can stem from ill-conceptual, logical, and/or physical database design decisions, violations to key design databases principles, use of anti-patterns etc. In this paper, we explore the problem of relational database design debt and define the problem. We develop a taxonomy, which classifies various types of debts that can relate to conceptual, logical and physical design of a database. We define the concept of Database Design Debt, discuss their origin, causes and preventive mechanisms. We draw on MediaWiki case study and examine its database schema evolution to support our work. The contribution hopes to make database designers and application developers aware of these debts so they can minimize/avoid their consequences on a given system.}, 
keywords={relational databases;database design debts;database schema evolution examination;ill-conceptual database design decisions;logical database design decisions;physical database design decisions;design databases principles;antipattern usage;relational database design debt;MediaWiki case study;Taxonomy;Relational databases;Redundancy;Physical design;Business;Maintenance engineering;Technical debt;Database design debt;Database debt}, 
doi={10.1109/MTD.2016.9}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7840924, 
author={M. {Klettke} and U. {Störl} and M. {Shenavai} and S. {Scherzinger}}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={NoSQL schema evolution and big data migration at scale}, 
year={2016}, 
volume={}, 
number={}, 
pages={2764-2774}, 
abstract={This paper explores scalable implementation strategies for carrying out lazy schema evolution in NoSQL data stores. For decades, schema evolution has been an evergreen in database research. Yet new challenges arise in the context of cloud-hosted data backends: With all database reads and writes charged by the provider, migrating the entire data instance eagerly into a new schema can be prohibitively expensive. Thus, lazy migration may be more cost-efficient, as legacy entities are only migrated in case they are actually accessed by the application. Related work has shown that the overhead of migrating data lazily is affordable when a single evolutionary change is carried out, such as adding a new property. In this paper, we focus on long-term schema evolution, where chains of pending schema evolution operations may have to be applied. Chains occur when legacy entities written several application releases back are finally accessed by the application. We discuss strategies for dealing with chains of evolution operations, in particular, the composition into a single, equivalent composite migration that performs the required version jump. Our experiments with MongoDB focus on scalable implementation strategies. Our lineup further compares the number of write operations, and thus, the operational costs of different data migration strategies.}, 
keywords={Big Data;cloud computing;database management systems;NoSQL schema evolution;Big Data migration;lazy schema evolution;NoSQL data stores;cloud-hosted data backends;data instance;MongoDB;Databases;Big data;Context;Data models;Software;Runtime;Production;NoSQL Databases;Schema Evolution;Data Migration Strategies;Lazy Migration;Lazy Composite Migration;Incremental Migration;Predictive Migration}, 
doi={10.1109/BigData.2016.7840924}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{1173183, 
author={R. {Motz}}, 
booktitle={12th International Conference of the Chilean Computer Science Society, 2002. Proceedings.}, 
title={Problems in the maintenance of a federated database schema}, 
year={2002}, 
volume={}, 
number={}, 
pages={124-132}, 
abstract={We characterize the problem of maintenance of a federated schema to cope with local schema evolution in a tightly coupled federation. By means of an example, we present the problems that local schema changes could cause on the federated schema and show proposed solutions.}, 
keywords={distributed databases;software maintenance;data models;federated database schema maintenance;local schema evolution;tightly coupled federation;local schema changes;Object oriented databases;Productivity;Information technology;Investments;Database systems;Object oriented modeling;Computer science;Societies}, 
doi={10.1109/SCCC.2002.1173183}, 
ISSN={1522-4902}, 
month={Nov},}
@ARTICLE{4067088, 
author={M. {Saadatmand-Tarzjan} and H. A. {Moghaddam}}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={A Novel Evolutionary Approach for Optimizing Content-Based Image Indexing Algorithms}, 
year={2007}, 
volume={37}, 
number={1}, 
pages={139-153}, 
abstract={Optimization of content-based image indexing and retrieval (CBIR) algorithms is a complicated and time-consuming task since each time a parameter of the indexing algorithm is changed, all images in the database should be indexed again. In this paper, a novel evolutionary method called evolutionary group algorithm (EGA) is proposed for complicated time-consuming optimization problems such as finding optimal parameters of content-based image indexing algorithms. In the new evolutionary algorithm, the image database is partitioned into several smaller subsets, and each subset is used by an updating process as training patterns for each chromosome during evolution. This is in contrast to genetic algorithms that use the whole database as training patterns for evolution. Additionally, for each chromosome, a parameter called age is defined that implies the progress of the updating process. Similarly, the genes of the proposed chromosomes are divided into two categories: evolutionary genes that participate to evolution and history genes that save previous states of the updating process. Furthermore, a new fitness function is defined which evaluates the fitness of the chromosomes of the current population with different ages in each generation. We used EGA to optimize the quantization thresholds of the wavelet-correlogram algorithm for CBIR. The optimal quantization thresholds computed by EGA improved significantly all the evaluation measures including average precision, average weighted precision, average recall, and average rank for the wavelet-correlogram method}, 
keywords={content-based retrieval;database indexing;genetic algorithms;image retrieval;visual databases;wavelet transforms;content-based image indexing algorithm;content-based image retrieval algorithm;database indexing;evolutionary group algorithm;time-consuming optimization problem;image database;genetic algorithm;fitness function;image quantization;wavelet-correlogram algorithm;Indexing;Biological cells;Partitioning algorithms;Image databases;Quantization;Information retrieval;Image retrieval;Content based retrieval;Optimization methods;Evolutionary computation;Content-based image indexing and retrieval (CBIR);evolutionary algorithms (EAs);evolutionary group algorithm (EGA);genetic algorithms (GAs);global optimization;wavelet correlogram;Algorithms;Artificial Intelligence;Biomimetics;Database Management Systems;Databases, Factual;Documentation;Evolution;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Genetic;Pattern Recognition, Automated}, 
doi={10.1109/TSMCB.2006.880137}, 
ISSN={1083-4419}, 
month={Feb},}
@INPROCEEDINGS{506064, 
author={R. {Sargent} and D. {Fuhrman} and T. {Critchlow} and T. {Di Sera} and R. {Mecklenburg} and G. {Lindstrom} and P. {Cartwright}}, 
booktitle={Proceedings of 8th International Conference on Scientific and Statistical Data Base Management}, 
title={The design and implementation of a database for human genome research}, 
year={1996}, 
volume={}, 
number={}, 
pages={220-225}, 
abstract={The Human Genome Project poses severe challenges in database design and implementation. These include comprehensive coverage of diverse data domains and user constituencies; robustness in the presence of incomplete, inconsistent and multi-version data; accessibility through many levels of abstraction, and scalability in content and organizational complexity. The paper presents a new data model developed to meet these challenges by the Utah Center for Human Genome Research. The central characteristics are: (i) a high level data model comprising five broadly applicable workflow notions; (ii) representation of those notions as objects in an extended relational model; (iii) expression of working database schemas as meta data in administration tables; (iv) population of the database through tables dependent on the meta data tables; and (v) implementation via a conventional relational database management system. The authors explore two advantages of this approach: the resulting representational flexibility, and the reflective use of meta data to accomplish schema evolution by ordinary updates. Implementation and performance pragmatics of this work are sketched, as well as implications for future database development.}, 
keywords={relational databases;data structures;genetics;cellular biophysics;molecular biophysics;scientific information systems;biology computing;object-oriented databases;Human Genome Project;database design;database implementation;diverse data domains;diverse user constituencies;incomplete data;multi-version data;inconsistent data;abstraction;content scalability;organizational complexity scalability;data model;Utah Center for Human Genome Research;high level data model;workflow notions;extended relational model;working database schemas;meta data;administration tables;database population;meta data tables;relational database management system;Humans;Genomics;Bioinformatics;Relational databases;Data models;Laboratories;Informatics;Protocols;Computer science;Cities and towns}, 
doi={10.1109/SSDM.1996.506064}, 
ISSN={}, 
month={June},}
@ARTICLE{76261, 
author={E. {Bertino} and L. {Martino}}, 
journal={Computer}, 
title={Object-oriented database management systems: concepts and issues}, 
year={1991}, 
volume={24}, 
number={4}, 
pages={33-47}, 
abstract={Requirements imposed on both the object data model and object management by the support of complex objects are outlined. The basic concepts of an object-oriented data model are discussed. They are objects and object identifiers, aggregation, classes and instantiation mechanisms, metaclasses, and inheritance. Object-oriented models are compared with semantic, relational, and Codasyl models. Object-oriented query languages and query processing are considered. Some operational aspects of data management in object-oriented systems are examined. Schema evolution is discussed.<<ETX>>}, 
keywords={object-oriented databases;object oriented DBMS;schema evolution;object data model;object management;complex objects;object identifiers;aggregation;classes;instantiation mechanisms;metaclasses;inheritance;query languages;query processing;operational aspects;Database systems;Object oriented modeling;Application software;Data models;Power system modeling;Object oriented databases;Computer languages;Research and development management;Technology management;Research and development}, 
doi={10.1109/2.76261}, 
ISSN={0018-9162}, 
month={April},}
@INPROCEEDINGS{14770, 
author={H. -. {Chou} and W. {Kim}}, 
booktitle={25th ACM/IEEE, Design Automation Conference.Proceedings 1988.}, 
title={Versions and change notification in an object-oriented database system}, 
year={1988}, 
volume={}, 
number={}, 
pages={275-281}, 
abstract={The authors have built a prototype object-oriented database system called ORION to support applications from the CAD/CAM (computer-aided-design/computer-aided-manufacturing), AI (artificial-intelligence), and office-information-system domains. Advanced functions supported in ORION include versions, change notification, composite objects, dynamic schema evolution, and multimedia data. The versions and change notification features are based on a model that the authors developed earlier. They have integrated their model of versions and change notification into the ORION object-oriented data model, and also provide an insight into system overhead that versions and change notification incur.<<ETX>>}, 
keywords={artificial intelligence;CAD/CAM;database management systems;office automation;object-oriented database system;ORION;CAD/CAM;computer-aided-design/computer-aided-manufacturing;AI;(artificial-intelligence);office-information-system;change notification;dynamic schema evolution;object-oriented data model;Object oriented databases;Object oriented modeling;Spatial databases;Prototypes;Database systems;Application software;Computer aided manufacturing;CADCAM;Artificial intelligence;Multimedia databases}, 
doi={10.1109/DAC.1988.14770}, 
ISSN={0738-100X}, 
month={June},}
@INPROCEEDINGS{65346, 
author={D. {Christodoulakis} and P. {Soupos} and S. {Goutas}}, 
booktitle={[Proceedings 1989] IEEE International Workshop on Tools for Artificial Intelligence}, 
title={Adaptive DB schema evolution via constrained relationships}, 
year={1989}, 
volume={}, 
number={}, 
pages={393-398}, 
abstract={A novel object-oriented data model and a tailored query language that enables the construction of evolving database (DB) schemas according to the information provided by transaction monitoring are presented. The data model and the query language are based on the notion of constrained relationships that enable interconnections between DB objects under constraints specified by rules. These relationships capture frequent transactions with the database and freeze them on the database schema. Thus, the database schema can evolve in terms of changing retrieval requirements. Furthermore, these frequent transactions are mapped at a lower level between object instances, so that future transactions may be answered more quickly. The schema adaptation mechanism is based on the identification of commonly used, simple query patterns.<<ETX>>}, 
keywords={database theory;object-oriented programming;query languages;transaction processing;adaptive database schema evolution;object interconnections;query pattern identification;constrained relationships;object-oriented data model;tailored query language;transaction monitoring;retrieval requirements;frequent transactions;schema adaptation mechanism;Data models;Transaction databases;Object oriented databases;Database languages;Object oriented modeling;LAN interconnection;Query processing;Data engineering;Computerized monitoring;Stability}, 
doi={10.1109/TAI.1989.65346}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{635287, 
author={ and and and and and }, 
booktitle={1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation}, 
title={Developing a data warehouse as the first step for information systems reengineering}, 
year={1997}, 
volume={3}, 
number={}, 
pages={2403-2408 vol.3}, 
abstract={All practitioners of information system know that building an industrial-strength information system is both a long-term commitment and a long-term investment for an organization. Any surviving system should encounter many times of changes either major or minor; and these changes might induce the system serious problems and a lot of maintenance cost. According to the recent development of database reengineering, data warehousing and business reengineering, many organizations have had their information system operating for years. They are facing the problems of introducing a new database system and the re-designation of the data processing procedure. For the information system to serve effectively and efficiently as time goes by, there are two ways to improve it. Firstly, re-design the whole system by using new technology and new database models; secondly, introduce a method that will make use of the existing systems to the maximum extent while the new requirements are still satisfied, i.e. Information systems reengineering. In this paper, we propose a framework for practitioners to complete the reengineering work more smoothly. The authors have applied this framework to the system reengineering work for a manufacturer. An EIS prototype is developed, according to the new requirements of high-level managers, and the system is operating currently.}, 
keywords={management information systems;systems re-engineering;systems analysis;database management systems;data warehouse;information systems reengineering;industrial-strength information system;database reengineering;business reengineering;EIS prototype;high-level managers;Data warehouses;Information systems;Investments;Costs;Warehousing;Business process re-engineering;Database systems;Data processing;Management information systems;Manufacturing}, 
doi={10.1109/ICSMC.1997.635287}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{8530076, 
author={J. {Delplanque} and A. {Etien} and N. {Anquetil} and O. {Auverlot}}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Relational Database Schema Evolution: An Industrial Case Study}, 
year={2018}, 
volume={}, 
number={}, 
pages={635-644}, 
abstract={Modern relational database management systems provide advanced features allowing, for example, to include behaviour directly inside the database (stored procedures). These features raise new difficulties when a database needs to evolve (e.g. adding a new table). To get a better understanding of these difficulties, we recorded and studied the actions of a database architect during a complex evolution of the database at the core of a software system. From our analysis, problems faced by the database architect are extracted, generalized and explored through the prism of software engineering. Six problems are identified: (1) difficulty in analysing and visualising dependencies between database's entities, (2) difficulty in evaluating the impact of a modification on the database, (3) replicating the evolution of the database schema on other instances of the database, (4) difficulty in testing database's functionalities, (5) lack of synchronization between the IDE's internal model of the database and the database actual state and (6) absence of an integrated tool enabling the architect to search for dependencies between entities, generate a patch or access an up to date PostgreSQL documentation. We suggest that techniques developed by the software engineering community could be adapted to help in the development and evolution of relational databases.}, 
keywords={relational databases;software engineering;SQL;software system;software engineering;relational database management systems;PostgreSQL documentation;Tools;Visual databases;Videos;Software engineering;Laboratories;Relational databases;relational database;software engineering;evolution}, 
doi={10.1109/ICSME.2018.00073}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{5202544, 
author={M. B. P. {Carneiro} and A. C. P. {Veiga} and F. C. {Castro} and E. L. {Flores} and G. A. {Carrijo}}, 
booktitle={2009 IEEE International Conference on Multimedia and Expo}, 
title={Application of evolutionary algorithms for iris localization}, 
year={2009}, 
volume={}, 
number={}, 
pages={506-509}, 
abstract={An iris recognition system requires efficient image processing techniques in order to duly represent and interpret the iris structural characteristics of an individual. The first processing stage should be the identification of the iris region in an eye image. This work introduces the application of evolutionary algorithms to localize the iris region in an eye image. A method based on memetic algorithms was proposed and used to find the circles that represent the external iris border and the pupil border in an edge map. The efficiency of the memetic algorithm in solving the problem was compared to the application of the Wildes' method, which uses the circular Hough transform, a well known algorithm employed to find circles in an edged image. To test the algorithms, images from a public database were used. The results show that the proposed application has an encouraging performance.}, 
keywords={edge detection;evolutionary computation;Hough transforms;evolutionary algorithm;iris recognition system;eye image processing technique;iris structural characteristics;pupil border;circular Hough transform;Wilde method;edged image;public database;Evolutionary computation;Pixel;Iris recognition;Image edge detection;Evolution (biology);Image processing;Image databases;Image segmentation;Biology computing;Biological cells;Biometry;Iris Recognition;Iris Segmentation;Evolutionary Algorithms;Memetic Algorithms}, 
doi={10.1109/ICME.2009.5202544}, 
ISSN={1945-7871}, 
month={June},}
@INPROCEEDINGS{7752305, 
author={A. {Kastrin} and T. C. {Rindflesch} and D. {Hristovski}}, 
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={Evolution of MEDLINE bibliographic database: Preliminary results}, 
year={2016}, 
volume={}, 
number={}, 
pages={644-646}, 
abstract={In this preliminary work we propose an approach to tracking network communities in time. We describe a methodology to study the dynamics and evolution of the MEDLINE bibliographic database using network-based analysis. We explore how the temporal characteristics of the network can be used to provide insight into the historical evolution of the broad field of biomedicine.}, 
keywords={bibliographic systems;medical information systems;network theory (graphs);social networking (online);MEDLINE bibliographic database evolution;network community tracking;MEDLINE bibliographic database;network-based analysis;network temporal characteristics;biomedicine field;Complex networks;Evolution (biology);Libraries;Complexity theory;Biomedical measurement;Life sciences;Network analysis;Community evolution;Network evolution;MEDLINE}, 
doi={10.1109/ASONAM.2016.7752305}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1402116, 
author={ and }, 
booktitle={Ninth European Conference on Software Maintenance and Reengineering}, 
title={Extracting entity relationship diagram from a table-based legacy database}, 
year={2005}, 
volume={}, 
number={}, 
pages={72-79}, 
abstract={Current database reverse engineering researches presume that the information regarding semantics of attributes, primary keys, and foreign keys in database tables is complete. However, this may not be the case. In this paper, we present a process that extracts an extended entity relationship diagram from a table-based database with little descriptions for the fields in its tables and no description for keys. The primary inputs of our approach are system display forms and table schema. An extended ER diagram is successfully extracted in a case study.}, 
keywords={entity-relationship modelling;relational databases;reverse engineering;systems re-engineering;formal specification;knowledge acquisition;entity relationship diagram extraction;table-based legacy database;database reverse engineering;primary keys;foreign keys;extended ER diagram;data model;software reengineering;Reverse engineering;Data mining;Data models;Data structures;Database systems;Data analysis;Software quality;Logic programming;Application software;Performance analysis;database;reverse engineering;entity relationship diagram;data model;case study}, 
doi={10.1109/CSMR.2005.31}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{5564636, 
author={ and }, 
booktitle={2010 3rd International Conference on Computer Science and Information Technology}, 
title={Notice of Retraction lt;br gt;Research on cutting data evolution based on cultural algorithm}, 
year={2010}, 
volume={3}, 
number={}, 
pages={70-72}, 
abstract={This article has been retracted by the publisher.}, 
keywords={cutting;database management systems;evolutionary computation;mechanical engineering computing;prototypes;cutting data evolution model;cultural algorithm;manufacturing environment;priori experience space;posterior knowledge space;optimization;prototype system;large-scale evolution;cutting parameter;cutting database design;Databases;cultural algorithm;cutting data evolution;cutting database}, 
doi={10.1109/ICCSIT.2010.5564636}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{617348, 
author={J. {Samos} and F. {Salter}}, 
booktitle={Database and Expert Systems Applications. 8th International Conference, DEXA '97. Proceedings}, 
title={External schemas in a schema-evolution environment for OODBs}, 
year={1997}, 
volume={}, 
number={}, 
pages={516-522}, 
abstract={External schemas are derived from the database conceptual schema; they can be used to simulate changes in it. Sometimes the final users' information requirements change: they need new information which cannot be derived from the information in the database. The solution put forward here is the definition of partially derived classes (capacity augmenting classes) which may contain non-derived information in the intension and moreover, unlike other systems, in the extension. When an external schema with partially derived classes is to be defined, the conceptual schema has to be modified in order to include the non-derived information. In order to avoid unnecessary modifications of the conceptual schema the use of a test environment for the definition of temporal external schemas is also proposed.}, 
keywords={object-oriented databases;abstract data types;data structures;database theory;schema-evolution environment;object-oriented databases;database conceptual schema;user information requirements;partially derived classes;nonderived information;external schema;test environment;temporal external schemas;Testing;Relational databases;Dictionaries}, 
doi={10.1109/DEXA.1997.617348}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{625708, 
author={A. {Pons} and R. K. {Keller}}, 
booktitle={Proceedings of the 1997 International Database Engineering and Applications Symposium (Cat. No.97TB100166)}, 
title={Schema evolution in object databases by catalogs}, 
year={1997}, 
volume={}, 
number={}, 
pages={368-376}, 
abstract={We are concerned with schema evolution in object oriented databases (OODB) that is processed by a modification of the classes of the schema. We present a new categorization of the different modifications in three categories: primitive, composite and complex modifications. On this basis, we propose a method by decomposition for addressing conceptual schema evolution: "real-life" complex schema modifications are solved by decomposition into simpler well known and controlled modifications, called composites, which in turn may be reduced to so-called primitives. A key step in making this approach practical, is the provision of two catalogs: one for the primitives based on a sound object model and one for the composites built on top of the primitive catalog. Such catalogs raise the level of abstraction, further reuse and are prerequisites for effective tool support. We define the three types of schema modifications, describe these catalogs, put them into the context of our decomposition approach and provide a process for schema evolution.}, 
keywords={object-oriented databases;cataloguing;database theory;software reusability;schema evolution;object oriented databases;catalogs;categorization;primitive modification;composite modification;complex modification;conceptual schema;object model;software reuse;software tool;ODMG;Object Database Management Group;Catalogs;Object oriented databases;Electronic mail;Object oriented modeling;Computer aided manufacturing;CADCAM;Councils;Automation}, 
doi={10.1109/IDEAS.1997.625708}, 
ISSN={1098-8068}, 
month={Aug},}
@INPROCEEDINGS{492251, 
author={V. {Crestana-Taube} and E. A. {Rundensteiner}}, 
booktitle={Proceedings RIDE '96. Sixth International Workshop on Research Issues in Data Engineering}, 
title={Consistent view removal in transparent schema evolution systems}, 
year={1996}, 
volume={}, 
number={}, 
pages={138-147}, 
abstract={We have developed the transparent schema evolution (TSE) system that, simulating schema evolution using object-oriented views, allows for the interoperability of applications with diverse and even changing requirements. TSE relieves users of the risk of making existing application programs obsolete when run against the modified schema, because the old view schema is maintained while a new view schema is generated to capture the changes desired by the user. However TSE may be generating a large number of schema versions (object-oriented view schemata) over time, resulting in an excessive build-up of classes and underlying object instances-some of which may potentially no longer be in use. We propose to solve this problem by developing techniques for effective and consistent schema removal. First, we characterize four potential problems of schema consistency that could be caused by removal of a single virtual class; and then outline our solution approach for each of these problems. Second, we demonstrate that view schema removal is sensitive to the order in which individual classes are processed. Our solution to this problem is the development of a dependency graph model for capturing the class relationships, used as a foundation for selecting among removal sequences. Designed to optimize the performance of the TSE system by effective schema version removal, the proposed techniques will enable more effective interoperability among evolving software applications.}, 
keywords={object-oriented databases;open systems;database theory;software performance evaluation;query processing;graph theory;consistent view removal;transparent schema evolution systems;object-oriented views;interoperability;application programs;view schema;object-oriented database;object instances;schema removal;schema consistency;virtual class;view schema removal;dependency graph model;performance;schema version removal;Application software;Object oriented modeling;Computer science;Software systems;Laboratories;Computational modeling;Marine vehicles;Design optimization;Software performance;Veins}, 
doi={10.1109/RIDE.1996.492251}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7184913, 
author={F. {Yang} and D. {Milosevic} and J. {Cao}}, 
booktitle={2015 IEEE First International Conference on Big Data Computing Service and Applications}, 
title={An Evolutionary Algorithm for Column Family Schema Optimization in HBase}, 
year={2015}, 
volume={}, 
number={}, 
pages={439-445}, 
abstract={Apache HBase is a column-oriented NoSQL key-value store built on top of the Hadoop distributed file-system. Logically, columns in HBase are grouped into column families. Physically, all columns in one column family are stored in the same set of files. Therefore the division of column families is closely related to the response time for a specific row query. In this paper, one new Evolutionary Algorithm is designed and applied to find the optimum column family schema for the given user queries. The reading performance of the optimized column family schema is evaluated on a real dataset provided by ZANOX AG, which contains 2.6 million rows of aggregated tracking data and 1.3 million user queries. It is shown that by using the found optimized column family schema, the reading performance of HBase is improved with a statistical significance. User queries from a testing set show that the average response time is reduced by up to 72% compared to un-optimized column family schemas.}, 
keywords={data handling;evolutionary computation;parallel processing;query processing;SQL;statistical analysis;evolutionary algorithm;column family schema optimization;Apache HBase;column-oriented NoSQL key-value store;Hadoop distributed file-system;user queries;ZANOX AG;user queries;statistical significance;Optimization;Layout;Conferences;Big data;Evolutionary computation;Genetic algorithms;Algorithm design and analysis;HBase;NoSQL;Column Family;Column Layout;Schema Optimization;Evolutionary Algorithm}, 
doi={10.1109/BigDataService.2015.20}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{796507, 
author={}, 
booktitle={Proceedings Technology of Object-Oriented Languages and Systems (Cat. No.PR00393)}, 
title={A survey of schema evolution in object-oriented databases}, 
year={1999}, 
volume={}, 
number={}, 
pages={362-371}, 
abstract={Changes in the real world may require both the database population and the database schema to evolve. Particularly, this is the case in CAD/CAM and CASE database systems, in which the design objects constantly evolve in every aspect. On the other hand, the prototyping of a database design may also involve changes to both the structure and behavior of a schema. Unfortunately, most of the current systems offer little support for schema evolution. In this paper, we survey the recent development of the research in the schema evolution of object-oriented databases. The main issues in the current research are identified.}, 
keywords={CAD/CAM;software prototyping;object-oriented databases;schema evolution;object-oriented databases;database population;database schema;CAD/CAM;CASE database systems;prototyping;database design;Object oriented databases;Database systems;Transaction databases;Computer aided manufacturing;Computer aided software engineering;Design automation;CADCAM;Prototypes;Management information systems;Technology management}, 
doi={10.1109/TOOLS.1999.796507}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6648199, 
author={H. {Schink}}, 
booktitle={2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={sql-schema-comparer: Support of multi-language refactoring with relational databases}, 
year={2013}, 
volume={}, 
number={}, 
pages={173-178}, 
abstract={Refactoring is a method to change a source-code's structure without modifying its semantics and was first introduced for object-oriented code. Since then refactorings were defined for relational databases too. But database refactorings must be treated differently because a database schema's structure defines semantics used by other applications to access the data in the schema. Thus, many database refactorings may break interaction with other applications if not treated appropriately. We discuss problems of database refactoring in regard to Java code and present sql-schema-comparer, a library to detect refactorings of database schemes. The sql-schema-comparer library is our first step to more advanced tools supporting developers in their database refactoring efforts.}, 
keywords={object-oriented methods;relational databases;software maintenance;SQL;SQL-schema-comparer library;multi-language refactoring;relational databases;object-oriented code;database schema structure;database refactorings;Java code;Libraries;Java;Semantics;Relational databases;Software;Conferences}, 
doi={10.1109/SCAM.2013.6648199}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5447778, 
author={G. {Papastefanatos} and P. {Vassiliadis} and A. {Simitsis} and Y. {Vassiliou}}, 
booktitle={2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)}, 
title={HECATAEUS: Regulating schema evolution}, 
year={2010}, 
volume={}, 
number={}, 
pages={1181-1184}, 
abstract={HECATAEUS is an open-source software tool for enabling impact prediction, what-if analysis, and regulation of relational database schema evolution. We follow a graph theoretic approach and represent database schemas and database constructs, like queries and views, as graphs. Our tool enables the user to create hypothetical evolution events and examine their impact over the overall graph before these are actually enforced on it. It also allows definition of rules for regulating the impact of evolution via (a) default values for all the nodes of the graph and (b) simple annotations for nodes deviating from the default behavior. Finally, HECATAEUS includes a metric suite for evaluating the impact of evolution events and detecting crucial and vulnerable parts of the system.}, 
keywords={graph theory;public domain software;relational databases;regulating schema evolution;HECATAEUS;open source software;relational database;graph theoretic approach;database construction;Relational databases;Event detection;Application software;Computer crashes;Remuneration;Open source software;Software tools;Embedded software;Control systems;Floods}, 
doi={10.1109/ICDE.2010.5447778}, 
ISSN={1063-6382}, 
month={March},}
@INPROCEEDINGS{65081, 
author={F. {Sadri}}, 
booktitle={[1989] Proceedings of the Thirteenth Annual International Computer Software Applications Conference}, 
title={Object-oriented database systems}, 
year={1989}, 
volume={}, 
number={}, 
pages={195-196}, 
abstract={The object model which is based on the abstract data type concept, provides a natural and more powerful modeling capability. This modeling power, coupled with efficiency of implementation, makes object-oriented database systems suitable for complex applications, such as engineering design applications. The author concentrates on: differences between object-oriented databases and object-oriented programming languages and differences between object-oriented databases and classical (relational) databases. The author argues the need for supporting schema evolution and object versions.<<ETX>>}, 
keywords={data structures;database management systems;high level languages;object-oriented programming;relational databases;abstract data type concept;powerful modeling capability;modeling power;object-oriented database systems;complex applications;engineering design applications;object-oriented programming languages;schema evolution;object versions;Database systems;Object oriented databases;Relational databases;Object oriented modeling;Concurrency control;Power system modeling;Database languages;Data engineering;Design engineering;Transaction databases}, 
doi={10.1109/CMPSAC.1989.65081}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{47269, 
author={F. {Oertly} and G. {Schiller}}, 
booktitle={[1989] Proceedings. Fifth International Conference on Data Engineering}, 
title={Evolutionary database design}, 
year={1989}, 
volume={}, 
number={}, 
pages={618-624}, 
abstract={An approach for evolutionary database design is presented which tries to remedy some of the shortcomings of previous design methods. The approach distinguishes clearly between a conceptual and a logical database design. A conceptual schema models the relevant aspects of reality. A logical schema describes the structure of the database as generic tables, and it reflects the design decisions taken to map the objects of the conceptual schema into the generic tables. To support this strategy with tools, it is necessary to have a version concept and a mechanism for recording design decisions called protocolling. These concepts are realized in Presto, a development environment for an evolutionary design of database applications which is described.<<ETX>>}, 
keywords={database management systems;programming environments;systems analysis;conceptual database design;reality modeling;evolutionary database design;logical database design;conceptual schema;logical schema;generic tables;design decisions;version concept;protocolling;Presto;development environment;database applications;Databases;Application software;Design methodology;Information retrieval;Software engineering;Computer applications;Process control;Software maintenance;Stress;Data engineering}, 
doi={10.1109/ICDE.1989.47269}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{5466932, 
author={M. {Nasiraghdam} and S. {Lotfi} and R. {Rashidy}}, 
booktitle={2010 International Conference on Information Retrieval Knowledge Management (CAMP)}, 
title={Query optimization in distributed database using hybrid evolutionary algorithm}, 
year={2010}, 
volume={}, 
number={}, 
pages={125-130}, 
abstract={Join execution order, suitable copy selection from a query tables, join execution location selection and semijoin strategy are effective agents of a query's execution plan's cost. The selection of an optimal hybrid among these for agents for finding an execution plan is whit lowest cost of NP-Complete problems that in this paper is used from a hybrid evolutionary algorithm (EALA) for solving of this problem. This algorithm has used combination genetic algorithm and learning automata for producing optimal execution plan for a query. In this paper we compare the results from hybrid algorithm whit the results of genetic algorithm execution meaningful results are achieved.}, 
keywords={distributed databases;genetic algorithms;learning automata;query processing;query optimization;distributed database;hybrid evolutionary algorithm;NP-complete problems;genetic algorithm;learning automata;Query processing;Distributed databases;Evolutionary computation;Genetic algorithms;Cost function;Learning automata;Distributed computing;Data engineering;Computer science;NP-complete problem;component;query optimizationg;distrbuted database;genetic algoritm;learning automata}, 
doi={10.1109/INFRKM.2010.5466932}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{1217993, 
author={A. {Telea} and F. {Frasincar} and G. -. {Houben}}, 
booktitle={Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.}, 
title={Visualisation of RDF(S)-based information}, 
year={2003}, 
volume={}, 
number={}, 
pages={294-299}, 
abstract={As Resource Description Framework (RDF) reaches maturity, there is an increasing need for tools that support it. A common and natural representation for RDF data is a directed labeled graph. Although there are tools to edit and/or browse RDF graph representations, we found their architecture rigid and not easily amenable to producing effective visual representations, especially for large RDF graphs. We discuss here how GViz, a general purpose graph visualisation tool, allows the easy construction and fine-tuning of various visual exploratory scenarios for RDF data. GViz's extended ability of customizing the visualisation's icons showed to be very useful in the context of RDF graph structures visualisation. Among the presented applications, we mention customizable selections, schema-instance comparison, instances comparison, and schemas comparison (schema evolution). GViz proved to be able not only to visualize large RDF data models, but also to be very flexible in designing scenario-specific queries to support the exploration process.}, 
keywords={data visualisation;directed graphs;data models;query processing;Internet;information resources;meta data;software tools;information retrieval;Resource Description Framework;directed labeled graph;RDF graph representation;GViz;graph visualisation tool;visual exploratory scenario;visualisation icon;RDF graph structure visualisation;customizable selection;schema-instance comparison;instance comparison;schema comparison;schema evolution;RDF data model;scenario-specific query;Resource description framework;Data visualization;Radar;Data models;Vocabulary;Knowledge representation;Packaging;Shape}, 
doi={10.1109/IV.2003.1217993}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1510113, 
author={I. {Garcia-Rodriguez de Guzman} and M. {Polo} and M. {Piattini}}, 
booktitle={21st IEEE International Conference on Software Maintenance (ICSM'05)}, 
title={An integrated environment for reengineering}, 
year={2005}, 
volume={}, 
number={}, 
pages={165-174}, 
abstract={This paper presents a tool specifically designed for database reengineering. As is well known, reengineering is the process of (1) applying reverse engineering to a software product to get higher-level specifications, and (2) using these specifications as the starting point for the development of a new version of the system. Thus, the complete process can be seen as a sequence of transformation functions that operate on the different sets of artifacts involved in the whole process. The starting point of the reengineering process is the physical schema of the database, which is translated into a vendor-independent metamodel; then, this is translated into a class diagram representing the possible conceptual schema used during the development of the database. This diagram is then taken as the starting point for the code generation process, which produces an executable application for four possible different platforms.}, 
keywords={systems re-engineering;reverse engineering;formal specification;data models;relational databases;programming environments;software reengineering integrated environment;database reengineering tool;software reverse engineering;software specifications;vendor-independent metamodel;class diagram;code generation process;Reverse engineering;Object oriented modeling;Erbium;Object oriented databases;Relational databases;Application software;Software systems;Distributed computing;Database systems;Unified modeling language;reengineering;reverse engineering;model-driven reengineering}, 
doi={10.1109/ICSM.2005.21}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{972772, 
author={A. {Rashid}}, 
booktitle={Proceedings IEEE International Conference on Software Maintenance. ICSM 2001}, 
title={A database evolution approach for object-oriented databases}, 
year={2001}, 
volume={}, 
number={}, 
pages={561-564}, 
abstract={The paper describes a composite evolution approach which integrates the evolution of the various types of entities in an object-oriented database into one model. The approach provides maintainers with a coherent and comprehensible view of the system and at the same time maintains change histories at a fine granularity. Links among meta-objects are implemented using dynamic relationships which are semantic constructs and first-class objects. Referential integrity is maintained by the relationships architecture reducing the evolution complexity at the meta-object level. A customisable and exchangeable instance adaptation approach is proposed. The approach is based on separating the instance adaptation code from class versions using aspects, abstractions used in Aspect-Oriented Programming to localise crosscutting concerns. A high level object-oriented model offering transparent access to the proposed evolution functionality is provided.}, 
keywords={software maintenance;object-oriented databases;data integrity;software prototyping;database evolution approach;object-oriented databases;composite evolution approach;maintainers;comprehensible view;change histories;meta-object links;dynamic relationships;semantic constructs;first-class objects;referential integrity;relationships architecture;evolution complexity;meta-object level;exchangeable instance adaptation approach;instance adaptation code;Aspect-Oriented Programming;crosscutting concerns;high level object oriented model;transparent access;evolution functionality;Object oriented databases;Spatial databases;Electrical capacitance tomography;Object oriented modeling;Manufacturing;Microwave integrated circuits;Database systems;EMP radiation effects;Table lookup}, 
doi={10.1109/ICSM.2001.972772}, 
ISSN={1063-6773}, 
month={Nov},}
@INPROCEEDINGS{6975839, 
author={Y. {Zhang} and Q. {Ma} and H. {Furutani}}, 
booktitle={2014 10th International Conference on Natural Computation (ICNC)}, 
title={Markov chain model of schema evolution and its application to stationary distribution}, 
year={2014}, 
volume={}, 
number={}, 
pages={225-229}, 
abstract={Markov chain is a powerful tool for analyzing the evolutionary process of a stochastic system. To select GA parameters such as mutation rate and population size are important in practical application. The value of this parameter has a big effect on the viewpoint of Markov chain. In this paper, we consider properties of stationary distribution with mutation in GAs. We used Markov chain to calculate distribution. If the population is in linkage equilibrium, we used Wright-Fisher model to get the distribution of first order schema. We define the mixing time is the time to arrive stationary distribution. We adopt Hunter's mixing time to estimate the mixing time T<sub>m</sub> of the first order schema.}, 
keywords={genetic algorithms;Markov processes;statistical distributions;stochastic systems;Markov chain model;schema evolution;stationary distribution;evolutionary process;stochastic system;GA parameters;mutation rate;population size;linkage equilibrium;Wright-Fisher model;first order schema;Hunter mixing time;Markov processes;Sociology;Statistics;Mathematical model;Equations;Genetic algorithms;Computational modeling;genetic algorithms;Markov chain;mixing time}, 
doi={10.1109/ICNC.2014.6975839}, 
ISSN={2157-9563}, 
month={Aug},}
@ARTICLE{4469888, 
author={R. C. {Romero-Zaliz} and C. {Rubio-Escudero} and J. P. {Cobb} and F. {Herrera} and Ó. {Cordon} and I. {Zwir}}, 
journal={IEEE Transactions on Evolutionary Computation}, 
title={A Multiobjective Evolutionary Conceptual Clustering Methodology for Gene Annotation Within Structural Databases: A Case of Study on the Gene Ontology Database}, 
year={2008}, 
volume={12}, 
number={6}, 
pages={679-701}, 
abstract={Current tools and techniques devoted to examine the content of large databases are often hampered by their inability to support searches based on criteria that are meaningful to their users. These shortcomings are particularly evident in data banks storing representations of structural data such as biological networks. Conceptual clustering techniques have demonstrated to be appropriate for uncovering relationships between features that characterize objects in structural data. However, typical conceptual clustering approaches normally recover the most obvious relations, but fail to discover the less frequent but more informative underlying data associations. The combination of evolutionary algorithms with multiobjective and multimodal optimization techniques constitutes a suitable tool for solving this problem. We propose a novel conceptual clustering methodology termed evolutionary multiobjective conceptual clustering (EMO-CC), relying on the NSGA-II multiobjective (MO) genetic algorithm. We apply this methodology to identify conceptual models in structural databases generated from gene ontologies. These models can explain and predict phenotypes in the immunoinflammatory response problem, similar to those provided by gene expression or other genetic markers. The analysis of these results reveals that our approach uncovers cohesive clusters, even those comprising a small number of observations explained by several features, which allows describing objects and their interactions from different perspectives and at different levels of detail.}, 
keywords={biology computing;data mining;genetic algorithms;genetics;ontologies (artificial intelligence);pattern clustering;scientific information systems;sensor fusion;very large databases;multiobjective evolutionary conceptual clustering methodology;gene annotation;structural gene ontology database;data association;multiobjective genetic algorithm;immunoinflammatory response problem;gene phenotype prediction;gene expression;large database;Ontologies;Spatial databases;Computer science;Artificial intelligence;Evolutionary computation;Gene expression;Biomedical imaging;Image databases;Genetic algorithms;Predictive models;Conceptual clustering;database annotation;evolutionary algorithms (EAs);gene expression profiles;gene ontology (GO);knowledge discovery;multiobjective (MO) optimization}, 
doi={10.1109/TEVC.2008.915995}, 
ISSN={1089-778X}, 
month={Dec},}
@INPROCEEDINGS{716710, 
author={J. -. {Jahnke} and U. A. {Nickel} and D. {Wagenblasst}}, 
booktitle={Proceedings. The Twenty-Second Annual International Computer Software and Applications Conference (Compsac '98) (Cat. No.98CB 36241)}, 
title={A case study in supporting schema evolution of complex engineering information systems}, 
year={1998}, 
volume={}, 
number={}, 
pages={513-520}, 
abstract={Information systems have to evolve continually in order to keep up with emerging requirements. Various problems arise with each such evolution step, e.g. the modification of the application's conceptual data structure, the migration of existing data, the adaption of application code, and the modification of technical documentation. Most database systems provide only limited support for schema evolution while problems like data migration and application migration are tackled manually by the programmers. This evolution process is unsatisfactory for a number of novel complex evolutionary information systems (CEIS) in the area of business and engineering applications. The paper describes our experiences with a case study in developing a CEIS in the domain of analysis and design of mixed signal printed circuit boards. We show that a meta schema approach combined with a well defined set of schema transformations is a practical way to cope with evolution. Based on this case study, we distinguish application specific from reusable architectural components and propose a systematic approach of building CEIS.}, 
keywords={engineering information systems;mixed analogue-digital integrated circuits;circuit CAD;software engineering;case study;schema evolution;complex engineering information systems;emerging requirements;evolution step;conceptual data structure;data migration;database systems;application code;technical documentation;complex evolutionary information systems;business;engineering applications;CEIS;mixed signal printed circuit boards;meta schema approach;schema transformations;reusable architectural components;systematic approach;Computer aided software engineering;Systems engineering and theory;Information systems;Object oriented modeling;Nickel;Cities and towns;Object oriented databases;Concatenated codes;Capacitance}, 
doi={10.1109/CMPSAC.1998.716710}, 
ISSN={0730-3157}, 
month={Aug},}
@INPROCEEDINGS{1020117, 
author={ and and }, 
booktitle={Proceedings of the 4th World Congress on Intelligent Control and Automation (Cat. No.02EX527)}, 
title={Rule induction based on a novel evolutionary strategy}, 
year={2002}, 
volume={4}, 
number={}, 
pages={3171-3174 vol.4}, 
abstract={With the development of data mining research, we gradually realize that the main task of the machine learning algorithm is not to find the knowledge without any error, but to find some novel and easy to understand knowledge, which is used for decision making in a higher level through people's participation. Based on this consideration, a novel data-mining method, an evolutionary strategy with immune selection and lifecycle, is designed for high level rule induction. In this novel algorithm, an immune selection operator and the crossover and mutation operators with lifecycle are employed, in which the former is used for resisting the degeneracy of the original evolutionary strategy, and the latter is for adjusting the probabilities of the crossover and mutation operators more effectively. Simulations on a large database show that the above method has good validity and rationality on rule induction.}, 
keywords={evolutionary computation;data mining;very large databases;probability;learning (artificial intelligence);rule induction;evolutionary strategy;data mining;machine learning algorithm;decision making;immune selection operator;crossover;mutation operators;probability;simulations;large database;Evolutionary computation;Immune system;Databases;Data mining;Machine learning algorithms;Genetic mutations;Learning systems;Decision making;Evolution (biology);Organisms}, 
doi={10.1109/WCICA.2002.1020117}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4529814, 
author={Z. {Brahmia} and R. {Bouaziz}}, 
booktitle={Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008)}, 
title={Schema Versioning in Multi-temporal XML Databases}, 
year={2008}, 
volume={}, 
number={}, 
pages={158-164}, 
abstract={Schema evolution keeps only the current data and the schema version after applying schema changes. On the contrary, schema versioning creates new schema versions and preserves old schema versions and their corresponding data. Much research work has recently focused on the problem of schema evolution in XML databases, but less attention has been devoted to schema versioning in such databases. In this paper, we present an approach for schema versioning in multitemporal XML databases. This approach is based on the XML Schema language for describing XML schema, and is database consistency-preserving.}, 
keywords={database languages;temporal databases;XML;Schema versioning;multitemporal XML database;Schema language;XML;Relational databases;Object oriented databases;Object oriented modeling;Spatial databases;Information science;Laboratories;Conference management;Context modeling;Data models;Temporal databases;XML databases;Schema versioning;XML Schema}, 
doi={10.1109/ICIS.2008.89}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6557874, 
author={A. C. B. L. {Moncao} and C. G. {Camilo} and L. T. {Queiroz} and C. L. {Rodrigues} and P. {de Sa Leitao} and A. M. R. {Vincenzi}}, 
booktitle={2013 IEEE Congress on Evolutionary Computation}, 
title={Shrinking a database to perform SQL mutation tests using an evolutionary algorithm}, 
year={2013}, 
volume={}, 
number={}, 
pages={2533-2539}, 
abstract={This paper tries to combine SQL mutation testing techniques with evolutionary computation aiming to improve the test data to SQL instructions. Based on a heuristic perspective it presents an approach that uses Genetic Algorithms (GA) to select tuples from an original database trying to reduce this one in an effective data set. The goal is to find a reduced data set which is able to detect a large number of faults in the SQL instructions of a given application. During the evolutionary process, the analysis of mutants is used to assess each set of data test selected by GA. The results obtained from experiments reveal a good performance using GA metaheuristic.}, 
keywords={database management systems;genetic algorithms;performance evaluation;SQL;database shrinking;evolutionary algorithm;SQL mutation testing techniques;evolutionary computation;SQL instructions;genetic algorithms;evolutionary process;mutant analysis;GA metaheuristic;Databases;Testing;Genetic algorithms;Sociology;Statistics;Production;Evolutionary computation}, 
doi={10.1109/CEC.2013.6557874}, 
ISSN={1089-778X}, 
month={June},}
@INPROCEEDINGS{7424406, 
author={S. {Mansha} and F. {Kamiran}}, 
booktitle={2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)}, 
title={Multi-query Optimization in Federated Databases Using Evolutionary Algorithm}, 
year={2015}, 
volume={}, 
number={}, 
pages={723-726}, 
abstract={Multi Query Optimization in federated database systems is a well-studied area. Studies have shown that similar problem arises in wide range of applications, e.g., distributed stream processing systems and wireless sensor networks. In this paper, a general distributed multiquery processing problem motivated by the need to speedup data acquisition in federated databases using evolutionary algorithm is studied. We setup a simple framework in which each individual in population is evolved in terms of cost, uniform labeling of hyper edges and validity of resource constraints through a number of generations. Variations of our general problem can be shown to be NP-Hard. Our extensive empirical evaluation over five different synthetic datasets shows a significant improvement of 8 percent in results as compared to the state-of-the-art methods.}, 
keywords={computational complexity;distributed databases;evolutionary computation;query processing;multiquery optimization;federated database system;evolutionary algorithm;general distributed multiquery processing problem;data acquisition;NP-hard problem;Biological cells;Linear programming;Load management;Communication networks;Sociology;Statistics;Distributed databases;Federated Database;Multi-Query Optimization;Load balancing;Evolutionary Algorithm}, 
doi={10.1109/ICMLA.2015.125}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6339502, 
author={C. {Szabó} and L. {Samuelis} and M. {Ivanovic} and T. {Fesic}}, 
booktitle={2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics}, 
title={Database refactoring and regression testing of Android mobile applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={135-139}, 
abstract={The widespread usage of mobile devices has made rapid progress recently in the society. The gradual development, modification and improvement of mobile applications is a rule. This contribution presents the application of refactoring and regression testing, which supports the software modification in a systematic and controlled manner. The database refactoring and regression testing is presented within the development of an experimental mobile Android-based application. Finally, the contribution evaluates the results of the experiments and verifies their correctness.}, 
keywords={database management systems;mobile computing;program testing;software maintenance;database refactoring;regression testing;Android mobile applications;mobile devices;software modification;Databases;Testing;Androids;Humanoid robots;Mobile communication;Software;Informatics;database refactoring;regression testing;Android-based application}, 
doi={10.1109/SISY.2012.6339502}, 
ISSN={1949-047X}, 
month={Sep.},}
@INPROCEEDINGS{765763, 
author={S. -. {Lautemann}}, 
booktitle={Proceedings. 6th International Conference on Advanced Systems for Advanced Applications}, 
title={Change management with roles}, 
year={1999}, 
volume={}, 
number={}, 
pages={291-300}, 
abstract={Various proposals have been made to extend object-oriented languages and database systems with roles because they allow to weaken strict typing concepts and therefore can provide mechanisms for an object to change its type during its lifetime. This so-called object migration is not the only advantage offered by a suitable role model. This paper studies the possibilities to use roles for a general change management system that also includes support for schema evolution. OPAQUE, as the selected role model, can be extended with schema update mechanisms in a surprisingly clean and straightforward way including the concepts of versions and views as well.}, 
keywords={object-oriented databases;object-oriented languages;management of change;data models;change management system;roles;object-oriented languages;object oriented database;strict typing concepts;object migration;schema evolution;OPAQUE;schema update;versions;views;Object oriented modeling;Taxonomy;Power system modeling;Database systems;Computer science;Object oriented databases;Management information systems;Proposals;Data models;Power system management}, 
doi={10.1109/DASFAA.1999.765763}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7965438, 
author={M. {de Jong} and A. {van Deursen} and A. {Cleve}}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)}, 
title={Zero-Downtime SQL Database Schema Evolution for Continuous Deployment}, 
year={2017}, 
volume={}, 
number={}, 
pages={143-152}, 
abstract={When a web service or application evolves, its database schema - tables, constraints, and indices - often need to evolve along with it. Depending on the database, some of these changes require a full table lock, preventing the service from accessing the tables under change. To deal with this, web services are typically taken offline momentarily to modify the database schema. However with the introduction of concepts like Continuous Deployment, web services are deployed into their production environments every time the source code is modified. Having to take the service offline - potentially several times a day - to perform schema changes is undesirable. In this paper we introduce QuantumDB - a tool-supported approach that abstracts this evolution process away from the web service without locking tables. This allows us to redeploy a web service without needing to take it offline even when a database schema change is necessary. In addition QuantumDB puts no restrictions on the method of deployment, supports schema changes to multiple tables using changesets, and does not subvert foreign key constraints during the evolution process. We evaluate QuantumDB by applying 19 synthetic and 95 industrial evolution scenarios to our open source implementation of QuantumDB. These experiments demonstrate that QuantumDB realizes zero downtime migrations at the cost of acceptable overhead, and is applicable in industrial continuous deployment contexts.}, 
keywords={software tools;SQL;Web services;zero-downtime SQL database schema evolution;Web service;source code;QuantumDB tool;industrial continuous deployment contexts;Databases;Web services;Servers;Switches;Tools}, 
doi={10.1109/ICSE-SEIP.2017.5}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{105472, 
author={K. {Narayanaswamy} and K. V. {Bapa Rao}}, 
booktitle={Proceedings. Fourth International Conference on Data Engineering}, 
title={An incremental mechanism for schema evolution in engineering domains}, 
year={1988}, 
volume={}, 
number={}, 
pages={294-301}, 
abstract={The authors focus on one class of schema revisions necessitated by a very basic phenomenon: a given individual object evolves into a family of objects which are similar to it in many ways. This is commonly called the version problem. In theoretical terms, one can handle the above schema change in the standard, object-oriented database models by the interposition of suitable abstractions into the existing type lattice. There are practical and engineering difficulties with such schema changes. The authors propose an incremental mechanism called instance inheritance which is well suited to handling the schema changes without the attendant practical costs. The authors formally characterize this augmentation to the standard database models, and show examples of its applications.<<ETX>>}, 
keywords={data structures;database management systems;engineering computing;incremental mechanism;schema evolution;engineering domains;schema revisions;individual object;family of objects;version problem;schema change;object-oriented database models;instance inheritance;Design engineering;Data engineering;Object oriented modeling;Automobiles;Object oriented databases;Lattices;Costs;Very large scale integration;Distributed computing;Image databases}, 
doi={10.1109/ICDE.1988.105472}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{1264962, 
author={H. {Bounif} and O. {Drutskyy} and F. {Jouanot} and S. {Spaccapietra}}, 
booktitle={10th International Multimedia Modelling Conference, 2004. Proceedings.}, 
title={A multimodal database framework for multimedia meeting annotations}, 
year={2004}, 
volume={}, 
number={}, 
pages={17-25}, 
abstract={The main objective of this paper is to present a flexible annotation management framework for a multimedia database system, applied to meeting recordings. Presented research and development activities are carried out within the scope of the IM2 project in which annotations play an important role in describing raw data from various points of view and in enhancing the query process. We focus on a database system capable of managing annotations (e.g. text transcriptions, dialog acts, speaker space position, etc.) and keeping links with raw data (audio, video, digital documents). This database provides a schema evolution mechanism and a meta-description layer ensuring flexible and incremental annotation definitions. To enhance this database system, some research works are currently in progress: a predictive methodology for schema evolution and a query technique that deals with fuzzy concepts and ontological commitments. We describe our on-going prototype development, in which we focus on data storage and interactive data access.}, 
keywords={multimedia databases;query processing;text analysis;speech processing;storage management;interactive systems;multimodal database;multimedia meeting annotation;annotation management;multimedia database;meeting recordings;IM2 Swiss national project;query processing;text transcriptions;dialog acts;speaker space position;audio documents;video documents;digital documents;fuzzy concepts;data storage;interactive data access;multimedia data complexity;Multimedia databases;Speech analysis;MPEG 7 Standard;Database systems;Streaming media;Speech recognition;Information retrieval;Video recording;Relational databases;Laboratories}, 
doi={10.1109/MULMM.2004.1264962}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{544586, 
author={W. W. {Song}}, 
booktitle={Proceedings of 20th International Computer Software and Applications Conference: COMPSAC '96}, 
title={Integration issues in information system reengineering}, 
year={1996}, 
volume={}, 
number={}, 
pages={328-335}, 
abstract={We address the problem of how to apply schema integration techniques in legacy system migration, since some issues in the legacy concept: legacy system migration and reengineering are closely related to schema integration methods. The importance of introducing integration issues to legacy system migration is two-fold. (1) To adapt legacy systems to a modern environment requires the integration (harmonization) of requirements for both legacy and modern systems. (2) Because a legacy information system is very large, it has to be decomposed into a number of smaller, relatively independent units in order to make migration easier to the target system. These decomposed units should be reintegrated again after the migration process. This situation becomes more complex and difficult when the legacy system was designed and modified using several requirements and data modelling methods. We pose the initial integration problems regarding these aspects: requirement and function schema integration, and database schema integration.}, 
keywords={systems re-engineering;information systems;software maintenance;integrated software;data structures;formal specification;software portability;database management systems;information system reengineering;schema integration techniques;legacy system migration;requirements;legacy information system;migration process;data modelling methods;function schema integration;database schema integration;Information systems;Computer languages;Data engineering;System testing;Documentation;Modems;Prototypes;Design methodology;Database languages;Reverse engineering}, 
doi={10.1109/CMPSAC.1996.544586}, 
ISSN={0730-3157}, 
month={Aug},}
@INPROCEEDINGS{995292, 
author={A. {Karahasanovic} and D. I. K. {Sjoberg}}, 
booktitle={Proceedings IEEE Symposia on Human-Centric Computing Languages and Environments (Cat. No.01TH8587)}, 
title={Visualizing impacts of database schema changes - A controlled experiment}, 
year={2001}, 
volume={}, 
number={}, 
pages={358-365}, 
abstract={Research in schema evolution has been driven by the need for more effective software development and maintenance. Finding impacts of schema changes on the applications and presenting them in an appropriate way are particularly challenging. We have developed a tool that finds impacts of schema changes on applications in object-oriented systems. This tool displays components (packages, classes, interfaces, methods and fields) of a database application system as a graph. Components potentially affected by a change are indicated by changing the shape of the boxes representing those components. Two versions of the tool are available. One version identifies affected parts of applications at the granularity of packages, classes, and interfaces, whereas the other version identifies affected parts at the finer granularity of fields and methods. This paper presents the design and results of a controlled student experiment testing these two granularity levels with respect to productivity and user satisfaction. There are indications that identifying impacts at the finer granularity can reduce the time needed to conduct schema changes and reduce the number of errors. Our results also show that the subjects of the experiment appreciated the idea of visualizing the impacts of schema changes.}, 
keywords={software maintenance;object-oriented programming;database management systems;data visualisation;database schema changes;schema evolution;software development;software maintenance;object-oriented systems;controlled student experiment;user satisfaction;productivity;Visualization;Visual databases;Application software;Packaging;Programming;Software maintenance;Displays;Object oriented databases;Shape;Student experiments}, 
doi={10.1109/HCC.2001.995292}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{539929, 
author={B. {Meyer}}, 
journal={Computer}, 
title={Schema evolution: Concepts, terminology, and solutions}, 
year={1996}, 
volume={29}, 
number={10}, 
pages={119-121}, 
abstract={Most applications must keep objects from one session to the next. This is known as persistence. But objects are not raw data: They are instances of classes. What happens if an object's class (its generator) changes from one session to the next? This problem is known as schema evolution (the term schema is borrowed from relational databases). This column defines a framework for addressing schema evolution in object technology.}, 
keywords={object-oriented programming;schema evolution;persistence;object technology;classes;Terminology;Relational databases;Object oriented databases;Libraries;Object detection}, 
doi={10.1109/2.539929}, 
ISSN={0018-9162}, 
month={Oct},}
@ARTICLE{6122034, 
author={M. {Piccioni} and M. {Oriol} and B. {Meyer}}, 
journal={IEEE Transactions on Software Engineering}, 
title={Class Schema Evolution for Persistent Object-Oriented Software: Model, Empirical Study, and Automated Support}, 
year={2013}, 
volume={39}, 
number={2}, 
pages={184-196}, 
abstract={With the wide support for object serialization in object-oriented programming languages, persistent objects have become commonplace and most large object-oriented software systems rely on extensive amounts of persistent data. Such systems also evolve over time. Retrieving previously persisted objects from classes whose schema has changed is, however, difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses these issues through an IDE-integrated approach that handles class schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of potentially corrupt objects. This paper describes a model for class attribute changes, a measure for class evolution robustness, four empirical studies, and the design and implementation of the ESCHER system.}, 
keywords={object-oriented languages;object-oriented programming;persistent objects;persistent object-oriented software;object serialization;object-oriented programming languages;object-oriented software systems;IDE-integrated approach;class schema evolution;automatic transformation function generation;potentially corrupt objects;class evolution robustness;ESCHER system implementation;ESCHER system design;Object oriented modeling;Java;Databases;Software;Robustness;Dictionaries;Atomic measurements;Versioning;persistence;serialization;object-oriented class schema evolution;IDE integration}, 
doi={10.1109/TSE.2011.123}, 
ISSN={0098-5589}, 
month={Feb},}
@INPROCEEDINGS{7498345, 
author={K. {Herrmann} and H. {Voigt} and T. {Seyschab} and W. {Lehner}}, 
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, 
title={InVerDa - co-existing schema versions made foolproof}, 
year={2016}, 
volume={}, 
number={}, 
pages={1362-1365}, 
abstract={In modern software landscapes multiple applications usually share one database as their single point of truth. All these applications will evolve over time by their very nature. Often former versions need to stay available, so database developers find themselves maintaining co-existing schema version of multiple applications in multiple versions. This is highly error-prone and accounts for significant costs in software projects, as developers realize the translation of data accesses between schema versions with hand-written delta code. In this demo, we showcase INVERDA, a tool for integrated, robust, and easy to use database versioning. We rethink the way of specifying the evolution to new schema versions. Using the richer semantics of a descriptive database evolution language, we generate all required artifacts automatically and make database versioning foolproof.}, 
keywords={Databases;Backpropagation;Software;Semantics;Manuals;Writing;Syntactics}, 
doi={10.1109/ICDE.2016.7498345}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1254473, 
author={ and C. {Zaniolo}}, 
booktitle={Proceedings of the Fourth International Conference on Web Information Systems Engineering, 2003. WISE 2003.}, 
title={Publishing and querying the histories of archived relational databases in XML}, 
year={2003}, 
volume={}, 
number={}, 
pages={93-102}, 
abstract={There is much current interest in publishing and viewing databases as XML documents. The general benefits of this approach follow from the popularity of XML and the tool set available for visualizing and processing information encoded in this universal standard. In this paper, we explore the additional and unique benefits achieved by this approach on temporal database applications. We show that XML views combined with XQuery can provide surprisingly effective solutions to the problem of supporting historical queries on past content of database relations and their evolution. Indeed, using XML, the histories of database relations can be naturally represented by temporally grouped data models. Thus, we identify mappings from relations to XML that are most conducive to modeling and querying database histories, and show that temporal queries that would be very difficult to express in SQL can be easily expressed in standard XQuery. Then, we turn to the problem of supporting efficiently the storage and the querying of relational table histories. We present an experimental study of the pros and cons of using native XML databases, versus using traditional databases, where the XML-represented histories are supported as views on the historical tables.}, 
keywords={temporal databases;relational databases;query processing;XML;information retrieval systems;electronic publishing;database publishing;database querying;relational databases;information visualization;temporal database;XQuery;historical queries;database relations;database evolution;database histories;temporal queries;SQL;XML databases;Publishing;History;Relational databases;XML;Visual databases;Information systems;Object oriented databases;Data models;Database systems;Computer science}, 
doi={10.1109/WISE.2003.1254473}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6160044, 
author={I. {Elek} and J. {Roden} and T. B. {Nguyen}}, 
booktitle={The Fourth International Workshop on Advanced Computational Intelligence}, 
title={Spatial database for digital evolutionary machines in an artificial world without knowledge-base}, 
year={2011}, 
volume={}, 
number={}, 
pages={426-431}, 
abstract={The evolution simulation is a popular problem not only in biology but in the computer science as well. This paper will introduce the principles of the evolution units. An artificial organism, a digital evolutionary machine (DEM) will be constructed based on these principles. Properties and abilities of DEMs will be shown with resolved and pending questions. After the theoretical approach some experimental results will be shown as thousands of DEMs were created. Their life was observed and stored in a database. Some interesting analysis and charts will be shown in order to understand how DEMs work in the artificial world. Finally the DEMs' future will be outlined.}, 
keywords={biology computing;evolution (biological);visual databases;spatial database;digital evolutionary machine;artificial world;evolution simulation;biology;computer science;evolution unit principles;Knowledge based systems;Organisms;Context;Evolution (biology);Meteorology;Databases;Internet}, 
doi={10.1109/IWACI.2011.6160044}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8456164, 
author={H. {Tufail} and K. {Zafar} and R. {Baig}}, 
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, 
title={Digital Watermarking for Relational Database Security Using mRMR Based Binary Bat Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={1948-1954}, 
abstract={Publically available relational data without security protection may cause data protection issues. Watermarking facilitates solution for remote sharing of relational database by ensuring data integrity and security. In this research, a reversible watermarking for numerical relational database by using evolutionary technique has been proposed that ensure the integrity of underlying data and robustness of watermark. Moreover, mRMR based feature subset selection technique has been used to select attributes for implementation of watermark instead of watermarking whole database. Binary Bat algorithm has been used as constraints optimization technique for watermark creation. Experimental results have shown the effectiveness of the proposed technique against data tempering attacks. In case of alteration attacks, almost 70% data has been recovered, 50% in deletion attacks and 100% data is retrieved after insertion attacks. The watermarking based on evolutionary technique (WET) i.e., mRMR based Binary Bat Algorithm ensures the data accuracy and it is resilient against malicious attacks.}, 
keywords={data integrity;evolutionary computation;relational databases;security of data;watermarking;digital watermarking;relational database security;mRMR based binary bat algorithm;security protection;data protection issues;data integrity;reversible watermarking;numerical relational database;evolutionary technique;mRMR based feature subset selection technique;constraints optimization technique;watermark creation;data tempering attacks;alteration attacks;deletion attacks;insertion attacks;data accuracy;relational data;Watermarking;Robustness;Constraint optimization;Data integrity;Relational databases;Clustering algorithms;Database-security;watermarking,;binary-bat-algorithm;feature-subset-selection}, 
doi={10.1109/TrustCom/BigDataSE.2018.00298}, 
ISSN={2324-9013}, 
month={Aug},}
@INPROCEEDINGS{7321548, 
author={S. {Ristic} and S. {Kordic} and M. {Celikovic} and V. {Dimitrieski} and I. {Lukovic}}, 
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
title={A model-driven approach to data structure conceptualization}, 
year={2015}, 
volume={}, 
number={}, 
pages={977-984}, 
abstract={Reengineering of an existing information system can be carried out: to improve its maintainability, to migrate to a new technology, to improve quality or to prepare for functional enhancement. An important phase of a data-oriented software system reengineering is a database reengineering process and, in particular, its subprocess - a database reverse engineering process. The reverse engineering process contains two main phases: data structure extraction and data structure conceptualization. In the paper we present a blueprint of a model-driven approach to database reengineering process that is one of the results of our research project on model-driven intelligent systems for software system development, maintenance and evolution. Within that process hereinafter we focus on the data structure conceptualization process and propose a model-driven approach to data structure conceptualization. Proposed process is based on model-to-model transformations implemented by means of Atlas Transformation Language.}, 
keywords={data structures;information systems;software maintenance;systems re-engineering;data structure conceptualization;information system reengineering;functional enhancement;data-oriented software system reengineering;database reengineering process;reverse engineering process;data structure extraction;data structure conceptualization;model-driven intelligent systems;software system development;software system maintenance;data structure conceptualization process;model-to-model transformations;Atlas transformation language;Data models;Data structures;Computer science;Information systems}, 
doi={10.15439/2015F224}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5260668, 
author={J. {Yang} and Y. {Chen} and Y. {Lo}}, 
booktitle={2009 International Joint Conference on Bioinformatics, Systems Biology and Intelligent Computing}, 
title={3D-interologs: A Protein-Protein Interacting Evolution Database across Multiple Species}, 
year={2009}, 
volume={}, 
number={}, 
pages={278-285}, 
abstract={The 3D-interologs database records the evolution of protein-protein interactions database across multiple species. Based on ldquo3D-domain interologsrdquoand a new scoring function, we infer 173,294 protein-protein interactions by using 1,895 three-dimensional (3D) structure heterodimers to search the UniProt database (4, 826,134 protein sequences). For a protein-protein interaction, the 3D-interologs database shows functional annotations (e.g. Gene Ontology annotations), interacting domains and binding models (e.g. hydrogen-bond interactions and conserved residues). Additionally, this database provides couple-conserved residues and the interacting evolution by exploring the interologs across multiple species. Experimental results reveal that the proposed scoring function obtains good agreement for the binding affinity of 275 mutated residues from the ASEdb. The precision and recall of our method are 0.52 and 0.34, respectively, by using 563 non-redundant heterodimers to search on the Integr8 database (549 completely deciphered genomes). The proposed method can infer many of the interactions that would not have been identified from sequence similarity alone. The 3D-interologs database comprises 15,024 species and 283,980 protein-protein interactions, including 173,294 interactions (61%) discovered from 3D-domain interologs and 110,686 interactions (39%) summarized from the IntAct database. The 3D-interologs database is available at http://3D-interologs.life.nctu.edu.tw.}, 
keywords={bioinformatics;database management systems;proteins;sequences;3D-domain interolog database;protein-protein interacting evolution database;multiple species;scoring function;UniProt database search;protein sequence;Proteins;Bioinformatics;Evolution (biology);Sequences;Libraries;Biology computing;Predictive models;Visual databases;Systems biology;Intelligent systems;protein-protein interaction;interologs}, 
doi={10.1109/IJCBS.2009.79}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1647902, 
author={ and A. {Lo} and R. {Alhajj} and K. {Barker}}, 
booktitle={21st International Conference on Data Engineering Workshops (ICDEW'05)}, 
title={Novel Approach for Reengineering Relational Databases into XML}, 
year={2005}, 
volume={}, 
number={}, 
pages={1284-1284}, 
abstract={In this paper, we present COCALEREX (converting relational to XML) which can address both catalog-based and legacy relational databases. It handles the latter category by first applying the reverse engineering approach described in [2] to extract the ER (Extended Entity Relationship) model from legacy relational databases. This reverse engineering approach is emplyed also not to extract from catalog-based databases meta-data not available in the catalog. Then, COCALEREX converts the ER to XML schema. Deriving the ER model empowers the proposed approach to smoothly consider many-to-many and nary relationships during the mapping into XML schema. COCALEREX provides a user-friendly interface that displays the result of each phase of the conversion process. Experimental results are encouraging, demonstrating the applicability and effectiveness of the proposed approach.}, 
keywords={reengineering;XML schema;relational database;conversion;user interface;Relational databases;XML;reengineering;XML schema;relational database;conversion;user interface}, 
doi={10.1109/ICDE.2005.249}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{1331177, 
author={M. L. {Goldstein} and G. G. {Yen}}, 
booktitle={Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)}, 
title={An evolutionary algorithm method for sampling n-partite graphs}, 
year={2004}, 
volume={2}, 
number={}, 
pages={2250-2257 Vol.2}, 
abstract={The growth of use of graph-structured databases modeled on n-partite graphs has increased the ability to generate more flexible databases. However, the calculation of certain features in these databases may be highly resource-consuming. This work proposes a method for approximating these features by sampling. A discussion of the difficulty of sampling in n-partite graphs is made and an evolutionary algorithm-based method is presented that uses the information from a smaller subset of the graph to infer the amount of sampling needed for the rest of the graph. Experimental results are shown on a publications database on Anthrax for finding the most important authors.}, 
keywords={graph theory;evolutionary computation;sampling methods;database theory;evolutionary algorithm;n-partite graph sampling;graph-structured databases;publications database;Anthrax;Evolutionary computation;Sampling methods;Spatial databases;Ontologies;Feature extraction;Pattern recognition;Intelligent systems;Intelligent control;Deductive databases;Control system synthesis}, 
doi={10.1109/CEC.2004.1331177}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4634241, 
author={E. P. {Farias} and J. C. {Nievola}}, 
booktitle={2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)}, 
title={Comparative of data base evolution in rule association algorithms in incremental and conventional way}, 
year={2008}, 
volume={}, 
number={}, 
pages={3131-3137}, 
abstract={Many results in the literature indicate that the incremental approach to association mining leads to gain regarding the time needed to obtain the rules, but there is no evaluation about their quality, compared to non-incremental algorithms. This paper presents the comparison of usage of two typical algorithms representing each approach: APriori and <i>ZigZag</i>. Execution time clearly shows the advantage of incremental approaches, but when someone needs accurate results concerning the association rules obtained, the matter should be taken with more caution, because the rules obtained are not necessarily in a relation one-to-one, according to the results obtained.}, 
keywords={data mining;database management systems;learning (artificial intelligence);incremental algorithm;association rule mining;database evolution;APriori strategy;ZigZag strategy;Association rules;Itemsets;Algorithm design and analysis;Databases;Data mining;Inference algorithms;Artificial neural networks}, 
doi={10.1109/IJCNN.2008.4634241}, 
ISSN={2161-4393}, 
month={June},}
@INPROCEEDINGS{632828, 
author={Y. {Cohen} and Y. A. {Feldman}}, 
booktitle={Proceedings 12th IEEE International Conference Automated Software Engineering}, 
title={Automatic high-quality reengineering of database programs by temporal abstraction}, 
year={1997}, 
volume={}, 
number={}, 
pages={90-97}, 
abstract={The relational database model is currently the target of choice for the conversion of legacy software that uses older models (such as indexed-sequential, hierarchical or network models). The relational model makes up for its lower efficiency by a greater expressive power and by optimization of queries, using indexes and other means. However, sophisticated analysis is required in order to take advantage of these features, since converting each database access operation separately does not use the greater expressive power of the target database and does not enable it to perform useful optimizations. By analyzing the behavior of the host program around the database access operations, it is possible to discover patterns such as filtering, joins and aggregative operations. It is then possible to remove those operations from the host program and re-implement them in the target database query language. This paper describes an automatic system, called MIDAS (MIgrator of Database Application Systems), that performs high-quality reengineering of legacy database programs in this way. The results of MIDAS were found to be superior to those of the naive one-to-one translation in terms of readability, size, speed and network data traffic.}, 
keywords={relational databases;systems re-engineering;query processing;temporal databases;computer aided software engineering;automatic high-quality reengineering;legacy database programs;temporal abstraction;legacy software conversion;relational database model;expressive power;query optimization;indexes;database access operations;filtering;joins;aggregative operations;target database query language;MIDAS;readability;network data traffic;Relational databases;Object oriented modeling;Spatial databases;Object oriented databases;Pattern analysis;Filtering;Application software;Computer science;Electronic mail;Database languages}, 
doi={10.1109/ASE.1997.632828}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7875879, 
author={ and and }, 
booktitle={2016 15th International Conference on Optical Communications and Networks (ICOCN)}, 
title={Efficient location proof with location tags in database-driven Cognitive Radio Networks}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Database-driven CRN is vulnerable to spoofing attacks. In this paper, we identified proactive location spoofing and passive location cheating attacks in database-driven CRN. Consequently, we proposed a novel and efficient location proof scheme via location tags collected from the LTE paging channel. Then, we adopted private equality test technology to test whether two location sketches matches, which generated by user and his proofer that we called Referee, using shingling technology. With the proposed scheme, the user can prove his location without revealing his accurate information to the database. Our experimental results showed that our approach could provide location proof effectively.}, 
keywords={cognitive radio;Long Term Evolution;database-driven cognitive radio networks;proactive location spoofing;passive location cheating attacks;CRN;LTE paging channel;private equality test technology;shingling technology;Databases;Mobile handsets;Poles and towers;Long Term Evolution;Encryption;Cognitive radio;Global Positioning System;location proof;database-driven CRN;shingling;private equality test}, 
doi={10.1109/ICOCN.2016.7875879}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6608367, 
author={M. {Antonelli} and P. {Ducange} and F. {Marcelloni} and A. {Segatori}}, 
booktitle={2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)}, 
title={Evolutionary fuzzy classifiers for imbalanced datasets: An experimental comparison}, 
year={2013}, 
volume={}, 
number={}, 
pages={13-18}, 
abstract={In this paper, we compare three state-of-the-art evolutionary fuzzy classifiers (EFCs) for imbalanced datasets. The first EFC performs an evolutionary data base learning with an embedded rule base generation. The second EFC builds a hierarchical fuzzy rule-based classifier (FRBC): first, a genetic programming algorithm is used to learn the rule base and then a post-process, which includes a genetic rule selection and a membership function parameters tuning, is applied to the generated FRBC. The third EFC is an extension of a multi-objective evolutionary learning scheme we have recently proposed: the rule base and the membership function parameters of a set of FRBCs are concurrently learned by optimizing the sensitivity, the specificity and the complexity. By performing non-parametric statistical tests, we show that, without re-balancing the training set, the third EFC outperforms, in terms of area under the ROC curve, the other comparison approaches.}, 
keywords={database management systems;fuzzy set theory;genetic algorithms;learning (artificial intelligence);pattern classification;statistical testing;evolutionary fuzzy classifiers;imbalanced datasets;EFC;evolutionary data base learning;embedded rule base generation;hierarchical fuzzy rule-based classifier;FRBC;genetic programming algorithm;rule base learning;genetic rule selection;membership function parameters tuning;multiobjective evolutionary learning scheme;sensitivity optimization;specificity optimization;complexity optimization;ROC curve;nonparametric statistical tests;Input variables;Genetics;Complexity theory;Training;Accuracy;Biological cells;Tuning;Genetic and Evolutionary Fuzzy Systems;Fuzzy Rule-based Classifiers;Imbalanced Datasets}, 
doi={10.1109/IFSA-NAFIPS.2013.6608367}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7751652, 
author={L. {Li} and W. {Hou} and H. {Zhang} and S. {Yao} and S. {Xia} and X. {Ji}}, 
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)}, 
title={Analysis of LTE traffic offloading based on big data}, 
year={2016}, 
volume={}, 
number={}, 
pages={367-370}, 
abstract={There are more inter-RAT (Radio Access Technology) operations between LTE network and GSM/UMTS networks at the early stage of the LTE network construction due to the insufficient LTE signal coverage. The proportion of camping on LTE network impacts customer ratings and revenue of mobile operators. In this paper, a new KPI (Key Performance Indicator) related to LTE camping rate was defined based on the data of BOSS (Business &amp; Operation Support System) and OMC (Operation &amp; Maintenance Center) from the Big Data Platform of Shanghai Unicom, and the scenarios or areas with low LTE camping rate were identified through a multi-indicator analysis and geographic display. Therefore, we can determine the priorities of construction and optimization in order to enhance user experiences.}, 
keywords={Big Data;Long Term Evolution;radio access networks;telecommunication computing;telecommunication traffic;LTE traffic offloading analysis;radio access technology;GSM-UMTS networks;LTE network construction;LTE signal coverage;key performance indicator;KPI;LTE camping rate;BOSS;business & operation support system;operation & maintenance center;OMC;Big Data platform;Shanghai Unicom;multiindicator analysis;geographic display;Long Term Evolution;Big data;Base stations;Mobile computing;Mobile communication;Business;Maintenance engineering;traffic offloading;big data;user analysis;terminal analysis}, 
doi={10.1109/ISCIT.2016.7751652}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{558286, 
author={S. -. {Lautemann}}, 
booktitle={Proceedings of 7th International Conference and Workshop on Database and Expert Systems Applications: DEXA 96}, 
title={An introduction to schema versioning in OODBMS}, 
year={1996}, 
volume={}, 
number={}, 
pages={132-139}, 
abstract={Object oriented database management systems (OODBMS) have enormous advantages in comparison to relational systems in modeling highly complex and dynamic application scenarios. Still they lack some flexibility which could help their promotion into widely spread industrial use. Especially engineering environments frequently require schema changes to handle evolving designs. The main contribution of the paper is twofold: firstly, we give an overview of schema evolution mechanisms, describing the state of the art in research. To the best of our knowledge, no such comprehensive overview has been published so far. Secondly, the study of a general application scenario led us to a list of requirements. We propose a general framework based on schema versioning to meet those requirements.}, 
keywords={object-oriented databases;schema versioning;OODBMS;object oriented database management systems;dynamic application scenarios;engineering environments;evolving designs;schema evolution mechanisms;general application scenario;Industrial relations;Database systems;Object oriented databases;Object oriented modeling;Design engineering;Control systems;History}, 
doi={10.1109/DEXA.1996.558286}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{4656423, 
author={J. {Henrard} and D. {Roland} and A. {Cleve} and J. {Hainaut}}, 
booktitle={2008 15th Working Conference on Reverse Engineering}, 
title={Large-Scale Data Reengineering: Return from Experience}, 
year={2008}, 
volume={}, 
number={}, 
pages={305-308}, 
abstract={This paper reports on a recent data reengineering project, the goal of which was to migrate a large CODASYL database towards a relational platform. The legacy system, made of one million lines of COBOL code, is in use in a Belgian federal administration. The approach followed combines program transformation, data reverse engineering, data analysis, wrapping and code generation techniques.}, 
keywords={COBOL;data analysis;program compilers;relational databases;reverse engineering;large-scale data reengineering;CODASYL database;relational platform;COBOL code;Belgian federal administration;program transformation;data reverse engineering;data analysis;data wrapping;code generation;Databases;Reverse engineering;Data analysis;Manuals;Servers;Data mining;Logic gates;Data reengineering;System migration;Data reverse engineering;Wrapping;Program transformation}, 
doi={10.1109/WCRE.2008.14}, 
ISSN={1095-1350}, 
month={Oct},}
@INPROCEEDINGS{754933, 
author={ and E. A. {Rundensteiner}}, 
booktitle={Proceedings 15th International Conference on Data Engineering (Cat. No.99CB36337)}, 
title={Data warehouse maintenance under concurrent schema and data updates}, 
year={1999}, 
volume={}, 
number={}, 
pages={253-}, 
abstract={Summary form only given. Data warehouses (DW) are built by gathering information from several information sources (IS) and integrating it into one repository customized to users' needs. ISs are typically owned by different information providers and hence are independent and autonomous. This implies they will update their data and schemas independently and without any concern for how this may affect the DW defined upon them. Hence, solutions for data warehouse maintenance are needed that can handle such IS updates. We have developed a first comprehensive solution that successfully tackles this problem (X. Zhang et al., 1998). In particular, to guarantee the correctness of concurrent view definition evolution and view extent maintenance of a data warehouse, we introduce a framework called the SDCC (Schema change and Data update Concurrency Control) system. SDCC integrates various algorithms designed to address different individual view maintenance subproblems into one system, such as algorithms for view extent maintenance after IS data updates, for view definition evolution after IS schema changes, and for view extent adaptation after view definition changes.}, 
keywords={data warehouses;software maintenance;concurrency control;data warehouse maintenance;concurrent schema;data updates;information sources;repository;information providers;IS updates;comprehensive solution;concurrent view definition evolution;view extent maintenance;SDCC;Schema change and Data update Concurrency Control;view maintenance subproblems;view definition evolution;IS schema change;view extent adaptation;view definition changes;Data warehouses;Concurrent computing;Protocols;Virtual manufacturing;Control systems;Algorithm design and analysis;Collaboration;Labeling;Timing;Writing}, 
doi={10.1109/ICDE.1999.754933}, 
ISSN={1063-6382}, 
month={March},}
@INPROCEEDINGS{6263182, 
author={R. H. R. {Pereira} and J. B. G. {Perez-Schofield}}, 
booktitle={7th Iberian Conference on Information Systems and Technologies (CISTI 2012)}, 
title={Database evolution on an orthogonal persistent programming system: A semi-transparent approach}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this paper the problem of the evolution of an object-oriented database in the context of orthogonal persistent programming systems is addressed. We have observed two characteristics in that type of systems that offer particular conditions to implement the evolution in a semi-transparent fashion. That transparency can further be enhanced with the obliviousness provided by the Aspect-Oriented Programming techniques. Was conceived a meta-model and developed a prototype to test the feasibility of our approach. The system allows programs, written to a schema, access semi-transparently to data in other versions of the schema.}, 
keywords={aspect-oriented programming;object-oriented databases;database evolution;orthogonal persistent programming system;semi-transparent approach;object-oriented database;aspect-oriented programming techniques;meta-model;Databases;Programming;Java;Object recognition;Semantics;Complexity theory;Prototypes;schema evolution;aspect-oriented programming;orthogonal persistence}, 
doi={}, 
ISSN={2166-0735}, 
month={June},}
@INPROCEEDINGS{4976612, 
author={A. C. {Moraes} and A. C. {Salgado} and P. A. {Tedesco}}, 
booktitle={2009 Fifth International Conference on Autonomic and Autonomous Systems}, 
title={AutonomousDB: A Tool for Autonomic Propagation of Schema Updates in Heterogeneous Multi-database Environments}, 
year={2009}, 
volume={}, 
number={}, 
pages={251-256}, 
abstract={One of the biggest challenges of building and maintaining applications with long life cycles, is dealing with the inevitable changes in requirements that occur over time. Many of these applications depend on DBMS that most often suffer direct consequences in their schemas due to changes in the reality that they model. In this work, we propose a new alternative to the issue of schema evolution in multi-database environments, which uses concepts of autonomic computing in DBMS to propagate updates in schemas replicated within the same environment, thus ensuring their consistency. Our prototype counts with a multi-agent system, which executes events for updating schemas in the target DBMS, which may be of different platforms. Repetitive tasks which are performed in different databases to ensure the evolution of replicated schemas are thus eliminated, freeing the DBA to do other tasks that are more important, such as analysis, database design and strategic management of data.}, 
keywords={multi-agent systems;replicated databases;software fault tolerance;AutonomousDB;autonomic propagation;heterogeneous multi database environments;DBMS;database management system;autonomic computing;schemas replicated;multiagent system;Databases;Optical propagation;Distributed computing;Humans;Prototypes;Multiagent systems;Data analysis;Performance analysis;Information technology;Acceleration;schema evolution;propagated updates;autonomy}, 
doi={10.1109/ICAS.2009.14}, 
ISSN={2168-1864}, 
month={April},}
@INPROCEEDINGS{7056938, 
author={B. B. {Mehta} and H. D. {Aswar}}, 
booktitle={2014 Conference on IT in Business, Industry and Government (CSIBIG)}, 
title={Watermarking for security in database: A review}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Digital multimedia watermarking technology was suggested in the last decade to embed copyright information in digital objects such as images, audio and video. However, the increasing use of relational database systems in many real-life applications created the need for database watermarking systems for protection of database. As a result, watermarking relational database systems is now merging as a research area that deals with the legal issue of copyright protection of database systems.Therefore, an evolution of watermarking has been started with Relational database. It all started with the first method proposed in 2002 by agrawal and kiernan for watermarking in relational database. Then there are so many methods have been proposed and implemented by many researchers. We are going to see the evolution of database watermarking for security in database in this paper.}, 
keywords={copyright;data protection;relational databases;watermarking;digital multimedia watermarking technology;copyright information;digital objects;database security;relational database systems;database watermarking systems;database protection;copyright protection;Databases;Watermarking;Cancer;Monitoring;Robustness;Chaotic communication;Speech;Database Watermarking;Database Security;Watermarking Evolution;Watermarking for Relational Database}, 
doi={10.1109/CSIBIG.2014.7056938}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6569780, 
author={F. {Mercaldo} and G. {Canfora} and C. A. {Visaggio}}, 
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={Identification of Anomalies in Processes of Database Alteration}, 
year={2013}, 
volume={}, 
number={}, 
pages={513-514}, 
abstract={Data, especially in large item sets, hide a wealth of information on the processes that have created and modified them. Often, a data-field or a set of data-fields are not modified only through well-defined processes, but also through latent processes; without the knowledge of the second type of processes, testing cannot be considered exhaustive. As a matter of fact, changes in the data deriving from unknown processes can cause anomalies not detectable by testing, which focuses on known data variation rules. History of data variations can yield information about the nature of the changes. In my work I focus on the elicitation of an evolution profile of data: the values data may assume, the change frequencies, the temporal variation of a piece of data in relation to other data, or other constraints that are directly connected to the reference domain. The profile of evolution is then used to detect anomalies in the database state evolution. Detecting anomalies in the database state evolution could strengthen the quality of a system, since a data anomaly could be the signal of a defect in the applications populating the database.}, 
keywords={database management systems;security of data;data anomaly;system quality;database state evolution;anomaly detection;reference domain;temporal variation;change frequency;evolution profile;data variations;data variation rules;latent processes;well-defined processes;data-fields;database alteration;anomaly identification;Databases;Noise;Testing;Association rules;Communities;History;anomaly detection;outlier;data mining;intrusion detection;pattern recognition}, 
doi={10.1109/ICST.2013.72}, 
ISSN={2159-4848}, 
month={March},}
@INPROCEEDINGS{8238166, 
author={A. {Jounaidi} and M. {Bahaj}}, 
booktitle={2017 International Conference on Wireless Networks and Mobile Communications (WINCOM)}, 
title={Designing and implementing XML schema inside OWL ontology}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={The extensible Markup Language (XML) has known since its beginnings an undeniable success. Defined since its origins as a meta-language facilitating the development of specialized tags Languages, nowadays, many documents benefit from the XML frame. But even if this language is strongly used in the web as a way of data exchange between applications, it still lack the capacity of defining the web resources and the system that uses them, and also the capacity of expressing the knowledge provided by XML documents. It's these lacks that proposes the Web Ontology Language (OWL) proposes to fill. In fact, OWL is a language for ontologies representation in the context of Semantic Web (SW). It's in this context that we're obliged to come up with a solution that allows migration to the SW in order to follow the WEB evolution. Among the suggested solutions, our approach models and implements a set of rules that allow transforming an XML Schema Definition (XSD) to Ontology. This mapping will transform not only the nodes of an XML file but also the relationships between these nodes in order to maintain the same structure.}, 
keywords={knowledge representation languages;ontologies (artificial intelligence);semantic Web;XML;OWL ontology;extensible Markup Language;meta-language;specialized tags Languages;XML frame;data exchange;web resources;XML documents;Web Ontology Language;ontologies representation;Semantic Web;WEB evolution;XML Schema Definition;XML file;XML;OWL;Ontologies;Relational databases;Transforms;Resource description framework;extensible Markup Language (XML);Complex type;Web Ontology Language (OWL);XML Schema Definition (XSD);Document Type Definition (DTD);Ontology;Resource Description Framework (RDF);eXtensible Style Language Transformations (XSLT)}, 
doi={10.1109/WINCOM.2017.8238166}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{581739, 
author={S. B. {Davidson} and A. S. {Kosky}}, 
booktitle={Proceedings 13th International Conference on Data Engineering}, 
title={WOL: a language for database transformations and constraints}, 
year={1997}, 
volume={}, 
number={}, 
pages={55-65}, 
abstract={The need to transform data between heterogeneous databases arises from a number of critical tasks in data management. These tasks are complicated by schema evolution in the underlying databases and by the presence of non-standard database constraints. We describe a declarative language called WOL (Well-founded Object Logic) for specifying such transformations, and its implementation in a system called Morphase (an "enzyme" for morphing data). WOL is designed to allow transformations between the complex data structures which arise in object-oriented databases as well as in complex relational databases, and to allow for reasoning about the interactions between database transformations and constraints.}, 
keywords={distributed databases;object-oriented databases;relational databases;data structures;database languages;object-oriented languages;specification languages;database theory;data integrity;WOL;database transformations;nonstandard database constraints;heterogeneous databases;data management;database schema evolution;declarative language;Well-founded Object Logic;Morphase;data morphing;complex data structures;object-oriented databases;complex relational databases;Cities and towns;Relational databases;Natural languages;Writing;Data conversion}, 
doi={10.1109/ICDE.1997.581739}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{810418, 
author={T. L. {Kunii} and J. C. {Browne} and H. S. {Kunii}}, 
booktitle={The IEEE Computer Society's Second International Computer Software and Applications Conference, 1978. COMPSAC '78.}, 
title={An architecture for evolutionary database system design}, 
year={1978}, 
volume={}, 
number={}, 
pages={382-386}, 
abstract={We outline herein an approach to evolutionary database systems design and evolutionary database design which is intended to lower the total costs of maintaining, developing and executing application programs against a database. The premise is that it should be possible to have an evolutionary database system which executes efficiently while providing explicit support for changes in application usage. The key concepts are: hierarchical layering of a database specification, inclusion of static/dynamic characteristics in specification of the database, and a hybrid compile/interpret system for the execution phase of the data management system itself.}, 
keywords={Database systems;Application software;Costs;Computer architecture;Information science;Software maintenance;Software systems}, 
doi={10.1109/CMPSAC.1978.810418}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{380397, 
author={R. J. {Peters} and M. {Tamer Ozsu}}, 
booktitle={Proceedings of the Eleventh International Conference on Data Engineering}, 
title={Axiomatization of dynamic schema evolution in object bases}, 
year={1995}, 
volume={}, 
number={}, 
pages={156-164}, 
abstract={The schema of a system consists of the constructs that model its entities. Schema evolution is the timely change and management of the schema. Dynamic schema evolution is the management of schema changes while the system is in operation. We propose a sound and complete axiomatic model for dynamic schema evolution in object-base management systems (OBMSs) that support subtyping and property inheritance. The model is formal, which distinguishes it from the traditional approach of informally defining a number of invariants and rules to enforce them. By reducing systems to the axiomatic model, their functionality with respect to dynamic schema evolution can be compared within a common framework.<<ETX>>}, 
keywords={object-oriented databases;type theory;inheritance;dynamic schema evolution;axiomatization;axiomatic model;object-base management systems;subtyping;property inheritance;Object oriented modeling;Power system modeling;Multimedia databases;Laboratories;Database systems;Image databases;Object oriented databases;Biomedical engineering;Data engineering;Design engineering}, 
doi={10.1109/ICDE.1995.380397}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{1375488, 
author={ and }, 
booktitle={2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)}, 
title={Optimal design of a 2-layer fuzzy controller using the schema co-evolutionary algorithm}, 
year={2004}, 
volume={2}, 
number={}, 
pages={721-726 vol.2}, 
abstract={The function of the robot is being complex and various. The robot with various inputs generates various outputs. A proper controller is needed to control such a complex robot. We introduce a 2-layer fuzzy controller (2LFC). It can control robustly and has the small number of rules in many input variables. It can not only deal with various inputs but also generate various outputs. The main problem in the fuzzy system is how to design the fuzzy knowledge base. For this specific problem, we optimize the fuzzy controller using the schema co-evolutionary algorithm (SCEA) to find a global optimal solution. Through the design of the optimal fuzzy controller using the SCEA and the simulation of the mobile robot, we verify the efficacy of the 2-layer fuzzy controller and the schema co-evolutionary algorithm.}, 
keywords={mobile robots;intelligent robots;fuzzy control;fuzzy systems;optimal control;robust control;control system synthesis;evolutionary computation;fuzzy reasoning;2 layer optimal fuzzy controller design;schema coevolutionary algorithm;complex robot control;robust control;fuzzy knowledge base system;mobile robot;optimization;fuzzy reasoning;Algorithm design and analysis;Fuzzy control;Optimal control;Robot sensing systems;Evolutionary computation;Robust control;Input variables;Robot control;Fuzzy reasoning;Fuzzy systems}, 
doi={10.1109/FUZZY.2004.1375488}, 
ISSN={1098-7584}, 
month={July},}
@INPROCEEDINGS{139410, 
author={J. {Takahashi}}, 
booktitle={Proceedings., Fourteenth Annual International Computer Software and Applications Conference}, 
title={Hybrid relations for database schema evolution}, 
year={1990}, 
volume={}, 
number={}, 
pages={465-470}, 
abstract={The author describes hybrid relations in relational databases that allow existing relations to be altered by the addition of new attributes without reorganization of the database scheme. The values of new attributes with respect to an existing relation are stored separately from the relation as a set of triples of tuple identifier, attribute name, and value. At query time, a hybrid relation, which has only the attributes requested in a query, is derived virtually by combining the relation and this set of triples. A relation can be reorganized by upgrading its attribute values from these triples. The hybrid relation is defined as an algebraic expression, and equivalent expressions of a query on the hybrid relations are shown for efficient query processing.<<ETX>>}, 
keywords={data structures;relational databases;database schema evolution;hybrid relations;relational databases;tuple identifier;attribute name;value;algebraic expression;query processing;Relational databases;Image databases;Laboratories;Query processing;Object oriented databases;Information retrieval;Navigation}, 
doi={10.1109/CMPSAC.1990.139410}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{4031507, 
author={D. {Kim} and K. {Jeong} and H. {Shin} and S. {Hwang}}, 
booktitle={2006 Fifth International Conference on Grid and Cooperative Computing (GCC'06)}, 
title={An XML Schema-based Semantic Data Integration}, 
year={2006}, 
volume={}, 
number={}, 
pages={522-525}, 
abstract={Cyber-infrastructures for scientific and engineering applications require integrating heterogeneous legacy data in different formats and from various domains. Such data integration raises challenging issues: (1) Support for multiple independently-managed schemas, (2) Ease of schema evolution, and (3) Simple schema mappings. In order to address these issues, we propose a novel approach to semantic integration of scientific data which uses XML schemas and RDF-based schema mappings. In this approach, XML schema allows scientists to manage data models intuitively and to use commodity XML DBMS tools. A simple RDF-based ontological representation scheme is used for only structural relations among independently-managed XML schemas from different institutes or domains. We present the design and implementation of a prototype system developed for the national cyber-environments for civil engineering research activities in Korea (similar to the NEES project in USA) which is called KOCEDgrid}, 
keywords={data integrity;data models;database management systems;ontologies (artificial intelligence);XML;XML schema-based semantic data integration;cyber-infrastructures;scientific applications;engineering applications;heterogeneous legacy data;multiple independently-managed schemas;RDF-based schema mappings;RDF-based ontological representation scheme;civil engineering research activities;KOCEDgrid;XML DBMS tool;XML;Data models;Ontologies;Resource description framework;Data engineering;Information retrieval;Relational databases;Internet;Computer science;Application software}, 
doi={10.1109/GCC.2006.28}, 
ISSN={2160-4908}, 
month={Oct},}
@INPROCEEDINGS{6044750, 
author={D. K. W. {Chiu} and Q. {Li} and K. {Karlapalem}}, 
booktitle={Proceedings Seventh International Conference on Database Systems for Advanced Applications. DASFAA 2001}, 
title={Facilitating workflow evolution in an advanced object environment}, 
year={2001}, 
volume={}, 
number={}, 
pages={148-149}, 
abstract={Workflow is automation of a business process. A Workflow Management Systems (WFMS) is a system that assists in defining, managing and executing workflows. To support flexible enactment and adapive features, such as on-line workflow evolutionand exception handling, a WFMS requires advanced modeling functionality. As workflow evolution requires the modification of workflow definitions or adding ECA rules to the system during work in progress, an advanced schema evolution capability is required at run-time. It should be noted that the resolutions based on schema evolution are general-purpose ones, which can help reduce the occurrence of additional exceptions. As such, we have developed ADOME-WFMS based on Advanced Object Modeling Evnironment (ADMOE [4] - - an active OODBMS with role and synamic schema suppport), with a novel exception centric apporach. The contribution and objectives of our research with respect to workflow evolution are as follows.}, 
keywords={Semantics;Computational modeling;Computer science;Database systems;Educational institutions;Workflow management software}, 
doi={10.1109/DASFAA.2001.6044750}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8509423, 
author={U. {Störl} and A. {Tekleab} and M. {Klettke} and S. {Scherzinger}}, 
booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)}, 
title={In for a Surprise When Migrating NoSQL Data}, 
year={2018}, 
volume={}, 
number={}, 
pages={1662-1662}, 
abstract={Schema-flexible NoSQL data stores lend themselves nicely for storing versioned data, a product of schema evolution. In this lightning talk, we apply pending schema changes to records that have been persisted several schema versions back. We present first experiments with MongoDB and Cassandra, where we explore the trade-off between applying chains of pending changes stepwise (one after the other), and as composite operations. Contrary to intuition, composite migration is not necessarily faster. The culprit is the computational overhead for deriving the compositions. However, caching composition formulae achieves a speed up: For Cassandra, we can cut the runtime by nearly 80%. Surprisingly, the relative speedup seems to be system-dependent. Our take away message is that in applying pending schema changes in NoSQL data stores, we need to base our design decisions on experimental evidence rather than on intuition alone.}, 
keywords={data handling;NoSQL databases;schema-flexible NoSQL data stores;versioned data;schema evolution;Cassandra;composite operations;composite migration;caching composition formulae;NoSQL data migration;MongoDB;Runtime;Tools;Conferences;Data engineering;Lightning;Indexes;NoSQL databases;schema evolution;data migration;composite migration}, 
doi={10.1109/ICDE.2018.00202}, 
ISSN={2375-026X}, 
month={April},}
@INPROCEEDINGS{5767627, 
author={S. {Wu} and I. {Neamtiu}}, 
booktitle={2011 IEEE 27th International Conference on Data Engineering Workshops}, 
title={Schema evolution analysis for embedded databases}, 
year={2011}, 
volume={}, 
number={}, 
pages={151-156}, 
abstract={Dynamic software updating research efforts have mostly been focused on updating application code and in-memory state. As more and more applications use embedded databases for storage, dynamic updating solutions will have to support changes to embedded database schemas. The first step towards supporting dynamic updates to embedded database schemas is understanding how these schemas change-so far, schema evolution studies have focused on large, enterprise-class databases. In this paper we propose an approach for automatically extracting embedded schemas from regular applications, e.g., written in C and C++, and automatically computing how schemas change as applications evolve. To showcase our approach, we perform a long-term schema evolution study on four popular open source programs that use embedded databases: Firefox, Monotone, BiblioteQ and Vienna. Our study spans 18 cumulative years of schema evolution and reveals that change patterns and frequency in embedded databases differ from schema changes in enterprise-class databases that formed the object of prior studies. Our platform can be used for performing long-term, large-scale embedded schema evolution studies that are potentially beneficial to dynamic updating and schema evolution researchers.}, 
keywords={database management systems;public domain software;software engineering;evolution analysis;embedded databases;dynamic software updating research;application code;open source program;Databases;Fires;Software;Internet;Encyclopedias;Electronic publishing}, 
doi={10.1109/ICDEW.2011.5767627}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6891569, 
author={W. {Li} and X. {Chen} and Z. M. {Ma}}, 
booktitle={2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
title={Reengineering fuzzy nested relational databases into fuzzy XML model}, 
year={2014}, 
volume={}, 
number={}, 
pages={1612-1617}, 
abstract={Data interchange on the Web is a common task today and XML has been the de-facto standard of information representation and exchange over the Web. Also information imperfection is inherent in the real-world applications. Fuzzy information has been extensively investigated in the context of database models. Also fuzzy XML modeling recently receives more attention. In order to present fuzzy data from the fuzzy databases with XML, this paper concentrates on fuzzy information modeling in the fuzzy XML model and the fuzzy nested relational database model. The formal approach to mapping a fuzzy nested relational database (FNRDB) schema into a fuzzy DTD model is developed in the paper.}, 
keywords={electronic data interchange;fuzzy set theory;Internet;relational databases;XML;fuzzy nested relational database reengineering;fuzzy XML model;data interchange;information representation;information exchange;Web;database model;fuzzy information modeling;fuzzy nested relational database model;formal approach;FNRDB schema;fuzzy DTD model;XML;Relational databases;Data models;Unified modeling language;Educational institutions;Fuzzy sets;fuzzy XML;fuzzy nested relational mode;fuzzy DTD;reengineering}, 
doi={10.1109/FUZZ-IEEE.2014.6891569}, 
ISSN={1098-7584}, 
month={July},}
@INPROCEEDINGS{5431717, 
author={M. {Piccioni} and M. {Orioly} and B. {Meyer} and T. {Schneider}}, 
booktitle={2009 IEEE/ACM International Conference on Automated Software Engineering}, 
title={An IDE-based, Integrated Solution to Schema Evolution of Object-Oriented Software}, 
year={2009}, 
volume={}, 
number={}, 
pages={650-654}, 
abstract={With the wide support for serialization in object-oriented programming languages, persistent objects have become common place. Retrieving previously ¿persisted¿ objects from classes whose schema changed is however difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses this issues through an IDE-based approach that handles schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of any corrupt objects. This article describes the principles behind invariant-safe schema evolution,and the design and implementation of the ESCHER system.}, 
keywords={object-oriented languages;object-oriented programming;IDE-based;integrated solution;schema evolution;object oriented software;object-oriented programming languages;ESCHER framework;transformation functions;Java;Packaging;Object oriented modeling;Software engineering;Computer languages;Runtime;Robustness;versioning;refactoring;persistence;serialization}, 
doi={10.1109/ASE.2009.100}, 
ISSN={1938-4300}, 
month={Nov},}
@INPROCEEDINGS{8258204, 
author={M. {Klettke} and H. {Awolin} and U. {Störl} and D. {Müller} and S. {Scherzinger}}, 
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, 
title={Uncovering the evolution history of data lakes}, 
year={2017}, 
volume={}, 
number={}, 
pages={2462-2471}, 
abstract={Data accumulating in data lakes can become inaccessible in the long run when its semantics are not available. The heterogeneity of data formats and the sheer volumes of data collections prohibit cleaning and unifying the data manually. Thus, tools for automated data lake analysis are of great interest. In this paper, we target the particular problem of reconstructing the schema evolution history from data lakes. Knowing how the data is structured, and how this structure has evolved over time, enables programmatic access to the lake. By deriving a sequence of schema versions, rather than a single schema, we take into account structural changes over time. Moreover, we address the challenge of detecting inclusion dependencies. This is a prerequisite for mapping between succeeding schema versions, and in particular, detecting nontrivial changes such as a property having been moved or copied. We evaluate our approach for detecting inclusion dependencies using the MovieLens dataset, as well an adaption of a dataset containing botanical descriptions, to cover specific edge cases.}, 
keywords={data analysis;data warehouses;meta data;query processing;data collections;automated data lake analysis;schema versions;data format heterogeneity;schema evolution history reconstruction;programmatic access;schema version;structural changes;inclusion dependency detection;MovieLens dataset;botanical descriptions;Protocols;Lakes;Grippers;NoSQL databases;History;Data mining;Tools;NoSQL databases;schema version extraction;evolution operations;integrity constraints;inclusion dependencies}, 
doi={10.1109/BigData.2017.8258204}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5284780, 
author={G. {Polese} and M. {Vacca}}, 
booktitle={2009 IEEE 5th International Conference on Intelligent Computer Communication and Processing}, 
title={A dialogue-based model for the query synchronization problem}, 
year={2009}, 
volume={}, 
number={}, 
pages={67-70}, 
abstract={The synchronization of queries is one of the schema evolution problems and it calls for the redefinition of those queries becoming undefined after a schema change, in order to keep them still working on the new schema. This problem is particularly difficult for changes that upset the schema, because it could not be possible to rewrite the queries exactly. In this paper we show that the dialogue can play an important role in these cases and we propose a model of dialogue for query synchronization based on the Hintikka interrogative logic.}, 
keywords={formal logic;interactive systems;query processing;synchronisation;query synchronization problem;dialogue-based model;Hintikka interrogative logic;database schema evolution;Databases;Humans;Logic design;Information systems;Semantic Web;Collaboration;Joining processes;dialogue;interrogative logic;query synchronization;schema evolution}, 
doi={10.1109/ICCP.2009.5284780}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{624730, 
author={R. A. {Paul} and T. L. {Kunii} and Y. {Shinagawa} and A. {Ghafoor}}, 
booktitle={Proceedings Twenty-First Annual International Computer Software and Applications Conference (COMPSAC'97)}, 
title={Object-oriented evolutionary database design for software metrics data}, 
year={1997}, 
volume={}, 
number={}, 
pages={32-37}, 
abstract={The authors present an approach to manage evolutionary changes that take place over time in an object-oriented software metrics database schema. The framework is based on the entity-relation (E-R) model and uses a recursive graph structure, known as R-graph, to support the views of software quality and risk. The fundamental mechanism for abstracting views is the introduction of two semantic operators for the R-graph. These operators are proposed based on graph-based predicates and Petri-net based predicate formalism for view abstraction.}, 
keywords={software metrics;software quality;object-oriented databases;entity-relationship modelling;Petri nets;data structures;risk management;object-oriented evolutionary database design;software metrics data;evolutionary change management;object-oriented software metrics database schema;entity-relation model;recursive graph structure;R-graph;software quality;risk;view abstraction;semantic operators;graph-based predicates;Petri net based predicate formalism;Object oriented databases;Software design;Software metrics;Object oriented modeling;Project management;Data models;Data analysis;Software quality;Monitoring;Risk management}, 
doi={10.1109/CMPSAC.1997.624730}, 
ISSN={0730-3157}, 
month={Aug},}
@INPROCEEDINGS{938551, 
author={J. {Fredriksson} and P. {Svensson}}, 
booktitle={Proceedings Thirteenth International Conference on Scientific and Statistical Database Management. SSDBM 2001}, 
title={Evolutionary design and development of image meta-analysis environments based on object-relational database mediator technology}, 
year={2001}, 
volume={}, 
number={}, 
pages={190-200}, 
abstract={Discusses how emerging object-relational database mediator technology can be used to integrate academic freeware and commercial-off-the-shelf software components to create a sequence of gradually more complex and powerful, yet always syntactically and semantically homogeneous, database-centred image meta-analysis environments. We show how this may be done by defining and utilising a use-case-based evolutionary design and development process. This process allows subsystems to be produced largely independently by several small specialist subprojects, turning the system integration work into a high-level domain modelling task.}, 
keywords={software prototyping;visual databases;meta data;object-oriented databases;relational databases;public domain software;software packages;integrated software;use-case-based evolutionary design;database-centred image meta-analysis environments;object-relational database mediator technology;academic freeware;commercial-off-the-shelf software components;independently produced subsystems;specialist subprojects;system integration;high-level domain modelling task;Image databases;Relational databases;Numerical analysis;Humans;Image storage;Object oriented databases;Visual databases;Computer science;Neuroscience;Process design}, 
doi={10.1109/SSDM.2001.938551}, 
ISSN={1099-3371}, 
month={July},}
@ARTICLE{1423977, 
author={M. L. {Goldstein} and G. G. {Yen}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Using evolutionary algorithms for defining the sampling policy of complex n-partite networks}, 
year={2005}, 
volume={17}, 
number={6}, 
pages={762-773}, 
abstract={N-partite networks are natural representations of complex multientity databases. However, processing these networks can be a highly memory and computation-intensive task, especially when positive correlation exists between the degrees of vertices from different partitions. In order to improve the scalability of this process, this paper proposes two algorithms that make use of sampling for obtaining less expensive approximate results. The first algorithm is optimal for obtaining homogeneous discovery rates with a low memory requirement, but can be very slow in cases where the combined branching factor of these networks is too large. A second algorithm that incorporates concepts from evolutionary computation aims toward dealing with this slow convergence in the case when it is more interesting to increase approximation convergence speed of elements with high feature values. This algorithm makes use of the positive correlation between "local" branching factors and the feature values. Two applications examples are demonstrated in searching for most influential authors in collections of journal articles and in analyzing most active earthquake regions from a collection of earthquake events.}, 
keywords={distributed databases;evolutionary computation;graph theory;convergence;data mining;sampling methods;approximation theory;earthquakes;N-partite networks;evolutionary algorithms;multientity databases;memory requirement;approximation convergence speed;local branching factors;journal articles;active earthquake regions;graphic-structured database;Evolutionary computation;Sampling methods;Spatial databases;Scalability;Partitioning algorithms;Convergence;Earthquakes;Computer networks;Approximation algorithms;Distributed databases;Index Terms- N-partite network;evolutionary algorithm;graphic-structured database.}, 
doi={10.1109/TKDE.2005.100}, 
ISSN={1041-4347}, 
month={June},}
@INPROCEEDINGS{891456, 
author={A. {Bianchi} and D. {Caivano} and G. {Visaggio}}, 
booktitle={Proceedings Seventh Working Conference on Reverse Engineering}, 
title={Method and process for iterative reengineering of data in a legacy system}, 
year={2000}, 
volume={}, 
number={}, 
pages={86-96}, 
abstract={This paper presents an iterative approach to database reengineering, starting from the assumption that for the user organization, the data are the most important assets in a legacy system. The most innovative feature of the proposed approach, in comparison with other rival approaches, is that it can eliminate all the ageing symptoms of the legacy database. The new database can therefore be readily used to integrate data used by new functions introduced in the legacy software. Moreover, the approach allows all the services offered by modern database management systems to be exploited. To test the effectiveness of the process described in this paper, it was experimented on a real legacy system; the results reported confirm its effectiveness.}, 
keywords={systems re-engineering;database management systems;data analysis;iterative data reengineering;legacy system;database reengineering;user organization;database management systems;Iterative methods;Aging;Spatial databases;Computer languages;Software systems;Wrapping;System testing;Investments}, 
doi={10.1109/WCRE.2000.891456}, 
ISSN={1095-1350}, 
month={Nov},}
@INPROCEEDINGS{957840, 
author={R. {Alhajj} and F. {Polat}}, 
booktitle={Proceedings Eighth Working Conference on Reverse Engineering}, 
title={Reengineering relational databases to object-oriented: constructing the class hierarchy and migrating the data}, 
year={2001}, 
volume={}, 
number={}, 
pages={335-344}, 
abstract={The object-oriented data model is predicted to be the heart of the next generation of database systems. Users want to move from old legacy databases into applying this new technology that provides extensibility and flexibility in maintenance. However, a major limitation on the wide acceptance of object-oriented databases is the amount of time and money invested on existing database applications, which are based on conventional legacy systems. Users do not want to loose the huge amounts of data present in conventional databases. This paper presents a novel approach to transform a given conventional database into an object-oriented database. It is assumed that the necessary characteristics of the conventional database to be re-engineered are known and available. The source of these characteristics might be the data dictionary and/or an expert in the given conventional database. We implemented a system that builds an understanding of a given conventional database by taking these characteristics as input and produces the corresponding object-oriented database as output. The system derives a graph that summarizes the conceptual model. Links in the graph are classified into inheritance links and aggregation links. This classification leads to the class hierarchy. Finally, we handle the migration of data from the conventional database to the constructed object-oriented database.}, 
keywords={relational databases;systems re-engineering;object-oriented databases;graphs;relational database reengineering;object- oriented databases;class hierarchy;data migration;object-oriented data model;legacy databases;extensibility;maintenance flexibility;data dictionary;database expert;graph links;inheritance links;aggregation links;conceptual model;algorithms;forward engineering;Relational databases;Object oriented databases;Database systems;Data models;Heart;Information systems;Computer science;Data engineering;Dictionaries;Object oriented modeling}, 
doi={10.1109/WCRE.2001.957840}, 
ISSN={1095-1350}, 
month={Oct},}
@INPROCEEDINGS{870716, 
author={L. {Cai} and D. {Juedes} and E. {Liakhovitch}}, 
booktitle={Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)}, 
title={Evolutionary computation techniques for multiple sequence alignment}, 
year={2000}, 
volume={2}, 
number={}, 
pages={829-835 vol.2}, 
abstract={Given a collection of biologically related protein or DNA sequences, the basic multiple sequence alignment problem is to determine the most biologically plausible alignment of these sequences. Under the assumption that the collection of sequences arose from some common ancestor, an alignment can be used to infer the evolutionary history among the sequences, i.e., the most likely pattern of insertions, deletions and mutations that transformed one sequence into another. The general multiple sequence alignment problem is known to be NP-hard, and hence the problem of finding the best possible multiple sequence alignment is intractable. However, this does not preclude the possibility of developing algorithms that produce near optimal multiple sequence alignments in polynomial time. We examine techniques to combine efficient algorithms for near optimal global and local multiple sequence alignment with evolutionary computation techniques to search for better near optimal sequence alignments. We describe our evolutionary computation approach to multiple sequence alignment and present preliminary simulation results on a set of 17 clusters of orthologous groups of proteins (COGs). We compare the fitness of the alignments given by the proposed techniques with the fitness of CLUSTAL W alignments given in the COG database.}, 
keywords={evolutionary computation;DNA;biology computing;computational complexity;evolutionary computation;multiple sequence alignment;biologically related protein sequences;DNA sequences;NP-hard;polynomial time;simulation;clusters of orthologous groups of proteins;COG database;Evolutionary computation;Sequences;Proteins;DNA;History;Genetic mutations;Polynomials;Clustering algorithms;Computational modeling;Databases}, 
doi={10.1109/CEC.2000.870716}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{380396, 
author={ and E. A. {Rundensteiner}}, 
booktitle={Proceedings of the Eleventh International Conference on Data Engineering}, 
title={A transparent object-oriented schema change approach using view evolution}, 
year={1995}, 
volume={}, 
number={}, 
pages={165-172}, 
abstract={When a database is shared by many users, updates to the database schema are almost always prohibited because there is a risk of making existing application programs obsolete when they run against the modified schema. This paper addresses the problem by integrating schema evolution with view facilities. When new requirements necessitate schema updates for a particular user, the user specifies schema changes to the personal view rather than to the shared base schema. Our view evolution approach then computes a new view schema that reflects the semantics of the desired schema change, and replaces the old view with the new one. We present algorithms that implement the set of schema evolution operations typically supported by OODB systems as view definitions. This approach provides the means for schema change without affecting other views (and thus without affecting existing application programs). The persistent data is shared by different views of the schema, i.e., both old as well as newly developed applications can continue to interoperate. In this paper, we present examples that demonstrate our approach.<<ETX>>}, 
keywords={object-oriented databases;data structures;transparent object-oriented schema change approach;view evolution;algorithms;persistent data;Object oriented databases;Multimedia databases;Computer science;Software systems;Electronic mail;Application software;Data models;Computer aided manufacturing;CADCAM;Multimedia systems}, 
doi={10.1109/ICDE.1995.380396}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{792162, 
author={M. {Kradolfer} and A. {Geppert}}, 
booktitle={Proceedings Fourth IFCIS International Conference on Cooperative Information Systems. CoopIS 99 (Cat. No.PR00384)}, 
title={Dynamic workflow schema evolution based on workflow type versioning and workflow migration}, 
year={1999}, 
volume={}, 
number={}, 
pages={104-114}, 
abstract={An important yet open problem in workflow management is the evolution of workflow schemas, i.e., the creation, deletion and modification of workflow types in such a way that the schema remains correct. This problem is aggravated when instances of modified workflow types are active at the time of modification because any workflow instance has to conform to the definition of its type. The paper presents a framework for dynamic workflow schema evolution that is based on workflow type versioning and workflow migration. Workflow types can be versioned, and a new version can be derived from an existing one by applying modification operations. Workflow type versions allow us to handle active instances in an elegant way whenever a schema is modified. If possible, an affected workflow instance is migrated to the new version of its type. Otherwise, it continues to execute under its old type. We introduce correctness criteria that must be met by workflow schemas and workflow schema modification operations. We also define under which conditions the migration of workflow instances to new workflow type versions is allowed.}, 
keywords={workflow management software;configuration management;type theory;dynamic workflow schema evolution;workflow type versioning;workflow migration;workflow management;workflow types;modified workflow types;workflow instance;modification operations;active instances;correctness criteria;workflow schema modification operations;workflow type versions;History;Taxonomy}, 
doi={10.1109/COOPIS.1999.792162}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{1019216, 
author={ and and }, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Exploring into programs for the recovery of data dependencies designed}, 
year={2002}, 
volume={14}, 
number={4}, 
pages={825-835}, 
abstract={Data dependencies play an important role in the design of a database. Many legacy database applications have been developed on old generation database management systems and conventional file systems. As a result, most of the data dependencies in legacy databases are not enforced in the database management systems. As such, they are not explicitly defined in database schema and are enforced in the transactions, which update the databases. It is very difficult and time consuming to find out the designed data dependencies manually during the maintenance and reengineering of database applications. In software engineering, program analysis has long been developed and proven as a useful aid in many areas. With the use of program analysis, this paper proposes a novel approach for the recovery of common data dependencies, i.e., functional dependencies, key constraints, inclusion dependencies, referential constraints, and sum dependencies, designed in a database from the behavior of transactions, which update the database. The approach is based on detecting program path patterns for implementing most commonly used methods to enforce these data dependencies.}, 
keywords={database theory;data mining;relational databases;systems re-engineering;reverse engineering;data dependencies;database design;legacy database applications;knowledge discovery;file systems;database schema;transactions;relational databases;application reengineering;software engineering;program analysis;functional dependencies;key constraints;inclusion dependencies;referential constraints;sum dependencies;program path patterns;database reverse engineering;design recovery;Transaction databases;Relational databases;Database systems;Reverse engineering;File systems;Application software;Software engineering;Data analysis;Design automation;Pattern analysis}, 
doi={10.1109/TKDE.2002.1019216}, 
ISSN={1041-4347}, 
month={July},}
@INPROCEEDINGS{542328, 
author={D. B. {Fogel} and A. {Ghozeil}}, 
booktitle={Proceedings of IEEE International Conference on Evolutionary Computation}, 
title={Using fitness distributions to design more efficient evolutionary computations}, 
year={1996}, 
volume={}, 
number={}, 
pages={11-19}, 
abstract={There is a need for methods to generate more efficient and effective evolutionary algorithms. Traditional techniques that rely on schema processing, minimizing expected losses, and an emphasis on particular genetic operators have failed to provide robust optimization performance. An alternative technique for enhancing both the expected rate and probability of improvement in evolutionary algorithms is proposed. The method is investigated empirically and is shown to provide a potentially useful procedure for assessing the suitability of various variation operators in light of a particular representation, selection operator, and objective function.}, 
keywords={probability;genetic algorithms;fitness distributions;evolutionary computation design;evolutionary algorithms;schema processing;expected loss minimization;genetic operators;robust optimization performance;probability;selection operator;objective function;Distributed computing;Evolutionary computation;Algorithm design and analysis;Genetic algorithms;Robustness;Performance loss;Design optimization;Parallel processing;Optimization methods;Genetic programming}, 
doi={10.1109/ICEC.1996.542328}, 
ISSN={}, 
month={May},}
@ARTICLE{6873284, 
author={S. {Sou} and S. {Wang}}, 
journal={IEEE Communications Letters}, 
title={Performance Improvements of Batch Data Model for Machine-to-Machine Communications}, 
year={2014}, 
volume={18}, 
number={10}, 
pages={1775-1778}, 
abstract={In recent years, rapid growth in the popularity of machine-to-machine (M2M) applications has enabled the deployment of a large number of M2M devices. In addition, 3GPP proposes the use of higher layer connections among M2M devices to link to LTE-advanced core networks. To support the massive numbers of device connections, it is essential that the cost imposed by the unnecessary transmission of data in core networks be reduced. This paper investigates how to apply the batch data model to reduce the data update frequency in M2M core networks. To achieve an acceptable ratio of updated devices while avoiding a high data update rate from the devices/gateway to the server, for each data access issuing by an M2M application, we dynamically pull not-yet-updated data from the gateway in respect to different kinds of application requirements. An analytical model is developed for the proposed batch data model with data pulling to evaluate the transmission cost and updated device ratio in M2M networks.}, 
keywords={Long Term Evolution;batch data model;machine-to-machine communications;machine-to-machine applications;M2M devices;LTE advanced core networks;device connections;data access;M2M application;Logic gates;Servers;Data models;Performance evaluation;Analytical models;Markov processes;Limiting;Machine-to-machine communications;data pulling;update frequency;transmission cost}, 
doi={10.1109/LCOMM.2014.2345656}, 
ISSN={1089-7798}, 
month={Oct},}
@INPROCEEDINGS{4594232, 
author={}, 
booktitle={2008 7th World Congress on Intelligent Control and Automation}, 
title={Research on virtual laboratory information management based on metadata}, 
year={2008}, 
volume={}, 
number={}, 
pages={8323-8327}, 
abstract={To enhance the effectiveness and efficiency in the virtual laboratory management, a VLIMS (virtual laboratory information management system) is needed. A meta-model and architecture for the VLIMS was discussed. The meta-model consisted of 6 basic entities. Using an expanded Web services description and search process, the architecture assembled the experimental workflow management of multi-heterogeneous system and supported data mining process, based on the metadata repository. The schema integration and evolution operation were analyzed, in order to implement schema matching and version management. The financial business example of Web services composition showed the services configuration and metadata function.}, 
keywords={configuration management;data mining;information systems;meta data;Web services;workflow management software;virtual laboratory information management system;Web services;experimental workflow management;multiheterogeneous system;data mining process;metadata repository;schema integration;evolution operation;schema matching;version management;financial business;Laboratories;Information management;Business;Web services;Data mining;Automation;Data models;virtual laboratory;metadata;model management}, 
doi={10.1109/WCICA.2008.4594232}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7969559, 
author={W. {Dhifli} and N. O. {Da Costa} and M. {Elati}}, 
booktitle={2017 IEEE Congress on Evolutionary Computation (CEC)}, 
title={An evolutionary schema for mining skyline clusters of attributed graph data}, 
year={2017}, 
volume={}, 
number={}, 
pages={2102-2109}, 
abstract={Graph clustering is one of the most important research topics in graph mining and network analysis. With the abundance of data in many real-world applications, the graph nodes and edges could be annotated with multiple sets of attributes that could be derived from heterogeneous data sources. Considering these attributes during the graph clustering could help in generating graph clusters with balanced and cohesive intra-cluster structure and nodes having homogeneous properties. In this paper, we propose a genetic algorithm-based graph clustering approach for mining skyline clusters over large attributed graphs based on the dominance relationship. Each skyline solution is optimized with respect to multiple fitness functions simultaneously where each function is defined over the graph topology or over a particular set of attributes that are derived from multiple data sources. We experimentally evaluate our approach on a real-world large protein-protein interaction network of the human interactome enriched with large sets of heterogeneous cancer associated attributes. The obtained results show the efficiency of our approach and how integrating node attributes of multiple data sources allows to obtain a more robust graph clustering than by considering only the graph topology.}, 
keywords={data mining;genetic algorithms;graph theory;pattern clustering;evolutionary schema;skyline clusters;attributed graph data mining;graph mining;network analysis;genetic algorithm-based graph clustering approach;graph topology;human interactome protein-protein interaction network;heterogeneous cancer associated attributes;Sociology;Statistics;Biological cells;Genetic algorithms;Clustering algorithms;Data mining;Topology}, 
doi={10.1109/CEC.2017.7969559}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6320793, 
author={S. M. {Jadhav} and V. {Patil}}, 
booktitle={2012 IEEE International Conference on Advanced Communication Control and Computing Technologies (ICACCCT)}, 
title={An effective content Based Image Retrieval (CBIR) system based on evolutionary programming (EP)}, 
year={2012}, 
volume={}, 
number={}, 
pages={310-315}, 
abstract={This paper introduces a content Based Image Retrieval (CBIR) based on evolutionary algorithm. Initially, the shape, color and texture feature is extracted for the given query image and also for the of the database images in a similar manner. Subsequently, similar images are retrieved utilizing an evolutionary algorithm based similarity. Thus, by means of the evolutionary algorithm, the required relevant images are retrieved from a large database based on the given query. The proposed CBIR system is evaluated by querying different images and the efficiency of the proposed system is evaluated by means of the precision-recall value of the retrieved results.}, 
keywords={content-based retrieval;evolutionary computation;image colour analysis;image retrieval;image texture;shape recognition;visual databases;effective content based image retrieval;CBIR;evolutionary programming;EP;texture feature;color feature;shape feature;query image;database images;evolutionary algorithm;Lead;Biomedical imaging;Image color analysis;Image segmentation;Biological cells;Ontologies;feature extraction;shape;color histogram;texture;query;Evolutionary Programming (EP)}, 
doi={10.1109/ICACCCT.2012.6320793}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{4425077, 
author={E. {Gonzales} and K. {Taboada} and and and and }, 
booktitle={2007 IEEE Congress on Evolutionary Computation}, 
title={Class association rule mining for large and dense databases with parallel processing of genetic network programming}, 
year={2007}, 
volume={}, 
number={}, 
pages={4615-4622}, 
abstract={Among several methods of extracting association rules that have been reported, a new evolutionary computation method named genetic network programming (GNP) has also shown its effectiveness for small datasets that have a relatively small number of attributes. The aim of this paper is to propose a new method to extract association rules from large and dense datasets with a huge amount of attributes using GNP. It consists of two-level of processing. Server level where conventional GNP based mining method runs in parallel and client level where files are considered as individuals and genetic operations are carried out over them. The algorithm starts dividing the large dataset into small datasets with appropriate size, and then each of them are dealt with GNP in parallel processing. The new association rules obtained in each generation are stored in a general global pool. We compared several genetic operators applied to the individuals in the global level. The proposed method showed remarkable improvements on simulations.}, 
keywords={client-server systems;data mining;database management systems;genetic algorithms;class association rule mining;large database;dense database;parallel processing;genetic network programming;association rule extraction;evolutionary computation;client-server system;genetic operators;Association rules;Data mining;Parallel processing;Parallel programming;Economic indicators;Genetic programming;Database systems;Evolutionary computation;File servers;Spatial databases}, 
doi={10.1109/CEC.2007.4425077}, 
ISSN={1089-778X}, 
month={Sep.},}
@INPROCEEDINGS{886239, 
author={M. {Watts} and N. {Kasabov}}, 
booktitle={2000 IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. Proceedings of the First IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks (Cat. No.00}, 
title={Simple evolving connectionist systems and experiments on isolated phoneme recognition}, 
year={2000}, 
volume={}, 
number={}, 
pages={232-239}, 
abstract={Evolving connectionist systems (ECoS) are systems that evolve their structure through online, adaptive learning from incoming data. This paradigm complements the paradigm of evolutionary computation based on population based search and optimisation of individual systems through generations of populations. The paper presents the theory and architecture of a simple evolving system called SECoS that evolves through one pass learning from incoming data. A case study of multi-modular SECoS systems evolved from a database of New Zealand English phonemes is used as an illustration of the method.}, 
keywords={neural nets;evolutionary computation;adaptive systems;search problems;learning (artificial intelligence);speech recognition;word processing;simple evolving connectionist systems;isolated phoneme recognition;ECoS;online adaptive learning;incoming data;evolutionary computation;population based search;evolving system;one pass learning;case study;multi-modular SECoS systems;New Zealand English phoneme database;Evolutionary computation;Radio access networks;Information science;Telephony;Computer architecture;Databases;Genetic algorithms;Optimization methods;Information retrieval;Fuzzy neural networks}, 
doi={10.1109/ECNN.2000.886239}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5716356, 
author={T. {Nguyen}}, 
booktitle={2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC)}, 
title={An evolutionary design for software systems}, 
year={2010}, 
volume={}, 
number={}, 
pages={255-260}, 
abstract={This paper explains Life Design, a bio-inspired approach towards building evolvable software, that is, software that can evolve itself similar to natural life forms. The design is informed by Darwinian evolution, gleaned from natural life, and based on object-orientation, meta-modelling and a web system. It can open a new and systematic way to develop customisable and adaptable systems, or evolve open source systems.}, 
keywords={bio-inspired materials;evolutionary computation;Internet;metacomputing;object-oriented methods;public domain software;software engineering;evolutionary design;software system;bio-inspired approach;Darwinian evolution;natural life design;object orientation;meta modelling;web system;open source system;Books;Genomics;Bioinformatics;Australia;evolvable;software design;object-orientation;schema evolution;meta-modelling;bio-inspired}, 
doi={10.1109/NABIC.2010.5716356}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6817784, 
author={J. {Stine} and D. {Swain-Walsh} and M. {Sherman}}, 
booktitle={2014 IEEE International Symposium on Dynamic Spectrum Access Networks (DYSPAN)}, 
title={IEEE 1900.5 enabled whitespace database architecture evolution}, 
year={2014}, 
volume={}, 
number={}, 
pages={103-112}, 
abstract={Increased demand for electromagnetic spectrum (licensed and unlicensed) has spurred the development of sharing techniques based on databases and the development of FCC rules requiring them for TV Whitespace bands. These concepts are also being considered for other sharing scenarios. Many standards groups have already begun standardizing solutions for TV Whitespace, including the IEEE, the IETF and ETSI. As these techniques are just starting to be deployed it is anticipated that there will be lessons learned which could prompt modifications to spectrum sharing requirements after initial deployment. The IEEE 1900.5 WG is developing a series of standards that will allow increased spectral flexibility when coupled with the sharing solutions being developed by other standards groups. This flexibility will allow new sharing requirements to be addressed as they emerge - even in deployed systems. This paper relates the policy-based control architectures in the IEEE 1900.5-2011 standard to proposed database sharing architectures, and goes on to describe the applicability of two IEEE 1900.5 standards which are currently under development-P1900.5.1 Draft Standard Policy Language for Dynamic Spectrum Access Systems and P1900.5.2 Standard Method for Modeling Spectrum Consumption - to requirements for database enabled spectrum sharing.}, 
keywords={IEEE standards;radio spectrum management;television;IEEE 1900.5 enabled whitespace database architecture evolution;electromagnetic spectrum;sharing techniques;FCC rules;TV whitespace bands;IETF;ETSI;IEEE 1900.5 WG;IEEE 1900.5-2011 standard;database sharing architectures;IEEE 1900.5 standards;dynamic spectrum access systems;P1900.5.2 standard method;spectrum consumption modeling;spectrum sharing;Databases;Standards;Geology;Radio spectrum management;Computer architecture;Cognition;Protocols;IEEE 1900.5;spectrum management;dynamic spectrum access;cognitive radio;spectrum consumption model;white space database;spectrum sharing}, 
doi={10.1109/DySPAN.2014.6817784}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7150714, 
author={M. J. {D'Souza} and B. {Barile} and A. F. {Givens}}, 
booktitle={2015 International Conference on Industrial Instrumentation and Control (ICIC)}, 
title={Evolution of a structure-searchable database into a prototype for a high-fidelity SmartPhone app for 62 common pesticides used in delaware}, 
year={2015}, 
volume={}, 
number={}, 
pages={71-76}, 
abstract={Synthetic pesticides are widely used in the modern world for human benefit. They are usually classified according to their intended pest target. In Delaware (DE), approximately 42 percent of the arable land is used for agriculture. In order to manage insectivorous and herbaceous pests (such as insects, weeds, nematodes, and rodents), pesticides are used profusely to biologically control the normal pest's life stage. In this undergraduate project, we first created a usable relational database containing 62 agricultural pesticides that are common in Delaware. Chemically pertinent quantitative and qualitative information was first stored in Bio-Rad's KnowItAll®Informatics System. Next, we extracted the data out of the KnowItAll®system and created additional sections on a Microsoft®Excel spreadsheet detailing pesticide use(s) and safety and handling information. Finally, in an effort to promote good agricultural practices, to increase efficiency in business decisions, and to make pesticide data globally accessible, we developed a mobile application for smartphones that displayed the pesticide database using Appery.io; a cloud-based HyperText Markup Language (HTML5), jQuery Mobile and Hybrid Mobile app builder.}, 
keywords={agriculture;agrochemicals;mobile computing;pest control;relational databases;smart phones;structure-searchable database evolution;high-fidelity smartphone application;Delaware;United States of America;synthetic pesticides;agriculture;pest management;relational database;agricultural pesticides;Bio-Rad KnowItAll Informatics System;Microsoft Excel spreadsheet;agricultural practices;cloud-based hypertext markup language;HTML5;jQuery Mobile;Hybrid Mobile app builder;Databases;Drugs;Agriculture;Informatics;Chemicals;Mobile communication;Wesley College;pesticide database;KnowItAll®Informatics System;Delaware;EPSCoR;IDeA;INBRE;undergraduate research;STEM;smartphone app}, 
doi={10.1109/IIC.2015.7150714}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1029679, 
author={L. {Ai-Jadir} and G. {Beydoun}}, 
booktitle={Proceedings International Database Engineering and Applications Symposium}, 
title={Using the F2 OODBMS to support incremental knowledge acquisition}, 
year={2002}, 
volume={}, 
number={}, 
pages={266-275}, 
abstract={Ripple down rules (RDR) is an incremental knowledge acquisition (KA) methodology, where a knowledge base (KB) is constructed as a collection of rules with exceptions. Nested ripple down rules (NRDR) is an extension of this methodology which allows the expert to enter her/his own domain concepts and later refine these concepts hierarchically. In this paper we show similarities between incremental knowledge acquisition and database schema evolution, and propose to use the F2 object-oriented database management system (OODBMS) to implement an NRDR knowledge based system. We use the existing non-standard features of F2 and show how multiple instantiation and object migration (known as multiobjects feature in F2), and schema evolution capabilities in F2 easily accommodate all the update mechanisms required to incrementally build an NRDR KB. We illustrate our approach with a KA session.}, 
keywords={knowledge acquisition;object-oriented databases;knowledge based systems;incremental knowledge acquisition;ripple down rules;knowledge base;exceptions;nested ripple down rules;domain concepts;database schema evolution;F2 object-oriented database management system;multiple instantiation;object migration;multiobjects feature;update mechanisms;Knowledge acquisition;Object oriented databases;Spatial databases;Database systems;Knowledge based systems;Data engineering;Knowledge engineering}, 
doi={10.1109/IDEAS.2002.1029679}, 
ISSN={1098-8068}, 
month={July},}
@INPROCEEDINGS{870303, 
author={ and G. {Chong}}, 
booktitle={Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)}, 
title={Evolving trajectory controller networks from linear approximation model networks}, 
year={2000}, 
volume={1}, 
number={}, 
pages={251-255 vol.1}, 
abstract={Simple, linear classical controllers are highly popular in industrial applications. However, most controllers have to be tuned and manually re-tuned on a trial and error basis at every operating level. This is particularly difficult when the plant to be controlled is significantly nonlinear. The deficiency in localised linearised models associated with 'local model networks' has been overcome by the introduction of 'linear approximation model (LAM) networks'. To address this problem and help in the design of industrial controllers for a wider range of operating trajectories,y, this paper develops a controller network design technique based upon a LAM network of a practical or nonlinear system to be controlled. This is called a 'Trajectory Controller Network (TCN)', which overcomes the deficiency associated with local controller networks. Each element of a TCN can be of a simple form, such as PID, and may be obtained directly from a set of step response data at several typical operating levels for fast prototyping. Since plant step response data are often readily available in control engineering practice, such TCNs can be automatically and optimally evolved from these data directly without the need for model identification. The overall controller is co-ordinated and evolved along the entire operating trajectory in the operating envelope, tackling the control problem of practical or nonlinear plants. Evolutionary computation provides global structural search for the network and multi-objective optimisation of the controllers. This novel technique is illustrated and validated through a nonlinear control example.}, 
keywords={nonlinear control systems;evolutionary computation;model reference adaptive control systems;evolving trajectory controller networks;linear approximation model networks;localised linearised models;nonlinear control systems;controller network design technique;PID;plant step response data;model identification;evolutionary computation;multiobjective optimisation;nonlinear control;Linear approximation;Nonlinear control systems;Control systems;Industrial control;Automatic control;Error correction;Electrical equipment industry;Nonlinear systems;Prototypes;Control engineering}, 
doi={10.1109/CEC.2000.870303}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{4296959, 
author={P. B. {Ambhore} and B. B. {Meshram} and V. B. {Waghmare}}, 
booktitle={5th ACIS International Conference on Software Engineering Research, Management Applications (SERA 2007)}, 
title={A Implementation of Object Oriented Database Security}, 
year={2007}, 
volume={}, 
number={}, 
pages={359-365}, 
abstract={This paper is to address high-level authorization specifications and its efficient implementation in object oriented database scenario. Premchand Ambhore is with higher and technical education, maharashtra state, india, e-mail: pbambhore@yahoo.com . The rest of the paper is organized as follows. Section 2 specifies object oriented database system; section 3 incorporates authorization rules into database and section 4 maps the specification into a logic programming language and implements the language. Section 5 concludes the paper. The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database system, therefore the problems involved in providing data security for object oriented databases have been receiving very great attention from researchers. As a matter of fact, data are the most sensitive part of any system and its loss or compromise can have disastrous consequences. We discuss three different types of access: discretionary access control (DAC), mandatory access control (MAC) and role based access control (RBAC) in relational DBMS and object oriented data bases. We have identified some of the issues and current proposals; current security models in object oriented database systems and outline possible direction issues for future research.}, 
keywords={authorisation;logic programming languages;object-oriented databases;object-oriented programming;object oriented database security;high-level authorization specification;authorization rules;logic programming language;object-oriented programming;discretionary access control;mandatory access control;role based access control;relational DBMS;Object oriented databases;Data security;Access control;Authorization;Educational programs;Logic programming;Object oriented programming;Database systems;Proposals;Object oriented modeling;Semantics;structural;and;dynamic;data model;access control;database;security;authorization policy;and logic-based specification.}, 
doi={10.1109/SERA.2007.17}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8570438, 
author={R. {Ei Hamdi} and M. {Njah}}, 
booktitle={2018 15th International Multi-Conference on Systems, Signals Devices (SSD)}, 
title={A Constraint Handling Technique for Implementing Multi-Objective Evolutionary Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={982-987}, 
abstract={Pareto-based multi-objective algorithms have been adopted for the design of artificial neural networks and they have attracted much interest, with promising results. Such approaches are usually implemented by means of error minimization while controlling the complexity of the model. This paper proposes a novel methodology for implementing Pareto-based multi-objective optimization within evolutionary neural networks. The originality of the proposed methodology lie in the use of a new constraint handling technique based on a self-adaptive penalty method in order to direct the entire search effort towards finding only Pareto optimal solutions that are acceptable. Experimental results show that the proposed approach outperforms other existing evolutionary neural networks approaches reported recently in the literature using the widely accepted Wisconsin breast cancer diagnosis (WBCD) database. The developed approach was also used to simulate chemical unit process and the outcomes of the results are quite encouraging.}, 
keywords={cancer;constraint handling;evolutionary computation;medical diagnostic computing;neural nets;Pareto optimisation;search problems;multiobjective evolutionary neural networks;artificial neural networks;error minimization;Pareto-based multiobjective optimization;self-adaptive penalty method;Pareto optimal solutions;experimental results;Wisconsin breast cancer diagnosis database;evolutionary neural networks approaches;WBCD;Sociology;Statistics;Complexity theory;Radial basis function networks;Optimization;Artificial neural networks;Artificial Neural Networks;Constrained Multi-Objective Optimization;Pareto Dominance Criterion;Differential Evolution}, 
doi={10.1109/SSD.2018.8570438}, 
ISSN={2474-0446}, 
month={March},}
@INPROCEEDINGS{289869, 
author={M. R. {Scalas} and A. {Cappelli} and C. {De Castro}}, 
booktitle={1993 CompEuro Proceedings Computers in Design, Manufacturing, and Production}, 
title={A model for schema evolution in temporal relational databases}, 
year={1993}, 
volume={}, 
number={}, 
pages={223-231}, 
abstract={In temporal databases, transaction time and valid time are added to the data. In this context, the modification of the schema which a database management system (DBMS) can undergo during its lifetime, known as schema changes, must develop into the higher concept of scheme evolution. Changes to the schema produce versions of previous schemas along both time axes and these versions must be maintained and managed during the whole database life. An overview on the issues of temporal relational databases and relational schema changes is presented. A basic outline for the integration of these two aspects into a temporal relational database model supporting schema evolution is given.<<ETX>>}, 
keywords={relational databases;temporal databases;temporal relational databases;transaction time;valid time;relational schema;Relational databases;Transaction databases;Object oriented modeling;Object oriented databases;US Department of Transportation;Computer aided manufacturing;CADCAM;Computer integrated manufacturing;Taxonomy;Proposals}, 
doi={10.1109/CMPEUR.1993.289869}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4811266, 
author={Y. {Kinoshita} and }, 
booktitle={2008 IEEE International Conference on Systems, Man and Cybernetics}, 
title={A tour route planning support system with consideration of the preferences of group members}, 
year={2008}, 
volume={}, 
number={}, 
pages={150-155}, 
abstract={Consideration of the preferences of multiple group members is a fundamental requirement for tour route planning. This paper describes the implementation of a tour route planning support system to simplify the planning process. The system recommends combinations of tourist attractions for a group considering the preferences of group members. First, a Kansei database linking tourist attractions and their characteristics is constructed based on the results of Kansei evaluation experiments. Next, the system is implemented using an evolutionary algorithm with the Kansei database. After the implementation, performance of the system is verified using several test samples. The results demonstrate that our system has sufficient ability to propose appropriate and wide variety of tourist attractions.}, 
keywords={database management systems;evolutionary computation;travel industry;tour route planning support system;group members;tourist attractions;Kansei database;evolutionary algorithm;Databases;Process planning;Evolutionary computation;Recommender systems;Joining processes;Collaboration;Filtering;Computer science;System testing;Marketing and sales;tourism;evolutionary algorithm;Kansei engineering;decision support}, 
doi={10.1109/ICSMC.2008.4811266}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{4212333, 
author={H. {Nobuhara} and C. {Han}}, 
booktitle={2006 International Symposium on Intelligent Signal Processing and Communications}, 
title={Evolutionary Computation Schemes based on Max Plus Algebra and Their Application to Image Processing}, 
year={2006}, 
volume={}, 
number={}, 
pages={538-541}, 
abstract={A hybrid genetic algorithm based learning method for the morphological neural networks (MNN) is proposed. The morphological neural networks are based on max-plus algebra, therefore, it is difficult to optimize the coefficients of MNN by the learning method with derivative operations. In order to solve the difficulty, a hybrid genetic algorithm based learning method to optimize the coefficients of MNN is proposed. Through the image compression/reconstruction experiment using test images extracted from standard image database (SIDBA), it is confirmed that the quality of the reconstructed images obtained by the proposed learning method is better than that obtained by the conventional method.}, 
keywords={algebra;data compression;feature extraction;genetic algorithms;image coding;image reconstruction;learning (artificial intelligence);mathematical morphology;neural nets;evolutionary computation schemes;max plus algebra;image processing;hybrid genetic algorithm;learning method;morphological neural networks;MNN;image compression;image reconstruction;test image extraction;standard image database;SIDBA;Evolutionary computation;Algebra;Image processing;Learning systems;Multi-layer neural network;Genetic algorithms;Neural networks;Optimization methods;Image reconstruction;Image coding}, 
doi={10.1109/ISPACS.2006.364715}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{1609809, 
author={R. {Morien}}, 
booktitle={Agile Development Conference (ADC'05)}, 
title={Agile development of the database: a focal entity prototyping approach}, 
year={2005}, 
volume={}, 
number={}, 
pages={103-110}, 
abstract={The agile development of the database and the application system is a highly productive and successful activity when undertaken in a coherent and organized manner. Agility does not preclude structure and order in development. The more serial thinking that the entire database schema must be developed once and for all, and before any processing development can take place, is seen to be incorrect and unnecessary. The evolution of the database, like other aspects of evolutionary and agile development of software systems, contributes significantly to schema quality, correctness and adaptability. Research and experience in both commercial development and in an academic project environment over 20 years has demonstrated the reasonableness and efficacy of this approach, and this paper elaborates upon a well ordered and well-organized, but clearly agile approach, to database development, which the author has termed "focal entity prototyping".}, 
keywords={software engineering;database management systems;agile development;database development;focal entity prototyping;software systems;database evolution;data modelling;Prototypes;Object oriented modeling;Business;Object oriented databases;Deductive databases;Australia;Software systems;Data models;Protection;Iterative methods;Entity Modelling;Data Modelling;Prototyping;Agile Development}, 
doi={10.1109/ADC.2005.7}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6257845, 
author={J. {Klímek} and M. {Necaský}}, 
booktitle={2012 IEEE 19th International Conference on Web Services}, 
title={Formal Evolution of XML Schemas with Inheritance}, 
year={2012}, 
volume={}, 
number={}, 
pages={496-503}, 
abstract={Today, web services are widely used for data exchange. The format of individual messages exchanged among them is usually described with XML schemas in WSDL documents. It is a common practice that there is not only one but a whole family of formats each, for example, tailored for a specific consumer. In such environments, the design and maintenance of the web service interfaces and, in particular, the XML schemas describing the structure of messages is not a simple task. In our previous work we developed a method based on the principles of Model-Driven Development for evolution of a family of XML schemas. It automated a portion of design and maintenance tasks to be done when a change in user requirements or surrounding environment of the system influences more XML schemas in the family. We provided a formal model of possible evolution changes and their propagation mechanism. In this paper, we extend our method with inheritance modeling. We formally extend our conceptual model and we introduce new evolution changes and update the current ones so that they keep the model in a consistent state.}, 
keywords={XML;formal evolution;XML schemas;Web services;data exchange;individual messages;WSDL documents;inheritance modeling;XML;Unified modeling language;Strontium;Semantics;Abstracts;Context;Electronic mail;XML schema modeling;model driven architecture;XML schema evolution;propagation of changes;inheritance}, 
doi={10.1109/ICWS.2012.11}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{4424784, 
author={ and }, 
booktitle={2007 IEEE Congress on Evolutionary Computation}, 
title={Extraction of hybrid trace features with evolutionary computation for face recognition}, 
year={2007}, 
volume={}, 
number={}, 
pages={2493-2500}, 
abstract={The hybrid trace features (HTF), a new face representation, is proposed for face authentication system. Trace transforms of multiple Trace functionals are used to construct the HTF, and Genetic Algorithms is implemented as the data fusion tool. In addition, rotation based hybrid trace features (r-HTF) is also introduced as facial features. The systemic evaluations on Cambridge ORL face database reveal that HTF and r-HTF present high discriminatory power and outperform the features extracted by principal component analysis (PCA), linear discriminant analysis (LDA) and kernel PCA in the task of classification.}, 
keywords={face recognition;feature extraction;genetic algorithms;image classification;image representation;principal component analysis;sensor fusion;visual databases;hybrid trace feature extraction;evolutionary computation;face recognition;face representation;face authentication system;trace functionals;genetic algorithms;data fusion;face database;principal component analysis;linear discriminant analysis;image classification;Cambridge ORL face database;Evolutionary computation;Face recognition;Principal component analysis;Data mining;Linear discriminant analysis;Authentication;Genetic algorithms;Facial features;Spatial databases;Feature extraction}, 
doi={10.1109/CEC.2007.4424784}, 
ISSN={1089-778X}, 
month={Sep.},}
@ARTICLE{249660, 
author={M. L. {Jaccheri} and R. {Conradi}}, 
journal={IEEE Transactions on Software Engineering}, 
title={Techniques for process model evolution in EPOS}, 
year={1993}, 
volume={19}, 
number={12}, 
pages={1145-1156}, 
abstract={The authors categorize some aspects of software process evolution and customization, and describe how they are handled in the EPOS PM system. Comparisons are made to other PM systems. A process model in EPOS consists of a schema of classes and meta-classes, and its model entities and relationships. There is an underlying software engineering database, EPOSDB, offering uniform versioning of all model parts and a context of nested cooperating transactions. Then, there is a reflective object-oriented process specification language, on top of the EPOSDB. Policies for model creation, composition, change, instantiation, refinement, and enaction are explicitly represented and are used by a set of PM automatic tools. The main tools are a planner to instantiate tasks, an execution manager to enact such tasks, and a PM manager to define, analyze, customize, and evolve the process schema.<<ETX>>}, 
keywords={configuration management;object-oriented databases;object-oriented languages;object-oriented programming;programming environments;specification languages;process model evolution;schema;software process evolution;EPOS PM system;meta-classes;model entities;underlying software engineering database;EPOSDB;uniform versioning;nested cooperating transactions;reflective object-oriented process specification language;model creation;PM automatic tools;planner;execution manager;PM manager;process schema;process support environment;SPELL;Object oriented modeling;Production;Software tools;Humans;Software engineering;Software libraries;Life testing;Software testing;Object oriented databases;Transaction databases}, 
doi={10.1109/32.249660}, 
ISSN={0098-5589}, 
month={Dec},}
@INPROCEEDINGS{882379, 
author={K. {Kuramitsu} and K. {Sakamura}}, 
booktitle={Proceedings of the First International Conference on Web Information Systems Engineering}, 
title={Distributed object-oriented schema for XML-based electronic catalog sharing semantics among businesses}, 
year={2000}, 
volume={1}, 
number={}, 
pages={87-96 vol.1}, 
abstract={Internet commerce is increasing the demands of service integrations by sharing XML-based catalogs. We propose the PCO (Portable Compound Object) data model supporting semantic inheritance to ensure the synonymy of heterogeneous semantics among distributed schemas that different authors define independently. Also, the PCO model makes semantic relationships independent of an initial class hierarchy, and it enables rapid schema evolution across the entire Internet business. This preserves semantic interoperability without changing pre-defined classes. We have also encoded the PCO model into two XML-based languages: the PCO Specification Language (PSL) and the Portable Composite Language (PCL). This paper demonstrates that intermediaries defining service semantics in PSL can automatically integrate multiple suppliers' PCL catalogs for their agent-mediated services.}, 
keywords={electronic commerce;distributed object management;hypermedia markup languages;cataloguing;Internet;inheritance;semantic networks;open systems;specification languages;software portability;object-oriented languages;management information systems;electronic data interchange;distributed object-oriented schema;XML-based electronic catalog sharing semantics;Internet business;Internet commerce;PCO data model;portable compound objects;semantic inheritance;synonymous heterogeneous semantics;semantic relationships;class hierarchy;rapid schema evolution;semantic interoperability;predefined classes;XML-based languages;PCO Specification Language;PSL;Portable Composite Language;electronic commerce;intermediaries;service semantics;multi-supplier PCL catalogs;agent-mediated services;Internet service integration;Electronic catalog;Object oriented modeling;XML;Web and internet services;Business;Data models;Electronic commerce;Semantic Web;HTML;Information science}, 
doi={10.1109/WISE.2000.882379}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8345459, 
author={R. {Egawa} and K. {Komatsu} and H. {Takizawa}}, 
booktitle={2017 Fifth International Symposium on Computing and Networking (CANDAR)}, 
title={Designing an Open Database of System-Aware Code Optimizations}, 
year={2017}, 
volume={}, 
number={}, 
pages={369-374}, 
abstract={The rapid evolution of HPC systems brings us a high computational capability. However, it is getting harder to exploit the potential of an HPC system due to the increases in system complexity. This fact forces computational scientists to create complicated programming, which causes low productivity and maintainability. To overcome this situation, we are aggregating various optimization patterns and building a knowhow database of system-aware code optimizations called an HPC refactoring catalog. This paper discusses the design and implementation of an HPC refactoring catalog and shows a case study to demonstrate the practicality of the catalog.}, 
keywords={database management systems;parallel processing;public domain software;software maintenance;open database;system-aware code optimizations;HPC system;system complexity;optimization patterns;knowhow database;HPC refactoring catalog;Optimization;Kernel;Databases;Guidelines;Software algorithms;Complexity theory;Tuning}, 
doi={10.1109/CANDAR.2017.102}, 
ISSN={2379-1896}, 
month={Nov},}
@INPROCEEDINGS{7820351, 
author={C. {Ji} and S. {Liu} and C. {Yang} and L. {Cui} and L. {Pan} and L. {Wu} and Y. {Liu}}, 
booktitle={2016 IEEE 9th International Conference on Cloud Computing (CLOUD)}, 
title={A Self-Evolving Method of Data Model for Cloud-Based Machine Data Ingestion}, 
year={2016}, 
volume={}, 
number={}, 
pages={814-819}, 
abstract={In the case of a cloud-based remote control system such as SCADA (Supervisory Control and Data Acquisition) that enables users to collect data from cloud-connected machines deployed anywhere at any time. However, machine data models may not be updated in a timely manner after the devices are upgrades or modified. This leads to mismatches between the machine data and data models. A key obstacle of the matching is that the machines can be modified. To address this, we present a self-evolving method for machine data model. We give the description of the evolution of machine data models and the self-evolving method for the models in details. The method detects the conflicts between the machine data and models, and transfer or derive models if necessary. Our method can thus facilitate the evolution of machine data models and ensure that every machine in the cloud corresponds to the correct machine data model automatically. At last, we present two case studies to validate our method.}, 
keywords={cloud computing;data models;self-evolving method;cloud-based machine data ingestion;machine data model evolution;Data models;Cloud computing;Conferences;device cloud;machine data;self-evolving;data model}, 
doi={10.1109/CLOUD.2016.0114}, 
ISSN={2159-6190}, 
month={June},}
@INPROCEEDINGS{4812597, 
author={J. {Yan} and B. {Zhang}}, 
booktitle={2009 IEEE 25th International Conference on Data Engineering}, 
title={Support Multi-version Applications in SaaS via Progressive Schema Evolution}, 
year={2009}, 
volume={}, 
number={}, 
pages={1717-1724}, 
abstract={Update of applications in SaaS is expected to be a continuous efforts and cannot be done overnight or over the weekend. In such migration efforts, users are trained and shifted from a existing version to a new version successively. There is a long period of time when both versions of applications co-exist. Supporting two systems at the same time is not a cost efficient option and two systems may suffer from slow response time due to continuous synchronization between two systems. In this paper, we focus on how to enable progressive migration of multi-version applications in SaaS via evolving schema. Instead of maintain two systems, our solution is to maintain an intermediate schema that is optimized for mixed workloads for new and old applications. With a application migration schedule, an genetic algorithm is used to find out the more effective intermediated schema as well as migration paths and schedule. A key advantage of our approach is optimum performance during the long migration period while maintaining the same level of data movement required by the migration. We evaluated the proposed progressive migration approach on a TPCW workload and results validated its effectiveness of across a variety of scenarios; Experimental results demonstrate that our incremental migration proposed in this paper could bring about 200% performance gain as compared to the existing system.}, 
keywords={configuration management;genetic algorithms;scheduling;support multiversion applications;software as a service;progressive schema evolution;application migration scheduling;genetic algorithm;progressive migration approach;TPCW workload;Costs;Databases;Delay;Genetic algorithms;Statistical distributions;Application software;Software systems;Data engineering;Performance gain;Job shop scheduling}, 
doi={10.1109/ICDE.2009.167}, 
ISSN={1063-6382}, 
month={March},}
@INPROCEEDINGS{1260817, 
author={A. Y. {Halevy} and Z. G. {Ives} and D. {Suciu} and I. {Tatarinov}}, 
booktitle={Proceedings 19th International Conference on Data Engineering (Cat. No.03CH37405)}, 
title={Schema mediation in peer data management systems}, 
year={2003}, 
volume={}, 
number={}, 
pages={505-516}, 
abstract={Intuitively, data management and data integration tools should be well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: they typically require a comprehensive schema design before they can be used to store or share information, and they are difficult to extend because schema evolution is heavyweight and may break backwards compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by nondatabase-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: we propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peer's schemas. PDMSs represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peer's individual schemas. We consider the problem of schema mediation in a PDMS. Our first contribution is a flexible language for mediating between peer schemas, which extends known data integration formalisms to our more complex architecture. We precisely characterize the complexity of query answering for our language. Next, we describe a reformulation algorithm for our language that generalizes both global-as-view and local-as-view query answering algorithms. Finally, we describe several methods for optimizing the reformulation algorithm, and an initial set of experiments studying its performance.}, 
keywords={distributed databases;query formulation;query languages;query processing;data structures;schema mediation;peer data management system;data management tool;data integration tool;semantic information;schema design;semantic mapping;query answering complexity;query reformulation;query optimisation;Mediation;XML;Optimization methods;Database languages;Investments;Memory;Standardization;Terminology;Cost function;HTML}, 
doi={10.1109/ICDE.2003.1260817}, 
ISSN={}, 
month={March},}
@ARTICLE{404032, 
author={V. J. {Tsotras} and B. {Gopinath} and G. W. {Hart}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Efficient management of time-evolving databases}, 
year={1995}, 
volume={7}, 
number={4}, 
pages={591-608}, 
abstract={Efficiently managing the history of a time-evolving system is one of the central problems in many database environments, like database systems that incorporate versioning, or object-oriented databases that implicitly or explicitly maintain the history of persistent objects. In this paper we propose algorithms that reconstruct past states of an evolving system for two general cases, i.e., when the system's state is represented by a set or by a hierarchy (a forest of trees). Sets are widely used as a canonical form of representing information in databases or program states. For more complex applications, like schema evolution in object-oriented databases, it becomes necessary to manage the history of data structures that have the form of forests or even graphs. The proposed algorithms use minimal space (proportional to the number of changes occurring in the evolution) and have the advantage of being on-line (in the amortized sense). Any past system state s(t) is reconstructed in time O.<<ETX>>}, 
keywords={temporal databases;tree data structures;query processing;object-oriented databases;database theory;computational complexity;time-evolving database management;history;versioning;object-oriented databases;persistent objects;sets;schema evolution;data structures;forests;graphs;random access;temporal database;queries;Object oriented databases;History;Transaction databases;Tree graphs;Data structures;Object oriented modeling;Companies;Environmental management;Database systems;Programming profession}, 
doi={10.1109/69.404032}, 
ISSN={1041-4347}, 
month={Aug},}
@INPROCEEDINGS{7969553, 
author={I. {Triguero} and M. {Galar} and H. {Bustince} and F. {Herrera}}, 
booktitle={2017 IEEE Congress on Evolutionary Computation (CEC)}, 
title={A first attempt on global evolutionary undersampling for imbalanced big data}, 
year={2017}, 
volume={}, 
number={}, 
pages={2054-2061}, 
abstract={The design of efficient big data learning models has become a common need in a great number of applications. The massive amounts of available data may hinder the use of traditional data mining techniques, especially when evolutionary algorithms are involved as a key step. Existing solutions typically follow a divide-and-conquer approach in which the data is split into several chunks that are addressed individually. Next, the partial knowledge acquired from every slice of data is aggregated in multiple ways to solve the entire problem. However, these approaches are missing a global view of the data as a whole, which may result in less accurate models. In this work we carry out a first attempt on the design of a global evolutionary undersampling model for imbalanced classification problems. These are characterised by having a highly skewed distribution of classes in which evolutionary models are being used to balance the dataset by selecting only the most relevant data. Using Apache Spark as big data technology, we have introduced a number of variations to the well-known CHC algorithm to work with very large chromosomes and reduce the costs associated to the fitness evaluation. We discuss some preliminary results, showing the great potential of this new kind of evolutionary big data model.}, 
keywords={Big Data;evolutionary computation;learning (artificial intelligence);pattern classification;Big Data learning models;global evolutionary undersampling model;imbalanced classification;highly skewed distribution;Apache Spark;CHC algorithm;very large chromosomes;fitness evaluation;Big Data;Sparks;Biological cells;Data models;Machine learning algorithms;Context;Data mining}, 
doi={10.1109/CEC.2017.7969553}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1251414, 
author={R. M. {Duwairi}}, 
booktitle={Proceedings Fifth IEEE Workshop on Mobile Computing Systems and Applications}, 
title={A framework for generating and maintaining global schemas in heterogeneous multidatabase systems}, 
year={2003}, 
volume={}, 
number={}, 
pages={200-207}, 
abstract={The problem of creating a global schema over a set of heterogeneous databases is important due to the availability of multiple databases within organizations. The global schema should provide a unified representation of local heterogeneous schemas. In this paper, we provide a general framework that supports the integration of local schemas into a global one. The framework takes into consideration the fact that local schemas are autonomous and may evolve over time, which makes the definition of the global schema obsolete. We define a set of integration operators that integrates local schemas based on the semantic relevance of their classes, and provide a model-independent representation of virtual classes of the global schema. We also define a set of modifications that can be applied to local schemas as a consequence of their local autonomy. For every local modification, we define a propagation rule that will automatically disseminate the effects of that modification to the global schema without having to regenerate it from scratch via integration.}, 
keywords={object-oriented databases;distributed databases;multidatabase system;database autonomy;schema evolution;query language;heterogeneous databases;local databases;DBMS;object-oriented database;global schemas;local heterogeneous schemas;local schemas;integration operators;local schema integration;model-independent representation;global schema virtual class;Data models;Computer science;Information systems;Database languages;Merging}, 
doi={10.1109/IRI.2003.1251414}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6909169, 
author={A. {Gosain} and }, 
booktitle={International Conference on Recent Advances and Innovations in Engineering (ICRAIE-2014)}, 
title={Query maintenance with hierarchical updates}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Data warehouse must evolve with changes in the schema, software components, user requirements or organization's business rules. Various authors have proposed different schema evolution operators for various level updates but none of them handled the syntactic and semantic adaptation of queries automatically. Few authors discussed this problem considering few changes in data sources schema and data warehouse structure schema. But they did not deal with the problem due to changes in the hierarchy and its levels in a dimension of the schema. In this paper, we perform what-if analysis for change in the hierarchy and its levels in the data warehouse schema. Queries, views and relations are modeled in a graph annotated with policies to manage the change event. Framework detects the affected part of graph due to change in hierarchical levels and readjusts according to the specified action without making the queries invalid.}, 
keywords={data warehouses;graph theory;query processing;query maintenance;hierarchical level update;graph modeling approach;schema evolution operators;data source schema;data warehouse structure schema;what-if analysis;view modelling;relation modelling;query modelling;graph annotation;change event management;structure software components;user requirements;organization business rules;automatic syntactic-semantic query adaptation;Manuals;data warehouse evolution;graph modeling;hierarchical update;policies}, 
doi={10.1109/ICRAIE.2014.6909169}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6200675, 
author={W. {Li} and J. {Wu}}, 
booktitle={2012 International Conference on Communication Systems and Network Technologies}, 
title={Multiple Evolutions of Business Process in Dynamic Environment}, 
year={2012}, 
volume={}, 
number={}, 
pages={468-471}, 
abstract={The process-aware information systems run in dynamic environments, so it must support changes at runtime. The changes of process model may result in process instances migrating to the modified process model, which is prone to introduce executing conflicts in the migrated process instances. It is difficult to determine the migrating condition of different process instances due to multi-paths existing in the process model. In this paper, on the one hand an efficient approach based on a novel correctness criterion for process evolution is proposed, which can determine the migrating strategies of process instances with diverse of routing structures and running states automatically, and ensure the most migrated process instances executing correctly without rollback tasks in them. It is based on the concept of point of change, and an algorism to find out these points is developed. On the other hand, the problem of how to migrate a process instance among many variants of process schemas is discussed. The algorism of process evolution based on the correctness criterion is also developed. The results in this paper are suitable for realizing an adaptable process middleware.}, 
keywords={business data processing;evolutionary computation;middleware;multiple evolutions;business process model;dynamic environment;process-aware information system;migrated process instance;migrating condition;correctness criterion;process evolution;process schema;adaptable process middleware;Bismuth;Business;Process control;Solid modeling;Routing;Semantics;Information systems;business process;dynamic change;process instance migration;correctness criteria;multiple evolution}, 
doi={10.1109/CSNT.2012.216}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{1214941, 
author={ and J. {Mylopoulos}}, 
booktitle={Seventh International Database Engineering and Applications Symposium, 2003. Proceedings.}, 
title={Automated EJB client code generation using database query rewriting}, 
year={2003}, 
volume={}, 
number={}, 
pages={308-317}, 
abstract={Enterprise JavaBean (hereafter EJB) technology has been widely adopted in software industry to develop Web information systems. However, most of EJB applications are reengineered from legacy database applications. This means that legacy SQL statements need to be translated into EJB client code. Since many methods in Enterprise Beans can be regarded as view definitions of the underlying database, the EJB client code generation can be mapped to the problem of query rewriting using views. This paper addresses the automatic generation of EJB client code using query rewriting.}, 
keywords={Java;query processing;rewriting systems;program compilers;language translation;object-oriented programming;software engineering;automatic EJB client code generation;database query rewriting;enterprise JavaBean;EJB technology;software industry;Web information system development;EJB application reengineering;legacy database application;legacy SQL statement translation;Enterprise JavaBeans;object-relational mapping;software engineering;information technology;legacy presentation code translation;Application software;Relational databases;Java;Computer science;Business;Computer industry;Information systems;Information technology;Industrial relations;Software engineering}, 
doi={10.1109/IDEAS.2003.1214941}, 
ISSN={1098-8068}, 
month={July},}
@INPROCEEDINGS{5767629, 
author={G. {Papastefanatos} and P. {Vassiliadis} and A. {Simitsis}}, 
booktitle={2011 IEEE 27th International Conference on Data Engineering Workshops}, 
title={Propagating evolution events in data-centric software artifacts}, 
year={2011}, 
volume={}, 
number={}, 
pages={162-167}, 
abstract={The success and wellbeing of large organizations rely on the smooth functionality and operability of their software. Such qualities are largely affected by evolution events and changes, like software upgrades. In this paper, we are dealing with handling evolution events in data management systems. We consider a data-centric ecosystem that captures relational tables, views and queries (the latter are seeing as software modules that are either internal to the database, e.g., stored procedures, or external software applications that access the database). We also consider policies dictating the response of a software module to a possible event. We investigate the impact of such events to the database and present a graph-based mechanism to control event propagation. We show that our mechanism terminates and that every database construct is annotated with a single status, regardless of the sequence of messages that the node receives.}, 
keywords={database management systems;software engineering;data centric software artifact;software upgrade;data management system;relational table;graph based mechanism;Semantics;Databases;Software;Ecosystems;Protocols;Biological system modeling;Computer architecture;database evolution;confluence;event propagation}, 
doi={10.1109/ICDEW.2011.5767629}, 
ISSN={}, 
month={April},}
@ARTICLE{250080, 
author={J. P. {Yoon} and L. {Kerschberg}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A framework for knowledge discovery and evolution in databases}, 
year={1993}, 
volume={5}, 
number={6}, 
pages={973-979}, 
abstract={A concept for knowledge discovery and evolution in databases is described. The key issues include: using a database query to discover new rules; using not only positive examples (answer to a query), but also negative examples to discover new rules; and harmonizing existing rules with the new rules. A tool for characterizing the exceptions in databases and evolving knowledge as a database evolves is developed.<<ETX>>}, 
keywords={database theory;deductive databases;query processing;relational databases;knowledge discovery;knowledge evolution;databases;database query;rules;exceptions;deductive database;relational database;Spatial databases;Object oriented databases;Relational databases;Artificial intelligence;Abstracts;Information retrieval;Information technology}, 
doi={10.1109/69.250080}, 
ISSN={1041-4347}, 
month={Dec},}
@INPROCEEDINGS{5336108, 
author={A. C. d. {Almeida} and G. {Boff} and J. L. d. {Oliveira}}, 
booktitle={2009 XXIII Brazilian Symposium on Software Engineering}, 
title={A Framework for Modeling, Building and Maintaining Enterprise Information Systems Software}, 
year={2009}, 
volume={}, 
number={}, 
pages={115-125}, 
abstract={An Enterprise Information System (EIS) software has three main aspects: data, which are processed to generate business information; application functions, which transform data into information; and business rules, which control and restrict the manipulation of data by functions. Traditional approaches to EIS software development consider data and application functions. Rules are second class citizens, embedded on the specification of either data (as database integrity constraints) or on the EIS functions (as a part of the application software). This work presents a new, integrated approach for the development and maintenance of EIS software. The main ideas are to focus on the conceptual modeling of the three aspects of the EIS software - application functions, business rules, and database schema - and to automatically generate code for each of these software aspects. This improves software quality, reducing redundancies by centralizing EIS definitions on a single conceptual model. Due to automatic generation of code, this approach increases the software engineering staff productivity, making it possible to respond to the continuous changes in the business domain.}, 
keywords={information systems;software development management;software maintenance;enterprise information systems software;business information generation;application functions;business rules;EIS software development;EIS software maintenance;conceptual modeling;database schema;Information systems;Software maintenance;Software systems;Application software;Databases;Control systems;Programming;Software quality;Software engineering;Productivity;Information Systems;Business Rules;Systems Maintenance;Model Driven Development;Schema Generation;Schema Evolution}, 
doi={10.1109/SBES.2009.24}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{217296, 
author={T. L. {Nixon} and K. {Lubenow} and J. {Srinivasan} and P. {Dewan} and B. {Bhargava}}, 
booktitle={Proceedings of the Second International Conference on Systems Integration}, 
title={Building a user-interface for the O-Raid database system using the Suite system}, 
year={1992}, 
volume={}, 
number={}, 
pages={248-257}, 
abstract={O-UI, the graphic interface to the O-Raid distributed object-oriented database system, supports access and direct manipulation of relations and objects. Since O-Raid supports a hybrid object-relation data model, an evolutionary approach to design was required. The use of Suite user interface construction tool led to rapid prototyping of the interface, which allowed the authors to experiment with the interface itself, leading to modifications and refinements to the initial design. They describe how O-UI was engineered using Suite, present their experiences, and discuss the related software integration issues.<<ETX>>}, 
keywords={distributed databases;graphical user interfaces;object-oriented databases;software tools;user interface management systems;graphic interface;O-Raid distributed object-oriented database system;direct manipulation;hybrid object-relation data model;evolutionary approach;Suite user interface construction tool;rapid prototyping;O-UI;software integration issues;Database systems;Displays;Data models;Buildings;User interfaces;Software prototyping;Application software;Operating systems;Relational databases;Computer interfaces}, 
doi={10.1109/ICSI.1992.217296}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1452432, 
author={H. {Ishibuchi} and Y. {Nojima}}, 
booktitle={The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.}, 
title={Comparison between Fuzzy and Interval Partitions in Evolutionary Multiobjective Design of Rule-Based Classification Systems}, 
year={2005}, 
volume={}, 
number={}, 
pages={430-435}, 
abstract={This paper compares fuzzy rules with interval rules through computational experiments on benchmark data sets from the UCI database using an evolutionary multiobjective rule selection method. In the design of fuzzy and interval rule-based systems for classification problems, we use three types of partitions: homogeneous fuzzy partitions, inhomogeneous entropy-based interval partitions, and inhomogeneous fuzzy partitions derived from the interval partitions. A large number of rule-based systems are designed from each type of partitions using our evolutionary multiobjective rule selection method with three objectives: to maximize the number of correctly classified training patterns, to minimize the number of rules, and to minimize the total number of antecedent conditions. Experimental results show that the fuzzification of interval rules improves their generalization ability for many data sets}, 
keywords={entropy;evolutionary computation;fuzzy set theory;fuzzy systems;generalisation (artificial intelligence);knowledge based systems;learning (artificial intelligence);pattern classification;evolutionary multiobjective design;rule-based classification systems;fuzzy rules;UCI database;evolutionary multiobjective rule selection method;fuzzy rule-based systems;interval rule-based systems;homogeneous fuzzy partitions;inhomogeneous entropy-based interval partitions;inhomogeneous fuzzy partitions;pattern classification;antecedent conditions;interval rule fuzzification;generalization;Fuzzy systems;Knowledge based systems;Fuzzy sets;Data mining;Neural networks;Genetic algorithms;Data engineering;Design engineering;Databases;Design methodology}, 
doi={10.1109/FUZZY.2005.1452432}, 
ISSN={1098-7584}, 
month={May},}
@INPROCEEDINGS{916370, 
author={L. {Al-Jadir}}, 
booktitle={Proceedings Seventh International Conference on Database Systems for Advanced Applications. DASFAA 2001}, 
title={Encapsulating classification in an OODBMS for data mining applications}, 
year={2001}, 
volume={}, 
number={}, 
pages={100-106}, 
abstract={Classification is an important task in data mining. Encapsulating classification in an object-oriented database system requires additional features: we propose multiobjects and schema evolution. Our approach allows us to store classification functions, and to store instances of each group in order to retrieve them later. Since the database is operational, it allows us also to perform dynamic classification, i.e. add/remove instances to/from groups over time. Moreover it allows us to update classification functions (if we choose another population sample or apply another classifier) and have the instances of groups consequently reclassified. We illustrate our approach with a target mailing application.}, 
keywords={data mining;object-oriented databases;marketing data processing;pattern classification;data mining;classification encapsulation;object-oriented database system;multiobjects;schema evolution;classification function storage;dynamic classification;classification function updating;target mailing application;Data mining;Information retrieval;History;Application software;Mathematics;Computer science;Database systems;Object oriented databases;Spatial databases;Medical diagnosis}, 
doi={10.1109/DASFAA.2001.916370}, 
ISSN={}, 
month={April},}
@ARTICLE{5390585, 
author={W. C. {McGee}}, 
journal={IBM Journal of Research and Development}, 
title={Data Base Technology}, 
year={1981}, 
volume={25}, 
number={5}, 
pages={505-519}, 
abstract={The evolution of data base technology over the past twenty-five years is surveyed, and major IBM contributions to this technology are identified and briefly described.}, 
keywords={}, 
doi={10.1147/rd.255.0505}, 
ISSN={0018-8646}, 
month={Sep.},}
@INPROCEEDINGS{5642219, 
author={R. {El Hamdi} and M. {Njah} and M. {Chtourou}}, 
booktitle={2010 IEEE International Conference on Systems, Man and Cybernetics}, 
title={An evolutionary neuro-fuzzy approach to breast cancer diagnosis}, 
year={2010}, 
volume={}, 
number={}, 
pages={142-146}, 
abstract={The important role that mammography is playing in breast cancer detection can be attributed largely to the technical improvements and dedication of radiologists to breast imaging. A lot of work is being done to ensure that these diagnosing steps are becoming smoother, faster and more accurate in classifying whether the abnormalities seen in mammogram images are benign or malignant. In this paper, an evolutionary approach for design of TSK-type fuzzy model (TFM) is proposed to solve the breast cancer diagnosis problem. In the proposed method, both the number of fuzzy rules and adjustable parameters in the TFM are designed concurrently combining the compact genetic algorithm (CGA) and the steady-state genetic algorithm (SSGA). The computational experiments show that the presented approach can obtain better generalization than some existing methods reported recently in the literature using the widely accepted Wisconsin breast cancer diagnosis (WBCD) database.}, 
keywords={cancer;genetic algorithms;mammography;medical image processing;patient diagnosis;evolutionary neuro-fuzzy approach;mammography;breast cancer detection;breast imaging;TSK-type fuzzy model;compact genetic algorithm;steady-state genetic algorithm;Wisconsin breast cancer diagnosis database;Breast;Niobium;Databases;Cancer;Breast Cancer Diagnosis;WBCD Database;TFM;Evolutionary Learning;CGA;SSGA}, 
doi={10.1109/ICSMC.2010.5642219}, 
ISSN={1062-922X}, 
month={Oct},}
@ARTICLE{542026, 
author={F. N. {Kesim} and M. {Sergot}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A logic programming framework for modeling temporal objects}, 
year={1996}, 
volume={8}, 
number={5}, 
pages={724-741}, 
abstract={We present a general approach for modeling temporal aspects of objects in a logic programming framework. Change is formulated in the context of a database which stores explicitly a record of all changes that have occurred to objects and thus (implicitly) all states of objects in the database. A snapshot of the database at any given time is an object-oriented database, in the sense that it supports an object-based data model. An object is viewed as a collection of simple atomic formulas, with support for an explicit notion of object identity, classes and inheritance. The event calculus is a treatment of time and change in first-order classical logic augmented with negation as failure. The paper develops a variant of the event calculus for representing changes to objects, including change in internal state of objects, creation and deletion of objects, and mutation of objects over time. The concluding sections present two natural and straightforward extensions, to deal with versioning of objects and schema evolution, and a sketch of implementation strategies for practical application to temporal object-oriented databases.}, 
keywords={temporal databases;temporal logic;logic programming;database theory;object-oriented databases;data structures;inheritance;deductive databases;logic programming framework;temporal object modeling;object-based data model;simple atomic formulas;object identity;object classes;inheritance;event calculus;change;time;first-order classical logic;negation as failure;object deletion;object creation;object mutation;object versioning;schema evolution;temporal object-oriented databases;temporal logic;deductive database;Logic programming;Object oriented databases;Object oriented modeling;Calculus;Deductive databases;Spatial databases;Relational databases;Computer Society;Data models;Genetic mutations}, 
doi={10.1109/69.542026}, 
ISSN={1041-4347}, 
month={Oct},}
@INPROCEEDINGS{6113896, 
author={H. {Assoudi} and H. {Lounis}}, 
booktitle={2011 IEEE 13th International Symposium on High-Assurance Systems Engineering}, 
title={Self-Healing Data Exchange Process under Evolving Schemas: A New Mapping Adaptation Approach Based on Self-Optimization}, 
year={2011}, 
volume={}, 
number={}, 
pages={188-190}, 
abstract={Today, more than ever, enterprises are relying on highly complex IT solutions to respond flexibly and rapidly to the constant changing business environment. Yet, the increasing complexity of IT solutions presents significant challenges. In this paper, we propose a solution to reduce the human intervention needed to maintain data exchange processes after a schema evolution (changes impacting source or target system schemas participating in a data exchange scenario). Our approach, toward reliable self-healed data exchange processes under evolving schemas, is called DEAM (Data Exchange Autonomic Manager).}, 
keywords={business data processing;electronic data interchange;fault tolerant computing;management of change;optimisation;self-healing data exchange process;mapping adaptation approach;self-optimization;complex IT solution;constant changing business environment;target system schema evolution;DEAM;data exchange autonomic manager;Humans;Degradation;Availability;Accuracy;Machine learning;Real time systems;data exchange;autonomic computing;selfmanaged systems;dependability;fault tolerance;sufficient correctness;schema matching;schema mapping;mapping adaptation;machine learning}, 
doi={10.1109/HASE.2011.42}, 
ISSN={1530-2059}, 
month={Nov},}
@ARTICLE{4510842, 
author={G. {Li} and J. F. {Wang} and K. H. {Lee} and K. {Leung}}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={Instruction-Matrix-Based Genetic Programming}, 
year={2008}, 
volume={38}, 
number={4}, 
pages={1036-1049}, 
abstract={In genetic programming (GP), evolving tree nodes separately would reduce the huge solution space. However, tree nodes are highly interdependent with respect to their fitness. In this paper, we propose a new GP framework, namely, instruction-matrix (IM)-based GP (IMGP), to handle their interactions. IMGP maintains an IM to evolve tree nodes and subtrees separately. IMGP extracts program trees from an IM and updates the IM with the information of the extracted program trees. As the IM actually keeps most of the information of the schemata of GP and evolves the schemata directly, IMGP is effective and efficient. Our experimental results on benchmark problems have verified that IMGP is not only better than those of canonical GP in terms of the qualities of the solutions and the number of program evaluations, but they are also better than some of the related GP algorithms. IMGP can also be used to evolve programs for classification problems. The classifiers obtained have higher classification accuracies than four other GP classification algorithms on four benchmark classification problems. The testing errors are also comparable to or better than those obtained with well-known classifiers. Furthermore, an extended version, called condition matrix for rule learning, has been used successfully to handle multiclass classification problems.}, 
keywords={feature extraction;genetic algorithms;learning (artificial intelligence);matrix algebra;pattern classification;trees (mathematics);instruction-matrix-based genetic programming;genetic programming;tree nodes;program trees;benchmark classification problems;rule learning;condition matrix;multiclass classification problems;Genetic programming;Data mining;Senior members;Classification algorithms;Benchmark testing;Evolutionary computation;Humans;Councils;Computer science;Classification;condition matrix for rule learning (CMRL);genetic programming (GP);instruction-matrix-based genetic programming (IMGP);schema evolution;Classification;condition matrix for rule learning (CMRL);genetic programming (GP);instruction-matrix-based genetic programming (IMGP);schema evolution;Algorithms;Artificial Intelligence;Computer Simulation;Feedback;Models, Genetic;Models, Theoretical;Pattern Recognition, Automated;Programming, Linear;Systems Theory}, 
doi={10.1109/TSMCB.2008.922054}, 
ISSN={1083-4419}, 
month={Aug},}
@INPROCEEDINGS{533986, 
author={N. {Stanger}}, 
booktitle={Proceedings 1996 International Conference Software Engineering: Education and Practice}, 
title={A language abstraction layer for relational DBMS}, 
year={1996}, 
volume={}, 
number={}, 
pages={93-96}, 
abstract={Current database definition techniques tend to be either purely language based or purely graphical. There is little or no integration between the two. This places artificial limits on the environment that developers must work in; ideally, they should be able to switch paradigms as the need arises. This lack of integration also causes problems with database reengineering. The paper describes an architecture for a relational abstraction layer, which isolates the front end "dialect" used from the underlying relational implementation. This allows tighter integration between different database definition techniques.}, 
keywords={relational databases;systems re-engineering;relational algebra;database languages;language abstraction layer;relational DBMS;database definition techniques;database reengineering;relational abstraction layer;front end dialect;relational implementation;Object oriented modeling;Relational databases;Natural languages;Encoding;Object oriented databases;Spatial databases}, 
doi={10.1109/SEEP.1996.533986}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7958577, 
author={B. {Fuller} and M. {Varia} and A. {Yerukhimovich} and E. {Shen} and A. {Hamlin} and V. {Gadepally} and R. {Shay} and J. D. {Mitchell} and R. K. {Cunningham}}, 
booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
title={SoK: Cryptographically Protected Database Search}, 
year={2017}, 
volume={}, 
number={}, 
pages={172-191}, 
abstract={Protected database search systems cryptographically isolate the roles of reading from, writing to, and administering the database. This separation limits unnecessary administrator access and protects data in the case of system breaches. Since protected search was introduced in 2000, the area has grown rapidly, systems are offered by academia, start-ups, and established companies. However, there is no best protected search system or set of techniques. Design of such systems is a balancing act between security, functionality, performance, and usability. This challenge is made more difficult by ongoing database specialization, as some users will want the functionality of SQL, NoSQL, or NewSQL databases. This database evolution will continue, and the protected search community should be able to quickly provide functionality consistent with newly invented databases. At the same time, the community must accurately and clearly characterize the tradeoffs between different approaches. To address these challenges, we provide the following contributions:(1) An identification of the important primitive operations across database paradigms. We find there are a small number of base operations that can be used and combined to support a large number of database paradigms.(2) An evaluation of the current state of protected search systems in implementing these base operations. This evaluation describes the main approaches and tradeoffs for each base operation. Furthermore, it puts protected search in the context of unprotected search, identifying key gaps in functionality.(3) An analysis of attacks against protected search for different base queries.(4) A roadmap and tools for transforming a protected search system into a protected database, including an open-source performance evaluation platform and initial user opinions of protected search.}, 
keywords={cryptography;data protection;information retrieval;NoSQL databases;public domain software;search problems;cryptographical protected database search;data access;data protection;NoSQL databases;NewSQL databases;open-source performance evaluation;searchable symmetric encryption;Cryptography;Arrays;Servers;Algebra;Database systems;searchable symmetric encryption;property preserving encryption;database search;oblivious random access memory;private information retrieval}, 
doi={10.1109/SP.2017.10}, 
ISSN={2375-1207}, 
month={May},}
@INPROCEEDINGS{994724, 
author={M. {Nakamura} and R. {Elmasri}}, 
booktitle={Proceedings 18th International Conference on Data Engineering}, 
title={Using Smodels (declarative logic programming) to verify correctness of certain active rules}, 
year={2002}, 
volume={}, 
number={}, 
pages={270-}, 
abstract={In this paper we show that the language of declarative logic programming (DLP) with answer sets and its extensions can be used to specify database evolution due to updates and active rules, and to verify correctness of active rules with respect to a specification described using temporal logic and aggregate operators. We classify the specification of active rules into four kind of constraints which can be expressed using a particular extension of DLP called Smodels. Smodels allows us to specify the evolution, to specify the constraints, and to enumerate all possible initial database states and initial updates. Together, these can be used to analyze all possible evolution paths of an active database system to verify if they satisfy a set of given constraints.}, 
keywords={logic programming;active databases;declarative logic programming;answer sets;database evolution;active rules;correctness verification;specification;temporal logic;aggregate operators;constraints;Smodels;initial database states;initial updates;Logic programming;Database systems;Data engineering;Design methodology;Engines;Computer science;Aggregates;Impedance;Prototypes;Specification languages}, 
doi={10.1109/ICDE.2002.994724}, 
ISSN={1063-6382}, 
month={Feb},}
@INPROCEEDINGS{4577987, 
author={P. {Chias} and T. {Abad}}, 
booktitle={2008 12th International Conference Information Visualisation}, 
title={Visualising Ancient Maps as Cultural Heritage: A Relational Database of the Spanish Ancient Cartography}, 
year={2008}, 
volume={}, 
number={}, 
pages={453-457}, 
abstract={The analysis of the historical evolution of the territories and landscapes has been seldom based upon the study of old cartographic documents; they have been always set in a second place after texts and writings. Trying to bridge this gap, we have designed and implemented a relational database of the ancient maps and charts that are already preserved in the main Spanish collections, archives and libraries, that includes the possibility to access to a a high resolution digital image of each one of them. The personalized queries to the database will allow the scholars and searchers to visualise the document together with its main features.}, 
keywords={cartography;computer vision;data visualisation;image resolution;relational databases;ancient map visualization;cultural heritage;Spanish ancient cartography;relational database;historical evolution analysis;personalized queries;Visualisation;Cultural Heritage;Databases;Multiformat}, 
doi={10.1109/IV.2008.46}, 
ISSN={1550-6037}, 
month={July},}
@INPROCEEDINGS{5768246, 
author={G. {Zhang} and Y. {Du} and G. {Li}}, 
booktitle={2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
title={Evaluation system of education quality in the evolution of information technology}, 
year={2011}, 
volume={}, 
number={}, 
pages={4064-4067}, 
abstract={This paper proposes a database based evaluation and management system. We takes the viewpoint of classroom teaching and information system in the research. The purpose of our efforts is based on the concept to improve the quality of education according to the new requirements set by the National Physical Education Curriculum Standard. The questionnaire surveys in this paper are imposed in the group of approximately 100 teachers, 300 students who entered the Wuhan Institute of Physical Education in 2008 and 2009. In order to cover the required qualities of the educators in Physical Education, this paper investigates the necessary quality as a physical educator, and proposes twenty two parameters to evaluate the educator in all perspectives.}, 
keywords={educational administrative data processing;social aspects of automation;teaching;education quality evaluation system;information technology evolution;database based evaluation;database based management system;classroom teaching;National Physical Education Curriculum Standard;Wuhan Institute of Physical Education;Ethics;Information technology;Educational institutions;Multimedia communication;Interviews;Art;Information system;educational technologies}, 
doi={10.1109/CECNET.2011.5768246}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6270579, 
author={Z. {Liu} and C. {Hu} and Y. {Li} and J. {Hu}}, 
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum}, 
title={DSDC: A Domain Scientific Data Cloud Based on Virtual Dataspaces}, 
year={2012}, 
volume={}, 
number={}, 
pages={2176-2182}, 
abstract={New challenges for large-scale data management and sharing have arisen due to the information evolution from massive data to "big data". Domain Scientific Data Cloud (DSDC) based on virtual data space is proposed in this paper to solve the problems of "big data" management, reusability and service in scientific field. The application of DSDC (MSDC) is also introduced, which is used in the field of materials science. Virtual data space is then defined. In order to build the data space, an ontology and logical resources model is discussed. A new mapping and evolutionary model is described as a "pay-as-you-go" fashion virtual data space, which introduces a "partial schema, semi-automatically evolution" method. A scientific data cloud application model is also proposed for defining service templates. Finally, the DSDC is verified by material service safety evaluation, the results show that DSDC is efficient in scientific data management and suitable for data-intensive application.}, 
keywords={cloud computing;data handling;evolutionary computation;materials science computing;ontologies (artificial intelligence);resource allocation;virtual storage;DSDC;domain scientific data cloud;large-scale data management;data sharing;information evolution;massive data;big data management;big data reusability;scientific field;materials science;ontology;logical resource model;mapping model;evolutionary model;pay-as-you-go fashion virtual data space;partial schema semiautomatic evolution;method;scientific data cloud application model;service template;material service safety evaluation;data-intensive application;Materials;Ontologies;Distributed databases;Semantics;Data models;Information management;Data handling;scientific data cloud;virtual dataspaces;semantic integration;ontology;pay-as-you-go}, 
doi={10.1109/IPDPSW.2012.269}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{558337, 
author={J. {Samos} and J. {Sistac}}, 
booktitle={Proceedings of 7th International Conference and Workshop on Database and Expert Systems Applications: DEXA 96}, 
title={Definition of deductive conceptual models of OODBs}, 
year={1996}, 
volume={}, 
number={}, 
pages={313-318}, 
abstract={The definition of deductive conceptual models (DCMs) using Prolog in order to specify different aspects of OODBs is proposed. The result of the specification process using this technique is an executable prototype of the system. Having a prototype directly available, along with the system specifications, is particularly useful in order to define additional elements in the context of OODBs (e.g. schema evolution, definition of derived classes definition of external schemas). The use of this technique is proposed mainly due to the difficulty of building prototypes of the mentioned elements over commercial OODBs. A brief example of a practical application of this technique-the specification of a conceptual schema definition system and its associated data model-is presented.}, 
keywords={object-oriented databases;OODB;deductive conceptual models;Prolog;specification;executable prototype;data model;conceptual schema definition system;object-oriented database systems;Prototypes;Proposals;Data models;Databases;Informatics;Buildings;Information systems;Formal specifications;Logic}, 
doi={10.1109/DEXA.1996.558337}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5620498, 
author={H. {Xie} and J. {Wei} and N. {Ma}}, 
booktitle={2010 International Conference on Computer Application and System Modeling (ICCASM 2010)}, 
title={Design of the WEB-based efficient and standard teaching management information system}, 
year={2010}, 
volume={7}, 
number={}, 
pages={V7-64-V7-68}, 
abstract={Teaching management information system is a very important tool and basis of the teaching management work and has been widely used in the educational institutions. According to the practice and the requirement of the management reform, an efficient and standard teaching management information system is needed which can be used by multi-users safely through the network, and the data must be complete and accurate. It should also have standardized database structure and support data sharing. The B/S model can be better used to realize it. In the paper, we have depicted the core E-R diagram and the data flow chart clearly, which can by used to guide the system development.}, 
keywords={computer aided instruction;educational institutions;Internet;management information systems;teaching;Web-based efficient;educational institutions;standard teaching management information system;standardized database structure;B/S model;Educational institutions;WEB-based;Efficient;Standard;Teaching Management;Management Information System;Business Process Reengineering;Database}, 
doi={10.1109/ICCASM.2010.5620498}, 
ISSN={2161-9069}, 
month={Oct},}
@INPROCEEDINGS{6137470, 
author={Y. {Chen} and C. {Wu} and S. {Lee}}, 
booktitle={2011 IEEE 11th International Conference on Data Mining Workshops}, 
title={Incremental Maintenance of Topological Patterns in Spatial-Temporal Database}, 
year={2011}, 
volume={}, 
number={}, 
pages={853-860}, 
abstract={Spatial temporal mining is an important research area with many interesting topics. Most spatial temporal databases are updating incrementally with time. Some discovered topological patterns may be invalidated and some new topological patterns may be introduced by the evolution of databases. However, the existing static algorithms are usually inefficient and not feasible to maintain topological patterns in an incremental environment. In this paper, we develop an efficient algorithm, Inc_TMiner (Incremental Topology Miner) to incrementally maintain topological patterns in spatial-temporal databases. The experimental results indicate that Inc_TMiner significantly outperforms state-of-the-art algorithms in execution time and possesses graceful scalability.}, 
keywords={data mining;temporal databases;topology;visual databases;spatial temporal database;topological patterns;incremental maintenance;database evolution;Inc_TMiner;Spatial databases;Data mining;Algorithm design and analysis;Meteorology;Maintenance engineering;Topology;spatial-temporal database;incremental mining;topological pattern;collocation pattern}, 
doi={10.1109/ICDMW.2011.99}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{5609679, 
author={K. {Haller}}, 
booktitle={2010 IEEE International Conference on Software Maintenance}, 
title={On the implementation and correctness of information system upgrades}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Information systems are applications incorporating a database for storing and processing data. Upgrading information systems requires updating the application logic, modifying the database schema, and adopting the data accordingly. Previous research focuses either on schema evolution or on application logic updates. In this paper, we take a holistic approach by addressing the combination. First, we elaborate the three main upgrade patterns: install & copy, rejuvenation/delta only, and rejuvenation/verified. Second, we introduce our upgrade correctness concept. It is a formal correctness criterion for deciding whether an upgrade succeeded. Third, we discuss implementation patterns. Our insights base on various upgrade projects from stand-alone applications to multi-tenant systems having affected more than one hundred banks.}, 
keywords={database management systems;software maintenance;information system upgrades;database schema;install-and-copy;rejuvenation-delta only;rejuvenation-verified;upgrade correctness concept;application logic;Information Systems;Database Applications;Upgrades;Correctness;Testing}, 
doi={10.1109/ICSM.2010.5609679}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{8453066, 
author={M. {Scavuzzo} and E. {Di Nitto} and D. {Ardagna}}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration}, 
year={2018}, 
volume={}, 
number={}, 
pages={93-93}, 
abstract={Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.}, 
keywords={Big Data;cloud computing;data analysis;data models;formal verification;NoSQL databases;program testing;quality of service;software quality;Hegira4Cloud;fault-tolerant data extraction;software design;verification tools;Data Intensive system;data loss;Mediation Data Model;software engineering;QoS analysis;big data frameworks;NoSQL databases;data migration;Quality of Service;developing- testing-reengineering;Database as a Service;Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software;Data intensive applications;Experiment driven action research;Big data;Data migration}, 
doi={10.1145/3180155.3182534}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{8241218, 
author={A. {Al-Thuhli} and M. {Al-Badawi}}, 
booktitle={2017 Second International Conference on Information Systems Engineering (ICISE)}, 
title={A Framework to Interface Enterprise Social Network into Running Business Process}, 
year={2017}, 
volume={}, 
number={}, 
pages={45-53}, 
abstract={Enterprise Social Network (ESN) gain widespread adoption and popularity by many organizations as it provides a new way of social business interactions. However, there are some limitations found in ESN from integration with running business processes (BP) perspective. BP service designers and developers can't utilize its generated structured and unstructured contents in running BPs. This leads organizations to spend more efforts and time in defining social BP and integrate it with running BPs manually. Building upon previous works on reengineering BP to fuse it with social interaction, this paper proposes a framework to interface ESN BP into organization running BP. The framework fits with the structure data of ESN as input to generate web service in virtual environment. It defines the refactoring process of ESN database before publishing it as a web service into SOA environment based on features of open source and commercial platform services available to researchers and experts.}, 
keywords={business data processing;social networking (online);Web services;business process;social business interactions;unstructured contents;social BP;social interaction;ESN BP;refactoring process;ESN database;structured contents;Web service;enterprise social network interface;running BP;Organizations;Social network services;Databases;Service-oriented architecture;Collaboration;business process;enterprise social network;database refactoring;SOA;web service}, 
doi={10.1109/ICISE.2017.12}, 
ISSN={2160-1291}, 
month={April},}
@INPROCEEDINGS{844970, 
author={J. L. {Pfaltz} and R. {Orlandic}}, 
booktitle={Proceedings 1999 International Symposium on Database Applications in Non-Traditional Environments (DANTE'99) (Cat. No.PR00496)}, 
title={A scalable DBMS for large scientific simulations}, 
year={1999}, 
volume={}, 
number={}, 
pages={271-275}, 
abstract={Scientific simulations evolve constantly. Both the logical organization of the underlying database and the scientist's view of data may change rapidly. The underlying DBMS must provide appropriate support for the evolution of scientific simulations, their rapidly increasing computational intensity, as well as the growing volumes and dimensionality of scientific data. ADAMS is a dynamic and scalable academic DBMS for scientific applications designed to address the evolutionary processes and the problems of scale. This paper presents innovative solutions adopted in ADAMS allowing the system to accommodate the increasing computational intensity of scientific simulations as well as the growing volumes and dimensionality of scientific data.}, 
keywords={scientific information systems;digital simulation;object-oriented databases;scalable database;scientific simulations;computational intensity;ADAMS;academic database;evolutionary processes;scientific database;object oriented database;Databases;Computational modeling;Computer simulation;Sliding mode control;Art}, 
doi={10.1109/DANTE.1999.844970}, 
ISSN={}, 
month={Nov},}
@INBOOK{8471041, 
author={M. {Hinchey}}, 
booktitle={Software Technology: 10 Years of Innovation in IEEE Computer}, 
title={Analyzing the Evolution of Database Usage in Data-Intensive Software Systems}, 
year={2018}, 
volume={}, 
number={}, 
pages={}, 
abstract={This chapter presents the research advancements in the field of data-intensive software system evolution, 5 years after the publication of theIEEE Computercolumn presenting the challenges in this field (A. Cleve, T. Mens, and J.-L. Hainaut (2010) Data-intensive system evolution.Computer,43(8), 110112). We present the state of the art in this research domain, and report on the evolution of open-source Java projects relying on relational database technologies. We empirically analyze how the usage of Java database technologies evolve over time. We report on a coarse-grained source-code analysis carried out over several thousands of Java projects, and complement this with a fine-grained longitudinal analysis of the coevolution between database schema changes and source code changes within three large Java projects. The presented results are the first steps toward a recommendation system supporting developers in writing database-centered code.}, 
keywords={Java;Software systems;Relational databases;Open source software;Business}, 
doi={10.1002/9781119174240.ch12}, 
ISSN={}, 
publisher={IEEE}, 
isbn={9781119174240}, 
url={https://ieeexplore.ieee.org/document/8471041},}
@INPROCEEDINGS{6976118, 
author={L. {Meurice} and F. J. B. {Ruiz} and J. H. {Weber} and A. {Cleve}}, 
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution}, 
title={Establishing Referential Integrity in Legacy Information Systems - Reality Bites!}, 
year={2014}, 
volume={}, 
number={}, 
pages={461-465}, 
abstract={Most modern relational DBMS have the ability to monitor and enforce referential integrity constraints (RICs). In contrast to new applications, however, heavily evolved legacy information systems may not make use of this important feature, if their design predates its availability. The detection of RICs in legacy systems has been a long-term research topic in the DB reengineering community and a variety of different methods have been proposed, analyzing schema, application code and data. However, empirical evidence on their application for reengineering large-scale industrial systems is scarce and all too often "problems" (case studies) are carefully selected to fit a particular "solution" (method), rather than the other way around. This paper takes a different approach. We analyze in detail the issues posed in reengineering a complex, mission-critical information system to support RICs. In our analysis, we find that many of the assumptions typically made in DB reengineering methods do not readily apply. Based on our findings, we design a process and tools for detecting RICs in context of our real-world problem and provide preliminary results on their effectiveness.}, 
keywords={data analysis;information systems;relational databases;software maintenance;referential integrity;legacy information systems;relational DBMS;referential integrity constraints;RIC detection;DB reengineering community;schema analysis;application code analysis;data analysis;mission-critical information system;Databases;Java;Algorithm design and analysis;Aging;Data analysis;Educational institutions}, 
doi={10.1109/ICSME.2014.74}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7051899, 
author={S. {Soomro} and A. {Matsunaga} and J. A. B. {Fortes}}, 
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)}, 
title={Mapping specifications for ranked hierarchical trees in data integration systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={269-276}, 
abstract={A popular approach to deal with data integration of heterogeneous data sources is to Extract, Transform and Load (ETL) data from disparate sources into a consolidated data store while addressing integration challenges including, but not limited to: structural differences in the source and target schemas, semantic differences in their vocabularies, and data encoding. This work focuses on the integration of tree-like hierarchical data or information that when modeled as a relational schema can take the shape of a flat schema, a self-referential schema or a hybrid schema. Examples include evolutionary taxonomies, geological time scales, and organizational charts. Given the observed complexity in developing ETL processes for this particular but common type of data, our work focuses on reducing the time and effort required to map and transform this data. Our research automates and simplifies the transformation from ranked self-referential to flat representations (and vice-versa), by: (a) proposing MSL+, an extension to IBM's Mapping Specification Language (MSL), to succinctly express the mapping between schemas while hiding the actual transformation implementation complexity from the user, and (b) implementing a transformation component for the Talend open-source ETL platform, called Tree Transformer (TT). We evaluated MSL+ and TT, in the context of biodiversity data integration, where this class of transformations is a recurring pattern. We demonstrate the effectiveness of MSL+ with respect to development time savings as well as a 2 to 25-fold performance improvement in transformation time achieved by TT when compared to existing implementations and to Talend built-in components.}, 
keywords={data integration;formal specification;ranked hierarchical trees;data integration systems;mapping specifications;heterogeneous data sources;extract transform and load;ETL data;structural differences;tree like hierarchical data;relational schema;flat schema;evolutionary taxonomies;geological time scales;organizational charts;observed complexity;IBM mapping specification language;MSL;transformation implementation complexity;transformation component;tree transformer;biodiversity data integration;Data integration;Cities and towns;Data models;Complexity theory;XML;Continents;Transforms;data integration;self-referential schema;mapping language;data transformation;ETL}, 
doi={10.1109/IRI.2014.7051899}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{1333482, 
author={R. {Schmidt}}, 
booktitle={Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.}, 
title={Enactment of inter-organizational workflows using aspect-element-oriented Web services}, 
year={2004}, 
volume={}, 
number={}, 
pages={254-258}, 
abstract={To support collaboration over the Web, workflows have to be enacted not only within an organization, but also between organizations. These inter-organizational workflows create new requirements to enactment architectures, as a dynamic set of workflow participants has to be supported. Therefore, a new enactment architecture based on Web services is developed. It uses a homogeneous and dynamic composition of so called aspect-element-oriented Web services to support inter-organizational workflows.}, 
keywords={workflow management software;object-oriented programming;organisational aspects;Internet;inter-organizational workflows;aspect-element-oriented Web services;workflow evolution;schema representation;Web services;Service oriented architecture;Fluctuations;Encapsulation;Computer science;Collaborative work;Computer architecture;Buildings;Conferences;Databases}, 
doi={10.1109/DEXA.2004.1333482}, 
ISSN={1529-4188}, 
month={Sep.},}
@INPROCEEDINGS{227631, 
author={F. {Fotouhi} and A. A. {Shah} and W. {Grosky}}, 
booktitle={Proceedings ICCI `92: Fourth International Conference on Computing and Information}, 
title={Complex objects in the temporal object system}, 
year={1992}, 
volume={}, 
number={}, 
pages={381-384}, 
abstract={In many engineering applications, changes to the state and/or structure of an object needs to be maintained over a period of time. Existing object-oriented data models allow such changes in the state (referred to as version management) and structure (referred to as schema evolution) of an object. However, when the structure changes, the old structure is replaced by the new one. The authors propose a temporal object system (TOS) which maintains changes to both the structure and the state of an object. Objects in this system are referred to as temporal objects and are allowed to evolve over time. The authors discuss how to extend TOS in order to construct complex temporal objects from an aggregation of temporal objects.<<ETX>>}, 
keywords={object-oriented databases;temporal databases;complex objects;temporal object system;engineering applications;object-oriented data models;version management;schema evolution;Relational databases;Object oriented databases;Object oriented modeling;Database systems;History;Computer science;Maintenance engineering;Application software;Data models;Computer aided manufacturing}, 
doi={10.1109/ICCI.1992.227631}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8393032, 
author={H. {Duan} and M. {Li} and H. {You} and F. {Chen} and J. {Jiang} and Q. {Wang}}, 
booktitle={2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={Tendency determining of knowledge-transfer evolution based on patent citation network}, 
year={2017}, 
volume={}, 
number={}, 
pages={1757-1763}, 
abstract={The citation network is considered as a hotpot to investigate the knowledge-transfer evolution tendency. Aiming to research the technology development trace, an advanced analysis method is proposed in this paper. Data of the patents granted between 1976 and 2017 are collected from the website database of United States Patent and Trademark Office (USPTO). In order to determine the knowledge-transfer evolution tendency, destabilization or consolidation, two novel indices, including mDtand mCt, are extracted through network structure analysis and the development trace is discussed with the statistical analysis for assessment results. It is demonstrated that destabilization dominates the citation network in 1980s while the patents granted after 1992 tend to consolidate the development trace. Besides, the results also validate the effectiveness of proposed indexes. Additionally, the contributions of this paper can be utilized and discussed further in detecting technology development opportunities.}, 
keywords={citation analysis;data analysis;patents;statistical analysis;tendency determining;patent citation network;technology development trace;advanced analysis method;network structure analysis;statistical analysis;United States Patent and Trademark Office;knowledge-transfer evolution;website database;data analysis;Patents;Indexes;Knowledge management;Citation analysis;Technological innovation;Market research;Trademarks;patent citation network;destabilization;consolidation;knowledge-transfer;evolution tendency}, 
doi={10.1109/FSKD.2017.8393032}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8576128, 
author={A. {Cuzzocrea} and F. {Martinelli} and F. {Mercaldo}}, 
booktitle={2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
title={Detecting and Analyzing Anomalies Across Historical Data Changes: A Data-Driven Approach}, 
year={2018}, 
volume={}, 
number={}, 
pages={832-837}, 
abstract={The history of data changes can yield information about the nature of the change processes. Often, data evolve according to rules and constraints, making it possible to identify a profile of evolution: the values a data item assumes over time, the frequencies at which it changes, the temporal variation in relation to other data, or other constraints that are directly connected to the reference domain. A violation of these rules could be the signal of different menaces that threat the system, including: attempts of a tampering or a cyber attack, a failure in the operation of the system, a bug in the applications which manage the lifecycle of data. Detecting such violations is not straightforward, as rules could be unknown or hard to extract. In this paper we propose an approach to extract the legal or expected evolution of a database, by observing it in a frame of its lifecycle. The obtained profile of evolution is then used to detect anomalies in the database state evolution. The approach has been validated by an experiment that produced encouraging outcomes about its precision and efficacy.}, 
keywords={data analysis;database management systems;feature extraction;security of data;database state evolution;historical data changes;data-driven approach;data alteration;data anomaly detection;application bug;system operation failure;cyber attack attempts;tampering attempts;data change history;Databases;Data mining;Wireless sensor networks;Monitoring;Tools;Cyberattack;Organizations;Anomaly Detection;Data Driven Approaches;Database Security}, 
doi={10.1109/ICTAI.2018.00130}, 
ISSN={2375-0197}, 
month={Nov},}
@ARTICLE{4118711, 
author={M. {Raghavachari} and O. {Shmueli}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Efficient Revalidation of XML Documents}, 
year={2007}, 
volume={19}, 
number={4}, 
pages={554-567}, 
abstract={We study the problem of schema revalidation where XML data known to conform to one schema must be validated with respect to another schema. Such revalidation algorithms have applications in schema evolution, query processing, XML-based programming languages, and other domains. We describe how knowledge of conformance to an XML Schema may be used to determine conformance to another XML Schema efficiently. We examine both the situation where an XML document is modified before it is revalidated and the situation where it is unmodified}, 
keywords={database management systems;XML;XML document;schema revalidation;conformance knowledge;Extensible Markup Language;XML;Computer languages;Databases;Pattern matching;Query processing;Web services;Information retrieval;Content based retrieval;XML;XML Schema;validation;updates;subtyping.}, 
doi={10.1109/TKDE.2007.1004}, 
ISSN={1041-4347}, 
month={April},}
@INPROCEEDINGS{1299766, 
author={ and and }, 
booktitle={The 2003 Congress on Evolutionary Computation, 2003. CEC '03.}, 
title={Applying sample weighting methods to genetic parallel programming}, 
year={2003}, 
volume={2}, 
number={}, 
pages={928-935 Vol.2}, 
abstract={We investigate the sample weighting effect on genetic parallel programming (GPP). GPP evolves parallel programs to solve the training samples in a training set. Usually, the samples are captured directly from a real-world system. The distribution of samples in a training set can be extremely biased. Standard GPP assigns equal weights to all samples. It slows down evolution because crowded regions of samples dominate the fitness evaluation causing premature convergence. We present 4 sample weighting (SW) methods, i.e. equal SW, class-equal SW, static SW (SSW) and dynamic SW (DSW). We evaluate the 4 methods on 7 training sets (3 Boolean functions and 4 UCI medical data classification databases). Experimental results show that DSW is superior in performance on all tested problems. In the 5-input symmetry Boolean function experiment, SSW and DSW boost the evolutionary performance by 465 and 745 times respectively. Due to the simplicity and effectiveness of SSW and DSW, they can also be applied to different population-based evolutionary algorithms.}, 
keywords={parallel programming;Boolean functions;genetic algorithms;learning (artificial intelligence);genetic parallel programming;sample weighting method;GPP;training sample;real-world system;equal SW method;class-equal SW method;static SW method;dynamic SW method;DSW;training set;Boolean function;UCI medical data classification database;SSW;evolutionary algorithm;Parallel programming;Genetic programming;Boolean functions;Concurrent computing;Evolutionary computation;Clocks;Silicon compounds;Computer science education;Educational programs;Computer science}, 
doi={10.1109/CEC.2003.1299766}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{283028, 
author={P. {Schwarz} and K. {Shoens}}, 
booktitle={Proceedings of 1994 IEEE 10th International Conference on Data Engineering}, 
title={Managing change in the Rufus system}, 
year={1994}, 
volume={}, 
number={}, 
pages={170-179}, 
abstract={Rufus is an information system that models user data with objects taken from a class system. Due to the importance of coping with changes to the schema, Rufus has adopted the conformity-based model of Melampus. This model enables Rufus to cope with schema changes more easily than traditional class- and inheritance-based data models. The paper reviews the Melampus data model and describes how it was implemented in the Rufus system. The authors show how changes to the schema can be accommodated with minimum disruption. They also review design decisions that contributed to streamlined schema evolution and compare this approach with those proposed in the literature.<<ETX>>}, 
keywords={database theory;object-oriented databases;Rufus system;information system;user data;conformity-based model;Melampus;schema changes;data models;Data models;Database systems;Management information systems;Markup languages;Bibliographies;Electronic mail;TV;Pixel;Text recognition}, 
doi={10.1109/ICDE.1994.283028}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7272391, 
author={W. {Kim} and D. {Woelk} and J. {Garza} and H. {Chou} and J. {Banerjee} and N. {Ballou}}, 
booktitle={1987 IEEE Third International Conference on Data Engineering}, 
title={Enhancing the object-oriented concepts for database support}, 
year={1987}, 
volume={}, 
number={}, 
pages={291-292}, 
abstract={In this paper, we elaborate on three major enhancements to the conventional object-oriented data model, namely, schema evolution, composite objects, and versions. Schema evolution is the ability to dynamically make changes to the class definitions and the structure of the class lattice. Composite objects are recursive collections of exclusive components that are treated as units of storage, retrieval, and integrity enforcement. Versions are variations of the same object that are related by the history of their derivation. These additional features are strongly motivated by data management requirements of object-oriented applications from the AI, CAD/CAM, and OIS (office information systems with multimedia documents) domains. An object-oriented data model, with these enhancements, has been incorporated into ORION, a prototype database system developed at MCC as a vehicle of research into object-oriented databases.}, 
keywords={Databases;Object oriented modeling;Solid modeling;Computer aided manufacturing;Design automation;Computational modeling}, 
doi={10.1109/ICDE.1987.7272391}, 
ISSN={}, 
month={Feb},}
@ARTICLE{5645623, 
author={M. G. {de Carvalho} and A. H. F. {Laender} and M. A. {Goncalves} and A. S. {da Silva}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A Genetic Programming Approach to Record Deduplication}, 
year={2012}, 
volume={24}, 
number={3}, 
pages={399-412}, 
abstract={Several systems that rely on consistent data to offer high-quality services, such as digital libraries and e-commerce brokers, may be affected by the existence of duplicates, quasi replicas, or near-duplicate entries in their repositories. Because of that, there have been significant investments from private and government organizations for developing methods for removing replicas from its data repositories. This is due to the fact that clean and replica-free repositories not only allow the retrieval of higher quality information but also lead to more concise data and to potential savings in computational time and resources to process this data. In this paper, we propose a genetic programming approach to record deduplication that combines several different pieces of evidence extracted from the data content to find a deduplication function that is able to identify whether two entries in a repository are replicas or not. As shown by our experiments, our approach outperforms an existing state-of-the-art method found in the literature. Moreover, the suggested functions are computationally less demanding since they use fewer evidence. In addition, our genetic programming approach is capable of automatically adapting these functions to a given fixed replica identification boundary, freeing the user from the burden of having to choose and tune this parameter.}, 
keywords={genetic algorithms;information retrieval;replicated databases;genetic programming;record deduplication;digital libraries;e-commerce brokers;replica removal;data repositories;replica-free repositories;information retrieval;computational time;fixed replica identification boundary;database administration;database integration;Training;Machine learning;Genetic programming;Probabilistic logic;Databases;Data mining;Database administration;evolutionary computing and genetic algorithms;database integration.}, 
doi={10.1109/TKDE.2010.234}, 
ISSN={1041-4347}, 
month={March},}
@ARTICLE{1318562, 
author={A. Y. {Halevy} and Z. G. {Ives} and and P. {Mork} and D. {Suciu} and I. {Tatarinov}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={The Piazza peer data management system}, 
year={2004}, 
volume={16}, 
number={7}, 
pages={787-798}, 
abstract={Intuitively, data management and data integration tools are well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: They typically require a comprehensive schema design before they can be used to store or share information and they are difficult to extend because schema evolution is heavyweight and may break backward compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by nondatabase-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: We propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peers' schemes. PDMSs represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peers' individual schemas. This paper describes-several aspects of the Piazza PDMS, including the schema mediation formalism, query answering and optimization algorithms, and the relevance of PDMSs to the semantic Web.}, 
keywords={semantic Web;data integrity;query processing;distributed databases;Piazza peer data management system;data integration tools;schema design;decentralized architecture;semantic mapping;query answering;optimization algorithm;semantic Web;database;schema mediation;Semantic Web;XML;Mediation;Large-scale systems;Resource description framework;Database languages;Investments;Memory;Standardization;Terminology}, 
doi={10.1109/TKDE.2004.1318562}, 
ISSN={1041-4347}, 
month={July},}
@ARTICLE{6793013, 
author={J. {Niehaus} and C. {Igel} and W. {Banzhaf}}, 
journal={Evolutionary Computation}, 
title={Reducing the Number of Fitness Evaluations in Graph Genetic Programming Using a Canonical Graph Indexed Database}, 
year={2007}, 
volume={15}, 
number={2}, 
pages={199-221}, 
abstract={In this paper we describe the genetic programming system GGP operating on graphs and introduce the notion of graph isomorphisms to explain how they influence the dynamics of GP. It is shown empirically how fitness databases can improve the performance of GP and how mapping graphs to a canonical form can increase these improvements by saving considerable evaluation time.}, 
keywords={graph representation;genetic programming;graph isomorphism;neutrality;fitness database}, 
doi={10.1162/evco.2007.15.2.199}, 
ISSN={1063-6560}, 
month={June},}
@INPROCEEDINGS{1552948, 
author={L. {Lim} and M. {Wang}}, 
booktitle={IEEE International Conference on e-Business Engineering (ICEBE'05)}, 
title={Managing e-commerce catalogs in a DBMS with native XML support}, 
year={2005}, 
volume={}, 
number={}, 
pages={564-571}, 
abstract={Electronic commerce is emerging as a major application area for database systems. A large number of e-commerce stores provide electronic product catalogs that allow customers to search products of interest and store owners to manage various product information. Due to the constant schema evolution and the sparsity of e-commerce data, most commercial e-commerce systems use the so-called vertical schema for data storage. However, query processing for data stored using vertical schema is extremely inefficient because current RDBMSs, especially its cost-based query optimizer, are specifically designed to deal with traditional horizontal schemas. In this paper, we show that e-catalog management can be naturally supported in IBM's system RX, the first DBMS that truly supports both XML and relational data in their native forms. By leveraging on system RX's hybrid nature, we present a novel solution for storing, managing, and querying e-catalog data. In addition to traditional queries, we show that our solution supports semantic queries as well. Our solution does not require a separate query optimization layer, because query optimization is handled within the hybrid DBMS engine itself}, 
keywords={cataloguing;electronic commerce;query processing;relational databases;retail data processing;XML;electronic commerce;e-commerce catalog management;DBMS engine;XML support;database system;electronic product catalog;vertical schema evolution;e-commerce system;data storage;query processing;RDBMS;cost-based query optimization;IBM system RX;Catalogs;XML;Consumer electronics;Query processing;Electronic commerce;Database systems;Information management;Memory;Design optimization;Engines}, 
doi={10.1109/ICEBE.2005.82}, 
ISSN={}, 
month={Oct},}
@ARTICLE{5467079, 
author={H. {Decker} and D. {Martinenghi}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Inconsistency-Tolerant Integrity Checking}, 
year={2011}, 
volume={23}, 
number={2}, 
pages={218-234}, 
abstract={All methods for efficient integrity checking require all integrity constraints to be totally satisfied, before any update is executed. However, a certain amount of inconsistency is the rule, rather than the exception in databases. In this paper, we close the gap between theory and practice of integrity checking, i.e., between the unrealistic theoretical requirement of total integrity and the practical need for inconsistency tolerance, which we define for integrity checking methods. We show that most of them can still be used to check whether updates preserve integrity, even if the current state is inconsistent. Inconsistency-tolerant integrity checking proves beneficial both for integrity preservation and query answering. Also, we show that it is useful for view updating, repairs, schema evolution, and other applications.}, 
keywords={database management systems;query processing;inconsistency tolerant integrity checking;query answering;integrity preservation;database schema;Database systems;Data mining;Warehousing;Data quality;Transaction databases;Knowledge engineering;Integrity checking;inconsistency tolerance.}, 
doi={10.1109/TKDE.2010.87}, 
ISSN={1041-4347}, 
month={Feb},}
@INPROCEEDINGS{5363351, 
author={Y. {Sun} and Z. {Li} and C. {Tang} and W. {Zhou} and R. {Jiang}}, 
booktitle={2009 Fifth International Conference on Natural Computation}, 
title={An Evolving Neural Network for Authentic Emotion Classification}, 
year={2009}, 
volume={2}, 
number={}, 
pages={109-113}, 
abstract={Nowadays, there are few international databases based on authentic gesture. Most of the facial expression databases are not naturally linked to the emotional state of the test subjects. In this work, we expand the authentic emotion database created in 2003 by adding more subjects. Meanwhile we combine evolutionary algorithms with neural networks and well improve the recognition rate. We also implement other classification methods like gene expression programming and decision trees in order to compare with the adjusted neural networks. The experiment results show that our way to evolve back propagation neural network is quick and it can achieve an average recognition rate of 97%. Besides, it is much faster and more accurate than the gene expression commercial software: GeneXproTools, which is usually very powerful in many common datasets' classification.}, 
keywords={backpropagation;decision trees;emotion recognition;face recognition;genetic algorithms;image classification;neural nets;visual databases;authentic emotion classification;international databases;authentic gesture recognition;facial expression databases;authentic emotion database;evolutionary algorithms;gene expression programming;decision trees;gene expression commercial software;GeneXproTools;backpropagation neural network;Neural networks;Testing;Gene expression;Computer science;Spatial databases;Cameras;Electronic mail;Evolutionary computation;Data mining;Computer networks;neural network;authentic Emotion classification;gene expression programming;GeneXproTools}, 
doi={10.1109/ICNC.2009.310}, 
ISSN={2157-9555}, 
month={Aug},}
@ARTICLE{476503, 
author={H. A. {Proper} and T. P. {van der Weide}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A general theory for evolving application models}, 
year={1995}, 
volume={7}, 
number={6}, 
pages={984-996}, 
abstract={Provides a general theory for evolving information systems. This theory makes a distinction between the underlying information structure at the conceptual level, its evolution on the one hand, and the description and semantics of operations on the information structure and its population on the other hand. The main issues within this theory are object typing, type relatedness and identification of objects. In terms of these concepts, we propose some axioms on the well-formedness of evolution. In this general theory, the underlying data model is a parameter, making the theory applicable for a wide range of modelling techniques, including object-role modelling and object-oriented techniques.}, 
keywords={data structures;entity-relationship modelling;object-oriented methods;type theory;abstract data types;application model evolution;information structure evolution;conceptual level;operational semantics;object typing;type relatedness;object identification;well-formedness;data model;object-role modelling;object-oriented techniques;evolving information systems;temporal infermation systems;schema evolution;predicator set model;entity-relationship model;Object oriented modeling;Information systems;Databases;Erbium;Data models;Data conversion;Information technology;Australia;History;Books}, 
doi={10.1109/69.476503}, 
ISSN={1041-4347}, 
month={Dec},}
@INPROCEEDINGS{927259, 
author={F. {Keienburg} and A. {Rausch}}, 
booktitle={Proceedings of the 34th Annual Hawaii International Conference on System Sciences}, 
title={Using XML/XMI for tool supported evolution of UML models}, 
year={2001}, 
volume={}, 
number={}, 
pages={10 pp.-}, 
abstract={Software components developed with modern tools and middleware infrastructures undergo considerable reprogramming before they become reusable. Tools and methodologies are needed to cope with the evolution of software components. We present some basic concepts and architectures to handle the impacts of the evolution of UML models. With the proposed concepts an infrastructure to support model evolution, data schema migration, and data instance migration based on UML models can be realized. To describe the evolution path we use XML/XMI files.}, 
keywords={hypermedia markup languages;software tools;specification languages;object-oriented programming;software reusability;distributed object management;XML;XMI;UML models;software components;software tools;middleware;software reuse;model evolution;data schema migration;data instance migration;XML;Unified modeling language;Object oriented modeling;Software tools;Middleware;Software engineering;Programming;Business communication;Java;Software reusability}, 
doi={10.1109/HICSS.2001.927259}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{1188933, 
author={J. {Bluis} and D. G. {Shin}}, 
booktitle={Third IEEE Symposium on Bioinformatics and Bioengineering, 2003. Proceedings.}, 
title={Nodal distance algorithm: calculating a phylogenetic tree comparison metric}, 
year={2003}, 
volume={}, 
number={}, 
pages={87-94}, 
abstract={Maintaining a phylogenetic relationship repository requires the development of tools that are useful for mining the data stored in the repository. One way to query a database of phylogenetic information would be to compare phylogenetic trees. Because the only existing tree comparison methods are computationally intensive, this is not a reasonable task. Presented here is the nodal distance algorithm which has significantly less computation time than the most widely used comparison method, the partition metric. When the metric is calculated for trees where one species has been repositioned to a distant part of the tree no further computation is required as is needed for the partition metric. The nodal distance algorithm provides a method for comparing large sets of phylogenetic trees in a reasonable amount of time.}, 
keywords={genetics;evolution (biological);biology computing;data mining;nodal distance algorithm;partition metric;database;molecular evolution studies;gene characteristics;protein characteristics;genome characteristics;repository data mining;computation time;phylogenetic tree comparison metric calculation;Phylogeny;Databases;Partitioning algorithms;Bioinformatics;Genomics;Computer science;Maintenance engineering;Data engineering;Data mining;Proteins}, 
doi={10.1109/BIBE.2003.1188933}, 
ISSN={}, 
month={March},}
@ARTICLE{1644730, 
author={S. {Chen} and X. {Zhang} and E. A. {Rundensteiner}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A compensation-based approach for view maintenance in distributed environments}, 
year={2006}, 
volume={18}, 
number={8}, 
pages={1068-1081}, 
abstract={Data integration over multiple heterogeneous data sources has become increasingly important for modern applications. The integrated data is usually stored as materialized views to allow better access, performance, and high availability. In loosely coupled environments, such as the data grid, the data sources are autonomous. Hence, tie source updates can be concurrent and cause erroneous results during view maintenance. State-of-the-art maintenance strategies apply compensating queries to correct such errors, making the restricting assumption that all source schemata remain static over time. However, in such dynamic environments, the data sources may change not only their data but also their schema. Consequently, either the maintenance queres or the compensating queries may fail. In this paper, we propose a novel framework called DyDa that overcomes these limitations and handles both source data updates and schema changes. We identify three types of maintenance anomalies, caused by either source data updates, data-preserving schema changes, or non-data-preserving schema changes. We propose a compensation algorithm to solve the first two types of anomalies. We show that the third type of anomaly is caused by the violation of dependencies between maintenance processes. Then, we propose dependency detection and correction algorithms to identify and resolve the violations. Put together, DyDa extends prior maintenance solutions to solve all types of view maintenance anomalies. The experimental results show that DyDa imposes a minimal overhead on data update processing while allowing for the extended functionality to handle concurrent schema changes}, 
keywords={concurrency control;data integrity;distributed databases;view maintenance;distributed environment;data integration;data grid;state-of-the-art maintenance strategy;DyDa framework;data-preserving schema change;nondata-preserving schema change;compensation algorithm;dependency detection algorithm;correction algorithm;source data update processing;concurrent schema change;Evolution (biology);Availability;Error correction;Concurrency control;Explosions;Information systems;XML;Query processing;Biological system modeling;Genomics;View maintenance;view synchronization;view adaptation;concurrency control;view maintenance anomaly.}, 
doi={10.1109/TKDE.2006.117}, 
ISSN={1041-4347}, 
month={Aug},}
@INPROCEEDINGS{7454387, 
author={J. {Turkka} and T. {Hiltunen} and R. U. {Mondal} and T. {Ristaniemi}}, 
booktitle={2015 International Symposium on Wireless Communication Systems (ISWCS)}, 
title={Performance evaluation of LTE radio fingerprinting using field measurements}, 
year={2015}, 
volume={}, 
number={}, 
pages={466-470}, 
abstract={This paper evaluates positioning accuracy of radio fingerprinting algorithm in commercially deployed LTE (Long Term Evolution) network operating on 800 MHz, 1800 MHz and 2600 MHz frequency bands. Training database required for fingerprint positioning is constructed from field measurements that resemble LTE Release 10 Minimization of Drive Test (MDT) trace records. Performance is compared between LTE and LTE+WLAN grid-based RF fingerprint measurements utilizing partial fingerprint matching. Results indicate that partial fingerprints that consist of LTE and WLAN radio measurements decrease positioning errors at least by a factor of 3.5x while the percentage of discarded samples is kept low. This emphasizes the importance of having generic MDT functionality for automating the collection and correlation of radio measurements in heterogeneous LTE and WLAN networks.}, 
keywords={Long Term Evolution;radio direction-finding;wireless LAN;radio fingerprinting algorithm;commercially deployed LTE network;Long Term Evolution;training database;fingerprint positioning;LTE Release 10 Minimization of Drive Test trace records;LTE Release 10 MDT trace records;grid-based RF fingerprint measurements;partial fingerprint matching;LTE radio measurements;WLAN radio measurements;positioning errors;generic MDT functionality;frequency 800 MHz;frequency 1800 MHz;frequency 2600 MHz;Fingerprint recognition;Wireless LAN;Training;Databases;Long Term Evolution;Correlation;Radio frequency;LTE;Minimization of Drive Tests;Field Measurements;Radio Fingerprint Positioning}, 
doi={10.1109/ISWCS.2015.7454387}, 
ISSN={2154-0225}, 
month={Aug},}
@INPROCEEDINGS{836632, 
author={G. {Psaila}}, 
booktitle={Proceedings 1999 Workshop on Knowledge and Data Engineering Exchange (KDEX'99) (Cat. No.PR00453)}, 
title={ERX: a data model for collections of XML documents}, 
year={1999}, 
volume={}, 
number={}, 
pages={99-104}, 
abstract={XML (eXtensible Mark-up Language) is able to represent any kind of structured or semi-structured document, such as papers, Web pages, database schemas and instances, style sheets, etc. However, the tree structure of XML documents, induced by mark-up nesting, does not provide a sufficiently expressive and general conceptual model of data in the documents, particularly when multiple source documents are processed at the same time. This paper proposes the ERX (Entity Relationship for XML) data model, an evolution of the entity-relationship model that copes with the peculiar features of XML. ERX is devised to provide effective support for the development of complex XML processors for advanced applications.}, 
keywords={hypermedia markup languages;entity-relationship modelling;data models;ERX data model;XML document collections;structured documents;tree structure;mark-up nesting;expressive conceptual model;multiple source document processing;entity-relationship model;Data models;XML;Hip;Internet;Web sites;Electrical capacitance tomography;Manuals;Writing;Web pages;Databases}, 
doi={10.1109/KDEX.1999.836632}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{4424618, 
author={ and J. {Vohradsky}}, 
booktitle={2007 IEEE Congress on Evolutionary Computation}, 
title={Binary classification using parallel genetic algorithm}, 
year={2007}, 
volume={}, 
number={}, 
pages={1281-1287}, 
abstract={Binary classification is one of the most popular methods in supervised pattern classification. In this paper, we would like to propose an algorithm based on genetic algorithm for binary classification. Binary classification here is presented in a nonlinear programming form. Genetic algorithm is then used to search solutions of nonlinear programming. Four databases (one transcriptomics, one proteomics, and two breast cancers) were used to test the algorithm and six other well-known methods. Parallel computing based on island model was also experimented. The results show that the algorithm could identify most similar patterns in the database with high precision. Island model not only increases computational speed but also gives high quality result.}, 
keywords={genetic algorithms;nonlinear programming;pattern classification;binary classification;parallel genetic algorithm;supervised pattern classification;nonlinear programming;parallel computing;island model;Genetic algorithms;Evolutionary computation;Database searching;Genetic algorithms;Pattern classification;Pattern recognition}, 
doi={10.1109/CEC.2007.4424618}, 
ISSN={1089-778X}, 
month={Sep.},}
@INPROCEEDINGS{792172, 
author={}, 
booktitle={Proceedings Fourth IFCIS International Conference on Cooperative Information Systems. CoopIS 99 (Cat. No.PR00384)}, 
title={Ad-Hoc recovery in workflows, a formal model and some system support aspects}, 
year={1999}, 
volume={}, 
number={}, 
pages={222-233}, 
abstract={How to increase flexibility has always been an issue relating to the applicability of any workflow system. We address this issue in one case where an agent needs to alter the control flow prescribed in the original definition to some previous executions. This phenomenon is termed ad-hoc recovery. Ad-hoc recovery is usually caused by unpredictable reasons, such as unexpected output Of some individual tasks, events or exceptions due to the changing environment. Due to its irregularity in nature, ad-hoc recovery in general cannot be dealt with by approaches to handling failure recoveries. On the other hand, since it allows normal control flows to be diverged from at run time without modifying workflow schema, it enjoys higher flexibility than the fixed schema systems while pays a lower cost than schema evolution. In this paper, we introduce the concept of ad-hoc recovery, and characterize its main features. We also discuss some implementation issues.}, 
keywords={workflow management software;system recovery;cooperative systems;workflow system;ad-hoc recovery;workflow schema;control flow;schema evolution;system support;formal model;Control systems;Costs}, 
doi={10.1109/COOPIS.1999.792172}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{5346033, 
author={H. {Zhou} and S. {Mabu} and K. {Shimada} and K. {Hirasawa}}, 
booktitle={2009 IEEE International Conference on Systems, Man and Cybernetics}, 
title={Backward time related association rule mining with database rearrangement in traffic volume prediction}, 
year={2009}, 
volume={}, 
number={}, 
pages={1021-1026}, 
abstract={In this paper, backward time related association rule mining using genetic network programming (GNP) with database rearrangement is introduced in order to find time related sequential association from time related databases effectively and efficiently. GNP is a kind of human brain like evolutionary model which represents solutions as directed graph structures. The concept of database rearrangement to better handle association rule extraction from the databases in the traffic volume prediction problems is proposed. The proposed algorithm and experimental results are also included.}, 
keywords={data mining;directed graphs;genetic algorithms;traffic engineering computing;traffic volume prediction;backward time related association rule mining;genetic network programming;database rearrangement;evolutionary model;directed graph structure;association rule extraction;Association rules;Data mining;Economic indicators;Telecommunication traffic;Genetics;Vehicle dynamics;Spatial databases;Traffic control;Real time systems;Cybernetics;Backwards;Time Related;Data Mining;Genetic Network Programming;Traffic Volume Prediction}, 
doi={10.1109/ICSMC.2009.5346033}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{6805417, 
author={J. {Malý} and M. {Necaský}}, 
booktitle={2013 20th Asia-Pacific Software Engineering Conference (APSEC)}, 
title={Semantic Document Adaptation with OCL Annotations}, 
year={2013}, 
volume={1}, 
number={}, 
pages={280-288}, 
abstract={The paper addresses the problem of XML document adaptation - transforming a document valid against an old version of an XML schema to a document valid against the new version of the schema. We propose a solution using a conceptual model, where the relationships between the old and new versions are specified declaratively. We focus on those scenarios, where mere between the structures of the schemas (corresponding to the parts of the XML documents) is not descriptive enough, because there are more intricate relationships between the data in the source and target document. We propose a use of OCL, an expression language and part of the UML standard, to declare the relationships as formal annotations of the mappings.}, 
keywords={document handling;XML;semantic document adaptation;OCL annotations;XML document adaptation;extensible markup language;XML schema;expression language;Object Constraints Language;XML;Unified modeling language;Adaptation models;Standards;Data models;Semantics;Computational modeling;ocl;xml;document adaptation;schema evolution}, 
doi={10.1109/APSEC.2013.46}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{7266269, 
author={N. {Minsky} and D. {Rozenshtein} and J. {Chomicki}}, 
booktitle={1986 IEEE Second International Conference on Data Engineering}, 
title={A controllable Prolog database system}, 
year={1986}, 
volume={}, 
number={}, 
pages={618-628}, 
abstract={This paper presents a model that provides a single comprehensive mechanism to control the use, operation and evolution of database systems. This model unifies several concepts generally considered to be quite distinct. In particular, it minimizes the formal distinction between the users of the database, the programs embedded in it and even the administrators and the programmers maintaining it. Furthermore, under this model, the concepts of subschema and of program module are replaced with a single concept of frame, which serves as the locus of power and of activity in the system. Moreover, the proposed control mechanism is closed, in the sense that the process of establishing controls is itself controllable by the same mechanism. This can be used to formalize and control managerial policies about the use and evolution of database systems.}, 
keywords={Database systems;Context;Standards;Syntactics;Control systems;Computer languages}, 
doi={10.1109/ICDE.1986.7266269}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6608478, 
author={S. -. {Yoo} and S. -. {Oh} and W. {Pedrycz}}, 
booktitle={2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)}, 
title={Design of face recognition algorithm realized with feature extraction from 2D-LDA and optimized polynomial-based RBF NNs}, 
year={2013}, 
volume={}, 
number={}, 
pages={655-660}, 
abstract={This study elaborates on a design of a face recognition algorithm realized with feature extraction from 2D-LDA and the use of polynomial-based radial basis function neural networks (P-RBF NNS). The overall face recognition system consists of two modules such as the preprocessing part and recognition part. The proposed polynomial-based radial basis function neural networks is used as an the recognition part of the overall face recognition system, while a data preprocessing algorithm presented of 2 dimensional linear discriminant analysis (2D-LDA) is exploited to data preprocessing. The essential design parameters are optimized by means of differential evolution (DE). The experimental results for benchmark face datasets - the Yale and ORL database - demonstrate the effectiveness and efficiency of 2D-LDA algorithm compared with other approaches such as principal component analysis (PCA), and fusion of PCA-LDA.}, 
keywords={evolutionary computation;face recognition;feature extraction;polynomials;radial basis function networks;statistical analysis;face recognition;feature extraction;2D-LDA;polynomial-based RBF NN;radial basis function neural network;P-RBF NNS;2 dimensional linear discriminant analysis;differential evolution;Yale database;ORL database;Artificial neural networks;Face;Databases;Training;Face recognition;Polynomials;Testing}, 
doi={10.1109/IFSA-NAFIPS.2013.6608478}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8088255, 
author={T. {Hoppe} and H. {Eisenmann} and A. {Viehl} and O. {Bringmann}}, 
booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
title={Guided systems engineering by profiled ontologies}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Engineering phase specific tailored views on data are required to prevent over-engineering and to discover missing elements. This paper presents an approach to enhance a semantic Conceptual Data Model (CDM) to define the specific behavior of engineering phases. Therefore, a CDM gets enhanced by an ontology profile to specify required and non-admissible features for each engineering phase. This is exploited to enhance existing transformations from Web Ontology Language (OWL) to Eclipse Modeling Language (EMF) to generate phase specific decorators representing the corresponding views on data items. The approach also supports engineering phase specific handling of constraints and rules that further augment the guidance of engineers through the system development process by applying knowledge management functions such as reasoning. As a result, the presented approach reveals a notably higher data quality and offers more analysis potential by enabling phase-specific definitions of views on data and specification of semantic checks on CDM level. Several use cases from aerospace engineering have been analyzed and the improvements in areas such as inconsistency detection, knowledge derivation, and guided system modeling are highlighted in this paper.}, 
keywords={data models;knowledge management;knowledge representation languages;ontologies (artificial intelligence);systems engineering;ontology profile;Web Ontology Language;Eclipse Modeling Language;phase specific decorators;data items;system development process;CDM level;aerospace engineering;system modeling;guided systems engineering;profiled ontologies;semantic conceptual data model;data quality;Data models;Ontologies;OWL;System analysis and design;Semantics;Model-based Systems Engineering;Conceptual Data Model Evolution;Ontology;Guided Systems Engineering}, 
doi={10.1109/SysEng.2017.8088255}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{246401, 
author={J. P. {Yoon}}, 
booktitle={Proceedings Fourth International Conference on Tools with Artificial Intelligence TAI '92}, 
title={Knowledge discovery for evolutionary systems}, 
year={1992}, 
volume={}, 
number={}, 
pages={192-199}, 
abstract={The issues addressed include using a database query to discover a new rule, using not only positive examples (answer to a query) but also negative examples to discover new rules, and harmonizing a new rule and previous rules to remove inconsistencies. The main contribution is the development of a new tool for characterizing the exceptions in databases and evolving knowledge as a database evolves.<<ETX>>}, 
keywords={deductive databases;exception handling;knowledge acquisition;knowledge based systems;learning systems;knowledge discovery;evolutionary systems;database query;new rule;positive examples;negative examples;exceptions;Object oriented databases;Relational databases;Spatial databases;Data engineering;Information technology;Knowledge engineering;Surveillance;Information retrieval}, 
doi={10.1109/TAI.1992.246401}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7165991, 
author={T. {Cerqueus} and E. C. d. {Almeida} and S. {Scherzinger}}, 
booktitle={2015 IEEE/ACM 1st International Workshop on Big Data Software Engineering}, 
title={Safely Managing Data Variety in Big Data Software Development}, 
year={2015}, 
volume={}, 
number={}, 
pages={4-10}, 
abstract={We consider the task of building Big Data software systems, offered as software-as-a-service. These applications are commonly backed by NoSQL data stores that address the proverbial Vs of Big Data processing: NoSQL data stores can handle large volumes of data and many systems do not enforce a global schema, to account for structural variety in data. Thus, software engineers can design the data model on the go, a flexibility that is particularly crucial in agile software development. However, NoSQL data stores commonly do not yet account for the veracity of changes when it comes to changes in the structure of persisted data. Yet this is an inevitable consequence of agile software development. In most NoSQL-based application stacks, schema evolution is completely handled within the application code, usually involving object mapper libraries. Yet simple code refactorings, such as renaming a class attribute at the source code level, can cause data loss or runtime errors once the application has been deployed to production. We address this pain point by contributing type checking rules that we have implemented within an IDE plug in. Our plug in ControVol statically type checks the object mapper class declarations against the code release history. ControVol is thus capable of detecting common yet risky cases of mismatched data and schema, and can even suggest automatic fixes.}, 
keywords={Big Data;cloud computing;programming environments;software engineering;Big Data software development;software-as-a-service;NoSQL data stores;type checking rules;IDE plugin;ControVol;object mapper class declarations;code release history;Runtime;Big data;Software;Java;Production;History;Loading;type checking;NoSQL data stores;object mapping;schema evolution}, 
doi={10.1109/BIGDSE.2015.9}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7523663, 
author={D. P. {Losey} and C. G. {McDonald} and M. K. {O'Malley}}, 
booktitle={2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}, 
title={A bio-inspired algorithm for identifying unknown kinematics from a discrete set of candidate models by using collision detection}, 
year={2016}, 
volume={}, 
number={}, 
pages={418-423}, 
abstract={Many robots are composed of interchangeable modular components, each of which can be independently controlled, and collectively can be disassembled and reassembled into new configurations. When assembling these modules into an open kinematic chain, there are some discrete choices dictated by the module geometry; for example, the order in which the modules are placed, the axis of rotation of each module with respect to the previous module, and/or the overall shape of the assembled robot. Although it might be straightforward for a human user to provide this information, there is also a practical benefit in the robot autonomously identifying these unknown, discrete forward kinematics. To date, a variety of techniques have been proposed to identify unknown kinematics; however, these methods cannot be directly applied during situations where we seek to identify the correct model amid a discrete set of options. In this paper, we introduce a method specifically for finding discrete robot kinematics, which relies on collision detection, and is inspired by the biological concepts of body schema and evolutionary algorithms. Under the proposed method, the robot maintains a population of possible models, stochastically identifies a motion which best distinguishes those models, and then performs that motion while checking for a collision. Models which correctly predicted whether a collision would occur produce candidate models for the next iteration. Using this algorithm during simulations with a Baxter robot, we were able to correctly determine the order of the links in 84% of trials while exploring around 0.01% of all possible models, and we were able to correctly determine the axes of rotation in 94% of trials while exploring &lt;; 0.1% of all possible models.}, 
keywords={collision avoidance;evolutionary computation;robot kinematics;bio-inspired algorithm;unknown kinematics;discrete set;collision detection;interchangeable modular components;open kinematic chain;module geometry;assembled robot;discrete forward kinematics;discrete robot kinematics;body schema;evolutionary algorithms;Collision avoidance;Kinematics;Biological system modeling;Predictive models;Robot sensing systems}, 
doi={10.1109/BIOROB.2016.7523663}, 
ISSN={2155-1782}, 
month={June},}
@ARTICLE{277771, 
author={E. B. {Fernandez} and E. {Gudes} and }, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A model for evaluation and administration of security in object-oriented databases}, 
year={1994}, 
volume={6}, 
number={2}, 
pages={275-292}, 
abstract={The integration of object-oriented programming concepts with databases is one of the most significant advances in the evolution of database systems. Many aspects of such a combination have been studied, but there are few models to provide security for this richly structured information. We develop an authorization model for object-oriented databases. This model consists of a set of policies, a structure for authorization rules, and algorithms to evaluate access requests against the authorization rules. User access policies are based on the concept of inherited authorization applied along the class structure hierarchy. We propose also a set of administrative policies that allow the control of user access and its decentralization. Finally, we study the effect of class structuring changes on authorization.<<ETX>>}, 
keywords={object-oriented databases;authorisation;database theory;object-oriented databases;object-oriented programming concepts;security evaluation;security administration;authorization model;inherited authorization;authorization rule structure;access requests;user access policies;class structure hierarchy;administrative policies;decentralization;class structuring changes;database security;Object oriented modeling;Data security;Object oriented databases;Authorization;Database systems;Object oriented programming;Information security;Relational databases;Encapsulation}, 
doi={10.1109/69.277771}, 
ISSN={1041-4347}, 
month={April},}
@INPROCEEDINGS{1515442, 
author={ and and }, 
booktitle={Fourth Annual ACIS International Conference on Computer and Information Science (ICIS'05)}, 
title={Mock objects framework for TDD in the network environment}, 
year={2005}, 
volume={}, 
number={}, 
pages={430-434}, 
abstract={TDD is a software development approach which is based on test TDD let us get improved code and refined design through lasting test with refactoring process. However, if network or database environment and other object were not developed, TDD could have a problem to make progress. If you will use the mock objects in this situation, TDD will be processed more effectively. To make mock objects needs a lot of cost and effort for network and database. Therefore this paper presents a mock objects frameworks for TDD which can save time and make safe mock objects.}, 
keywords={object-oriented programming;program testing;software engineering;mock objects;network environment;TDD;software code;software design;lasting test;refactoring process;database environment;software testing;agile software development;extreme programming;test driven development;Intelligent networks;Databases;Programming;Software testing;Computer science;User interfaces;Costs;Information science;Network servers;Software Testing;TDD;Agile Software Development;XP}, 
doi={10.1109/ICIS.2005.88}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{1452496, 
author={R. {Alcala} and J. {Alcala-Fdez} and F. {Herrera} and J. {Otero}}, 
booktitle={The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.}, 
title={Genetic Learning of the Knowledge Base of a Fuzzy System by Using the Linguistic 2-Tuples Representation}, 
year={2005}, 
volume={}, 
number={}, 
pages={797-802}, 
abstract={One of the problems associated to linguistic fuzzy modeling is its lack of accuracy when modeling some complex systems. To overcome this problem, many different possibilities of improving the accuracy of linguistic fuzzy modeling have been considered in the specialized literature, maintaining the desired trade-off between accuracy and interpretability. Recently, a new linguistic rule representation model was presented to perform a genetic lateral tuning of membership functions. It is based on the linguistic 2-tuples representation model that allows the lateral displacement of a label considering a unique parameter. It involves a reduction of the search space that eases the derivation of optimal models. Based on the linguistic 2-tuples representation model, in this work, we present a new method to obtain linguistic fuzzy systems by means of an evolutionary learning of the data base a priori (granularity and lateral displacements of the membership functions) and on the use of a basic rule generation method to obtain the whole knowledge base. In this way, the search space reduction provided by the linguistic 2-tuples representation helps to the evolutionary search technique to obtain more precise and compact knowledge bases. Moreover, we analyze this approach considering 21 real-world problems}, 
keywords={computational linguistics;fuzzy set theory;fuzzy systems;genetic algorithms;knowledge acquisition;knowledge based systems;knowledge representation;learning (artificial intelligence);knowledge base genetic learning;fuzzy system;linguistic 2-tuple representation;linguistic fuzzy modeling;linguistic rule representation;genetic lateral tuning;membership functions;label lateral displacement;search space reduction;optimal model;linguistic fuzzy systems;database evolutionary learning;granularity;rule generation;evolutionary search technique;Genetics;Fuzzy systems;Computer science;Learning;Artificial intelligence;Buildings;Humans;Fuzzy reasoning;Shape;Knowledge based systems}, 
doi={10.1109/FUZZY.2005.1452496}, 
ISSN={1098-7584}, 
month={May},}
@ARTICLE{1033775, 
author={M. {Dahchour} and A. {Pirotte} and E. {Zimanyi}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Materialization and its metaclass implementation}, 
year={2002}, 
volume={14}, 
number={5}, 
pages={1078-1094}, 
abstract={Materialization is a powerful and ubiquitous abstraction pattern for conceptual modeling that relates a class of categories (e.g., models of cars) and a class of more concrete objects (e.g., individual cars). This paper presents materialization as a generic relationship between two classes of objects and describes an abstract implementation of it. The presentation is abstract in that it is not targeted at a specific object system. The target system is supposed to provide: 1) basic object-modeling facilities, supplemented with an explicit metaclass concept and 2) operations for dynamic schema evolution like creation or deletion of a subclass of a given class and modification of the type of an attribute of a class. The presentation is generic in that the semantics of materialization is implemented in a metaclass, which is a template to be instantiated in applications. Application classes are created as instances of the metaclass and they are thereby endowed with structure and behavior consistent with the generic semantics of materialization.}, 
keywords={object-oriented methods;category theory;meta data;data models;conceptual modeling;materialization;classes of objects;abstraction pattern;object orientation;metaclass;inheritance;object-modeling;schema evolution;Object oriented modeling;Power system modeling;Concrete;Application software;Database systems;Computer Society;Instruction sets;Object oriented programming}, 
doi={10.1109/TKDE.2002.1033775}, 
ISSN={1041-4347}, 
month={Sep.},}
@INPROCEEDINGS{7738266, 
author={I. {Sakly} and M. A. {Mezghich} and S. {M'hiri} and F. {Ghorbel}}, 
booktitle={2016 IEEE International Conference on Imaging Systems and Techniques (IST)}, 
title={A hybrid shape constraint for level set-based active contours}, 
year={2016}, 
volume={}, 
number={}, 
pages={440-443}, 
abstract={In this paper, we present a new method to incorporate a hybrid shape prior into a level set-based active contour in order to improve its robustness to partial occlusion, clutter and noise. The proposed shape constraint is constructed in two steps. The first one consists in the alignment of the given training data according to the target shape. Then, the Hausdorff distance is used as criteria to choose the most suitable ones to construct the statistical map for being used as a reference model. This constraint is then incorporated, using an evolution schema, into an active contour model to promote the convergence towards the statistical model. Experiments on synthetic and real data show satisfactory segmentation results.}, 
keywords={image segmentation;set theory;shape recognition;statistical analysis;hybrid shape constraint;level set;active contour model;Hausdorff distance;statistical map;reference model;evolution schema;image segmentation;Shape;Active contours;Image segmentation;Level set;Training data;Mathematical model;Robustness;Shape alignment;Active contour;Shape prior;Statistical map}, 
doi={10.1109/IST.2016.7738266}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6649582, 
author={G. H. {Alférez} and V. {Pelechano}}, 
booktitle={2013 IEEE 20th International Conference on Web Services}, 
title={Facing Uncertainty in Web Service Compositions}, 
year={2013}, 
volume={}, 
number={}, 
pages={219-226}, 
abstract={Web service compositions run in complex computing infrastructures where arising events may affect the quality of the system. However, crucial Web service compositions cannot be stopped to apply changes to deal with problematic events. Therefore, the trend is moving towards context-aware Web service compositions, which use context information as a basis for autonomic changes. Under the closed-world assumption, the context and possible adaptations are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts. In this paper, we leverage models at runtime to guide the dynamic evolution of context-aware Web service compositions to deal with unexpected events in the open world. In order to manage uncertainty, a model that abstracts the Web service composition, self-evolves to preserve requirements. The evolved model guides changes in the underlying WS-BPEL composition schema. A prototype and an evaluation demonstrate the feasibility of our approach.}, 
keywords={software fault tolerance;specification languages;ubiquitous computing;Web services;WS-BPEL composition schema;dynamic evolution;closed-world assumption;context information;context-aware Web service composition;complex computing infrastructures;Context;Web services;Context modeling;Engines;Uncertainty;Runtime;Adaptation models;Uncertainty;Web service compositions;models at runtime;dynamic evolution;open world}, 
doi={10.1109/ICWS.2013.38}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7811871, 
author={M. {Kvet} and K. {Matiasko}}, 
booktitle={2016 International Conference on Systems Informatics, Modelling and Simulation (SIMS)}, 
title={Trigger Performance Characteristics in Temporal Environment}, 
year={2016}, 
volume={}, 
number={}, 
pages={72-77}, 
abstract={Data characteristics stored in the database evolve over the time. Nowadays, it is inevitable to store all states of the objects over the time. Paradigms of conventional database are based on managing actual states, which are inappropriate. Therefore, temporal approach has been developed. In this paper, we deal with our proposed temporal solution based on attribute level architecture, which can be used effectively, if the granularity of the attribute change frequency is not the same. Another aspect of the data stored in the database is based on security, therefore, it is necessary to provide access to the data and perform changes only by privileged users. This requirement is mostly provided by the triggers. Multiple trigger characteristics influenced by individual clauses can be declared. Therefore, decision and development must be based on suitability and correctness to provide best performance. In this paper, we define clauses for triggers and compare performance.}, 
keywords={data handling;data models;database management systems;security of data;software architecture;trigger performance characteristics;temporal environment;data characteristic evolution;conventional database paradigms;attribute level architecture;data security;data access;multiple trigger characteristics;Databases;Security;Informatics;Monitoring;Servers;Fires;Data models;temporal approach;temporal attribute granularity;triggers;performance analysis}, 
doi={10.1109/SIMS.2016.16}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6946899, 
author={S. {Réjichi} and F. {Chaabane}}, 
booktitle={2014 IEEE Geoscience and Remote Sensing Symposium}, 
title={Knowledge-based approach for VHR satellite image time series analysis}, 
year={2014}, 
volume={}, 
number={}, 
pages={2177-2180}, 
abstract={As satellite data volumes are growing thanks to technological evolution, there are more needs to automatic approaches for Satellite Image Time Series (SITS) analysis. In this article, we propose a new approach based on experts knowledge for land cover monitoring. This approach helps geoscientists to overcome direct interpretation difficulties by modeling expert knowledge. As a first step, using predefined nomenclature and a prior information, concepts are set. Then, each SITS region temporal evolution, represented by a graph, is assigned to the most similar reference one in the knowledge database using the marginalized graph kernel similarity measure.}, 
keywords={geophysical image processing;land cover;time series;knowledge-based approach;VHR satellite image time series analysis;satellite data volume;technological evolution;SITS analysis automatic approach;satellite image time series analysis;land cover monitoring;direct interpretation difficulty;modeling expert knowledge;predefined nomenclature;SITS region temporal evolution;knowledge database;marginalized graph kernel similarity measure;Knowledge based systems;Satellites;Semantics;Kernel;Image segmentation;Time series analysis;Monitoring;Spatio-temporal analysis;Very High Resolution Satellite Image Time Series VHR-SITS;knowledge modeling;semantic;graph kernel}, 
doi={10.1109/IGARSS.2014.6946899}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{384768, 
author={M. {Leonard} and F. {Adreit}}, 
booktitle={Proceedings of IEEE Systems Man and Cybernetics Conference - SMC}, 
title={Strategic issues of information systems modelling}, 
year={1993}, 
volume={1}, 
number={}, 
pages={343-348 vol.1}, 
abstract={We consider the information system as a computerized materialization of the collective intelligence of the institution. From this point of view, the information system design process is a collective, progressive, learning process where evolution takes a central role. We examine the needs and the pitfalls of this process and we propound methodological bases for the design and the evolution management of an information system.<<ETX>>}, 
keywords={information systems;database management systems;learning (artificial intelligence);strategic planning;systems analysis;social aspects of automation;information systems;modelling;design process;evolution management;database management system;computerized materialization;collective intelligence;learning process;Information systems;Management information systems;Humans;Computer network management;Object oriented modeling;Process design;Information management;Computer science;Application software;Design methodology}, 
doi={10.1109/ICSMC.1993.384768}, 
ISSN={}, 
month={Oct},}
@ARTICLE{6792136, 
author={R. {Poli} and C. R. {Stephens}}, 
journal={Evolutionary Computation}, 
title={Understanding the Biases of Generalised Recombination: Part I}, 
year={2006}, 
volume={14}, 
number={4}, 
pages={411-432}, 
abstract={This is the first part of a two-part paper where we propose, model theoretically and study a general notion of recombination for fixed-length strings, where homologous recombination, inversion, gene duplication, gene deletion, diploidy and more are just special cases. The analysis of the model reveals that the notion of schema emerges naturally from the model's equations. In Part I, after describing and characterising the notion of generalised recombination, we derive both microscopic and coarse-grained evolution equations for strings and schemata and illustrate their features with simple examples. Also, we explain the hierarchical nature of the schema evolution equations and show how the theory presented here generalises past work in evolutionary computation. In Part II, the study provides a variety of fixed points for evolution in the case where recombination is used alone, which generalise Geiringer's theorem. In addition, we numerically integrate the infinite-population schema equations for some interesting problems, where selection and recombination are used together to illustrate how these operators interact. Finally, to assess by how much genetic drift can make a system deviate from the infinite-population-model predictions we discuss the results of real GA runs for the same model problems with generalised recombination, selection and finite populations of different sizes.}, 
keywords={}, 
doi={10.1162/evco.2006.14.4.411}, 
ISSN={1063-6560}, 
month={Dec},}
@INPROCEEDINGS{7973641, 
author={D. {Jakic} and V. {Jovanovic} and P. {Pocic}}, 
booktitle={2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
title={Integrating evolving MDM and EDW systems by data vault based system catalog}, 
year={2017}, 
volume={}, 
number={}, 
pages={1401-1406}, 
abstract={The paper presents results of a research on integration of enterprise data warehouse (EDW) and a master data management (MDM) system. The primary goal was solving a schema evolution problem, and the corner stone of our approach was utilization of a data vault modeling of an integrated meta-model of EDW and MDM as an expansion of a traditional relational database system catalog. The main contributions of this paper are: a) common integration architecture, b) new system catalog based on a meta-model for DW and MDM integration, and c) research prototype used for empirical validation of the effectiveness of the proposed solution.}, 
keywords={cataloguing;data warehouses;relational databases;evolving MDM;EDW systems;data vault based system catalog;enterprise data warehouse;master data management;MDM system;schema evolution problem solving;data vault modeling;integrated metamodel;traditional relational database system catalog;integration architecture;Data models;Satellites;Data warehouses;History;Organizations;Prototypes}, 
doi={10.23919/MIPRO.2017.7973641}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7791889, 
author={Y. {Nishimura} and T. {Amagasa} and Y. {Inagaki} and T. {Hashimoto} and H. {Kitagawa}}, 
booktitle={2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)}, 
title={A System for Phylogenetic Analyses over Alignments of Next Generation Sequence Data}, 
year={2016}, 
volume={}, 
number={}, 
pages={230-237}, 
abstract={A large quantity of DNA sequence data is being generated at high speed and in low cost by next generation sequencing (NGS) technology in recent years. NGS influences wide range of biology, including evolutionary biology, which is the field that infers the evolutionary relationships of genes or organisms from sequence data. In particular, the phylogenetic analyses using massive amount of data generated by NGS have been actively conducted. To infer the phylogenetic relationship, a number of alignments that comprise sets of sequences need to be maintained, i.e., they need to be updated whenever new sequence data become available. However, there have been no database that support updates of alignments, i.e., addition and/or removal of sequences from existing alignments. Instead, individual researchers independently update their alignments manually. To cope with this problem, we propose a system for phylogenetic analyses over alignments of NGS data. It takes as input NGS data, predicts the orthologue that each sequence belongs to, and updates the alignments. Moreover, by describing the related alignments in tree structure, it can maintain stored alignments in a systematic way. To prove the concept, we implement a prototype web application. We expect that our system help biological researchers carry out phylogenetic analysis on large scale data including those from NGS).}, 
keywords={bioinformatics;data analysis;DNA;genetics;Internet;phylogenetic analysis;next generation sequence data;NGS technology;DNA sequence data alignment;prototype Web application;Organisms;Phylogeny;Databases;Sequential analysis;Bioinformatics;DNA;Alignment database;Bioinformatics;Evolutionary biology;Phylogeny;Next generation sequencing}, 
doi={10.1109/CISIS.2016.51}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6240908, 
author={I. {Zouari Turki} and F. {Ghozzi Jedidi} and R. {Bouaziz}}, 
booktitle={2012 Proceedings of the 35th International Convention MIPRO}, 
title={Constraints to manage consistency in multiversion data warehouse}, 
year={2012}, 
volume={}, 
number={}, 
pages={1607-1612}, 
abstract={Defining structural constraints in multidimensional models is certainly a necessity to ensure the consistency of such models, especially the hierarchical structure of dimensions. This necessity increases when considering temporal and multiversion multidimensional models due to their temporal extension. Our proposed model for managing multiversion data warehouses (MV-DW) supports several structural and temporal constraints. In this paper, we enrich these constraints by the temporal fact-dimension dependency constraint which deals with relationship between fact and dimensions in a temporal context. Besides, schema and instance evolution operators can introduce inconsistencies in MV-DW schema and/or data. Such inconsistencies can be avoided by checking the defined MV-DW constraints. To this end, we define two classes of technical issues. The first one deals with algorithms that have to be run following to schema evolution operators to check the consistency of the changed DW schema. The second issue deals with triggers that have to cancel data propagation when detecting any instance constraint violation.}, 
keywords={data models;data warehouses;consistency management;multiversion data warehouse;structural constraints;hierarchical structure;multiversion multidimensional models;temporal constraints;temporal fact-dimension dependency constraint;instance evolution operators;MV-DW schema;Context;Marketing and sales;Data warehouses;Data models;Registers;Databases;Classification algorithms}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{5339986, 
author={H. {Chen} and D. H. {Summerville} and Y. {Chen}}, 
booktitle={2009 7th International Workshop on Design of Reliable Communication Networks}, 
title={Two-stage decomposition of SNORT rules towards efficient hardware implementation}, 
year={2009}, 
volume={}, 
number={}, 
pages={359-366}, 
abstract={The performance gap between the execution speed of security software and the amount of data to be processed is ever widening. A common solution is to close the performance gap through hardware implementation of security functions. However, continuously expanding signature databases have become a major impediment to achieving scalable hardware based pattern matching. Additionally, evolutionary rule databases have necessitated real time online updating for reconfigurable hardware implementations. Based on the observation that signature patterns are constructed from combinations of a limited number of primary patterns, we propose to decompose the Snort signature patterns. These smaller primary pattern sets can be stored along with their associations to allow dynamic signature pattern reconstruction. Not only does the matching operation potentially become more scalable, but the real time online updating task is simplified. The approach is verified with patterns from the latest version of the Snort rule database. The experimental results show that after decomposition, a reduction in size of over 77% can be achieved on Snort signature patterns.}, 
keywords={digital signatures;field programmable gate arrays;pattern matching;reconfigurable architectures;security software;signature database;pattern matching;evolutionary rule database;Snort signature pattern;dynamic signature pattern reconstruction;Snort rule database;network intrusion detection system;reconfigurable hardware;FPGA;Hardware;Decision support systems;Virtual reality;Network Intrusion Detection Systems (NIDS);Security;Finite State Machine;Scalability;Decompose;FPGAs}, 
doi={10.1109/DRCN.2009.5339986}, 
ISSN={}, 
month={Oct},}
@ARTICLE{5387694, 
author={P. G. {Selinger}}, 
journal={IBM Systems Journal}, 
title={Database technology}, 
year={1987}, 
volume={26}, 
number={1}, 
pages={96-106}, 
abstract={Computers were originally invented and used to ease and automate the task of computation. As the word computer implies, these early machines were used for calculations, such as tabulating census data. As a side effect, the technology needed for storing data was also invented to provide the computational engine with input data and allow it to output results. This means of permanently storing data included punched cards, tape, and disks. Throughout the 1950s and most of the 1960s, the management of stored data was done as required; file systems stored data according to user-defined formats and kept a table of contents. Users shared data by equally hoc means, generally by taking turns accessing the same device. Over the years, database technology has evolved through at least three generations to a diverse and sophisticated set of data management tools, as discussed in this paper. This paper has three major sections. Presented first is an introduction to database technology. Presented next is a description of the evolution of database technology from early computing to the sophisticated systems of today. The third section presents a view of both the driving forces that will influence the database technology of the future and also the resulting new directions for the future.}, 
keywords={}, 
doi={10.1147/sj.261.0096}, 
ISSN={0018-8670}, 
month={},}
@INPROCEEDINGS{5413556, 
author={L. {Canini} and S. {Benini} and P. {Migliorati} and R. {Leonardi}}, 
booktitle={2009 16th IEEE International Conference on Image Processing (ICIP)}, 
title={Emotional identity of movies}, 
year={2009}, 
volume={}, 
number={}, 
pages={1821-1824}, 
abstract={In the field of multimedia analysis, attempts that lead to an emotional characterization of content have been proposed. In this work we aim at defining the emotional identity of a feature movie by positioning it into an emotional space, as if it was a piece of art. The multimedia content is mapped into a trajectory whose coordinates are connected to filming and cinematographic techniques used by directors to convey emotions. The trajectory evolution over time provides a strong characterization of the movie, by locating different movies into different regions of the emotional space. The ability of this tool in characterizing content has been tested by retrieving emotionally similar movies from a large database, using IMDb genre classification for the evaluation of results.}, 
keywords={cinematography;emotion recognition;image classification;multimedia computing;video retrieval;movies;multimedia analysis;emotional characterization;emotional identity;cinematographic technique;trajectory evolution;database;IMDb genre classification;video retrieval;Motion pictures;Multimedia systems;Information retrieval;Multimedia databases;Art;Trajectory;Testing;Content based retrieval;Hidden Markov models;Computer industry;Emotional identity;Video retrieval}, 
doi={10.1109/ICIP.2009.5413556}, 
ISSN={1522-4880}, 
month={Nov},}
@ARTICLE{6792333, 
author={R. {Poli} and C. R. {Stephens}}, 
journal={Evolutionary Computation}, 
title={Understanding the Biases of Generalised Recombination: Part II}, 
year={2007}, 
volume={15}, 
number={1}, 
pages={95-131}, 
abstract={This is the second part of a two-part paper where we propose, model theoretically and study a general notion of recombination for fixed-length strings where homologous recombination, inversion, gene duplication, gene deletion, diploidy and more are just special cases. In Part I, we derived both microscopic and coarse-grained evolution equations for strings and schemata for a selecto-recombinative GA using generalised recombination, and we explained the hierarchical nature of the schema evolution equations. In this part, we provide a variety of fixed points for evolution in the case where recombination is used alone, thereby generalising Geiringer's theorem. In addition, we numerically integrate the infinite-population schema equations for some interesting problems, where selection and recombination are used together to illustrate how these operators interact. Finally, to assess by how much genetic drift can make a system deviate from the infinite-population-model predictions we discuss the results of real GA runs for the same model problems with generalised recombination, selection and finite populations of different sizes.}, 
keywords={}, 
doi={10.1162/evco.2007.15.1.95}, 
ISSN={1063-6560}, 
month={March},}
@ARTICLE{710971, 
author={R. {Jones} and L. {Mapelli} and Y. {Ryabov} and I. {Soloviev}}, 
journal={IEEE Transactions on Nuclear Science}, 
title={The OKS persistent in-memory object manager}, 
year={1998}, 
volume={45}, 
number={4}, 
pages={1958-1964}, 
abstract={The OKS (Object Kernel Support) is a library to support a simple, active persistent in-memory object manager. It is suitable for applications which need to create persistent structured information with fast access but do not require full database functionality. It can be used as the frame of configuration databases and real-time object managers for Data Acquisition and Detector Control Systems in such fields as setup, diagnostics and general configuration description. OKS is based on an object model that supports objects, classes, associations, methods, inheritance, polymorphism, object identifiers, composite objects, integrity constraints, schema evolution, data migration and active notification. OKS stores the class definitions and their instances in portable ASCII files. It provides query facilities, including indices support. The OKS has a C++ API (Application Program Interface) and includes Motif based GUI applications to design class schema and to manipulate objects. OKS has been developed on top of the Rogue Wave Tools h++ C++ class library.}, 
keywords={object-oriented databases;software libraries;data acquisition;inheritance;high energy physics instrumentation computing;OKS persistent in-memory object manager;Object Kernel Support;persistent structured information;configuration databases;real-time object managers;objects;classes;associations;methods;inheritance;polymorphism;object identifiers;composite objects;integrity constraints;schema evolution;data migration;active notification;class definitions;portable ASCII files;query facilities;Motif based GUI applications;Data acquisition;Prototypes;Relational databases;Object oriented modeling;Distributed databases;Object oriented databases;Nuclear physics;Libraries;Real time systems;Knowledge management}, 
doi={10.1109/23.710971}, 
ISSN={0018-9499}, 
month={Aug},}
@ARTICLE{485848, 
author={P. {Desfray}}, 
journal={Computer}, 
title={Automated object design: the client-server case}, 
year={1996}, 
volume={29}, 
number={2}, 
pages={62-66}, 
abstract={As more enterprises move to client-server environments, object technology is gaining favour as a means of migrating legacy applications. The object paradigm is well suited for modeling real world business processes and has the flexibility to integrate emerging technologies. However, the most difficult aspect of object-oriented applications development is not programming but technical design. This article explores a methodology that formalizes and automates object-based technical design in the domain of information management systems.}, 
keywords={object-oriented programming;systems re-engineering;business data processing;relational databases;client-server systems;object design;client-server systems;object technology;legacy applications;business process model;object-oriented applications development;technical design;object-oriented programming;information management systems;relational database;system reengineering;Computer aided software engineering;Object oriented modeling;Analytical models;Application software;Information systems;Large-scale systems;Information management;Object oriented programming;Design methodology;Relational databases}, 
doi={10.1109/2.485848}, 
ISSN={0018-9162}, 
month={Feb},}
@INPROCEEDINGS{5474093, 
author={Z. {Peng} and H. {Wang} and Y. {Peng} and B. {Xu} and Z. {Huang}}, 
booktitle={2010 12th International Asia-Pacific Web Conference}, 
title={A Three Layer System Architecture for Web-Based Unstructured Data Management}, 
year={2010}, 
volume={}, 
number={}, 
pages={447-450}, 
abstract={With the rapid growing of data on Web, we are facing three serious problems. Firstly, there are a huge number of data resources which are heterogeneous and dynamic.Secondly, most of data on Web are unstructured. Thirdly, there are various kinds of Web users who have different interests and requirements. In this paper, we proposed a new system architecture for unstructured data management on Web to solve these problems by integrating data spaces, database and meta search engine. The system architecture consists of three layers for data gathering on demand, dynamic management and personalized service, respectively. Data servicing layer allows Web users to create data spaces with advanced functions to manipulate and access Web data, eg, cross media query and automatic recommendation. Data managing layer models both Web data and their semantic relationships using our object deputy database named as TOTEM; it also supports schema evolution and dynamic classification. Data gathering layer extracts user's interest from his or her data space; gathers the related data on Web and further analyzes their semantic relationships. Finally, we implemented a prototype system Tmusic based on this new system architecture to show its availability.}, 
keywords={data handling;Internet;meta data;search engines;unstructured data management;Web users;data spaces integration;meta search engine;dynamic management;data servicing layer;object deputy database;TOTEM database;three layer system architecture;Databases;Search engines;Service oriented architecture;Metasearch;Data mining;Computer architecture;Prototypes;Data analysis;Conference management;Engineering management}, 
doi={10.1109/APWeb.2010.21}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{1053103, 
author={J. {Palma} and R. {Marin} and M. {Campos} and A. {Carceles}}, 
booktitle={Proceedings of the Second Joint 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society] [Engineering in Medicine and Biology}, 
title={ACUDES: architecture for intensive care units decision support}, 
year={2002}, 
volume={3}, 
number={}, 
pages={1938-1939 vol.3}, 
abstract={The development of data-intensive systems in medical domains has received increasing attention in recent years. In this work we present ACUDES (Architecture for Intensive Care Units Decision Support) in which we have combined traditional techniques for managing and representing time, together with a temporal diagnosis task in a decision support platform. ACUDES has been designed to manage the patient's temporal evolution data base and to describe the patient's evolution in terms of the temporal sequence diseases suffered. These functionalities are supported by an ontology which simplifies the knowledge acquisition and sharing and guarantees the semantic consistency of the patient's data.}, 
keywords={patient care;decision support systems;health care;computer architecture;medical diagnostic computing;diseases;temporal information management;ontologies;temporal diagnosis;data semantic consistency;data-intensive systems development;patient's evolution;temporal sequence diseases;ACUDES;architecture for intensive care units decision support;Ontologies;Diseases;Terminology;Artificial intelligence;Knowledge engineering;Medical diagnostic imaging;Graphical user interfaces;Unified modeling language;Biomedical engineering;Knowledge acquisition}, 
doi={10.1109/IEMBS.2002.1053103}, 
ISSN={1094-687X}, 
month={Oct},}
@INPROCEEDINGS{4269814, 
author={K. C. {Davis} and S. {Banerjee}}, 
booktitle={24th British National Conference on Databases (BNCOD'07)}, 
title={Teaching and Assessing a Data Warehouse Design Course}, 
year={2007}, 
volume={}, 
number={}, 
pages={22-31}, 
abstract={This paper describes a course in data warehouse design that includes conceptual, logical, and physical design research. The instructional modes and assessment techniques are described and illustrated. The scope of the course is given via a discussion of the course bibliography; the bibliography includes a workshop report, "Data Warehousing at the Crossroads," created at a week-long international meeting by 18 co-authors. The paper concludes with discussion of a data warehouse conceptual modeling and schema evolution software tool that can be used to supplement the course materials. The tool is part of a larger research effort and we anticipate that it will have educational value for building student intuition and visualising the effects of schema changes.}, 
keywords={computer aided instruction;computer science education;data warehouses;software tools;data warehouse design course;instructional modes;assessment techniques;course bibliography;schema evolution software tool;Education;Data warehouses;Relational databases;Distributed databases;Object oriented databases;Object oriented modeling;Bibliographies;Springs;Unified modeling language;Writing}, 
doi={10.1109/BNCOD.2007.18}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8327078, 
author={C. R. {Valencio} and J. C. {de Freitas} and R. C. {Gratao de Souza} and L. A. {Neves} and G. F. {Donega Zafalon} and A. C. {Colombini} and W. {Tenorio}}, 
booktitle={2017 18th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)}, 
title={An Efficient Parallel Optimization for Co-Authorship Network Analysis}, 
year={2017}, 
volume={}, 
number={}, 
pages={127-134}, 
abstract={Co-authorship analysis in science and technology partnerships provides a vision of cooperation patterns between individuals and organizations and is still widely used to understand and assess scientific collaboration patterns. This analysis is conducted by means of bibliometry, which is the quantitative study of scientific production. However, with the evolution of database management systems, there was a significant increase in the volume of stored data, which could difficult the analysis. In this context, the developed work presents an efficient parallel optimization of bibliometric information, in order to allow this scientific analysis in a Big Data environment. Our results show that the time taken to calculate the transitivity value using the sequential approach grows 4.08 times faster than the parallel proposed approach when the number of nodes tends to infinity; the time taken to calculate the average distance and diameter values using the sequential approach grows 5.27 times faster than the parallel proposed approach when the number of nodes tends to infinity. Also, the results found present good values of speed up and efficiency.}, 
keywords={Big Data;citation analysis;data analysis;database management systems;information analysis;parallel algorithms;scientific information systems;social networking (online);storage management;Co-Authorship Network Analysis;Co-authorship analysis;cooperation patterns;scientific production;database management systems;scientific analysis;Big Data environment;parallel optimization;collaboration patterns;data storage;Collaboration;Databases;Bibliometrics;Production;Big Data;Measurement;Data mining;bibliometrics;graphs;knowledge extraction;coauthorship network;big data;NoSQL}, 
doi={10.1109/PDCAT.2017.00030}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{614047, 
author={R. {McClatchey} and F. {Estrella} and J. -. {Le Goff} and Z. {Kovacs} and N. {Baker}}, 
booktitle={Proceedings of the Third Basque International Workshop on Information Technology - BIWIT'97 - Data Management Systems}, 
title={Object databases in a distributed scientific workflow application}, 
year={1997}, 
volume={}, 
number={}, 
pages={11-21}, 
abstract={The construction of large scientific experiments (such as the Compact Muon Solenoid experiment being undertaken for CERN) necessitates the use of complex production management operations, often distributed over many geographically separated research institutes. These workflow operations are often only loosely-defined at the outset of the construction and can evolve rapidly as the results of experiments are analysed. Existing commercial workflow management systems are largely based on relational database management systems (RDBMSs) and are unable to cope with the requirements of such scientific workflow environments and, in particular, with the dynamic schema evolution which results from the rapidly evolving scientific workflow definitions. This paper reports on the requirements for object repositories in the implementation of a prototype scientific workflow management system entitled CRISTAL (Cooperating Repositories and an Information System for Tracking Assembly Lifecycles) and considers issues surrounding data duplication between object repositories and scientific workflow management in CRISTAL.}, 
keywords={scientific information systems;object-oriented databases;distributed databases;high energy physics instrumentation computing;natural sciences;production engineering computing;nuclear electronics;object databases;distributed scientific workflow application;Compact Muon Solenoid experiment;complex production management operations;geographically separated research institutes;scientific workflow management system;dynamic schema evolution;object repositories;CRISTAL;cooperating repositories;information system;assembly lifecycle tracking;data duplication;Distributed databases;Workflow management software;Mesons;Solenoids;Production management;Relational databases;Environmental management;Prototypes;Management information systems;Assembly systems}, 
doi={10.1109/BIWIT.1997.614047}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{4283687, 
author={M. S. {Bouguelid} and M. S. {Mouchaweh} and P. {Billaudel}}, 
booktitle={2007 11th International Conference on Intelligent Engineering Systems}, 
title={Adaptive and Predictive Diagnosis Based on Pattern Recognition}, 
year={2007}, 
volume={}, 
number={}, 
pages={139-144}, 
abstract={Systems work in either normal or abnormal functioning modes. Pattern Recognition (PR) is a set of methods used to classify a pattern into one of a set of predefined classes. Each class is associated to a functioning mode. If the pattern is considered as the observation of the actual functioning mode, then the diagnosis by PR is realized by deciding the class of the actual pattern. PR is particularly adapted to realize the diagnosis when the prior information about system functioning modes is not sufficient to construct an analytical or structural model of the system functioning. However this knowledge is often incomplete because it cannot contain information about all system functioning modes. Thus the diagnosis method must be adaptive to include into its database all the new functioning modes. In addition, the diagnosis must be predictive in order to follow the evolution of the system from one mode to another one. We use the supervised classification method Fuzzy Pattern Matching (FPM) to realize the diagnosis of non-evolutionary systems with a complete database. Indeed, FPM cannot realize the adaptive and predictive diagnosis. Therefore, a solution to this problem is proposed in this paper. The performance of the proposed approach is illustrated using different examples.}, 
keywords={fuzzy set theory;learning (artificial intelligence);pattern classification;pattern matching;system monitoring;pattern recognition;predictive diagnosis;adaptive diagnosis;abnormal functioning mode;normal functioning mode;predefined class set;fuzzy pattern matching;supervised classification method;non evolutionary system;database management system;Pattern recognition;Databases;Information analysis;Analytical models;Fuzzy systems;Pattern matching;Design methodology;Particle measurements;Sensor phenomena and characterization;Acoustic sensors}, 
doi={10.1109/INES.2007.4283687}, 
ISSN={1543-9259}, 
month={June},}
@INPROCEEDINGS{4092226, 
author={S. {Rinderle} and U. {Kreher} and M. {Lauer} and P. {Dadam} and M. {Reichert}}, 
booktitle={15th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE'06)}, 
title={On Representing Instance Changes in Adaptive Process Management Systems}, 
year={2006}, 
volume={}, 
number={}, 
pages={297-304}, 
abstract={By separating the process logic from the application code process management systems (PMS) offer promising perspectives for automation and management of business processes. However, the added value of PMS strongly depends on their ability to support business process changes which can affect the process type as well as the process instance level. This does not only impose challenging conceptual issues (e.g., correctness of process schemata after changes) but also requires sophisticated implementation concepts with respect to efficient algorithms, flexible architectures, and reasonable treatment of resources. In this paper we sketch the general implementation concepts for representing process type and process instance data as well as for realizating process schema evolution. All these concepts have been developed and are currently implemented in the ADEPT2 prototype within the AristaFlow project}, 
keywords={business data processing;adaptive business process management systems;business process logic;business process instance data;business process schema evolution;Adaptive systems;Prototypes;Resource management;Logic;Automation;Electric breakdown;Engines;Technology management;Companies;Control systems}, 
doi={10.1109/WETICE.2006.51}, 
ISSN={1524-4547}, 
month={June},}
@INPROCEEDINGS{8487265, 
author={J. {Wu} and Q. {Zhang} and L. {Yu} and W. {Ma} and Y. {Wu} and S. {Deng} and H. {Huang}}, 
booktitle={2017 4th Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)}, 
title={Multi-typed Community Discovery in Dynamic Heterogeneous Information Networks through Tensor Method}, 
year={2017}, 
volume={}, 
number={}, 
pages={59-65}, 
abstract={Community discovery in a dynamic heterogeneous information network is a challenging topic and quite more difficult than that in a traditional static homogeneous information network. Community in heterogeneous information network, named multi-typed community, contains multiple types of dynamic objects and links, which brings three challenges. Firstly, the multi-typed communities are heterogeneous. Secondly, The communities are constantly changing along time. Finally, the network schemas for different heterogeneous information networks are various. To overcome these challenges, we propose a multi-typed community discovery method for dynamic heterogeneous information networks through tensor method without the restriction of network schema. A tensor decomposition framework is designed to model the multi-typed community and address the community evolution. Experimental result on a real-world dataset demonstrates the efficiency of our framework.}, 
keywords={information networks;network theory (graphs);tensors;network schema;community evolution;dynamic heterogeneous information network;tensor method;multityped community discovery method;static homogeneous information network;tensor decomposition framework;Tensile stress;Heuristic algorithms;Computer science;Semantics;Stochastic processes;Optimization;Linear programming;community-discovery;tensor-decomposition;multi-typed-community;heterogeneous-information-network;dynamic-network}, 
doi={10.1109/APWConCSE.2017.00019}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7821807, 
author={D. {Plase} and L. {Niedrite} and R. {Taranovs}}, 
booktitle={2016 IEEE 4th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)}, 
title={Accelerating data queries on Hadoop framework by using compact data formats}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={There are massive amounts of data generated from IoT, online transactions, click streams, emails, logs, posts, social networking interactions, sensors, mobile phones and their applications etc. The question is where and how to store these data in order to provide faster data access. Understanding and handling Big Data is a big challenge. The research direction in Big Data projects using Hadoop Technology, MapReduce kind of framework and compact data formats such as RCFile, SequenceFile, ORC, Avro, Parquet shows that only two data formats (Avro and Parquet) support schema evolution and compression in order to utilize less storage space. In this paper, file formats like Avro and Parquet are compared with text formats to evaluate the performance of the data queries. Different data query patterns have been evaluated. Cloudera's open-source Apache Hadoop distribution CDH 5.4 has been chosen for the experiments presented in this article. The results show that compact data formats (Avro and Parquet) take up less storage space when compared with plain text data formats because of binary data format and compression advantage. Furthermore data queries from the column based data format Parquet are faster when compared with text data formats and Avro.}, 
keywords={data compression;data handling;query processing;data query acceleration;Hadoop framework;compact data formats;data storage;data access;Big Data;MapReduce framework;RCFile;SequenceFile;ORC;Avro;Parquet;schema evolution;text formats;data query patterns;Cloudera open-source Apache Hadoop distribution CDH 5;binary data format;compression advantage;Big data;Memory;Java;Protocols;Distributed databases;Business;Big Data;Hadoop;HDFS;Hive;Avro;Parquet}, 
doi={10.1109/AIEEE.2016.7821807}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{659898, 
author={ and and N. {Rishe}}, 
booktitle={1998 IEEE International Performance, Computing and Communications Conference. Proceedings (Cat. No.98CH36191)}, 
title={Performance comparison of three alternatives of distributed multidatabase systems: a global query perspective}, 
year={1998}, 
volume={}, 
number={}, 
pages={53-59}, 
abstract={Diversity and evolution in database applications often result in a multidatabase environment in which corporate data are stored in multiple distributed data sources, each managed by an independent database management system. One of the essential functions of a multidatabase system is to provide inter database access: the capability of evaluating global queries that require access to multiple data sources. The paper compares three common relational multidatabase approaches: the federated approach, the gateway approach, and the middleware approach from the perspective of global query performance. In particular, we examine their architectural impact on the applicability of pipelined query processing techniques and load balancing. We present a performance comparison based on a detailed simulation. The study suggests that the middleware approach, which is the most cost effective solution among the three, provides better or comparable performance to the other two approaches.}, 
keywords={distributed databases;query processing;relational databases;client-server systems;pipeline processing;resource allocation;software performance evaluation;performance comparison;distributed multidatabase systems;global query perspective;multidatabase environment;corporate data;multiple distributed data sources;independent database management system;inter database access;global queries;multiple data source access;relational multidatabase approaches;federated approach;gateway approach;middleware approach;global query performance;architectural impact;pipelined query processing techniques;load balancing;cost effective solution;Middleware;Network servers;Query processing;Databases;Collaborative software;Software performance;Costs;Assembly;Distributed computing;Computer architecture}, 
doi={10.1109/PCCC.1998.659898}, 
ISSN={1097-2641}, 
month={Feb},}
@INPROCEEDINGS{1423306, 
author={}, 
booktitle={Proceedings. IEEE SoutheastCon, 2005.}, 
title={Content-based retrieval using a multi-objective genetic algorithm}, 
year={2005}, 
volume={}, 
number={}, 
pages={561-569}, 
abstract={Content-based retrieval from multimedia databases is an important multimedia research area where traditional keyword-based approaches are not adequate. Multimedia data is significantly different from alphanumeric data because multimedia data is generally meaningless to a human and multimedia objects are typically large. Moreover, the traditional keyword-based approaches require an enormous amount of human effort during manual annotation and maintaining the consistency of annotations throughout database evolution. Research on content-based retrieval focus on using low-level features like color and texture for image representation, and a geometric framework of distances in the feature space for similarity. However, systematic retrieval of the best matches in a large multimedia database requires exhaustive and exponential search and does not guarantee worst-case performance. In addition, it has been observed that certain image representation schemes perform better than others under certain query situations, and these schemes should be somehow integrated and adjusted on the fly to facilitate effective and efficient image retrieval. Some work has been done applying simple genetic algorithms for content-based retrieval to provide good, but not necessary optimal solutions. However, these simple genetic algorithms can find only one optimum solution in a single run. This research proposes a new content-based retrieval method based on a multi-objective genetic algorithm (MOGA), which is capable of finding multiple trade-off solutions in one run and providing a natural way for integrating multiple image representation schemes. This research focuses on structural similarity framework that addresses topological, directional and distance relations of image objects.}, 
keywords={image retrieval;visual databases;content-based retrieval;image representation;genetic algorithms;multimedia databases;query formulation;content-based retrieval;multi-objective genetic algorithm;multimedia databases;keyword-based approaches;multimedia data;alphanumeric data;multimedia objects;human effort;manual annotation;database evolution;annotation consistency;low-level features;image representation;image texture;image color;geometric distance framework;feature space;similarity;systematic retrieval;exponential search;worst-case performance;query situations;image retrieval;MOGA;trade-off solutions;multiple image representation scheme integration;structural similarity framework;topological relations;directional relations;distance relations;image objects;Content based retrieval;Genetic algorithms;Multimedia databases;Information retrieval;Image retrieval;Humans;Image representation;Prototypes;Multimedia systems;Performance evaluation}, 
doi={10.1109/SECON.2005.1423306}, 
ISSN={1091-0050}, 
month={April},}
@INPROCEEDINGS{638369, 
author={G. {Fiol-Roig} and M. {Ferrer-Gili}}, 
booktitle={1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation}, 
title={Expert system for supervision of real time control processes}, 
year={1997}, 
volume={2}, 
number={}, 
pages={1966-1971 vol.2}, 
abstract={The general task of an expert system for real time monitoring and supervision consists of evaluating the quality of the current situation of a dynamic system and acting consequently, or proposing an action upon the system to an operator, so that its future behaviour leads to an acceptable situation. The particular functions of such an expert system are reduced to the monitoring and supervision processes. While some software packages allow the monitoring function to be made practical, real time supervision of dynamic systems arises as an attempt to aid the human operator in developing its functions, constituting the main way to guarantee the quality of the system response. Guaranteeing the constraints about the response time of the expert system demands adequate information in an optimal state of processing about the functions of the control system. A generic expert system model to implement the monitoring and supervision functions based on inductive learning techniques, together with a data structure to store information from the expert and the control systems, called evolutionary database for supervision, are presented.}, 
keywords={process control;expert systems;intelligent control;computerised monitoring;real-time systems;deductive databases;knowledge acquisition;real time control;monitoring;supervisory control;expert system;dynamic system;system response;inductive learning;data structure;evolutionary database;process control;knowledge acquisition;Expert systems;Real time systems;Process control;Control systems;Monitoring;Software packages;Humans;Delay;Optimal control;Data structures}, 
doi={10.1109/ICSMC.1997.638369}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{5946750, 
author={X. {Dong} and P. {Frossard} and P. {Vandergheynst} and N. {Nefedov}}, 
booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={A regularization framework for mobile social network analysis}, 
year={2011}, 
volume={}, 
number={}, 
pages={2140-2143}, 
abstract={Mobile phone data provides rich dynamic information on human activities in social network analysis. In this paper, we represent data from two different modalities as a graph and functions defined on the vertex set of the graph. We propose a regularization framework for the joint utilization of these two modalities of data, which enables us to model evolution of social network information and efficiently classify relationships among mobile phone users. Simulations based on real world data demonstrate the potential application of our model in dynamic scenarios, and present competitive results to baseline methods for combining multimodal data in the learning and clustering communities.}, 
keywords={mobile computing;social networking (online);regularization framework;mobile social network analysis;mobile phone data;model evolution;social network information;multimodal data;Bluetooth;Mobile handsets;Mobile communication;Social network services;Global Positioning System;Data mining;Mobile computing;Multimodal data;regularization on graphs;classification and clustering}, 
doi={10.1109/ICASSP.2011.5946750}, 
ISSN={2379-190X}, 
month={May},}
@INPROCEEDINGS{5568322, 
author={ and and and and }, 
booktitle={2010 The 2nd Conference on Environmental Science and Information Application Technology}, 
title={Dynamic changes and its factor analysis of urban wetlands in Wuhan, China from 1950s to 2005 year}, 
year={2010}, 
volume={3}, 
number={}, 
pages={401-405}, 
abstract={Urban wetlands play a significant role in the sustainable development. But the effect of human action on urban wetlands has been expanded in greater scope and depth, which may cause the accelerated degeneration of urban wetlands seriously. Based on the historical data of land using maps, topographic maps and TM images of Wuhan from 1930s to 2005, which were through the Man-Machine Interaction in ArcGIS System and ERDAS IMAGINE, and the field data through the fieldwork and questionnaire research in 2009 and 2010, the urban wetland database from 1930s to 2005 in Wuhan had been established. Then, the spatio-temporal evolution of urban wetlands was quantitatively and qualitatively analyzed. Urban wetlands have been degenerating seriously, and the tendency of its degeneration is all but irreversible. The results showed that: (1) the area of natural urban wetlands decreased: lake wetlands decreased by 55% from 1930s to 2005; (2) pollution state of urban wetlands worsened: no lakes reached Grade I, and the remaining 65.4% exceeded Grade V; (3) decline in biodiversity had been becoming worse and worse; (4) the urban wetland landscape fragmentation had augmented a lot. Based on the statistic data of socio-economic factors from 1930s to 2005, we analyzed dynamic changes of urban wetlands, and by making use of the correlation analysis of SPSS, analyzed the direct factors consisting of agriculture-inning, engineering construction, resource utilization and biological invasion, and the basic factors consisting of population explosion, urbanization, industrialization, mass consciousness and public policy.}, 
keywords={ecology;geographic information systems;lakes;spatiotemporal phenomena;sustainable development;topography (Earth);water pollution;water quality;sustainable development;topographic maps;TM images;AD 1930 to 2005;Man-Machine Interaction;ArcGIS System;ERDAS IMAGINE;field data;AD 2009;AD 2010;urban wetland database;spatiotemporal evolution;natural urban wetlands;lake wetlands;biodiversity;urban wetland landscape fragmentation;statistic data;socio-economic factors;correlation analysis;SPSS;agriculture-inning;engineering construction;resource utilization;biological invasion;population explosion;public policy;degradation mechanism;Wuhan city;China;urbanization;industrialization;Lakes;Artificial neural networks;Temperature control;Temperature;Biological system modeling;Organisms;urban wetlands;degradation mechanism;factor analysis;Wuhan city}, 
doi={10.1109/ESIAT.2010.5568322}, 
ISSN={}, 
month={July},}
@ARTICLE{4663866, 
author={H. {Greenspan} and S. {Gordon} and G. {Zimmerman} and S. {Lotenberg} and J. {Jeronimo} and S. {Antani} and R. {Long}}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Automatic Detection of Anatomical Landmarks in Uterine Cervix Images}, 
year={2009}, 
volume={28}, 
number={3}, 
pages={454-468}, 
abstract={The work focuses on a unique medical repository of digital cervicographic images (ldquoCervigramsrdquo) collected by the National Cancer Institute (NCI) in longitudinal multiyear studies. NCI, together with the National Library of Medicine (NLM), is developing a unique Web-accessible database of the digitized cervix images to study the evolution of lesions related to cervical cancer. Tools are needed for automated analysis of the cervigram content to support cancer research. We present a multistage scheme for segmenting and labeling regions of anatomical interest within the cervigrams. In particular, we focus on the extraction of the cervix region and fine detection of the cervix boundary; specular reflection is eliminated as an important preprocessing step; in addition, the entrance to the endocervical canal (the ldquoosrdquo), is detected. Segmentation results are evaluated on three image sets of cervigrams that were manually labeled by NCI experts.}, 
keywords={biological organs;cancer;edge detection;feature extraction;gynaecology;image segmentation;medical image processing;tumours;automatic anatomical landmark detection;uterine cervix images;digital cervicographic images;cervigrams;National Cancer Institute;NCI;National Library of Medicine;NLM;Web-accessible database;lesion evolution;cervical cancer;image segmentation;regions-of-anatomical interest labeling;cervix boundary detection;specular reflection;image preprocessing step;endocervical canal;cervix region extraction;Cervical cancer;Biomedical imaging;Medical diagnostic imaging;Biomedical engineering;Cancer detection;Image segmentation;Software libraries;Medical treatment;Image databases;Lesions;Cervical cancer;curvature features;image segmentation;landmark extraction;medical image analysis;Algorithms;Cervix Uteri;Cervix Uteri;Female;Humans;Image Interpretation, Computer-Assisted;Image Processing, Computer-Assisted;Information Storage and Retrieval;Normal Distribution;Pattern Recognition, Automated;Sensitivity and Specificity;Uterine Cervical Neoplasms}, 
doi={10.1109/TMI.2008.2007823}, 
ISSN={0278-0062}, 
month={March},}
@INPROCEEDINGS{8531309, 
author={I. {Codreanu} and A. M. {Florea} and I. {Mocanu}}, 
booktitle={2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
title={A Home Based Health-Care Solution for Older Adults Using Kinect}, 
year={2017}, 
volume={}, 
number={}, 
pages={352-355}, 
abstract={The gradual decline in physical activity in older people may lead to multiple affections that sometimes translate into functional disability and need for assistance and home treatments. This paper proposes a self-management health-care game solution for kinesiology issues that aims to increase the quality of life in Ambient Assisted Living (AAL) using Kinect. The presented story line of the game aims to minimize the physic effort by offering a motivating virtual world where the patient performs kinesiology exercises. The kinesiology exercises are represented by successive body members in 3-dimensional graphics using gesture detection with Microsoft Kinect. The raw data obtained from the avatar interactions with the Kinect sensor is stored in the system database and processed to evolution and progress measurements through the amplitude and the duration of movement reaction time, the number of peak velocity, the precision of the trajectory and the potential level of negligence. The web application of the system allows to one kinesiology therapist to guide, manage and monitor many patients, in order to store the personal and medical data about patients, to configure the specific types of exercises for each user, to show statistics and to detect and notify possible issues.}, 
keywords={assisted living;avatars;computer games;geriatrics;gesture recognition;health care;home computing;Internet;medical computing;medical information systems;home based health-care solution;older adults;physical activity;multiple affections;functional disability;home treatments;self-management health-care game solution;story line;physic effort;motivating virtual world;patient;kinesiology exercises;successive body members;3-dimensional graphics;gesture detection;Microsoft Kinect;raw data;avatar interactions;Kinect sensor;system database;evolution;progress measurements;movement reaction time;kinesiology therapist;ambient assisted living;AAL;Web application;medical data;personal data;Games;Three-dimensional displays;Monitoring;Senior citizens;Tools;Computer science;Avatars;serious games;gesture detection;older adults;health care;Kinect}, 
doi={10.1109/SYNASC.2017.00063}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7727300, 
author={C. H. {Bennett} and S. {La Barbera} and A. F. {Vincent} and J. {Klein} and F. {Alibart} and D. {Querlioz}}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={Exploiting the short-term to long-term plasticity transition in memristive nanodevice learning architectures}, 
year={2016}, 
volume={}, 
number={}, 
pages={947-954}, 
abstract={Memristive nanodevices offer new frontiers for computing systems that unite arithmetic and memory operations on-chip. Here, we explore the integration of electrochemical metallization cell (ECM) nanodevices with tunable filamentary switching in nanoscale learning systems. Such devices offer a natural transition between short-term plasticity (STP) and long-term plasticity (LTP). In this work, we show that this property can be exploited to efficiently solve noisy classification tasks. A single crossbar learning scheme is first introduced and evaluated. Perfect classification is possible only for simple input patterns, within critical timing parameters, and when device variability is weak. To overcome these limitations, a dual-crossbar learning system partly inspired by the extreme learning machine (ELM) approach is then introduced. This approach outperforms a conventional ELM-inspired system when the first layer is imprinted before training and testing, and especially so when variability in device timing evolution is considered: variability is therefore transformed from an issue to a feature. In attempting to classify the MNIST database under the same conditions, conventional ELM obtains 84% classification, the imprinted, uniform device system obtains 88% classification, and the imprinted, variable device system reaches 92% classification. We discuss benefits and drawbacks of both systems in terms of energy, complexity, area imprint, and speed. All these results highlight that tuning and exploiting intrinsic device timing parameters may be of central interest to future bio-inspired approximate computing systems.}, 
keywords={learning (artificial intelligence);memristors;nanoelectronics;neural nets;pattern classification;semiconductor device metallisation;switching circuits;short-term plasticity transition;long-term plasticity transition;memristive nanodevice learning architectures;arithmetic operations;memory operations;electrochemical metallization cell nanodevice;ECM nanodevice;tunable filamentary switching;nanoscale learning system;noisy classification task;critical timing parameter;device variability;dual-crossbar learning system;extreme learning machine;ELM-inspired system;device timing evolution;MNIST database;bio-inspired approximate computing system;Neurons;Electronic countermeasures;Computer architecture;Nanobioscience;Microprocessors;Learning systems;Nanoscale devices}, 
doi={10.1109/IJCNN.2016.7727300}, 
ISSN={2161-4407}, 
month={July},}
@INPROCEEDINGS{4620485, 
author={}, 
booktitle={2008 International Conference on Machine Learning and Cybernetics}, 
title={A service-oriented architecture for Virtual Laboratory Integration and Management}, 
year={2008}, 
volume={2}, 
number={}, 
pages={649-654}, 
abstract={Virtual laboratory information management system (VLIMS) is crucial to a remote virtual laboratory integration and management. An architecture for the remote VLIMS was proposed. The architecture integrates heterogeneous experimental system, using expanded Web services mechanism, based on a real-time metadata management. In implementation, the schema evolution with version was proposed in the abstract by model management operators; Web services composition was shown by a business process example. It satisfies the requirements for information management of a virtual laboratory.}, 
keywords={meta data;software architecture;virtual reality;Web services;service-oriented architecture;virtual laboratory integration;virtual laboratory information management system;Web services;real-time metadata management;Evolution (biology);Business;Computational modeling;Laboratories;Web services;Service oriented architecture;Information management;Meta-model;Web services;Virtual laboratory}, 
doi={10.1109/ICMLC.2008.4620485}, 
ISSN={2160-133X}, 
month={July},}
@INPROCEEDINGS{6354673, 
author={S. {Ishikawa} and A. {Tanaka} and T. {Miyazaki}}, 
booktitle={2012 IEEE 6th International Symposium on Embedded Multicore SoCs}, 
title={Hardware Accelerator for BLAST}, 
year={2012}, 
volume={}, 
number={}, 
pages={16-22}, 
abstract={The basic local alignment search tool (BLAST) is one of the most popular sequence alignment tools available. Sequence alignment is used to extract similar parts of an input protein (or DNA) sequence from protein (or DNA) databases, in order to investigate biological evolution and genomic genealogy. It is a very important and difficult task in bioinformatics. Even though BLAST is an efficient sequence alignment algorithm, it cannot cope with the rapid growth of databases. Initially, in a preprocessing step, BLAST creates query words and a neighborhood word list. Next, it performs three processing steps: a) seed search, b) ungapped extension, and c) gapped extension. In this paper, we propose a hardware accelerator to speed up all processing steps of BLAST, including preprocessing, related works speed up only a subset of them. Each processing step is realized by a hardware module. Thus, we can easily speed up each processing step by duplicating the corresponding module. By implementing the proposed accelerator in a field programmable gate array (FPGA), we demonstrate that a high-performance accelerator can be realized with reasonable hardware cost.}, 
keywords={bioinformatics;field programmable gate arrays;genomics;hardware accelerator;BLAST;basic local alignment search tool;sequence alignment tools;input protein;DNA;protein database;biological evolution;genomic genealogy;bioinformatics;query words;neighborhood word list;seed search;ungapped extension;hardware module;field programmable gate array;FPGA;Proteins;Matrices;Hardware;DNA;Table lookup;Databases;Sequence alignment;Smith-Waterman algorithm;Dynamic programming;Array processor;FPGA}, 
doi={10.1109/MCSoC.2012.22}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{1265370, 
author={E. D. {Browne} and M. {Schrefl} and J. R. {Warren}}, 
booktitle={37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the}, 
title={Goal-focused self-modifying workflow in the healthcare domain}, 
year={2004}, 
volume={}, 
number={}, 
pages={10 pp.-}, 
abstract={This paper introduces the concept of self-modifying workflow in the context of health care planning. Certain tasks in the workflow schema are devoted to modifying the downstream workflow on an instance by instance basis. Such self-modifying schemas provide the necessary flexibility to suit the evolving diagnostic and therapeutic processes encountered in chronic disease management (CDM), particularly in complex areas requiring significant individualization. The management of diabetes mellitus in a community care setting provides an example to illustrate this complexity. Over the past few years, object-oriented modeling tools of inheritance and specialization have been applied to workflow modeling to assist in schema evolution and workflow migration, thereby potentially empowering new workflow management systems (WfMSs) with the functionality to allow the tailoring of guideline-based care plans to individual patient requirements. However, schema evolution is a necessary but insufficient requirement for such tailoring. Healthcare WfMSs need to support a paradigm whereby schema evolution becomes a de facto operation for each workflow instance (i.e. patient episode of care). Self-modifying workflows provide this paradigm. In order to facilitate self-modification of workflow schemas, we annunciate a set of valid operations that can be applied to downstream components of a workflow schema. These operations are primarily concerned with turning abstract subworkflows into concrete ones through completion and alteration of template primitives.}, 
keywords={health care;medical information systems;workflow management software;diseases;self-modifying workflow;health care planning;chronic disease management;diabetes mellitus management;object-oriented modeling tools;workflow migration;workflow management systems;Medical services;Object oriented modeling;Guidelines;Australia;Diseases;Concrete;Lakes;Diabetes;Workflow management software;Turning}, 
doi={10.1109/HICSS.2004.1265370}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{1398267, 
author={ and and }, 
booktitle={2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)}, 
title={Graphical user interface based on multidatabase query optimization}, 
year={2004}, 
volume={1}, 
number={}, 
pages={24-29 vol.1}, 
abstract={Diversity and evolution in database applications often result in multidatabase environment in which corporate data are stored in multiple, distributed data source. It is a formidable work to formulate query to access those multidatabase, especially when some complex constraints are involved. This article presents MQDP (multidatabase query design and processing), a graphical user interface to multidatabase. This tool allows the user to handle the overwhelming information from different data sources. It has supplied common interface to multidatabase and implemented the multidatabase query optimization in query processing. It has wrapped the heterogeneity of the data and put a uniform manner to the user. In this paper we discuss critical issues of this graphical user interface development for multidatabase}, 
keywords={distributed databases;graphical user interfaces;query processing;graphical user interface;multidatabase query optimization;distributed data source;query processing;Graphical user interfaces;Query processing;Computer science;Process design;Database systems;User interfaces;Displays;Dictionaries;Distributed databases;Application software}, 
doi={10.1109/ICSMC.2004.1398267}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{8115612, 
author={A. {van Deursen}}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Software engineering without borders}, 
year={2017}, 
volume={}, 
number={}, 
pages={3-3}, 
abstract={DevOps approaches software engineering by advocating the removal of borders between development and operations. DevOps emphasizes operational resilience, continuous feedback from operations back to development, and rapid deployment of features developed. In this talk we will look at selected (automation) aspects related to DevOps, based on our collaborations with various industrial partners. For example, we will explore (automated) methods for analyzing log data to support deployments and monitor REST API integrations, (search-based) test input generation for reproducing crashes and testing complex database queries, and zero downtime database schema evolution and deployment. We will close by looking at borders beyond those between development and operations, in order to see whether there are other borders we need to remove in order to strengthen the impact of software engineering research.}, 
keywords={}, 
doi={10.1109/ASE.2017.8115612}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{472664, 
author={A. E. {Wade}}, 
booktitle={Proceedings of ELECTRO '94}, 
title={The ODBMS role in 64 bit distributed client-server computing}, 
year={1994}, 
volume={}, 
number={}, 
pages={603-608}, 
abstract={Two trends in today's corporate world demand distribution: downsizing from centralized mainframe single-database environments; and wider integration, connecting finance, engineering, and manufacturing information systems for enterprise-wide modeling and operations optimization. The resulting environment consists of multiple databases, at the group level, department level, and corporate level, but with the need for dependencies among data in all of them. The solution is full distribution, providing a single logical view to objects anywhere, from anywhere. Users see a logical model of objects connected to objects, with atomic transactions and propagating methods, even though composite objects are split among multiple databases, each under separate administrative control, on multiple, heterogeneous platforms, operating systems, and network protocols. 32-bit address spaces are not sufficient for this level of integration, so Objectivity/DB is based on 64 bits, providing access to millions of tera-objects, each of which may be many gigabytes. Support for production environments includes multiple schemas, which may be shared among databases or private, encrypted schemas, dynamic addition of schemas, and schema evolution. Integration must include legacy databases, such as RDBMSs, in this same transparent logical view of objects, and must cooperate with standards such as ODMG-93 and the OMG CORBA. Finally, the logical view must remain valid, and applications must continue to work, as the mapping to the physical environment changes, moving objects and databases to new platforms.<<ETX>>}, 
keywords={object-oriented databases;protocols;standards;client-server systems;64 bit distributed client-server computing;downsizing;manufacturing information systems;enterprise-wide modeling;operations optimization;atomic transactions;operating systems;network protocols;ODMG-93;OMG CORBA;Distributed computing;Transaction databases;Joining processes;Finance;Virtual manufacturing;Information systems;Operating systems;Access protocols;Production;Cryptography}, 
doi={10.1109/ELECTR.1994.472664}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{4221726, 
author={A. {Chandel} and N. {Koudas} and K. Q. {Pu} and D. {Srivastava}}, 
booktitle={2007 IEEE 23rd International Conference on Data Engineering}, 
title={Fast Identification of Relational Constraint Violations}, 
year={2007}, 
volume={}, 
number={}, 
pages={776-785}, 
abstract={Logical constraints, (e.g., `phone numbers in Toronto can have prefixes 416, 647, 905 only'), are ubiquitous in relational databases. Traditional integrity constraints, such as functional dependencies, are examples of such logical constraints as well. However, under frequent database updates, schema evolution and transformations, they can be easily violated. As a result, tables become inconsistent and data quality is degraded. In this paper we study the problem of validating collections of user defined constraints on a number of relational tables. Our primary goal is to quickly identify which tables violate such constraints. Logical constraints are potentially complex logical formuli, and we demonstrate that they cannot be efficiently evaluated by SQL queries. In order to enable fast identification of constraint violations, we propose to build and maintain specialized logical indices on the relational tables. We choose Boolean Decision Diagrams (BDD) as the index structure to aid in this task. We first propose efficient algorithms to construct and maintain such indices in a space efficient manner. We then describe a set of query re-write rules that aid in the efficient utilization of logical indices during constraint validation. We have implemented our approach on top of a relational database and tested our techniques using large collections of real and synthetic data sets. Our results indicate that utilizing our techniques in conjunction with logical indices during constraint validation offers very significant performance advantages.}, 
keywords={Boolean functions;decision diagrams;relational databases;fast identification;relational constraint violations;logical constraints;relational databases;Boolean decision diagrams;synthetic data sets;Relational databases;Degradation;Binary decision diagrams;Testing}, 
doi={10.1109/ICDE.2007.367923}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{7549548, 
author={K. P. {Marimuthu} and F. {Rickhey} and H. {Lee} and J. H. {Lee}}, 
booktitle={2016 7th International Conference on Mechanical and Aerospace Engineering (ICMAE)}, 
title={Spherical indentation cracking in brittle materials: An XFEM study}, 
year={2016}, 
volume={}, 
number={}, 
pages={267-273}, 
abstract={This work aims at characterizing the formation of cone-cracks in brittle materials upon spherical indentation. The cone-cracking is simulated by the extended finite element method (XFEM) in Abaqus / Standard. The element size-dependency is reduced by scaling the damage initiation strength based on mean stress criterion and calibration techniques. The formation of a kinked-cone-crack is observed when the indenter comes into (second) contact with the surface part outside the ring crack. After analyzing the effects of friction, Poisson's ratio on cone-crack evolution, a database for enhanced Roesler's constant, which considers the effect of cone-crack-kinking, is provided by performing systematic XFE analyses. This database can be used for the fracture toughness evaluation in brittle materials.}, 
keywords={brittleness;finite element analysis;fracture mechanics;fracture toughness;friction;indentation;kink bands;Poisson ratio;spherical indentation cracking;brittle materials;XFEM;cone-cracking;extended finite element method;ABAQUS;element size-dependency;damage initiation strength;mean stress criterion;kinked-cone-crack;friction;Poisson's ratio;Roesler's constant;Stress;Surface cracks;Finite element analysis;Glass;Loading;Friction;Shape;fracture toughness;spherical indentation cracking;kinked-cone-cracks;XFEM;damage initiation}, 
doi={10.1109/ICMAE.2016.7549548}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{4281581, 
author={C. {Tsai} and K. {Chao}}, 
booktitle={2007 11th International Conference on Computer Supported Cooperative Work in Design}, 
title={An Effective Chromosome Representation for Optimising Product Quality}, 
year={2007}, 
volume={}, 
number={}, 
pages={1032-1037}, 
abstract={Optimising variables in the quality control of production can be a complex issue, as it may involve many different constraints and expectations on the quality of products which are normally provided from different organisations to form a multiple supply chain. The challenge of measuring product interdependence across various supply chains and identifying a trade-off between quality and cost is not trivial. In this research, which applies dynamic genetic algorithms, we propose a new approach to representing the problem domain within the chromosome which takes advantage of schema evolution and domain knowledge to refine the chromosome structure. As a result, different weightings can be derived and applied to the genes in order to improve searching efficiency of the genetic algorithms (GA). An example of multiple supply chains has been used to evaluate the proposed approach. The results show that the proposed approach outperforms traditional GA approaches.}, 
keywords={genetic algorithms;quality control;supply chain management;chromosome representation;product quality optimization;production quality control;genetic algorithm;multiple supply chain optimization;Biological cells;Genetic algorithms;Supply chains;Computer networks;Constraint optimization;Design optimization;Supply chain management;Costs;Space exploration;Collaborative work;multiple supply chain optimizations;dynamic genetic algorithms;weight rankings and contribution ratio}, 
doi={10.1109/CSCWD.2007.4281581}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{4031028, 
author={D. {Kim} and K. {Jeong} and S. {Hwang} and K. W. {Cho}}, 
booktitle={2006 Second IEEE International Conference on e-Science and Grid Computing (e-Science'06)}, 
title={X-SIGMA: XML Based Simple Data Integration System for Gathering, Managing, and Accessing Scientific Experimental Data in Grid Environments}, 
year={2006}, 
volume={}, 
number={}, 
pages={55-55}, 
abstract={Effective scientific data management is crucial for e-Science applications. Scientific data management raises challenging requirements: (1) support for not only experimental data but also context data, (2) both schema evolution and integration, (3) and compatibility with legacy data management conventions or environments. In this paper, we present a scientific data management system (called XSIGMA) which is designed to address those issues. X-SIGMA is a Grid-based integrated system for managing, integrating, and accessing scientific experimental data. A prototype system is implemented and has been being used to develop the scientific data management system for the national cyberinfrastructure project called KOCED in Korea.}, 
keywords={XML;Environmental management}, 
doi={10.1109/E-SCIENCE.2006.261139}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{4115469, 
author={}, 
booktitle={2006 International Conference on Computer Engineering and Systems}, 
title={[Front cover]}, 
year={2006}, 
volume={}, 
number={}, 
pages={C1-C1}, 
abstract={The following topics are dealt with: computer engineering and systems; computer architecture; computer aided design; control systems; embedded systems; hardware-software codesign; computer networks and security; signal processing; multimedia; Web applications; mobile and ubiquitous computing; artificial intelligence; evolutionary computing; database and data mining}, 
keywords={artificial intelligence;CAD;computer architecture;computer networks;control systems;data mining;database management systems;embedded systems;evolutionary computation;hardware-software codesign;Internet;mobile computing;multimedia computing;security of data;signal processing;computer engineering;computer architecture;computer aided design;control systems;embedded systems;hardware-software codesign;computer networks;security;signal processing;multimedia;Web application;mobile computing;ubiquitous computing;artificial intelligence;evolutionary computing;database;data mining}, 
doi={10.1109/ICCES.2006.320409}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{283010, 
author={}, 
booktitle={Proceedings of 1994 IEEE 10th International Conference on Data Engineering}, 
title={Proceedings of 1994 IEEE 10th International Conference on Data Engineering}, 
year={1994}, 
volume={}, 
number={}, 
pages={0_1-}, 
abstract={}, 
keywords={database management systems;data handling;database theory;data engineering;data distribution;analytical modelling;data replication;query optimization;disk storage management;multidatabase systems;schema evolution;knowledge bases;rule processing;query processing;heterogeneity;parallel databases;temporal databases;knowledge mining;indexing techniques;object-oriented databases}, 
doi={10.1109/ICDE.1994.283010}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{624570, 
author={}, 
booktitle={Proceedings of the Fourth Working Conference on Reverse Engineering}, 
title={Proceedings of the Fourth Working Conference on Reverse Engineering}, 
year={1997}, 
volume={}, 
number={}, 
pages={iii-}, 
abstract={}, 
keywords={reverse engineering;systems re-engineering;database management systems;software engineering;reverse engineering;tool evaluation;object identification;module identification;architectural understanding;program understanding;program scalability;tool support;reengineering;database reverse engineering;enabling technologies}, 
doi={10.1109/WCRE.1997.624570}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{5466954, 
author={}, 
booktitle={2010 International Conference on Information Retrieval Knowledge Management (CAMP)}, 
title={[Front cover]}, 
year={2010}, 
volume={}, 
number={}, 
pages={c1-c1}, 
abstract={The following topics are dealt with: content-based retrieval; image database; image compression; centre-based clustering; user interface; video retrieval; XML; image transmission; image resolution; supervised learning; ontological technique; query optimization; distributed database; evolutionary algorithm; peer-to-peer networks; information dissemination; computer game; 3D colors representation; data extraction; search engine; augmented reality; information security; data mining; intrusion detection; artificial immune systems; shape recognition; bipartite graph; edge coloring; particle swarm optimization; feature extraction; fuzzy heuristic; software maintenance; information retrieval; and computer simulation.}, 
keywords={artificial immune systems;augmented reality;computer games;content-based retrieval;data compression;data mining;digital simulation;distributed databases;evolutionary computation;feature extraction;graph theory;image representation;image resolution;information dissemination;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;peer-to-peer computing;search engines;security of data;shape recognition;software maintenance;user interfaces;video retrieval;visual communication;visual databases;XML;content-based retrieval;image database;image compression;centre-based clustering;user interface;video retrieval;XML;image transmission;image resolution;supervised learning;ontology;query optimization;distributed database;evolutionary algorithm;peer-to-peer networks;information dissemination;computer game;3D colors representation;data extraction;search engine;augmented reality;information security;data mining;intrusion detection;artificial immune system;shape recognition;bipartite graph;edge coloring;particle swarm optimization;feature extraction;fuzzy heuristic;software maintenance;information retrieval;computer simulation}, 
doi={10.1109/INFRKM.2010.5466954}, 
ISSN={}, 
month={March},}