% Encoding: UTF-8

@InCollection{Chen1992,
  author    = {I-Min Amy Chen and Dennis McLeod},
  title     = {A Unified Approach to Data and Meta-data Modification for Data/Knowledge Bases},
  booktitle = {Theoretical Studies in Computer Science},
  publisher = {Academic Press},
  year      = {1992},
  editor    = {Jeffrey D. Ullman},
  pages     = {287 - 313},
  isbn      = {978-0-12-708240-0},
  abstract  = {A unified approach to the modification of the spectrum of information facts in a data/knowledge base is proposed. Derived data update, conceptual schema evolution, and exception handling are indeed different policies to resolve database inconsistency due to update. We propose a unified update approach that utilizes and balances the above three update policies to handle updates at both data and conceptual schema (meta-data) levels. An experimental prototype embodies the unified update algorithm; this prototype supports a two-level database model (a higher level object model and a lower level triplet representation), unified update algorithm, update heuristics, and an end-user interface. Several examples are described to demonstrate the prototype as well as the techniques underlying it.},
  doi       = {https://doi.org/10.1016/B978-0-12-708240-0.50015-0},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780127082400500150},
}

@Article{Lakshmanan1997,
  author   = {Laks V.S. Lakshmanan and Fereidoon Sadri and Iyer N. Subramanian},
  title    = {Logic and algebraic languages for interoperability in multidatabase systems},
  journal  = {The Journal of Logic Programming},
  year     = {1997},
  volume   = {33},
  number   = {2},
  pages    = {101 - 149},
  issn     = {0743-1066},
  abstract = {Developing a declarative approach to interoperability in the context of multidatabase systems is a major goal of this research. We take a first step toward this goal in this paper, by developing a simple logic called SchemaLog which is syntactically higher-order but has a first-order semantics. SchemaLog can provide for interoperability among multiple relational databases in a federation of database systems. We develop a fixpoint theory for the definite clause fragment of SchemaLog and show its equivalence to the model-theoretic semantics. We also develop a sound and complete proof procedure for all clausal theories. We establish the correspondence between SchemaLog and first-order predicate calculus and provide a reduction of SchemaLog to predicate calculus. We propose an extension to classical relational algebra, capable of retrieving and manipulating data and schema from databases in a multidatabase system, and prove its equivalence to a form of relational calculus inspired by SchemaLog syntax. We illustrate the simplicity and power of SchemaLog with a variety of applications involving database programming (with schema browsing), schema integration, schema evolution, cooperative query answering, and sophisticated forms of aggregation in the spirit of OLAP (On-Line Analytical Processing). We also highlight our implementation of SchemaLog realized on a federation of INGRES databases.},
  doi      = {https://doi.org/10.1016/S0743-1066(96)00146-X},
  url      = {http://www.sciencedirect.com/science/article/pii/S074310669600146X},
}

@Article{Watson1991,
  author   = {A.S. Watson and S.H. Chan},
  title    = {A prolog-based object oriented engineering DBMS},
  journal  = {Computers \& Structures},
  year     = {1991},
  volume   = {40},
  number   = {1},
  pages    = {11 - 21},
  issn     = {0045-7949},
  abstract = {In this paper we present the primary concepts of PBASE, a prototype object oriented database system. PBASE is intended to support the needs of engineering applications with specific reference to structural engineering. To address the engineering requirements the object oriented data model used in PBASE incorporates several enhancements, including Schema Evolution, Composite Objects, Declarative Methods and Version Management. Schema evolution allows dynamic changes to the class definitions and the class lattice. Composite objects support the is-part-of relationship between assemblies and components. Declarative methods introduce semantics into objects while version management supports the tracking of objects' versions and alternatives as they evolve during the design process.},
  doi      = {https://doi.org/10.1016/0045-7949(91)90451-Q},
  url      = {http://www.sciencedirect.com/science/article/pii/004579499190451Q},
}

@Article{Chung1992,
  author   = {Yunkung Chung and Gary W. Fischer},
  title    = {Illustration of object-oriented databases for the structure of a bill of materials},
  journal  = {Computers in Industry},
  year     = {1992},
  volume   = {19},
  number   = {3},
  pages    = {257 - 270},
  issn     = {0166-3615},
  abstract = {In this tutorial paper, the basic concepts of applying an object-oriented database (OODB) system, called orion, to the management of a bill of materials (BOM) is addressed. The illustration attempts to demonstrate the linkage between the needs of a CAD/CAM environment and the production planning environment with specific reference to the material requirements planning (MRP) system of the future. The structure of a composite object made up of objects is the major consideration of the proposed object-oriented BOM (OOBOM). The concentration is on composite objects in the OOBOM schema evolution, their propagations of changing object instances, and the unique object identity of each manufacturing part. The proposed OOBOM system is rudimentary, and may be embodied by using the Itasca™ system, which is a commercial OODB system whose prototype is orion, or by using the c++ language. In this paper, both viewpoints, orion and c++, are discussed.},
  doi      = {https://doi.org/10.1016/0166-3615(92)90063-S},
  keywords = {Object-oriented database, Bill of materials, Composite objects, Schema evolution, , Itasca™, c language},
  url      = {http://www.sciencedirect.com/science/article/pii/016636159290063S},
}

@Article{Abiteboul1995,
  author   = {S. Abiteboul and P. Kanellakis and S. Ramaswamy and E. Waller},
  title    = {Method Schemas},
  journal  = {Journal of Computer and System Sciences},
  year     = {1995},
  volume   = {51},
  number   = {3},
  pages    = {433 - 455},
  issn     = {0022-0000},
  abstract = {A method schema is a simple programming formalism for object-oriented databases with features such as classes, methods, inheritance, name overloading, and late binding. An important problem is to check whether a given method schema can lead to an inconsistency in some interpretation. This consistency question is shown to be undecidable in general. Decidability is obtained for monadic and/or recursion-free method schemas. In particular, consistency of monadic method schemas is shown to be decidable in O(nc3) time, where n is the size of the method definitions and c is the size of the class hierarchy; also, it is logspace-complete in PTIME, even for monadic, recursion-free schemas. Method signature covariance is shown to simplify the computational complexity of key decidable cases. For example, one coded method in the context of base methods with covariant signatures can be tested for consistency in O(n + c) time for the monadic case (without covariance this problem is in O(nc2) time) and in PTIME for the fixed arity polyadic case (without covariance this problem is NP-complete). Incremental consistency checking of method schemas is a formalization of the database schema evolution problem, for which a sound, but necessarily incomplete, heuristic is proposed.},
  doi      = {https://doi.org/10.1006/jcss.1995.1080},
  url      = {http://www.sciencedirect.com/science/article/pii/S002200008571080X},
}

@Article{Proper1994,
  author   = {H.A Proper and Th.P van der Weide},
  title    = {EVORM: A conceptual modelling technique for evolving application domains},
  journal  = {Data \& Knowledge Engineering},
  year     = {1994},
  volume   = {12},
  number   = {3},
  pages    = {313 - 359},
  issn     = {0169-023X},
  abstract = {In this paper we present EVORM, a data modelling technique for evolving application domains. EVORM is the result of applying a general theory for the evolution of application domains to the object role modelling technique PSM, a generalisation of ER, EER, FORM and NIAM. First the general theory is presented. This theory describes a general approach to the evolution of application domains, abstracting from details of specific modelling techniques. This theory makes a distinction between the underlying information structure and its evolution on the one hand, and the description and semantics of operations on the information structure and its population on the other hand. Main issues within this theory are object typing, type relatedness and identification of objects. After a (short) introduction to PSM, this general theory is applied, resulting in EVORM. Besides having a right of its own, the usefulness of the general theory is demonstrated by interpreting its abstract results, resulting in more intuitive rules for EVORM.},
  doi      = {https://doi.org/10.1016/0169-023X(94)90031-0},
  keywords = {Schema evolution, Conceptual modelling, Evolving information systems, Temporal information systems, Data modelling, Predicator set model},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X94900310},
}

@Article{Quintarelli2015,
  author   = {Elisa Quintarelli and Emanuele Rabosio and Letizia Tanca},
  title    = {A principled approach to context schema evolution in a data management perspective},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {49},
  pages    = {65 - 101},
  issn     = {0306-4379},
  abstract = {Context-aware data tailoring studies the means for the system to furnish the users, at any moment, only with the set of data which is relevant for their current context. These data may be from traditional databases, sensor readings, environmental information, close-by people, points of interest, etc. To implement context-awareness, we use a formal representation of a conceptual context model, used to design the context schema, which intensionally represents all the contexts in which the user may be involved in the considered application scenario. Following this line of thought, in this paper we develop a formal approach and the corresponding strategy to manage the evolution of the context schema of a given context-aware application, when the context perspectives initially envisaged by the system designer are not applicable any more and unexpected contexts are to be activated. Accordingly, when the context schema evolves also the evolution of the corresponding context-aware data portions must be taken care of. The aim of this paper is thus to provide the necessary conceptual and formal notions to manage the evolution of a context schema in the perspective of data tailoring: after introducing a set of operators to manage evolution and proving their soundness and completeness, we analyze the impact that context evolution has on the context-based data tailoring process. We then study how sequences of operator applications can be optimized and finally present a prototype validating the feasibility of the approach.},
  doi      = {https://doi.org/10.1016/j.is.2014.11.008},
  keywords = {Context-awareness, Schema evolution, Evolution operators},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437914001847},
}

@Article{Nikolov2015,
  author   = {Nikolay Nikolov and Alessandro Rossini and Kyriakos Kritikos},
  title    = {Integration of DSLs and Migration of Models: A Case Study in the Cloud Computing Domain},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {68},
  pages    = {53 - 66},
  issn     = {1877-0509},
  note     = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
  abstract = {Domain-specific languages (DSLs) are high-level software languages representing concepts in a particular domain. In real-world scenarios, it is common to adopt multiple DSLs to solve different aspects of a specific problem. As any other software artefact, DSLs evolve independently in response to changing requirements, which leads to two challenges. First, the concepts from the DSLs have to be integrated into a single language. Second, models that conform to an old version of the language have to be migrated to conform to its current version. In this paper, we discuss how we tackled the challenge of integrating the DSLs that comprise the Cloud Application Modelling and Execution Language (CAMEL) by leveraging upon Eclipse Modeling Framework (EMF) and Object Constraint Language (OCL). Moreover, we propose a solution to the challenge of persisting and automatically migrating CAMEL models based on Connected Data Objects (CDO) and Edapt.},
  doi      = {https://doi.org/10.1016/j.procs.2015.09.223},
  keywords = {model-driven engineering, domain-specific language, metamodel migration, model co-evolution, cloud computing, CAMEL, EMF, OCL, CDO, Edapt},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915030689},
}

@Article{Ewald1996,
  author   = {C.A. Ewald and M.E. Orlowska},
  title    = {Characterization of the effects of schema change},
  journal  = {Information Sciences},
  year     = {1996},
  volume   = {94},
  number   = {1},
  pages    = {23 - 39},
  issn     = {0020-0255},
  abstract = {Schema evolution occurs frequently in practice. In this paper, we show that the entire schema does not need to be redesigned every time a change is made. We characterize the subschema affected by basic schema operations, which allows us to limit the scope of checking required and have a guarantee of the overall design being correct (safe changes). In addition, we characterize the effects on the actual relations of the addition and removal of constraints. In some cases, relations may have to be added to or removed from the database in order to preserve correctness.},
  doi      = {https://doi.org/10.1016/0020-0255(96)00134-X},
  url      = {http://www.sciencedirect.com/science/article/pii/002002559600134X},
}

@Article{Skoulis2015,
  author   = {Ioannis Skoulis and Panos Vassiliadis and Apostolos V. Zarras},
  title    = {Growing up with stability: How open-source relational databases evolve},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {53},
  pages    = {363 - 385},
  issn     = {0306-4379},
  abstract = {Like all software systems, databases are subject to evolution as time passes. The impact of this evolution can be vast as a change to the schema of a database can affect the syntactic correctness and the semantic validity of all the surrounding applications. In this paper, we have performed a thorough, large-scale study on the evolution of databases that are part of larger open source projects, publicly available through open source repositories. Lehman׳s laws of software evolution, a well-established set of observations on how the typical software systems evolve (matured during the last forty years), has served as our guide towards providing insights on the mechanisms that govern schema evolution. Much like software systems, we found that schemata expand over time, under a stabilization mechanism that constraints uncontrolled expansion with perfective maintenance. At the same time, unlike typical software systems, the growth is typically low, with long periods of calmness interrupted by bursts of maintenance and a surprising lack of complexity increase.},
  doi      = {https://doi.org/10.1016/j.is.2015.03.009},
  keywords = {Schema evolution, Software evolution, Lehman׳s laws},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915000691},
}

@Article{Perret2016,
  author  = {C Perret and L Perrier and M Castex and C Acquadro},
  title   = {PRM66 - Proqolid Database: Evolution Of Content, Structure, And Functionalities (2012-2016) - Integration In Eprovide, A New Online Platform Dedicated To Clinical Outcome Assessment (Coa) Research},
  journal = {Value in Health},
  year    = {2016},
  volume  = {19},
  number  = {7},
  pages   = {A369},
  issn    = {1098-3015},
  doi     = {https://doi.org/10.1016/j.jval.2016.09.131},
  url     = {http://www.sciencedirect.com/science/article/pii/S109830151631498X},
}

@Article{Kim1990,
  author   = {Won Kim and Jay Banerjee and Hong-Tai Chou and Jorge F. Garza},
  title    = {Object-oriented database support for CAD},
  journal  = {Computer-Aided Design},
  year     = {1990},
  volume   = {22},
  number   = {8},
  pages    = {469 - 479},
  issn     = {0010-4485},
  abstract = {ORION is a prototype object-oriented database system built in the Advanced Computer Technology (ACT) Program at MCC. It is intended to support the data management needs of applications in such domains as computer-aided design, artificial intelligence, and office information systems. The paper describes features of ORION which have been implemented specifically to support CAD environments. These include dynamic schema evolution, version control and change notification, and transaction management.},
  doi      = {https://doi.org/10.1016/0010-4485(90)90063-I},
  keywords = {computer-aided design, object-orientism, databases},
  url      = {http://www.sciencedirect.com/science/article/pii/001044859090063I},
}

@Article{Engels1992,
  author   = {Gregor Engels and Martin Gogolla and Uwe Hohenstein and Klaus Hülsmann and Perdita Löhr-Richter and Gunter Saake and Hans-Dieter Ehrich},
  title    = {Conceptual modelling of database applications using an extended ER model},
  journal  = {Data \& Knowledge Engineering},
  year     = {1992},
  volume   = {9},
  number   = {2},
  pages    = {157 - 204},
  issn     = {0169-023X},
  abstract = {In this paper, we motivate and present a data model for conceptual design of structural and behavioural aspects of databases. We follow an object centered design paradigm in the spirit of semantic data models. The specification of structural aspects is divided into modelling of object structures and modelling of data types used for describing object properties. The specification of object structures is based on an Extended Entity-Relationship (EER) model. The specification of behavioural aspects is divided into the modelling of admissible database state evolutions by means of temporal integrity constraints and the formulation of database (trans)actions. The central link for integrating these design components is a descriptive logic-based query language for the EER model. The logic part of this language is the basis for static constraints and descriptive action specifications by means of pre- and postconditions. A temporal extension of this logic is the specification language for temporal integrity constraints. We emphasize that the various aspects of a database application are specified using several appropriate, but yet compatible formalisms, which are integrated by a unifying common semantic.},
  doi      = {https://doi.org/10.1016/0169-023X(92)90008-Y},
  keywords = {Conceptual data model, conceptual database design, Entity-Relationship model, database evolution, integrity constraints, query language, transactions},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9290008Y},
}

@Article{Shah2008,
  author   = {Jay B. Shah and James M. McKiernan and Eric P. Elkin and Peter R. Carroll and Maxwell V. Meng},
  title    = {Prostate Biopsy Patterns in the CaPSURE Database: Evolution With Time and Impact on Outcome After Prostatectomy},
  journal  = {The Journal of Urology},
  year     = {2008},
  volume   = {179},
  number   = {1},
  pages    = {136 - 140},
  issn     = {0022-5347},
  abstract = {Purpose
Significant variability exists in the urological community regarding the number of cores that should be taken during prostate biopsy. Using CaPSURE™ we determined trends in prostate biopsy patterns during the last decade and assessed whether changes in biopsy number have had an impact on outcomes after radical prostatectomy.
Materials and Methods
In CaPSURE between 1995 and 2004 we identified 6,450 men with newly diagnosed prostate cancer who underwent biopsy with 6 cores or greater. The number of cores removed, number of cores positive for cancer and percent of cores containing cancer were analyzed by year of diagnosis. For 1,757 men who underwent radical prostatectomy these variables were entered into Cox proportional hazards models controlling for preoperative prostate specific antigen, biopsy Gleason sum and clinical stage to predict recurrence-free survival.
Results
The mean number of removed cores increased from 6.9 in 1995 to 10.2 in 2004 (p <0.0001). The mean number of positive cores remained unchanged from 2.9 in 1995 to 3.2 in 2004 (p = 0.40). The percent of positive cores decreased from 42.6% in 1995 to 32.1% in 2004 (p <0.0001). The number and percent of positive cores were associated with recurrence-free survival after radical prostatectomy throughout the study period (each p <0.001).
Conclusions
The percent of positive cores is an independent predictor of disease recurrence after radical prostatectomy. The total number of tissue cores sampled increased during the last decade, thereby driving down the mean percent of positive cores from 42.6% to 32.1%. The trend toward an increasing number of removed cores may have contributed indirectly to improved outcomes after radical prostatectomy in the last decade.},
  doi      = {https://doi.org/10.1016/j.juro.2007.08.126},
  keywords = {prostate, prostatic neoplasms, biopsy, outcome assessment (health care), prostatectomy},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022534707022926},
}

@Article{Nguyen1989,
  author   = {G.T. Nguyen and D. Rieu},
  title    = {Schema evolution in object-oriented database systems},
  journal  = {Data \& Knowledge Engineering},
  year     = {1989},
  volume   = {4},
  number   = {1},
  pages    = {43 - 67},
  issn     = {0169-023X},
  abstract = {Object-oriented database systems ususally exhibit specific advantages over traditional database management systems and programming languages. Among them stand the ease of writing, maintaining and debugging application programs, code modularity, inheritance, persistency and sharability. Of particular interest to software engineering and computer-aided design applications is also the ability to dynamically change the object definitions and the opportunity to define incrementally composite objects. This paper gives an overview of current research efforts directed towards evolving data definitions in object-oriented database systems. The emphasis is on their ability to support two complementary aspects: supporting evolving schemas, and propagating the changes on the object instances. Several projects are analyzed: Cadb, Encore, GemStone, Orion and Sherpa. Current results indicate that if most of them provide schema evolution facilities, they seldom support automatic propagation mechanisms. A proposal is described that enables Sherpa to fully support the propagation of changes and the dynamic classification of the instances whose class definitions are modified. This approach is an extension of techniques used in artificial intelligence for knowledge representation. It extends previous classification mechanisms with a dynamic capability which adequately supports evolving class definitions and instances.},
  doi      = {https://doi.org/10.1016/0169-023X(89)90004-9},
  keywords = {Object-oriented models, Databases, Dynamic schemas, Inheritance Propagation, Classification},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X89900049},
}

@InCollection{BERTHOLD2003,
  author    = {BERTHOLD DAUM},
  title     = {12 - Schema Evolution},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {445 - 469},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-155860816-0/50014-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500148},
}

@Article{Ortin2014,
  author   = {Francisco Ortin and Miguel A. Labrador and Jose M. Redondo},
  title    = {A hybrid class- and prototype-based object model to support language-neutral structural intercession},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {2},
  pages    = {199 - 219},
  issn     = {0950-5849},
  abstract = {Context
Dynamic languages have turned out to be suitable for developing specific applications where runtime adaptability is an important issue. Although .Net and Java platforms have gradually incorporated features to improve their support of dynamic languages, they do not provide intercession for every object or class. This limitation is mainly caused by the rigid class-based object model these platforms implement, in contrast to the flexible prototype-based model used by most dynamic languages.
Objective
Our approach is to provide intercession for any object or class by defining a hybrid class- and prototype-based object model that efficiently incorporates structural intercession into the object model implemented by the widespread .Net and Java platforms.
Method
In a previous work, we developed and evaluated an extension of a shared-source implementation of the .Net platform. In this work, we define the formal semantics of the proposed reflective model, and modify the existing implementation to include the hybrid model. Finally, we assess its runtime performance and memory consumption, comparing it to existing approaches.
Results
Our platform shows a competitive runtime performance compared to 9 widespread systems. On average, it performs 73% and 61% better than the second fastest system for short- and long-running applications, respectively. Besides, it is the JIT-compiler approach that consumes less average memory. The proposed approach of including a hybrid object-model into the virtual machine involves a 444% performance improvement (and 65% less memory consumption) compared to the existing alternative of creating an extra software layer (the DLR). When none of the new features are used, our platform requires 12% more execution time and 13% more memory than the original .Net implementation.
Conclusion
Our proposed hybrid class- and prototype-based object model supports structural intercession for any object or class. It can be included in existing JIT-compiler class-based platforms to support common dynamic languages, providing competitive runtime performance and low memory consumption.},
  doi      = {https://doi.org/10.1016/j.infsof.2013.09.002},
  keywords = {Structural intercession, Duck typing, Prototype-based object model, Reflection, Virtual machine, Dynamic languages},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584913001778},
}

@Article{Vries2007,
  author   = {Denise de Vries and John F. Roddick},
  title    = {The case for mesodata: An empirical investigation of an evolving database system},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {9},
  pages    = {1061 - 1072},
  issn     = {0950-5849},
  abstract = {Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute’s specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system.},
  doi      = {https://doi.org/10.1016/j.infsof.2006.11.001},
  keywords = {Database evolution, Domain evolution, Mesodata},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584906001881},
}

@Article{Sjoeberg1993,
  author   = {D Sjøberg},
  title    = {Quantifying schema evolution},
  journal  = {Information and Software Technology},
  year     = {1993},
  volume   = {35},
  number   = {1},
  pages    = {35 - 44},
  issn     = {0950-5849},
  abstract = {Achieving correct changes is the dominant activity in the application software industry. Modification of database schemata is one kind of change which may have severe consequences for database applications. The paper presents a method for measuring modifications to database schemata and their consequences by using a thesaurus tool. Measurements of the evolution of a large-scale database application currently running in several hospitals in the UK are presented and interpreted. The kind of measurements provided by this in-depth study is useful input to the design of change management tools.},
  doi      = {https://doi.org/10.1016/0950-5849(93)90027-Z},
  keywords = {schema evolution, change statistics, change management tools},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499390027Z},
}

@Article{2006,
  title   = {Subject index to volume 59},
  journal = {Data \& Knowledge Engineering},
  year    = {2006},
  volume  = {59},
  number  = {3},
  pages   = {791 - 792},
  issn    = {0169-023X},
  note    = {Including: ER 2003},
  doi     = {https://doi.org/10.1016/S0169-023X(06)00176-5},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X06001765},
}

@Article{Zaeschke2015,
  author   = {Tilmann Zäschke and Stefania Leone and Tobias Gmünder and Moira C. Norrie},
  title    = {Improving conceptual data models through iterative development},
  journal  = {Data \& Knowledge Engineering},
  year     = {2015},
  volume   = {98},
  pages    = {54 - 73},
  issn     = {0169-023X},
  note     = {Research on conceptual modeling},
  abstract = {Agile methods promote iterative development with short cycles, where user feedback from the previous iteration is used to refactor and improve the current version. To facilitate agile development of information systems, this paper offers three contributions. First, we introduce the concept of evolvability as a model quality characteristic. Evolvability refers to the expected implications of future model refactorings, both in terms of complexity of the required database evolution algorithm and in terms of the expected volume of data to evolve. Second, we propose extending the agile development cycle by using database profiling information to suggest adaptations to the conceptual model to improve performance. For every software release, the database profiler identifies and analyses navigational access patterns, and proposes model optimisations based on data characteristics, access patterns and a cost–benefit model. Based on an experimental evaluation of the profiler we discuss why the quality of conceptual models can generally benefit from profiling and how performance measurements convey semantic information. Third, we discuss the flow of semantic information when developing and using information systems. Beyond these contributions, we also make a case for using object databases in agile development environments. However, most of the presented concepts are also applicable to other database paradigms.},
  doi      = {https://doi.org/10.1016/j.datak.2015.07.005},
  keywords = {Database profiling, Evolvability, Object database, Agile development, Conceptual models, Model quality, Semantic verification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X15000506},
}

@Article{Li1991,
  author   = {Q Li},
  title    = {Extending semantic object model: Towards more unified view of information objects},
  journal  = {Information and Software Technology},
  year     = {1991},
  volume   = {33},
  number   = {2},
  pages    = {106 - 112},
  issn     = {0950-5849},
  abstract = {Object-based approaches to information management exhibits uniform treatment of all information objects. Two kinds of uniformity supported by existing models and systems can be classified: ‘horizontal’ and ‘vertical’. The former is realised by the single concept of object for modelling all conceptual entities at various level of complexity and abstraction. The latter is embodied by the notion of class hierarchy and inheritance of attributes and instances along the class hierarchy. The paper demonstrates that a third kind of uniformity, termed ‘diagonal’ uniformity, is also desired by many applications. By allowing inter-mixing and inter-changing among different modelling constructs, the system exhibits a more consistent and uniform view of all information objects, and provides more flexible and powerful modelling capabilities. Such a feature is especially needed in areas such as schema evolution, information sharing, and integration among multiple databases. A framework for supporting such diagonal uniformity is described.},
  doi      = {https://doi.org/10.1016/0950-5849(91)90055-G},
  keywords = {information management, databases, object databases, semantic modelling, uniformity, diagonal uniformity},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499190055G},
}

@Article{Castellani2001,
  author   = {Xavier Castellani and Hong Jiang and Alain Billionnet},
  title    = {Method for the analysis and design of class characteristic migrations during object system evolution},
  journal  = {Information Systems},
  year     = {2001},
  volume   = {26},
  number   = {4},
  pages    = {237 - 257},
  issn     = {0306-4379},
  abstract = {We propose a method to assist designers in the analysis and design of the migrations of class characteristics (attributes and methods) during object system evolution. This method advocates:•to analyze a migration by identifying transfer links that will be used to transfer class characteristics;•to design the graph of the transfer links of a migration by decomposing it into components that minimize the amount of work required to carry out the modifications of the classes concerned by the migration, that can be studied separately and then possibly dispatching to designers; these components are groups of classes strongly coupled and separate (i.e. the classes making up each component are strongly coupled by transfer links, and no links occur between the classes of different components). Among the partitions of a migration obtained with such components, the best ones are those whose components minimize the amount of work required to carry out the modifications of the classes concerned by the migration. These partitions are such that classes are implicated in the minimum number components. In case several best partitions are found, we choose the ones which have the fewest components. The components of a migration are represented by maximal bipartite complete subgraphs of the graph of transfers of a migration that defines a partition of a set of its arcs. Efficient algorithms for building a partition of a directed graph by searching for such subgraphs do not exist in the graph theory. We propose three approaches for building a partition of the graph of transfers of a migration: a departure approach that places emphasis on the classes from which class characteristics are transferred, an arrival approach that places emphasis on the classes to which class characteristics are transferred, and a simulated annealing approach used in operations research.},
  doi      = {https://doi.org/10.1016/S0306-4379(01)00019-9},
  keywords = {Evolution, Maintenance, Migration, Object-oriented methods, Transfer of characteristics, Partitions of migration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437901000199},
}

@Article{Rodriguez2001,
  author   = {L. Rodríguez and H. Ogata and Y. Yano},
  title    = {A temporal versioned object-oriented data schema model},
  journal  = {Computers \& Mathematics with Applications},
  year     = {2001},
  volume   = {41},
  number   = {1},
  pages    = {177 - 192},
  issn     = {0898-1221},
  abstract = {This paper describes in a formal way a data schema model which introduces temporal and versioning schema features in an object-oriented environment. In our model, the schema is time dependent and the history of the changes which occur on its elements are kept into version hierarchies. A fundamental assumption behind our approach is that a new schema specification should not define a new database, so that previous schema definitions are considered as alternative design specifications, and consequently, existing data can be accessed in a consistent way using any of the defined schemas.},
  doi      = {https://doi.org/10.1016/S0898-1221(01)85015-X},
  keywords = {Object-oriented databases, Temporal databases, Schema versioning},
  url      = {http://www.sciencedirect.com/science/article/pii/S089812210185015X},
}

@InCollection{Falkner2014,
  author    = {Andreas Falkner and Herwig Schreiner},
  title     = {Chapter 16 - SIEMENS: Configuration and Reconfiguration in Industry},
  booktitle = {Knowledge-Based Configuration},
  publisher = {Morgan Kaufmann},
  year      = {2014},
  editor    = {Alexander Felfernig and Lothar Hotz and Claire Bagley and Juha Tiihonen},
  pages     = {199 - 210},
  address   = {Boston},
  isbn      = {978-0-12-415817-7},
  abstract  = {Whereas the configuration of consumer products such as PCs, cars, insurances, and such is well understood and supported by commercial tools, large-scale industrial systems still raise considerable challenges concerning modeling, solving, and performance. After a short explanation of the importance of this topic to Siemens, this chapter presents the domain of railway interlocking systems as an example of such complex industrial systems and discusses detailed requirements for their configuration. It then reports on techniques used for solving those requirements at Siemens as well as on results of their application.},
  doi       = {https://doi.org/10.1016/B978-0-12-415817-7.00016-5},
  keywords  = {Knowledge-based Configuration, Reconfiguration, Knowledge Base, Knowledge Evolution, Large-scale Systems, Industrial Systems, Complex Systems, Siemens, Railway Interlocking Systems, Variability},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124158177000165},
}

@Article{Proper1997,
  author   = {H.A. Proper},
  title    = {Data schema design as a schema evolution process},
  journal  = {Data \& Knowledge Engineering},
  year     = {1997},
  volume   = {22},
  number   = {2},
  pages    = {159 - 189},
  issn     = {0169-023X},
  abstract = {In an information system a key role is played by the underlying data schema. This article starts out from the view that the entire modelling process of an information system's data schema can be seen as a schema transformation process. A transformation process that starts out with an initial draft conceptual schema and ends with an internal database schema for some implementation platform. This allows us to describe the transformation process of a database design as an evolution of a schema through a universe of data schemas. Doing so allows a better understanding of the actual design process, countering the problem of ‘software development under the lamppost’. Even when the information system design is finalised, the data schema can evolve further due to changes in the requirements on the system. We present a universe of data schemas that allows us to describe the underlying data schemas at all stages of their development. This universe of data schemas is used as a case study on how to describe the complete evolution of a data schema with all its relevant aspects. The theory is general enough to cater for more modelling concepts, or different modelling approaches. To actually model the evolution of a data schema we present a versioning mechanism that allows us to model the evolutions of the elements of data schemas and their interactions, leading to a better understanding of the schema design process as a whole. Finally, we also discuss the relationship between this simple versioning mechanism and general-purpose version-management systems.},
  doi      = {https://doi.org/10.1016/S0169-023X(96)00045-6},
  keywords = {Conceptual modelling, Version management, Schema evolution, Object role modelling, Entity relationship modelling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X96000456},
}

@Article{Corwin2007,
  author   = {John Corwin and Avi Silberschatz and Perry L. Miller and Luis Marenco},
  title    = {Dynamic Tables: An Architecture for Managing Evolving, Heterogeneous Biomedical Data in Relational Database Management Systems},
  journal  = {Journal of the American Medical Informatics Association},
  year     = {2007},
  volume   = {14},
  number   = {1},
  pages    = {86 - 93},
  issn     = {1067-5027},
  abstract = {Data sparsity and schema evolution issues affecting clinical informatics and bioinformatics communities have led to the adoption of vertical or object-attribute–value-based database schemas to overcome limitations posed when using conventional relational database technology. This paper explores these issues and discusses why biomedical data are difficult to model using conventional relational techniques. The authors propose a solution to these obstacles based on a relational database engine using a sparse, column-store architecture. The authors provide benchmarks comparing the performance of queries and schema-modification operations using three different strategies: (1) the standard conventional relational design; (2) past approaches used by biomedical informatics researchers; and (3) their sparse, column-store architecture. The performance results show that their architecture is a promising technique for storing and processing many types of data that are not handled well by the other two semantic data models.},
  doi      = {https://doi.org/10.1197/jamia.M2189},
  url      = {http://www.sciencedirect.com/science/article/pii/S1067502706002052},
}

@Article{Menezes2002,
  author   = {Elizabete W. Menezes and Fábio A. R. Gonçalves and Eliana B. Giuntini and Franco M. Lajolo},
  title    = {Brazilian Food Composition Database: Internet Dissemination and Other Recent DevelopmentsSTUDY REVIEW},
  journal  = {Journal of Food Composition and Analysis},
  year     = {2002},
  volume   = {15},
  number   = {4},
  pages    = {453 - 464},
  issn     = {0889-1575},
  abstract = {A database of food composition was created by BRASILFOODS/USP to centralize the information obtained in the country and is available on the Internet (http://www.fcf.usp.br/tabela). This is the best device to launch this information, as it enables a dynamic data update and also provides an easier approach at reduced cost. The first version of the Web site was created in 1998 and has been changed according to the availability of new data and to the users' needs. The database was elaborated considering the aims and criteria conceived by INFOODS/LATINFOODS/SAMFOODS, targeting data validation and interchange. The main intent of the paper is to illustrate aspects related to the database evolution (content, data quality evaluation, structure and difficulties) and its dissemination on the Internet. The awareness of positive and negative aspects of the database and also the identification of the reasons which determined them, can contribute to improve the database as a whole. Based on these observations, a great number of activities is already being carried out as follows: the optimization of the search by foods, the procedure of data compilation and analyses of new foods and nutrients, and also the search for new partnerships and government financial support.},
  doi      = {https://doi.org/10.1006/jfca.2002.1083},
  keywords = {food composition, database, compilation, analysis, Internet approach.},
  url      = {http://www.sciencedirect.com/science/article/pii/S0889157502910835},
}

@Article{Li1991a,
  author   = {Qing Li},
  title    = {Object data model = object-oriented + semantic models},
  journal  = {Computer Standards \& Interfaces},
  year     = {1991},
  volume   = {13},
  number   = {1},
  pages    = {99 - 103},
  issn     = {0920-5489},
  abstract = {The term ‘object-oriented database’ has been used to refer to different characteristics and mechanisms supported by semantic databases and (behaviorally) object-oriented ones. While this indicates the current confusion due to the lack of a common model, it also suggests the potential desirability of combining these two kinds of modeling facilities into the same nutshell. This paper summarizes from these two paradigms the features common to both, the features influenced/adopted by each other, and the features which are presented in one paradigm but missing from the other. After a brief analysis and discussion of these features, we conclude this paper by proposing our major theme: a complete object data model should take the union of the facilities offered by these two kinds of models.},
  doi      = {https://doi.org/10.1016/0920-5489(91)90015-R},
  keywords = {Semantic data modeling, object-oriented databases, object data model},
  url      = {http://www.sciencedirect.com/science/article/pii/092054899190015R},
}

@Article{2005,
  title   = {Subject index to volume 53},
  journal = {Data \& Knowledge Engineering},
  year    = {2005},
  volume  = {53},
  number  = {3},
  pages   = {341},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/S0169-023X(05)00008-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X0500008X},
}

@Article{Cho2000,
  author   = {E.-S Cho and H.-J Kim},
  title    = {LOD∗: A C++ extension for OODBMSs with orthogonal persistence to class hierarchies},
  journal  = {Information and Software Technology},
  year     = {2000},
  volume   = {42},
  number   = {5},
  pages    = {347 - 356},
  issn     = {0950-5849},
  abstract = {There exist some preprocessing based language extensions for database management where persistence is orthogonal to the class hierarchy. They allow a class hierarchy to be built from both database classes and non-database classes together. Such a property is important in that classes can be reused in implementing database classes, and vice versa. In this paper, we elaborate on the orthogonality of persistence to class-hierarchies, and find that the existing method to achieve this is not satisfactory because of the side-effects of the heterogeneosity of the links in a class hierarchy; some links represent subset(IsA) relationships between database classes, while the others denote inheritance for code-reuse. Finally, we propose LOD∗, a C++ extension to database access, which separates the different categories of links into independent hierarchies, and supports orthogonal persistence to the class hierarchy, overcoming the limitations in the previous methods.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00094-4},
  keywords = {Object-oriented database programming language, C++, Class interface, Class implementation, IsA relationships, Code-reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000944},
}

@Article{Penna2006,
  author   = {Giuseppe Della Penna and Antinisca Di Marco and Benedetto Intrigila and Igor Melatti and Alfonso Pierantonio},
  title    = {Interoperability mapping from XML schemas to ER diagrams},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {59},
  number   = {1},
  pages    = {166 - 188},
  issn     = {0169-023X},
  abstract = {The eXtensible Markup Language (XML) is a de facto standard on the Internet and is now being used to exchange a variety of data structures. This leads to the problem of efficiently storing, querying and retrieving a great amount of data contained in XML documents. Unfortunately, XML data often need to coexist with historical data. At present, the best solution for storing XML into pre-existing data structures is to extract the information from the XML documents and adapt it to the data structures’ logical model (e.g., the relational model of a DBMS). In this paper, we introduce a technique called Xere (XML entity–relationship exchange) to assist the integration of XML data with other data sources. To this aim, we present an algorithm that maps XML schemas into entity–relationship diagrams, discuss its soundness and completeness and show its implementation in XSLT.},
  doi      = {https://doi.org/10.1016/j.datak.2005.08.002},
  keywords = {Data models, Schema evolution and maintenance, Interoperability and heterogeneity, Web applications/XML, Entity–relationship diagrams},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05001102},
}

@Article{Lee2006,
  author   = {Sang-Won Lee and Jung-Ho Ahn and Hyoung-Joo Kim},
  title    = {A schema version model for complex objects in object-oriented databases},
  journal  = {Journal of Systems Architecture},
  year     = {2006},
  volume   = {52},
  number   = {10},
  pages    = {563 - 577},
  issn     = {1383-7621},
  abstract = {In this paper, we propose a schema version model which allows to restructure complex object hierarchy in object-oriented databases. This model extends a schema version model, called RiBS, which is based on the concept of Rich Base Schema. In the RiBS model, each schema version is in the form of updatable class hierarchy view over one base schema, called the RiBS layer, which has richer schema information than any existing schema version in the database. In this paper, we introduce new operations for restructuring composite object hierarchy in schema versions, and explain their semantics. We also touch upon the ways to transform queries posed against a restructured composite object hierarchy into one against the base schema. In addition, we identify several types of conflicts during schema version merging which result from the restructuring operations, and provide a semi-automatic algorithm to resolve the conflicts. The originality of this paper lies in that (1) we introduce several new operations to restructure composite object hierarchy, and (2) this extended RiBS model operations raise the concept of data independence in OODBs upto the schema level.},
  doi      = {https://doi.org/10.1016/j.sysarc.2006.04.001},
  keywords = {Complex objects, Schema versions, Object-oriented databases, Schema version merging},
  url      = {http://www.sciencedirect.com/science/article/pii/S1383762106000440},
}

@Article{McGinnes2015,
  author   = {Simon McGinnes and Evangelos Kapros},
  title    = {Conceptual independence: A design principle for the construction of adaptive information systems},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {47},
  pages    = {33 - 50},
  issn     = {0306-4379},
  abstract = {This paper examines the problem of conceptual dependence, the coupling of software applications׳ internal structures and logic with their underlying conceptual models. Although conceptual dependence is almost universal in information system design, it produces a range of unintended negative consequences including system inflexibility and increased maintenance costs. Many information systems contain components, such as database tables and classes, whose design reflects the entity types and relationships in underlying, domain-oriented conceptual models. When the models change, work is involved in altering the software components. For example, an e-commerce system might include tables and classes representing product types, customers and orders, with associated code in methods, stored procedures and other scripts. The structure of the entity types and their relationships will be implicit in the tables, classes and code, coupling the system to its conceptual model. Any change to the model (such as the introduction of a new entity type, representing order lines) invalidates existing structures and code, causing rework. In large systems, this rework can be time-consuming and expensive. Research shows that schema change is common, and that it contributes significantly to the high cost of software maintenance. We argue that much of the cost may be avoidable if alternative design strategies are used. The paper describes an alternative design approach based on the principle of conceptual independence, which can be used to produce adaptive information systems (AIS). It decouples the internal structures and logic of information systems from the domain-specific entity types and relationships in the conceptual models they implement. An architecture for AIS is presented which includes soft schemas (conceptual models stored as data), an end-user conceptual modelling tool, a set of archetypal categories (predefined semantic categories), and an adaptive data model which allows data to be stored without conceptual dependence. The archetypal categories allow domain-specific run time behaviour to be provided, despite the absence of domain-specific software structure and logic. An advantage of AIS over conventionally-designed applications is that each AIS can be used in a wide variety of domains. AIS offer the prospect of significantly reduced maintenance costs, as well as increased scope for the development and modification of systems by end users. Work to date on implementation of the AIS architecture is discussed, and an agenda for future research is outlined including development and evaluation of a fully-featured AIS. The paper discusses challenges to be overcome and barriers to adoption.},
  doi      = {https://doi.org/10.1016/j.is.2014.06.001},
  keywords = {Conceptual modelling, Conceptual independence, Data independence, Schema evolution, Software design, Adaptive information systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437914000933},
}

@Article{Casanova1993,
  author   = {Marco A. Casanova and Luiz Tucherman and Alberto H.F. Laender},
  title    = {On the design and maintenance of optimized relational representations of entity-relationship schemas},
  journal  = {Data \& Knowledge Engineering},
  year     = {1993},
  volume   = {11},
  number   = {1},
  pages    = {1 - 20},
  issn     = {0169-023X},
  abstract = {A method for obtaining optimized relational representations of database conceptual schemas in an extended entity-relationship model is first proposed. The method incorporates and generalizes a familiar heuristics to obtain good relational representations and also produces, for each relational structure, an explanation indicating which concepts it represents. Then, a redesign method that, given changes to the conceptual schema, generates a plan to modify the original representation and to organize the database state is described.},
  doi      = {https://doi.org/10.1016/0169-023X(93)90043-O},
  keywords = {Conceptual database design, extended entity-relationship model, relational schema optimization, database schema evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9390043O},
}

@Article{Amer2017,
  author   = {Said Amer and Ahmed ElKhatam and Yasuhiro Fukuda and Lamia I. Bakr and Shereif Zidan and Ahmed Elsify and Mostafa A. Mohamed and Chika Tada and Yutaka Nakai},
  title    = {Prevalence and Identity of Taenia multiceps cysts “Coenurus cerebralis” in Sheep in Egypt},
  journal  = {Acta Tropica},
  year     = {2017},
  volume   = {176},
  pages    = {270 - 276},
  issn     = {0001-706X},
  abstract = {Coenurosis is a parasitic disease caused by the larval stage (Coenurus cerebralis) of the canids cestode Taenia multiceps. C. cerebralis particularly infects sheep and goats, and pose a public health concerns. The present study aimed to determine the occurrence and molecular identity of C. cerebralis infecting sheep in Egypt. Infection rate was determined by postmortem inspection of heads of the cases that showed neurological manifestations. Species identification and genetic diversity were analyzed based on PCR-sequence analysis of nuclear ITS1 and mitochondrial cytochrome oxidase (COI) and nicotinamide adenine dinucleotide dehydrogenase (ND1) gene markers. Out of 3668 animals distributed in 50 herds at localities of Ashmoun and El Sadat cities, El Menoufia Province, Egypt, 420 (11.45%) sheep showed neurological disorders. Postmortem examination of these animals after slaughter at local abattoirs indicated to occurrence of C. cerebralis cysts in the brain of 111 out of 420 (26.4%), with overall infection rate 3.03% of the involved sheep population. Molecular analysis of representative samples of coenuri at ITS1 gene marker showed extensive intra- and inter-sequence diversity due to deletions/insertions in the microsatellite regions. On contrast to the nuclear gene marker, considerably low genetic diversity was seen in the analyzed mitochondrial gene markers. Phylogenetic analysis based on COI and ND1 gene sequences indicated that the generated sequences in the present study and the reference sequences in the database clustered in 4 haplogroups, with more or less similar topologies. Clustering pattern of the phylogenetic tree showed no effect for the geographic location or the host species.},
  doi      = {https://doi.org/10.1016/j.actatropica.2017.08.012},
  keywords = {, , Sheep, Egypt},
  url      = {http://www.sciencedirect.com/science/article/pii/S0001706X1730743X},
}

@Article{Hess2006,
  author   = {Claudia Hess and Christoph Schlieder},
  title    = {Ontology-based verification of core model conformity in conceptual modeling},
  journal  = {Computers, Environment and Urban Systems},
  year     = {2006},
  volume   = {30},
  number   = {5},
  pages    = {543 - 561},
  issn     = {0198-9715},
  note     = {Cadastral Systems IV},
  abstract = {Reference models, often called core models are developed in various application domains. Until now, no computational support exists for the task of verifying the conformity between such core models and their domain models. The approach developed at Bamberg University uses Semantic Web technologies to examine whether or not a domain model is a derivation of a core model. This ontology-based conformity verification supports an iterative modeling process in which core or domain models are modified. Inference services as provided by ontologies can be used to analyze the relationships between core and domain models. For example, it is possible to formally prove which specific relations hold between two types of models and compare the result with the intentions of the domain experts involved in the modeling. As a consequence, knowledge not explicitly represented is revealed. In case that the domain model does not conform to the core model, an interpretation of the inference results is provided in ordinary language giving the domain experts hints on how to modify either the core model, the domain model or both. We evaluated our approach by applying it to a core model and a domain, hence national model from the cadastral domain. Conformity was verified between the core cadastral model proposed by [Lemmen, C., van der Molen, P., van Oosterom, P., Ploeger, H., Quak, W., Stoter, J., et al. (2003). A modular standard for the cadastral domain. In Proceedings of digital earth 2003: Information resources for global sustainability: knowledge, networks, technology, economy, society, natural and human resources, policy and strategy. Brno, Czech Republic, pp. 108–117] and the Greek cadastral model [Tzani, A. (2003). Object-oriented modeling of the Greek Cadastre. Master’s thesis, School of Rural and Surveying Engineering of Aristotle University of Thessaloniki], which both are results of research activities related to the European COST Action G9 “Modeling Real Property Transactions”. Although our approach to conformity verification was only evaluated with the cadastral models, it can be used for conformity verification in various applications domains due to its generality.},
  doi      = {https://doi.org/10.1016/j.compenvurbsys.2005.08.009},
  keywords = {Conformity verification, Schema evolution, Ontological modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0198971505000840},
}

@Article{Cunha2011,
  author   = {Alcino Cunha and Joost Visser},
  title    = {Transformation of structure-shy programs with application to XPath queries and strategic functions},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {6},
  pages    = {516 - 539},
  issn     = {0167-6423},
  note     = {Special issue on Partial Evaluation and Program Manipulation (selected paper of PEPM 2007)},
  abstract = {Various programming languages allow the construction of structure-shy programs. Such programs are defined generically for many different datatypes and only specify specific behavior for a few relevant subtypes. Typical examples are XML query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags. Other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object-oriented programming. In this paper, we present an algebraic approach to transformation of declarative structure-shy programs, in particular for strategic functions and XML queries. We formulate a rich set of algebraic laws, not just for transformation of structure-shy programs, but also for their conversion into structure-sensitive programs and vice versa. We show how subsets of these laws can be used to construct effective rewrite systems for specialization, generalization, and optimization of structure-shy programs. We present a type-safe encoding of these rewrite systems in Haskell which itself uses strategic functional programming techniques. We discuss the application of these rewrite systems for XPath query optimization and for query migration in the context of schema evolution.},
  doi      = {https://doi.org/10.1016/j.scico.2010.01.003},
  keywords = {Algebraic program transformation, Strategic functional programming, XML query languages, Point-free program calculation, Type specialization, Type generalization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310000146},
}

@Article{Roddick1995,
  author   = {John F Roddick},
  title    = {A survey of schema versioning issues for database systems},
  journal  = {Information and Software Technology},
  year     = {1995},
  volume   = {37},
  number   = {7},
  pages    = {383 - 393},
  issn     = {0950-5849},
  abstract = {Schema versioning is one of a number of related areas dealing with the same general problem—that of using multiple heterogeneous schemata for various database related tasks. In particular, schema versioning, and its weaker companion, schema evolution, deal with the need to retain current data and software system functionality in the face of changing database structure. Schema versioning and schema evolution offer a solution to the problem by enabling intelligent handling of any temporal mismatch between data and data structure. This survey discusses the modelling, architectural and query language issues relating to the support of evolving schemata in database systems. An indication of the future directions of schema versioning research is also given.},
  doi      = {https://doi.org/10.1016/0950-5849(95)91494-K},
  keywords = {schema evolution, schema versioning, evolving database systems},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499591494K},
}

@Article{Gosain2015,
  author   = {Anjana Gosain and Sangeeta Sabharwal and Rolly Gupta},
  title    = {Model Based Materialized View Evolution: A Review},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {57},
  pages    = {1273 - 1280},
  issn     = {1877-0509},
  note     = {3rd International Conference on Recent Trends in Computing 2015 (ICRTC-2015)},
  abstract = {Materialized views evolve in order to meet the user's requirement in the dynamically changing data warehouse environment. Therefore, materialized view evolution approach focuses on choosing materialized views in the design process of data warehouses or maintaining a materialized view in response to data changes or to data sources changes and sometimes to monitor the DW quality under schema evolution. Although few researchers have addressed materialized view evolution problem for evolving an appropriate set of views. But, none of the surveys provides a classification of materialized view evolution approaches in order to identify their advantages and disadvantages. This survey tries to fill this gap. The present paper provides a review of model based materialized view evolution methods by identifying the three main dimensions namely; (i) Framework, (ii) Architecture and (iii) Model/Design Model, that are the basis in the classification of materialized view evolution methods. The goal of this paper is to provide a comparative study on model based materialized view evolution methods, by identifying respective potentials and limits.},
  doi      = {https://doi.org/10.1016/j.procs.2015.07.432},
  keywords = {Model, View Maintenanc, Materialized view evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915019614},
}

@Article{Shoaran2011,
  author   = {M. Shoaran and A. Thomo},
  title    = {Evolving schemas for streaming XML},
  journal  = {Theoretical Computer Science},
  year     = {2011},
  volume   = {412},
  number   = {35},
  pages    = {4545 - 4557},
  issn     = {0304-3975},
  abstract = {In this paper we model schema evolution for XML by defining formal language operators on Visibly Pushdown Languages (VPLs). Our goal is to provide a framework for efficient validation of streaming XML in the realistic setting where the schemas of the exchanging parties evolve and thus diverge from one another. We show that Visibly Pushdown Languages are closed under the defined language operators and this enables us to expand the schemas (for XML) in order to account for flexible or constrained evolution.},
  doi      = {https://doi.org/10.1016/j.tcs.2011.04.037},
  keywords = {XML schemas, Evolution, Streaming data, Visibly pushdown languages},
  url      = {http://www.sciencedirect.com/science/article/pii/S030439751100346X},
}

@Article{Shengqi2012,
  author   = {Wu Shengqi and Zhang Shidong and Kong Lanju},
  title    = {Schema Evolution via Multi-Version Metadata in SaaS},
  journal  = {Procedia Engineering},
  year     = {2012},
  volume   = {29},
  pages    = {1107 - 1112},
  issn     = {1877-7058},
  note     = {2012 International Workshop on Information and Electronics Engineering},
  abstract = {The Basic-Table combined with Extension-Table (BT&ET) layout has become the popular data storage architecture for SaaS currently. For the sake of improving the processing efficiency we improve the BT&ET layout, and migrate some tenants’ frequently accessed extension fields into the basic table based on the tenants’ constantly need on data access. With the development of cloud computing, Multi-Tenant data need to be stored in multiple data nodes. When tenant's data storage schema evolution happens, all data nodes hava to do data migration simultaneously and tenants may perceive the influence. In order to minimize the costs and negative load of schema evolution we propose the Multi-Version metadata technology. Tenant's data on each data node may contain different version metadata, and schema envolution can run asynchronously. We carry out experiments and the results figure out that the workload decreases significantly.},
  doi      = {https://doi.org/10.1016/j.proeng.2012.01.096},
  keywords = {Multi-Version metadata, schema evolution, SAAS},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877705812001063},
}

@Article{Hick2006,
  author   = {Jean-Marc Hick and Jean-Luc Hainaut},
  title    = {Database application evolution: A transformational approach},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {59},
  number   = {3},
  pages    = {534 - 558},
  issn     = {0169-023X},
  note     = {Including: ER 2003},
  abstract = {While recent data management technologies, such as object oriented techniques, address the problem of database schema evolution, standard information systems currently in use raise challenging evolution problems. This paper examines database evolution from the developer point of view. It shows how requirements changes are propagated to database schemas, to data and to programs through a general strategy. This strategy requires the documentation of database design. When absent, such documentation has to be rebuilt through reverse engineering techniques. Our approach, called DB-MAIN, relies on a generic database model and on transformational paradigm that states that database engineering processes can be modeled by schema transformations. Indeed, a transformation provides both structural and instance mappings that formally define how to modify database structures and contents. We describe both the complete and a simplified approaches, and compare their merits and drawbacks. We then analyze the problem of program modification and describe a CASE tool that can assist developers in their task of system evolution. We illustrate our approach with Biomaze, a biochemical knowledge-based the database of which is rapidly evolving.},
  doi      = {https://doi.org/10.1016/j.datak.2005.10.003},
  keywords = {Evolution, Database conversion, Schema transformation, History, Reverse engineering, CASE tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05001631},
}

@Article{Weber2008,
  author   = {Barbara Weber and Manfred Reichert and Stefanie Rinderle-Ma},
  title    = {Change patterns and change support features – Enhancing flexibility in process-aware information systems},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {66},
  number   = {3},
  pages    = {438 - 466},
  issn     = {0169-023X},
  abstract = {Companies increasingly adopt process-aware information systems (PAISs), which offer promising perspectives for more flexible enterprise computing. The emergence of different process support paradigms and the lack of methods for comparing existing approaches enabling PAIS changes have made the selection of adequate process management technology difficult. This paper suggests a set of 18 change patterns and seven change support features to foster the systematic comparison of existing process management technology in respect to process change support. While the proposed patterns are all based on empirical evidence from several large case studies, the suggested change support features constitute typical functionalities provided by flexible PAISs. Based on the proposed change patterns and features, we provide a detailed analysis and evaluation of selected approaches from both academia and industry. The presented work will not only facilitate the selection of technologies for realizing flexible PAISs, but can also be used as a reference for implementing flexible PAISs.},
  doi      = {https://doi.org/10.1016/j.datak.2008.05.001},
  keywords = {Workflow management, Process-aware information systems, Patterns, Process change, Process flexibility},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X0800058X},
}

@Article{Wang1996,
  author   = {Huaiqing Wang},
  title    = {Repositories for co-operative information systems},
  journal  = {Information and Software Technology},
  year     = {1996},
  volume   = {38},
  number   = {5},
  pages    = {333 - 341},
  issn     = {0950-5849},
  abstract = {Repository technology has become a pervasive part of the software development process. The role played by the repository within the modern software development process is to support a particular software engineering (SE) environment, and to provide an extensive set of services that allows CASE tools to interface with the repository. The next generation of Information Systems (ISs), often referred to as Co-operative Information Systems (CISs), will involve a number of information agents distributed over large computer and telecommunications networks. One of the most prominent problems faced by such CISs is the problem of co-operation and collaboration. It is realized that such problems are analogous to those in software development process addressed by repository technology. This paper presents the experience of introducing repository technology into a CIS. The goal of developing a shareable and reusable central repository is achieved by deploying the object-oriented data model paradigm. This paper also presents a novel way to deal with real-time CISs by separating the services of a repository into two distinct subsets provided by its off-line component and its on-line component. The successful incorporation of repository technology with a real-world CIS application has demonstrated both the feasibility and the benefits of such integration.},
  doi      = {https://doi.org/10.1016/0950-5849(95)01063-7},
  keywords = {Co-operative information systems, Information repository, Information system architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584995010637},
}

@Article{Vassiliadis2017,
  author   = {Panos Vassiliadis and Apostolos V. Zarras and Ioannis Skoulis},
  title    = {Gravitating to rigidity: Patterns of schema evolution – and its absence – in the lives of tables},
  journal  = {Information Systems},
  year     = {2017},
  volume   = {63},
  pages    = {24 - 46},
  issn     = {0306-4379},
  abstract = {Like all software maintenance, schema evolution is a process that can severely impact the lifecycle of a data-intensive software projects, as schema updates can drive depending applications crushing or delivering incorrect data to end users. In this paper, we study the schema evolution of eight databases that are part of larger open source projects, publicly available through open source repositories. In particular, the focus of our research was the understanding of which tables evolve and how. We report on our observations and patterns on how evolution related properties, like the possibility of deletion, or the amount of updates that a table undergoes, are related to observable table properties like the number of attributes or the time of birth of a table. A study of the update profile of tables, indicates that they are mostly rigid (without any updates to their schema at all) or quiet (with few updates), especially in databases that are more mature and heavily updated. Deletions are significantly outnumbered by table insertions, leading to schema expansion. Delving deeper, we can highlight four patterns of schema evolution. The Γ pattern indicating that tables with large schemata tend to have long durations and avoid removal, the Comet pattern indicating that the tables with most updates are the ones with medium schema size, the Inverse Γ pattern, indicating that tables with medium or small durations produce amounts of updates lower than expected, and, the Empty Triangle pattern indicating that deletions involve mostly early born, quiet tables with short lives, whereas older tables are unlikely to be removed. Overall, we believe that the observed evidence strongly indicates that databases are rigidity-prone rather than evolution-prone. We call the phenomenon gravitation to rigidity and we attribute it to the implied impact to the surrounding code that a modification to the schema of a database has.},
  doi      = {https://doi.org/10.1016/j.is.2016.06.010},
  keywords = {Schema evolution, Database evolution, Analysis of evolution history, Patterns in schema evolution, Software rigidity, Software repository mining, Exploratory study, Software maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S030643791630120X},
}

@Article{Sachdeva2017,
  author   = {Shelly Sachdeva and Shivani Batra and Subhash Bhalla},
  title    = {Evolving large scale healthcare applications using open standards},
  journal  = {Health Policy and Technology},
  year     = {2017},
  volume   = {6},
  number   = {4},
  pages    = {410 - 425},
  issn     = {2211-8837},
  abstract = {Electronic Health Records (EHRs) are becoming more prevalent in health care. Worldwide exchange of healthcare data demands adherence to semantic interoperable standards to overcome the language and platform barriers. Various healthcare organizations in developing countries such as, India adopt their own independent information systems without adhering to standard guidelines. Thus, this tends to sacrifice interoperability. This affects permanent persistence of longitudinal health records for future reference and research purpose. Current research implements a standard based clinical application to be used for healthcare domain in India. The study has been done for enhancing the data quality through standardization. It aims at providing a generic permanent persistence to track life-long interoperable health records of patients. This is the first effort for exploring its adoption for various regional languages in India. The user interfaces have been generated for various Indian languages for testing on a sample set of archetypes. The clinical application deployed in ‘Hindi’ language can be easily deployed for other people in ‘Tamil’ language, while maintaining semantic interoperability. The persistence will also be maintained, with the same meaning (of data) for both the regions. Implementing these standard based healthcare applications helps in reducing the costs while enhancing patient care. Thus, this study aims to build a standard based, and platform independent healthcare application to provide support for interoperability, usability and generic persistence.},
  doi      = {https://doi.org/10.1016/j.hlpt.2017.10.001},
  keywords = {Electronic Health Records (EHRs), Indian languages, User interface, Healthcare Application, Large Scale Application, open Standards},
  url      = {http://www.sciencedirect.com/science/article/pii/S2211883717300679},
}

@Article{Necasky2012,
  author   = {Martin Nečaský and Jakub Klímek and Jakub Malý and Irena Mlýnková},
  title    = {Evolution and change management of XML-based systems},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {3},
  pages    = {683 - 707},
  issn     = {0164-1212},
  note     = {Novel approaches in the design and implementation of systems/software architecture},
  abstract = {XML is de-facto a standard language for data exchange. Structure of XML documents exchanged among different components of a system (e.g. services in a Service-Oriented Architecture) is usually described with XML schemas. It is a common practice that there is not only one but a whole family of XML schemas each applied in a particular logical execution part of the system. In such systems, the design and later maintenance of the XML schemas is not a simple task. In this paper we aim at a part of this problem – evolution of the family of the XML schemas. A single change in user requirements or surrounding environment of the system may influence more XML schemas in the family. A designer needs to identify the XML schemas affected by a change and ensure that they are evolved coherently with each other to meet the new requirement. Doing this manually is very time consuming and error prone. In this paper we show that much of the manual work can be automated. For this, we introduce a technique based on the principles of Model-Driven Development. A designer is required to make a change only once in a conceptual schema of the problem domain and our technique ensures semi-automatic coherent propagation to all affected XML schemas (and vice versa). We provide a formal model of possible evolution changes and their propagation mechanism. We also evaluate the approach on a real-world evolution scenario.},
  doi      = {https://doi.org/10.1016/j.jss.2011.09.038},
  keywords = {XML data modeling, Model driven architecture, XML schema evolution, Propagation of changes},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002524},
}

@Article{Cunha2002,
  author   = {Raimundo R.M. da Cunha and A. Dias},
  title    = {A feature-based database evolution approach in the design process},
  journal  = {Robotics and Computer-Integrated Manufacturing},
  year     = {2002},
  volume   = {18},
  number   = {3},
  pages    = {275 - 281},
  issn     = {0736-5845},
  note     = {11th International Conference on Flexible Automation and Intelligent Manufacturing},
  abstract = {Design data are assigned in geometric and non-geometric form in order to meet design requirements. These data and information must be encapsulated in a data structure that has significance for design applications in each design process phase. The main goal of this research is to find design data groups that represent each mechanical design phase, which will be called phase's design signature. In addition, current data should be an evolution of the geometric and non-geometric information of the previous design phase. In this paper, the purpose is to identify and model a set of design features that encapsulate the design data and their transformations which occurred during the mechanical design phases. This database must capture the designer's intents that can be modeled and implemented using feature-based model in the conventional CAD systems, object-oriented modeling, and Java classes.},
  doi      = {https://doi.org/10.1016/S0736-5845(02)00018-2},
  keywords = {Design data, Feature, Object-oriented modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0736584502000182},
}

@InCollection{Lichao1990,
  author    = {Lichao TAN and Takuya KATAYAMA},
  title     = {Meta Operations for Type Management in Object-Oriented Databases: — A Lazy Mechanism for Schema Evolution},
  booktitle = {Deductive and Object-Oriented Databases},
  publisher = {North-Holland},
  year      = {1990},
  editor    = {Won KIM and Jean-Marie NICOLAS and Shojiro NISHIO},
  pages     = {241 - 258},
  address   = {Amsterdam},
  isbn      = {978-0-444-88433-6},
  abstract  = {In object-oriented database systems, type definitions are used as the basis of object manipulation. They may change causing the systems' schemata evolve dynamically. In this paper, we first clarify the concept of schema evolution in databases and discuss its existing solutions. Next we propose a lazy evaluation method of schema evolution which minimize the amount of object manipulation. It is realized in a system which incorporates the concept of persistent meta-object, while meta-object interprets meta-message and principally maintains type validness while schema evolves. Better balance between the availability of object and the speed of accessing object is obtained by our method.},
  doi       = {https://doi.org/10.1016/B978-0-444-88433-6.50021-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444884336500216},
}

@Article{Park2012,
  author  = {Henry S. Park and Shane Lloyd and Roy H. Decker and Lynn D. Wilson and James B. Yu},
  title   = {Overview of the Surveillance, Epidemiology, and End Results Database: Evolution, Data Variables, and Quality Assurance},
  journal = {Current Problems in Cancer},
  year    = {2012},
  volume  = {36},
  number  = {4},
  pages   = {183 - 190},
  issn    = {0147-0272},
  note    = {Working with the SEER data: Opportunities and Cautions},
  doi     = {https://doi.org/10.1016/j.currproblcancer.2012.03.007},
  url     = {http://www.sciencedirect.com/science/article/pii/S0147027212000451},
}

@Article{Roennbaeck2010,
  author   = {L. Rönnbäck and O. Regardt and M. Bergholtz and P. Johannesson and P. Wohed},
  title    = {Anchor modeling — Agile information modeling in evolving data environments},
  journal  = {Data \& Knowledge Engineering},
  year     = {2010},
  volume   = {69},
  number   = {12},
  pages    = {1229 - 1253},
  issn     = {0169-023X},
  note     = {Special issue on 28th International Conference on Conceptual Modeling (ER 2009)},
  abstract = {Maintaining and evolving data warehouses is a complex, error prone, and time consuming activity. The main reason for this state of affairs is that the environment of a data warehouse is in constant change, while the warehouse itself needs to provide a stable and consistent interface to information spanning extended periods of time. In this article, we propose an agile information modeling technique, called Anchor Modeling, that offers non-destructive extensibility mechanisms, thereby enabling robust and flexible management of changes. A key benefit of Anchor Modeling is that changes in a data warehouse environment only require extensions, not modifications, to the data warehouse. Such changes, therefore, do not require immediate modifications of existing applications, since all previous versions of the database schema are available as subsets of the current schema. Anchor Modeling decouples the evolution and application of a database, which when building a data warehouse enables shrinking of the initial project scope. While data models were previously made to capture every facet of a domain in a single phase of development, in Anchor Modeling fragments can be iteratively modeled and applied. We provide a formal and technology independent definition of anchor models and show how anchor models can be realized as relational databases together with examples of schema evolution. We also investigate performance through a number of lab experiments, which indicate that under certain conditions anchor databases perform substantially better than databases constructed using traditional modeling techniques.},
  doi      = {https://doi.org/10.1016/j.datak.2010.10.002},
  keywords = {Anchor Modeling, Database modeling, Normalization, 6NF, Data warehousing, Agile development, Temporal databases, Table elimination},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X10001229},
}

@Article{Mantz2015,
  author   = {Florian Mantz and Gabriele Taentzer and Yngve Lamo and Uwe Wolter},
  title    = {Co-evolving meta-models and their instance models: A formal approach based on graph transformation},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {104},
  pages    = {2 - 43},
  issn     = {0167-6423},
  note     = {Special Issue on Graph Transformation and Visual Modeling Techniques (GT-VMT 2013)},
  abstract = {Model-driven engineering focuses on models as primary artifacts of the software development process, which means programs are mainly generated by model-to-code transformations. In particular, modeling languages tailored to specific domains promise to increase the productivity of software developers and the quality of generated software. Modeling languages, however, evolve over time and therefore, existing models have to be migrated accordingly. The manual migration of models tends to be tedious and error-prone, therefore tools have been developed to (partly) automate this process. Nevertheless, the migration results may not always be well-defined. In this article, we provide a formal framework for model migration which is independent of specific modeling approaches. We treat modeling languages, formalized by meta-models, as well as models as graphs and consider their co-evolutions as coupled graph transformations. In the same line, we study the conditions under which model migrations are well-defined. Existing solutions to model migration are either handwritten or default solutions that can hardly be customized. Here, we introduce a high-level specification approach, called model migration schemes, that supports automation and customization. Starting from a meta-model evolution rule, a default migration scheme can be automatically deduced and customized.},
  doi      = {https://doi.org/10.1016/j.scico.2015.01.002},
  keywords = {Meta-model evolution, Model migration, Graph transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642315000106},
}

@Article{Chau2008,
  author   = {Vo Thi Ngoc Chau and Suphamit Chittayasothorn},
  title    = {A temporal object relational SQL language with attribute timestamping in a temporal transparency environment},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {67},
  number   = {3},
  pages    = {331 - 361},
  issn     = {0169-023X},
  abstract = {In this paper, a temporal object relational SQL language is proposed. It is intended to facilitate the non-procedural data definitions, queries, and modifications of temporal databases with attribute timestamping in a temporal transparency environment. With attribute timestamping, temporal data of any type is naturally handled. With temporal logic, users can issue temporal queries in an intuitively expressive manner. The language provides valid time support for upward compatibility, temporal upward compatibility, sequenced and non-sequenced variants to serve both non-temporal and temporal users. Using object relational technology, the implementation is made for embedded and interactive SQL modes. All existing interfaces of an ORDBMS are inherited with temporal transparency.},
  doi      = {https://doi.org/10.1016/j.datak.2008.06.008},
  keywords = {Temporal database, Attribute timestamping, SQL, Object relational database, Temporal transparency},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X0800089X},
}

@InCollection{Johnston2014,
  author    = {Tom Johnston},
  title     = {Chapter 16 - Bitemporal Data and the Inmon Data Warehouse},
  booktitle = {Bitemporal Data},
  publisher = {Morgan Kaufmann},
  year      = {2014},
  editor    = {Tom Johnston},
  pages     = {261 - 276},
  address   = {Boston},
  isbn      = {978-0-12-408067-6},
  abstract  = {This chapter begins with a brief history of data warehousing. It then deconstructs Inmon’s famous definition of a data warehouse, discards certain parts, modifies other parts, adds bitemporality, and reconstructs a new definition. In it, I argue that an Inmon data warehouse, lacking bitemporality, cannot be both time-variant and nonvolatile.},
  doi       = {https://doi.org/10.1016/B978-0-12-408067-6.00016-4},
  keywords  = {ambiguous-time table, as-was data about things, as-was states of things, enterprise data warehouse, federated data warehouse, integrated, nonvolatile, operational data store, physical instantiation, semantic integration, subject-oriented, time-variant, virtual data warehouse},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124080676000164},
}

@Article{Ariav1991,
  author   = {Gad Ariav},
  title    = {Temporally oriented data definitions: Managing schema evolution in temporally oriented databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {1991},
  volume   = {6},
  number   = {6},
  pages    = {451 - 467},
  issn     = {0169-023X},
  abstract = {A simplifying — yet unrealistic — assumption widely held throughout the research of Temporally Oriented Data Models (TODM) is that the associated schema never changes. The implications of allowing data structures to evolve over time within a TODM and related databases are examined in this paper, and key issues and concepts are identified. Specifically, Temporally Oriented Data Definition (TODD) raises questions with respect to (1) the evolution of meanings in databases, (2) the nature of the temporal prevalence of database schema, and (3) the general principles that may guide the implementation of a TODM database with TODD.},
  doi      = {https://doi.org/10.1016/0169-023X(91)90023-Q},
  keywords = {Temporally oriented databases, schema evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9190023Q},
}

@Article{MatosGalante2005,
  author   = {Renata de Matos Galante and Clesio Saraiva dos Santos and Nina Edelweiss and Álvaro Freitas Moreira},
  title    = {Temporal and versioning model for schema evolution in object-oriented databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {2005},
  volume   = {53},
  number   = {2},
  pages    = {99 - 128},
  issn     = {0169-023X},
  abstract = {In this paper we define the Temporal and Versioning Model for Schema Evolution (TVSE), a model that uses time and version concepts to manage dynamic schema evolution in object-oriented databases. The proposed model is able to manage the schema evolution process considering: schema versioning, schema modification, change propagation and data manipulation. TVSE differs from other schema evolution models by enabling the homogeneous and simultaneous management of the evolution history concerning both intentional and extensional databases. Besides defining the model, we also propose a language to derive and modify schema versions, and also to update data associated with them, creating either new object versions or just keeping the history of these data modifications. We provide an operational semantics for this language which is an essential step towards for establishing the preservation of complex time integrity constraints.},
  doi      = {https://doi.org/10.1016/j.datak.2004.07.001},
  keywords = {Temporal object-oriented database, Schema evolution, Schema versioning, Operational semantics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X04001284},
}

@Article{Zhao2013,
  author   = {Xiaohui Zhao and Chengfei Liu},
  title    = {Version management for business process schema evolution},
  journal  = {Information Systems},
  year     = {2013},
  volume   = {38},
  number   = {8},
  pages    = {1046 - 1069},
  issn     = {0306-4379},
  abstract = {The current business environment changes rapidly, dictated by user requirements and market opportunities. Organisations are therefore driven to continuously adapt their business processes to new conditions. Thus, management of business process schema evolution, particularly process version control, is in great demand to capture the dynamics of business process schema changes. This paper aims to facilitate version control for business process schema evolution, with an emphasis on version compatibility, co-existence of multiple versions and dynamic version shifts. A multi-level versioning approach is established to specify dependency between business process schema evolutions, and a novel version preserving graph model is proposed to record business process schema evolutions. A set of business process schema updating operations is devised to support the entire set of process change patterns. By maintaining sufficient and necessary schema and version information, our approach provides comprehensive support for navigating process instance executions of different and changing versions, and deriving the process schema of a certain version. A prototype is also implemented for the proof-of-concept purpose.},
  doi      = {https://doi.org/10.1016/j.is.2013.03.006},
  keywords = {Business process management, Business process schema evolution, Business process version control},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437913000471},
}

@Article{Ordonez2014,
  author   = {Carlos Ordonez and Sofian Maabout and David Sergio Matusevich and Wellington Cabrera},
  title    = {Extending ER models to capture database transformations to build data sets for data mining},
  journal  = {Data \& Knowledge Engineering},
  year     = {2014},
  volume   = {89},
  pages    = {38 - 54},
  issn     = {0169-023X},
  abstract = {In a data mining project developed on a relational database, a significant effort is required to build a data set for analysis. The main reason is that, in general, the database has a collection of normalized tables that must be joined, aggregated and transformed in order to build the required data set. Such scenario results in many complex SQL queries that are written independently from each other, in a disorganized manner. Therefore, the database grows with many tables and views that are not present as entities in the ER model and similar SQL queries are written multiple times, creating problems in database evolution and software maintenance. In this paper, we classify potential database transformations, we extend an ER diagram with entities capturing database transformations and we introduce an algorithm which automates the creation of such extended ER model. We present a case study with a public database illustrating database transformations to build a data set to compute a typical data mining model.},
  doi      = {https://doi.org/10.1016/j.datak.2013.11.002},
  keywords = {ER model, Data mining, Transformation, Denormalization, Aggregation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X13001298},
}

@InCollection{BERTHOLD2003a,
  title     = {Praise for Modeling Business Objects with XML Schema},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {i},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-1-55860-816-0.50022-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500227},
}

@Article{Amer2018,
  author   = {Said Amer and Ahmed ElKhatam and Yasuhiro Fukuda and Lamia I. Bakr and Shereif Zidan and Ahmed Elsify and Mostafa A. Mohamed and Chika Tada and Yutaka Nakai},
  title    = {Clinical, pathological, and molecular data concerning Coenurus cerebralis in Sheep in Egypt},
  journal  = {Data in Brief},
  year     = {2018},
  volume   = {16},
  pages    = {1 - 9},
  issn     = {2352-3409},
  abstract = {This article contains information related to a recent study “Prevalence and Identity of Taenia multiceps cysts “Coenurus cerebralis” in Sheep in Egypt” (Amer et al., 2017) [1]. Specifically, affected sheep showed neurological disorders manifested as depression, head shaking and circling, altered head position, incoordination and paralysis in some cases. Brain-derived cysts were molecularly identified by PCR-sequence analysis at mitochondrial 12S rRNA gene marker. Cyst-induced pathological changes included degenerative changes and demyelination in brain tissue, infiltration of lymphocytes and histiocytes. Cystic fluids were biochemically analyzed for protein, lipids and electrolytes. The data of this study provides more understanding on phylogeny, epidemiology and pathology of coenurosis in sheep.},
  doi      = {https://doi.org/10.1016/j.dib.2017.10.070},
  keywords = {, , Sheep, Pathology, 12S rRNA, Egypt},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352340917305917},
}

@Article{Grandi2003,
  author   = {Fabio Grandi and Federica Mandreoli},
  title    = {A formal model for temporal schema versioning in object-oriented databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {2003},
  volume   = {46},
  number   = {2},
  pages    = {123 - 167},
  issn     = {0169-023X},
  abstract = {In this paper we present a formal model for the support of temporal schema versions in object-oriented databases. Its definition is partially based on a generic (ODMG compatible) object model and partially introduces new concepts. The proposed model supports all the schema changes which are usually considered in the OODB literature, for which an operational semantics and a formal analysis of their correct behaviour is provided. Semantic issues arising from the introduction of temporal schema versioning in a conventional or temporal database (concerning the interaction between the intensional and extensional levels of versioning and the management of data in the presence of multiple schema versions) are also considered.},
  doi      = {https://doi.org/10.1016/S0169-023X(02)00207-0},
  keywords = {Schema versioning, Schema evolution, Temporal versioning, Temporal database},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X02002070},
}

@Article{Rathy2018,
  author   = {Rashmi Rathy and Sayan Paul and Vasanthakumar Ponesakki and Paulkumar Kanniah and Suriya Prabha Muthu and Arun Arumugaperumal and Emmanuel Joshua Jebasingh Sathiya Balasingh Thangapandi and Subburathinam Balakrishnan and Rajendhran Jeyaprakash and Sudhakar Sivasubramaniam},
  title    = {Data on genome sequencing, analysis and annotation of a pathogenic Bacillus cereus 062011msu},
  journal  = {Data in Brief},
  year     = {2018},
  volume   = {17},
  pages    = {15 - 23},
  issn     = {2352-3409},
  abstract = {Bacillus species 062011 msu is a harmful pathogenic strain responsible for causing abscessation in sheep and goat population studied by Mariappan et al. (2012) [1]. The organism specifically targets the female sheep and goat population and results in the reduction of milk and meat production. In the present study, we have performed the whole genome sequencing of the pathogenic isolate using the Ion Torrent sequencing platform and generated 458,944 raw reads with an average length of 198.2bp. The genome sequence was assembled, annotated and analysed for the genetic islands, metabolic pathways, orthologous groups, virulence factors and antibiotic resistance genes associated with the pathogen. Simultaneously the 16S rRNA sequencing study and genome sequence comparison data confirmed that the strain belongs to the species Bacillus cereus and exhibits 99% sequence homo;logy with the genomes of B. cereus ATCC 10987 and B. cereus FRI-35. Hence, we have renamed the organism as Bacillus cereus 062011msu. The Whole Genome Shotgun (WGS) project has been deposited at DDBJ/ENA/GenBank under the accession NTMF00000000 (https://www.ncbi.nlm.nih.gov/bioproject/PRJNA404036(SAMN07629099)).},
  doi      = {https://doi.org/10.1016/j.dib.2017.12.054},
  keywords = {, Genome sequencing, Abscessation, Virulence factors},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352340917307497},
}

@Article{Du2001,
  author   = {Timon C Du and Jen-Long Wu},
  title    = {Using object-oriented paradigm to develop an evolutional vehicle routing system},
  journal  = {Computers in Industry},
  year     = {2001},
  volume   = {44},
  number   = {3},
  pages    = {229 - 249},
  issn     = {0166-3615},
  abstract = {Since the customer requirements and implementation environments change rapidly, the conventional application models have difficulties in satisfying the needs of users. Fortunately, the object-oriented paradigm represents things in terms of objects, and has the advantages of integrating application functions and data management. This advantage provides the evolutional features. Therefore, this study adopts a component assembly model to develop an evolutional system development process including database schema evolution, application object reuse, and the integration. Since a vehicle routing system is a computer system that uses a database to maintain data and application functions to implement delivering algorithms that focus on how to deliver customer orders under different circumstances, the evolution capability is demonstrated in various vehicle routing problems.},
  doi      = {https://doi.org/10.1016/S0166-3615(01)00073-2},
  keywords = {Data management, Information technology, Object-orientated technology, Vehicle routing system},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361501000732},
}

@Article{Dominguez2011,
  author   = {Eladio Domínguez and Jorge Lloret and Beatriz Pérez and Áurea Rodrı´guez and Ángel L. Rubio and Marı´a A. Zapata},
  title    = {Evolution of XML schemas and documents from stereotyped UML class models: A traceable approach},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {1},
  pages    = {34 - 50},
  issn     = {0950-5849},
  abstract = {Context
UML and XML are two of the most commonly used languages in software engineering processes. One of the most critical of these processes is that of model evolution and maintenance. More specifically, when an XML schema is modified, the changes should be propagated to the corresponding XML documents, which must conform with the new, modified schema.
Objective
The goal of this paper is to provide an evolution framework by which the XML schema and documents are incrementally updated according to the changes in the conceptual model (expressed as a UML class model). In this framework, we include the transformation and evolution of UML profiles specified in UML class models because they are widely used to capture domain specific semantics.
Method
We have followed a metamodeling approach which allowed us to achieve a language independent framework, not tied to the specific case of UML–XML. Besides, our proposal considers a traceability setting as a key aspect of the transformation process which allows changes to be propagated from UML class models to both XML schemas and documents.
Results
As a general framework, we propose a Generic Evolution Architecture (GEA) for the model-driven engineering context. Within this architecture and for the particular case of the UML-to-XML setting, our contribution is a UML-to-XML framework that, to our knowledge, is the only approach that incorporates the following four characteristics. Firstly, the evolution tasks are carried out in a conceptual model. Secondly, our approach includes the transformation to XML of UML profiles. Thirdly, the proposal allows stereotyped UML class models to be evolved, propagating changes to XML schemas and documents in such a way that the different elements are kept in synch. Finally, we propose a traceability setting that enables evolution tasks to be performed seamlessly.
Conclusions
Generic frameworks such as that proposed in this paper help to reduce the work overload experienced by software engineers in keeping different software artifacts synchronized.},
  doi      = {https://doi.org/10.1016/j.infsof.2010.08.001},
  keywords = {Model evolution and maintenance, UML profiles, XML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910001333},
}

@Article{Nadal2019,
  author   = {Sergi Nadal and Oscar Romero and Alberto Abelló and Panos Vassiliadis and Stijn Vansummeren},
  title    = {An integration-oriented ontology to govern evolution in Big Data ecosystems},
  journal  = {Information Systems},
  year     = {2019},
  volume   = {79},
  pages    = {3 - 19},
  issn     = {0306-4379},
  note     = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
  abstract = {Big Data architectures allow to flexibly store and process heterogeneous data, from multiple sources, in their original format. The structure of those data, commonly supplied by means of REST APIs, is continuously evolving. Thus data analysts need to adapt their analytical processes after each API release. This gets more challenging when performing an integrated or historical analysis. To cope with such complexity, in this paper, we present the Big Data Integration ontology, the core construct to govern the data integration process under schema evolution by systematically annotating it with information regarding the schema of the sources. We present a query rewriting algorithm that, using the annotated ontology, converts queries posed over the ontology to queries over the sources. To cope with syntactic evolution in the sources, we present an algorithm that semi-automatically adapts the ontology upon new releases. This guarantees ontology-mediated queries to correctly retrieve data from the most recent schema version as well as correctness in historical queries. A functional and performance evaluation on real-world APIs is performed to validate our approach.},
  doi      = {https://doi.org/10.1016/j.is.2018.01.006},
  keywords = {Data integration, Evolution, Semantic web},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437917304660},
}

@Article{Filteau1988,
  author   = {M.C. Filteau and S.K. Kassicieh and R.S. Tripp},
  title    = {Evolutionary database design and development in very large scale MIS},
  journal  = {Information \& Management},
  year     = {1988},
  volume   = {15},
  number   = {4},
  pages    = {203 - 212},
  issn     = {0378-7206},
  abstract = {Developing flexible databases that can accommodate the changes and enhancements necessary in development of large scale Management Information Systems (MIS) is crucial to the success of the system and ultimately to the survival of the organization. Since the MIS is constantly being modified and enhanced, it is necessary for the database to be designed to adapt to change as it occurs. Numerous articles point to the need for large scale systems decomposition into smaller subsystems for implementation purposes. They also require an incremental development strategy which coincides with the database evolutionary strategy. This paper describes a methodology for large scale database development that assists MIS managers in building databases for large scale systems. It describes how these concepts were used for implementation of the US Air Force's largest distributed processing MIS—“The Requirements Data Bank” (RDB).},
  doi      = {https://doi.org/10.1016/0378-7206(88)90046-8},
  keywords = {Large scale MIS systems, Incremental development approach, Database design, MIS prototype, MIS development methodology},
  url      = {http://www.sciencedirect.com/science/article/pii/0378720688900468},
}

@Article{Grolinger2011,
  author   = {Katarina Grolinger and Miriam A.M. Capretz},
  title    = {A unit test approach for database schema evolution},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {2},
  pages    = {159 - 170},
  issn     = {0950-5849},
  abstract = {Context
The constant changes in today’s business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.
Objective
This work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database.
Method
The approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively simple database schema. Although the database schema in this study was simple, it was nevertheless able to demonstrate the advantages of the proposed approach.
Results
After changes in the database schema, the proposed approach found all SELECT statements as well as the majority of other statements requiring modifications in the application code. Due to its efficiency with SELECT statements, the proposed approach is expected to be more successful with database warehouse applications where SELECT statements are dominant.
Conclusion
The unit test approach that accesses databases has proven to be successful in evaluating the application code against the evolved database. In particular, the approach is simple and straightforward to implement, which makes it easily adoptable in practice.},
  doi      = {https://doi.org/10.1016/j.infsof.2010.10.002},
  keywords = {Database schema evolution, Database testing, Unit testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910001734},
}

@Article{Alhajj2003,
  author   = {Reda Alhajj and Faruk Polat},
  title    = {Rule-based schema evolution in object-oriented databases},
  journal  = {Knowledge-Based Systems},
  year     = {2003},
  volume   = {16},
  number   = {1},
  pages    = {47 - 57},
  issn     = {0950-7051},
  abstract = {In this paper, a rule-based mechanism for schema evolution in object-oriented databases is presented. We have benefited from having an object algebra maintaining closure that makes it possible to have the output from a query persistent in the hierarchy. The actual class hierarchy and the corresponding hierarchy which reflects the relationship between operands and results of queries are utilized. In order to have query results reflected into the class hierarchy and classes reflected into the operands hierarchy, we also define mappings between the two hierarchies. As a result, it is possible to maximize reusability in object-oriented databases. The object algebra is utilized to handle basic schema evolution functions without requiring any special set of built-in functions. The invariants and the conflict resolving rules are specified. It is also shown how other schema functions are derivable from the basic ones.},
  doi      = {https://doi.org/10.1016/S0950-7051(02)00051-5},
  keywords = {Rule-based systems, Object-oriented data model, Schema evolution, Conflict resolution, Reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950705102000515},
}

@Article{Goralwalla1998,
  author   = {Iqbal A. Goralwalla and Duane Szafron and M.Tamer Özsu and Randal J. Peters},
  title    = {A temporal approach to managing schema evolution in object database systems},
  journal  = {Data \& Knowledge Engineering},
  year     = {1998},
  volume   = {28},
  number   = {1},
  pages    = {73 - 105},
  issn     = {0169-023X},
  note     = {16th International Conference on Conceptual Modelling},
  abstract = {The issues of schema evolution and temporal object models are generally considered to be orthogonal and are handled independently. However, to properly model applications that need incremental design and experimentation, the evolutionary histories of the schema objects should be traceable rather than corrective so that historical queries can be supported. In this paper we propose a method for managing schema changes, and propagating these changes to object instances by exploiting the functionality of a temporal object model. The result is a uniform treatment of schema evolution and temporal support for many object database management systems applications that require both.},
  doi      = {https://doi.org/10.1016/S0169-023X(98)00014-7},
  keywords = {Schema evolution, Change propagation, Temporal model, Object-oriented, Database},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X98000147},
}

@Article{Hartung2010,
  author   = {Michael Hartung and Frank Loebe and Heinrich Herre and Erhard Rahm},
  title    = {Management of evolving semantic grid metadata within a collaborative platform},
  journal  = {Information Sciences},
  year     = {2010},
  volume   = {180},
  number   = {10},
  pages    = {1837 - 1849},
  issn     = {0020-0255},
  note     = {Special Issue on Intelligent Distributed Information Systems},
  abstract = {Grid environments, providing distributed infrastructures, computing resources and data storage, usually show a high degree of heterogeneity and change in their metadata. We propose a platform for collaborative management and maintenance of common metadata for grids. As the conceptual foundation of this platform, a meta model is presented which distinguishes structured descriptions and classification structures that both are modifiable. On this basis, the system allows for the creation and editing of grid relevant metadata and provides various search and navigation facilities for grid participants. We applied the platform to the German D-Grid initiative by establishing the D-Grid Ontology (DGO).},
  doi      = {https://doi.org/10.1016/j.ins.2009.08.008},
  keywords = {Semantic metadata, Grids, Collaborative environments, Schema evolution, Data migration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025509003417},
}

@Article{Castro1997,
  author   = {Cristina De Castro and Fabio Grandi and Maria Rita Scalas},
  title    = {SCHEMA VERSIONING FOR MULTITEMPORAL RELATIONAL DATABASES††Recommended by Peri Loucopoulos},
  journal  = {Information Systems},
  year     = {1997},
  volume   = {22},
  number   = {5},
  pages    = {249 - 290},
  issn     = {0306-4379},
  abstract = {In order to follow the evolution of application needs, a database management system is easily expected to undergo changes involving database structure after implementation. Schema evolution concerns the ability of maintaining extant data in response to changes in database structure. Schema versioning enables the use of extensional data through multiple schema interface as created by a history of schema changes. However, schema versioning has been considered only to a limited extent in current literature. Also in the field of temporal databases, whereas a great deal of work has been done concerning temporal versioning of extensional data, a thorough investigation of schema versioning potentialities has not yet been made. In this paper we consider schema versioning in a broader perspective and introduce new design options whose distinct semantic properties and functionalities will be discussed. First of all, we consider solutions for schema versioning along transaction time but also along valid time. Moreover, the support of schema versioning implies operations both at intensional and extensional level. Two distinct design solutions (namely single- and multi-pool) are presented for the management of extensional data in a system supporting schema versioning. Finally, a further distinction is introduced to define synchronous and asynchronous management of versioned data and schemata. The proposed solutions differ in their semantics and in the possible operations they support. The mechanisms for the selection of data through a schema version are in many cases strictly related to the particular schema versioning solution adopted, that also affects the data definition and manipulation language at user-interface level. In particular, we show how the temporal language TSQL2, originally designed to support basic functionalities of transaction-time schema versioning, can accordingly be extended. ©1997 Elsevier Science Ltd},
  doi      = {https://doi.org/10.1016/S0306-4379(97)00017-3},
  keywords = {Transaction-Time},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437997000173},
}

@Article{Oubelli2018,
  author   = {Lynda Ait Oubelli and Yamine Aït Ameur and Judicaël Bedouet and Romain Kervarc and Benoît Chausserie-Laprée and Béatrice Larzul},
  title    = {A scalable model based approach for data model evolution: Application to space missions data models},
  journal  = {Computer Languages, Systems \& Structures},
  year     = {2018},
  volume   = {54},
  pages    = {358 - 385},
  issn     = {1477-8424},
  abstract = {During the development of a complex system, data models are the key to a successful engineering process, as they contain and organize all the information manipulated by the different functions involved in the design of the system. Moreover, these data models evolve throughout the design, as the development raises issues that have to be solved through a restructuration of data organization. But any such data model evolution has a deep impact on the functions that have already being defined. Recent research tries to deal with this issue by studying how complex industrial data models evolve from one version to another and how their data instances co-evolve. Complexity and scalability issues make this problem a major scientific challenge, leading to huge gains in development efficiency. This problem is of particular interest in the field of aeronautics and space systems. Indeed, the development of these systems produces many complex data models associated to the designed systems and/or to the systems under design, hence on the one hand data models are available. On the other hand, it is well known that these systems are developed in the context of collaborative projects that may last for decades. In such projects, specifications together with the associated data models are bound to evolve and engineering processes shall take into account this evolution. Our work addresses the problem of data model evolution in a model-driven engineering setting. We focus on minimizing the impact of model evolution on the system development processes in the specific context on the space engineering area, where data models may involve thousands of concepts and relationships, and we investigate the performance of the model-based development (MBD) approach we propose for data model evolution over two space missions, namely PHARAO and MICROSCOPE.},
  doi      = {https://doi.org/10.1016/j.cl.2018.08.001},
  keywords = {Data model comparison, Data model evolution, Data migration, Data conservation, Model driven engineering (MDE), Composite evolution operators},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842418300447},
}

@Article{Genero2008,
  author   = {Marcela Genero and Geert Poels and Mario Piattini},
  title    = {Defining and validating metrics for assessing the understandability of entity–relationship diagrams},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {64},
  number   = {3},
  pages    = {534 - 557},
  issn     = {0169-023X},
  abstract = {Database and data model evolution cause significant problems in the highly dynamic business environment that we experience these days. To support the rapidly changing data requirements of agile companies, conceptual data models, which constitute the foundation of database design, should be sufficiently flexible to be able to incorporate changes easily and smoothly. In order to understand what factors drive the maintainability of conceptual data models and to improve conceptual modelling processes, we need to be able to assess conceptual data model properties and qualities in an objective and cost-efficient manner. The scarcity of early available and thoroughly validated maintainability measurement instruments motivated us to define a set of metrics for Entity–Relationship (ER) diagrams. In this paper we show that these easily calculated and objective metrics, measuring structural properties of ER diagrams, can be used as indicators of the understandability of the diagrams. Understandability is a key factor in determining maintainability as model modifications must be preceded by a thorough understanding of the model. The validation of the metrics as early understandability indicators opens up the way for an in-depth study of how structural properties determine conceptual data model understandability. It also allows building maintenance-related prediction models that can be used in conceptual data modelling practice.},
  doi      = {https://doi.org/10.1016/j.datak.2007.09.011},
  keywords = {Conceptual data modelling, ER diagram, Understandability, Structural properties, Metrics, Measurement theory, Experimental validation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001796},
}

@InCollection{Kapuruge2014,
  author    = {Malinda Kapuruge and Jun Han and Alan Colman},
  title     = {9 - Evaluation},
  booktitle = {Service Orchestration As Organization},
  publisher = {Elsevier},
  year      = {2014},
  editor    = {Malinda Kapuruge and Jun Han and Alan Colman},
  pages     = {213 - 242},
  address   = {Boston},
  isbn      = {978-0-12-800938-3},
  abstract  = {In this chapter we evaluate the Serendip process support. To evaluate the benefits and the viability of the proposed Serendip process modelling approach, three types of evaluations have been performed.},
  doi       = {https://doi.org/10.1016/B978-0-12-800938-3.00009-6},
  keywords  = {Evaluation, adaptation patterns, performance, results, analysis},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780128009383000096},
}

@Article{Klimek2012,
  author   = {Jakub Klímek and Martin Nečaský},
  title    = {On Inheritance in Conceptual Modeling for XML},
  journal  = {Procedia Computer Science},
  year     = {2012},
  volume   = {10},
  pages    = {54 - 61},
  issn     = {1877-0509},
  note     = {ANT 2012 and MobiWIS 2012},
  abstract = {Modern information systems may exploit numerous XML formats for communication. Each message may have its own XML format for data representation which causes problems with evolution of their schemas. Manual change management of the XML formats may be error-prone and time consuming. We tackled this problem in our previous work with the introduction of a formal two level conceptual model for XML which interconnects multiple XML schemas describing parts of a common problem domain on a conceptual level. This allows for well-deﬁned and automated change management of XML schemas. In this paper, we extend our previous work with inheritance modeling. Because inheritance is common in XML schemas and conceptual models in general, its modeling is needed and makes our conceptual model more usable in real world situations. There are two basic types of inheritance when it comes to modeling: structural and conceptual inheritance. We discuss the differences and how these two types need to be reﬂected in our model.},
  doi      = {https://doi.org/10.1016/j.procs.2012.06.011},
  keywords = {XML schema modeling, model driven architecture, inheritance, conceptual model},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050912003687},
}

@Article{Chen1995,
  author   = {Jia-Lin Chen and Dennis McLeod and Daniel O'Leary},
  title    = {Domain-knowledge-guided schema evolution for accounting database systems},
  journal  = {Expert Systems with Applications},
  year     = {1995},
  volume   = {9},
  number   = {4},
  pages    = {491 - 501},
  issn     = {0957-4174},
  note     = {Expert systems in accounting, auditing, and finance},
  abstract = {The static meta-data view of accounting database management is that the schema of a database is designed before the database is populated and remains relatively fixed over the life cycle of the system. However, the need to support accounting database evolution is clear: a static meta-data view of an accounting database cannot support next generation dynamic environment where system migration, organization reengineering, and heterogeneous system interoperation are essential. This paper presents a knowledge-based approach and mechanism to support dynamic accounting database schema evolution in an object-based data modeling context. When an accounting database schema does not meet the requirements of a firm, the schema must be changed. Such schema evolution can be realized via a sequence of evolution operators. As a result, this paper considers the question: what heuristics and knowledge are necessary to guide a system to choose a sequence of operators to complete a given evolution task for an accounting database? In particular, we first define a set of basic evolution schema operators, employing heuristics to guide the evolution process. Second, we explore how domain-specific knowledge can be used to guide the use of the operators to complete the evolution task. A well-known accounting data model, REA model, is used here to guide the schema evolution process. Third, we discuss a prototype system, REAtool, to demonstrate and test our approach.},
  doi      = {https://doi.org/10.1016/0957-4174(95)00019-4},
  url      = {http://www.sciencedirect.com/science/article/pii/0957417495000194},
}

@Article{Karanikolas2009,
  author   = {Nikitas N. Karanikolas and Maria Nitsiou and Emmanuel J. Yannakoudakis and Christos Skourlas},
  title    = {CUDL language semantics: Updating FDB data},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {6},
  pages    = {947 - 962},
  issn     = {0164-1212},
  abstract = {The semantics for data manipulation of the database language CUDL – Conceptual Universal Database Language – designed to manage dynamic database environments, are presented. This language conforms to the FDB (Frame DataBase) data model, offering a simple, easy and efficient platform for the use of the FDB model. Otherwise the management and operation of FDB data is laborious and time-consuming and it requires from the user a very good acquaintance of the proposed model, the structures and organisation of it as well as the processes of the management of elements that compose it. In this paper we present in depth the semantics of the way of handling the data, in order to search and transform information, in an FDB data source. We present the analysis of simple and complex cases that led us to synthesize valid and simple semantic rules that determine the data manipulation operations. The more sophisticated and demanding constructs, used in the language, for query specification, query processing and object manipulation are discussed and evaluated.},
  doi      = {https://doi.org/10.1016/j.jss.2008.12.031},
  keywords = {Database models, Database schema evolution, Database language, Language syntax, Language semantics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121208002744},
}

@Article{Faisal2014,
  author   = {Sidra Faisal and Mansoor Sarwar},
  title    = {Temporal and multi-versioned XML documents: A survey},
  journal  = {Information Processing \& Management},
  year     = {2014},
  volume   = {50},
  number   = {1},
  pages    = {113 - 131},
  issn     = {0306-4573},
  abstract = {Extensible Markup Language (XML) documents are associated with time in two ways: (1) XML documents evolve over time and (2) XML documents contain temporal information. The efficient management of the temporal and multi-versioned XML documents requires optimized use of storage and efficient processing of complex historical queries. This paper provides a comparative analysis of the various schemes available to efficiently store and query the temporal and multi-versioned XML documents based on temporal, change management, versioning, and querying support. Firstly, the paper studies the multi-versioning control schemes to detect, manage, and query change in dynamic XML documents. Secondly, it describes the storage structures used to efficiently store and retrieve XML documents. Thirdly, it provides a comparative analysis of the various commercial tools based on change management, versioning, collaborative editing, and validation support. Finally, the paper presents some future research and development directions for the multi-versioned XML documents.},
  doi      = {https://doi.org/10.1016/j.ipm.2013.08.003},
  keywords = {XML documents, Multi-version, Evolving, Temporal, Data model, Change detection},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457313000939},
}

@Article{Lo2010,
  author   = {Anthony Lo and Tansel Özyer and Radwan Tahboob and Keivan Kianmehr and Jamal Jida and Reda Alhajj},
  title    = {XML materialized views and schema evolution in VIREX},
  journal  = {Information Sciences},
  year     = {2010},
  volume   = {180},
  number   = {24},
  pages    = {4940 - 4957},
  issn     = {0020-0255},
  abstract = {Web-based databases are gaining increased popularity. This has positively influenced the availability of structured and semi-structured databases for access by a variety of users ranging from professionals to naive users. The number of users accessing online databases will continue to increase if the visual tools connected to web-based databases are flexible and user-friendly enough to meet the expectations of naive users and professionals. Further, XML is accepted as the standard for platform independent data exchange. This motivated for the development of the conversion tools between structured databases and XML. Realizing that such a need has not been well handled by the available tools, including Clio from IBM, we developed VIREX as a visual tool for converting relational databases into XML, and since then has been empowered with further capabilities to manipulate the produced XML schema including the maintenance of materialized views and schema evolution functions. VIREX provides an interactive approach for querying and integrating relational databases to produce XML documents and the corresponding XML schema(s). VIREX supports VRXQuery as a visual naive users-oriented query language that allows users to specify queries and define views directly on the interactive diagram as a sequence of mouse clicks with minimum keyboard input. As the query result, VIREX displays on the screen the XML schema that satisfies the specified characteristics and generates colored (easy to read) XML document(s). The main contribution described in this paper is the novel approach for turning query results into materialized views which are maintained to remain consistent with the underlying database. VIREX supports deferred update of XML views by keeping an ordered summary of the necessary and sufficient information required for the process. Each view has a corresponding marker in the ordered summary to indicate the start of the information to be reflected onto the view when it is accessed. When a view is accessed, its marker moves to the head of the list to mark for the next update. In addition, VIREX supports some basic schema evolution functions include renaming, adding and dropping of elements and attributes, among others. The supported schema evolution functions add flexibility to the view maintenance and materialization process.},
  doi      = {https://doi.org/10.1016/j.ins.2010.08.025},
  keywords = {Data conversion, Structured databases, Visual query language, Materialized views, Schema evolution, Deferred update, XML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025510003981},
}

@Article{Pomarede2006,
  author   = {D. Pomarède},
  title    = {Detector description of the ATLAS muon spectrometer and H8 muon testbeam},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2006},
  volume   = {559},
  number   = {1},
  pages    = {181 - 184},
  issn     = {0168-9002},
  note     = {Proceedings of the X International Workshop on Advanced Computing and Analysis Techniques in Physics Research},
  abstract = {The Muon Spectrometer of the ATLAS experiment is a large and complex system of gaseous detectors. The simulation and the reconstruction of muon events require a careful description of these detectors, which either participate in the trigger or in the precision measurements of tracks. A thorough description of the passive materials, such as the toroidal magnet systems, is also needed to account for Coulomb scattering and energy losses. The operation of the muon spectrometer relies on the alignment of its precision chambers, so the geometrical model must fully implement their misalignments and deformations. We present the Detector Description chain employed in the Muon system and its integration in the software framework. The Muon Detector Description has been used successfully in the context of the Data Challenges, where it provides a unique and coherent geometry source for the simulation and reconstruction algorithms. It has also been validated in the context of the experimental program of the H8 testbeams, where analyses of the treatment of chamber alignment in track reconstruction relies crucially upon the detector description model.},
  doi      = {https://doi.org/10.1016/j.nima.2005.11.201},
  keywords = {Detector description, Simulation, Visualization, Software engineering, Tracking detectors},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900205022655},
}

@Article{Krishna2016,
  author   = {P. Radha Krishna and Anushree Khandekar and Kamalakar Karlapalem},
  title    = {Modeling dynamic relationship types for subsets of entity type instances and across entity types},
  journal  = {Information Systems},
  year     = {2016},
  volume   = {60},
  pages    = {114 - 126},
  issn     = {0306-4379},
  abstract = {In a traditional ER model, once we specify a subclass or superclass relationship, any changes to that relationship are treated as schema evolution. Further, ER models are rigid in the sense that once a relationship type is specified across a set of entity types, an instance of relationship type occur when one instance of all participating entity types are specified. Therefore, it is difficult to introduce in a simplified manner all relationship types across subsets of given set of entity types. In this paper, we provide mechanisms to model in our extended ER model: (i) specification of dynamic relationship types across subsets of instances of entity types, (ii) a simplified specification of relationships across subsets of given set of entity types, and (iii) mapping our extended ER model to relational database schema. We also show through an e-contract example the utility of our extended ER model.},
  doi      = {https://doi.org/10.1016/j.is.2016.03.010},
  keywords = {ER model, Star constructs, Relational model, e-Contract meta-model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437916301272},
}

@Article{McKenzie1990,
  author   = {Edwin McKenzie and Richard Snodgrass},
  title    = {Schema evolution and the relational algebra},
  journal  = {Information Systems},
  year     = {1990},
  volume   = {15},
  number   = {2},
  pages    = {207 - 232},
  issn     = {0306-4379},
  abstract = {In this paper we discuss extensions to the conventional relational algebra to support both aspects of transaction time, evolution of a database's contents and evolution of a database's schema. We define a relation's schema to be the relation's temporal signature, a function mapping the relation's attribute names onto their value domains and class, indicating the extent of support for time. We also introduce commands to change a relation, now defined as a triple consisting of a sequence of classes, a sequence of signatures, and a sequence of states. A semantic type of system is required to identify semantically incorrect expressions and to enforce consistency constraints among a relation's class, signature and state following update. We show that these extensions are applicable, without change, to historical algebras that support valid time, yielding an algebraic language for the query and update of temporal databases. The additions preserve the useful properties of the conventional algebra.},
  doi      = {https://doi.org/10.1016/0306-4379(90)90036-O},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799090036O},
}

@Article{Hsu1986,
  author   = {Cheng Hsu},
  title    = {A decision support system for database evolution using data model independent architecture},
  journal  = {Computers \& Operations Research},
  year     = {1986},
  volume   = {13},
  number   = {4},
  pages    = {427 - 436},
  issn     = {0305-0548},
  abstract = {The existing corpus of scientific knowledge in the field of database systems (DBS) has been concerned mostly with such problems as database technologies and system development methodologies. Relatively few efforts have been devoted to the problem of adapting an ongoing DBS in a systematic fashion. Notwithstanding the lack of sufficient prior knowledge, this adaptation problem is critical in DBS management, since a DBS should really be conceived as an evolving structure, rather than a stable one-shot phenomenon. Toward this end, this paper proposes a reliability-based decision support framework for evolving a DBS systematically. Both user satisfaction with the DBS and the usage pattern of it are monitored on a real-time basis and used for controlling it adaptively. Chance-constrained models are proposed to characterize suitable decision rules for DBS evolution decisions ranging from file reorganization to DBS restructing. A four-schema architecture is also proposed for achieving data model independence in a DBS, whereby facilitating the control and evolution of DBS.},
  doi      = {https://doi.org/10.1016/0305-0548(86)90030-4},
  url      = {http://www.sciencedirect.com/science/article/pii/0305054886900304},
}

@Article{Luz2007,
  author   = {Robson da Luz and Mírian Halfeld Ferrari and Martin A. Musicante},
  title    = {Regular expression transformations to extend regular languages (with application to a Datalog XML schema validator)},
  journal  = {Journal of Algorithms},
  year     = {2007},
  volume   = {62},
  number   = {3},
  pages    = {148 - 167},
  issn     = {0196-6774},
  abstract = {An XML schema is a set of rules for defining the allowed sub-elements of any element in an XML document. These rules use regular expressions to define the language of the element's children. Updates to an XML schema are updates to the regular expressions defined by the schema rules. We consider an interactive, data administration tool for XML databases. In this tool, changes on an XML schema are activated by updates that violate the validity of an XML document. Our schema validator is a Datalog program, resulting from the translation of a given XML schema. Changing the schema implies changing the validator. The main contribution of this paper is an algorithm allowing the evolution of XML schemas. This algorithm is based on the computation of new regular expressions to extend a given regular language in a conservative way, trying to foresee the needs of an application. A translation function from schema constraints to Datalog programs is introduced. The validation of an XML tree corresponds to the evaluation of the Datalog program over the tree. Our method allows the maintenance of the Datalog program in an incremental way, i.e., without redoing the entire translation.},
  doi      = {https://doi.org/10.1016/j.jalgor.2007.04.004},
  keywords = {XML, Schema for XML, Schema evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0196677407000326},
}

@Article{Reis2015,
  author   = {Julio Cesar Dos Reis and Cédric Pruski and Chantal Reynaud-Delaître},
  title    = {State-of-the-art on mapping maintenance and challenges towards a fully automatic approach},
  journal  = {Expert Systems with Applications},
  year     = {2015},
  volume   = {42},
  number   = {3},
  pages    = {1465 - 1478},
  issn     = {0957-4174},
  abstract = {In several domains, software applications have intensively used Knowledge Organization Systems (KOS) like database schemas, ontologies, taxonomies and thesauri and their associated semantic correspondences (i.e., mappings). This underlines the relevance and capabilities of KOS and mappings to manage and integrate vast amounts of data. However, the dynamic nature of domain knowledge forces knowledge engineers to constantly modify KOS, to keep them up to date and useful. In this context, the maintenance of mappings affected by KOS evolution still remains an open research issue. Although this problem appears relevant for many different computer science fields, ranging from database to artificial intelligence, literature has so far only superficially addressed it to enable more flexible, automatic and precise solutions. This article presents, discusses and compares existing approaches for maintaining mappings and describes open research challenges.},
  doi      = {https://doi.org/10.1016/j.eswa.2014.08.047},
  keywords = {Knowledge management, Mapping evolution, Mapping adaptation, Mapping maintenance, Ontology alignment, Ontology evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417414005296},
}

@Article{Haubert2018,
  author   = {Louise Haubert and Carlos Eduardo Pouey da Cunha and Graciela Völz Lopes and Wladimir Padilha da Silva},
  title    = {Food isolate Listeria monocytogenes harboring tetM gene plasmid-mediated exchangeable to Enterococcus faecalis on the surface of processed cheese},
  journal  = {Food Research International},
  year     = {2018},
  volume   = {107},
  pages    = {503 - 508},
  issn     = {0963-9969},
  abstract = {The genetic basis of tetracycline resistance in a food isolate Listeria monocytogenes (Lm16) was evaluated. Resistance to tetracycline was associated with the presence of the tetM gene in plasmid DNA. The sequence of tetM showed 100% of similarity with the Enterococcus faecalis sequences found in the EMBL database, suggesting that Lm16 received this gene from E. faecalis. Various size bands were detected in the DNA plasmid analysis, the largest being approximately 54.38 kb. Transferability of the tetM gene was achieved in vitro by agar matings between Lm16 and E. faecalis JH2-2, proving the potential for the spread of tetM by horizontal gene transfer. Furthermore, the conjugation experiments were performed on the surface of processed cheese, confirming the transferability in a food matrix. PCR assays were used to confirm the identity of E. faecalis and to detect the tetM gene in transconjugant bacteria. Additionally, the minimal inhibitory concentration for tetracycline and rifampicin and plasmid profiling were performed. This is the first report of a food isolate L. monocytogenes carrying the tetM gene in plasmid DNA, and it highlights the potential risk of spreading antimicrobial resistance genes between different bacteria.},
  doi      = {https://doi.org/10.1016/j.foodres.2018.02.062},
  keywords = {Antimicrobial resistance, Mobile genetic element, , Conjugation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0963996918301558},
}

@Article{Marenco2004,
  author   = {Luis Marenco and Tzuu-Yi Wang and Gordon Shepherd and Perry L. Miller and Prakash Nadkarni},
  title    = {QIS: A framework for biomedical database federation},
  journal  = {Journal of the American Medical Informatics Association},
  year     = {2004},
  volume   = {11},
  number   = {6},
  pages    = {523 - 534},
  issn     = {1067-5027},
  abstract = {Query Integrator System (QIS) is a database mediator framework intended to address robust data integration from continuously changing heterogeneous data sources in the biosciences. Currently in the advanced prototype stage, it is being used on a production basis to integrate data from neuroscience databases developed for the SenseLab project at Yale University with external neuroscience and genomics databases. The QIS framework uses standard technologies and is intended to be deployable by administrators with a moderate level of technological expertise: It comes with various tools, such as interfaces for the design of distributed queries. The QIS architecture is based on a set of distributed network-based servers, data source servers, integration servers, and ontology servers, that exchange metadata as well as mappings of both metadata and data elements to elements in an ontology. Metadata version difference determination coupled with decomposition of stored queries is used as the basis for partial query recovery when the schema of data sources alters.},
  doi      = {https://doi.org/10.1197/jamia.M1506},
  url      = {http://www.sciencedirect.com/science/article/pii/S1067502704001100},
}

@Article{Claypool2001,
  author   = {Kajal T. Claypool and Elke A. Rundensteiner and George T. Heineman},
  title    = {ROVER: flexible yet consistent evolution of relationships},
  journal  = {Data \& Knowledge Engineering},
  year     = {2001},
  volume   = {39},
  number   = {1},
  pages    = {27 - 50},
  issn     = {0169-023X},
  note     = {19th International Conference on Conceptual Modeling (ER2000)},
  abstract = {Relationships have been repeatedly identified as an important object-oriented modeling construct. Most emerging modeling standards such as the object database management group (ODMG) object model and UML have some support for relationships. However object-oriented database (OODB) systems have largely ignored the existence of relationships during schema evolution. We are the first to propose comprehensive support for relationship evolution. A complete schema evolution facility for any OODB system must provide primitives to manipulate all object model constructs, and maintenance strategies for the structural and referential integrity of the database under such evolution. We propose a set of basic evolution primitives for relationships as well as a compound set of changes that can be applied to the same. However, given the myriad of possible change semantics a user may desire in the future, any pre-defined set is not sufficient. Rather we present a flexible schema evolution framework that allows the user to define new relationship transformations as well as to extend existing ones. Addressing the second problem, namely of updating schema evolution primitives to conform to the new set of invariants, can be a very expensive re-engineering effort. In this paper we present an approach that de-couples the constraints from the schema evolution code, thereby enabling their update without any re-coding effort. We also present an approach that can be used to verify the correctness of these complex evolution operations using the de-coupled constraints.},
  doi      = {https://doi.org/10.1016/S0169-023X(01)00029-5},
  keywords = {Schema evolution, Relationships, Object-oriented databases, Consistency management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X01000295},
}

@Article{LEITCH1998,
  author   = {ILIA J. LEITCH and MARK W. CHASE and MICHAEL D. BENNETT},
  title    = {Phylogenetic Analysis of DNA C-values provides Evidence for a Small Ancestral Genome Size in Flowering Plants},
  journal  = {Annals of Botany},
  year     = {1998},
  volume   = {82},
  pages    = {85 - 94},
  issn     = {0305-7364},
  abstract = {DNA C-value is a highly variable aspect of plant biodiversity whose origin and significance has often attracted general interest. Evaluation of the phylogenetic component of genome size variation is essential for a full explanation of its evolutionary significance but was previously prevented by insufficient data and lack of phylogenetic consensus. However, the recent development for the angiosperms of a DNA C-values database for 2802 species and a robust phylogenetic tree based on a three-gene DNA sequence matrix and 252 non-molecular characters allows meaningful new investigations of genome size in a phylogenetic context. Superimposing data from the former onto the latter shows that whereas all 15 higher order groups for which data are available contain species with small C-values, very large C-values occur in only two distantly related groups. At the lower taxonomic levels within these two groups similar trends were detected, with very large C-values restricted to species in the more derived families. The most parsimonious explanation for these observations is that ancestral angiosperms almost certainly had small genomes, and the possession of very large genomes represents a derived condition that has arisen independently at least twice. In contrast, gymnosperms (sister group to the angiosperms) are characterized by larger C-values than angiosperms. Thus within extant seed plants the possession of a small genome is a character unique to the angiosperms that was not only present in the ancestral species but has also been retained in most living taxa.},
  doi      = {https://doi.org/10.1006/anbo.1998.0783},
  keywords = {DNA C-value database, evolution, phylogeny, angiosperms, ancestral genome size.},
  url      = {http://www.sciencedirect.com/science/article/pii/S0305736498907831},
}

@Article{Zadahmad2015,
  author   = {Manouchehr Zadahmad and Parisa YousefzadehFard and Mortaza Abbaszadeh},
  title    = {A Data Model to Support Context-aware Mobile Cloud Collaboration Scenarios},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2015},
  volume   = {195},
  pages    = {1602 - 1608},
  issn     = {1877-0428},
  note     = {World Conference on Technology, Innovation and Entrepreneurship},
  abstract = {In mobile cloud computing systems, collaborators share the software and hardware infrastructure. Achieving a satisfactory throughput partly depends on providing a well-designed data model. It helps to achieve fewer database objects and improve data access, mobile cloud integration and service customization. This study uses a XSD (XML Schema Definition) approach to provide a flexible Meta data. Exploiting such pliable model, mobile clouds will be able to achieve important objectives such as effective cloud integration and customization.},
  doi      = {https://doi.org/10.1016/j.sbspro.2015.06.200},
  keywords = {Mobile Cloud Computing (MCC), Cloud collaboration, XSD (XML Schema Definition), Metamodel, Context awareness.},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877042815036794},
}

@Article{Dong2010,
  author   = {Gan Dong and Zhipeng Gao and Xuesong Qiu},
  title    = {Automatic Approach to Ontology Evolution Based on Change Impact Comparisons},
  journal  = {Tsinghua Science \& Technology},
  year     = {2010},
  volume   = {15},
  number   = {6},
  pages    = {716 - 723},
  issn     = {1007-0214},
  abstract = {Ontology evolution is the timely adaptation of ontologies to changing requirements, which is becoming more and more important as ontologies become widely used in different fields. This paper shows how to address the problem of evolving ontologies with less manual case-based reasoning using an automatic selection mechanism. An automatic ontology evolution strategy selection framework is presented that automates the evolution. A minimal change impact algorithm is also developed for the framework. The method is shown to be effective in a case study.},
  doi      = {https://doi.org/10.1016/S1007-0214(10)70120-6},
  keywords = {ontology, ontology evolution, automatic ontology evolution strategy selection (AOESS), minimal change impact},
  url      = {http://www.sciencedirect.com/science/article/pii/S1007021410701206},
}

@Article{Fahrner1997,
  author   = {Christian Fahrner and Thomas Marx and Stephan Philippi},
  title    = {DICE: declarative integrity constraint embedding into the object database standard ODMG-93},
  journal  = {Data \& Knowledge Engineering},
  year     = {1997},
  volume   = {23},
  number   = {2},
  pages    = {119 - 145},
  issn     = {0169-023X},
  abstract = {In this paper we present a new approach for embedding integrity constraints into object-oriented database systems (OODBS), which can not be specified implicitly by structure or explicitly by keywords of the system. To take those integrity constraints into consideration they have to be integrated into the behavioral part of an object-oriented schema. This paper gives a survey of the existing proposals for this integration and presents a new one, DICE, to overcome the drawbacks of the reviewed solutions. As an example we describe the integration of DICE into object-oriented schema according to ODMG-93.},
  doi      = {https://doi.org/10.1016/S0169-023X(96)00046-8},
  keywords = {Integrity constraint, Constraint checking, Object-oriented database system, ODMG, Schema evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X96000468},
}

@Article{Talwar2012,
  author   = {Kanika Talwar and Anjana Gosain},
  title    = {Hierarchy Classification for Data Warehouse: A Survey},
  journal  = {Procedia Technology},
  year     = {2012},
  volume   = {6},
  pages    = {460 - 468},
  issn     = {2212-0173},
  note     = {2nd International Conference on Communication, Computing \&amp; Security [ICCCS-2012]},
  abstract = {In data warehouse systems, the hierarchies play a key role in processing and monitoring information. These hierarchies dynamically analyze huge volumes of historical data in data warehouses at various granularity levels using OLAP operations like roll-up and drill-down. Through these operations we can get summarized as well as detailed data which aids in analysis as well as decision making process. Several authors have defined hierarchies deriving from real-world applications in order to represent broad range of business scenarios. But there is a need to properly categorize dimension hierarchies so as to adequately model them during evolution. In this paper we have provided a comprehensive comparison of different categories of hierarchies proposed by various researchers based on certain parameters.},
  doi      = {https://doi.org/10.1016/j.protcy.2012.10.055},
  keywords = {Data warehouse systems, Multi-dimensional schema, Dimension hierarchies},
  url      = {http://www.sciencedirect.com/science/article/pii/S2212017312006007},
}

@Article{Javed2008,
  author   = {Faizan Javed and Marjan Mernik and Jeff Gray and Barrett R. Bryant},
  title    = {MARS: A metamodel recovery system using grammar inference},
  journal  = {Information and Software Technology},
  year     = {2008},
  volume   = {50},
  number   = {9},
  pages    = {948 - 968},
  issn     = {0950-5849},
  abstract = {Domain-specific modeling (DSM) assists subject matter experts in describing the essential characteristics of a problem in their domain. When a metamodel is lost, repositories of domain models can become orphaned from their defining metamodel. Within the purview of model-driven engineering, the ability to recover the design knowledge in a repository of legacy models is needed. In this paper we describe MARS, a semi-automatic grammar-centric system that leverages grammar inference techniques to solve the metamodel recovery problem. The paper also contains an applicative case study, as well as experimental results from the recovery of several metamodels in diverse domains.},
  doi      = {https://doi.org/10.1016/j.infsof.2007.08.003},
  keywords = {Domain-specific modeling, Metamodeling, Reverse engineering, Grammar inference},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584907000882},
}

@Article{Roddick1992,
  author   = {John F Roddick and Jon D Patrick},
  title    = {Temporal semantics in information systems—A survey},
  journal  = {Information Systems},
  year     = {1992},
  volume   = {17},
  number   = {3},
  pages    = {249 - 267},
  issn     = {0306-4379},
  abstract = {If a computer system is to deal with temporal semantics, it must understand the nature of time and have the ability to accept and reason with time-related facts. This reasoning ranges from the knowledge and use of the chronological nature of a given calendar system, to the more complex nature of inductive reasoning between related events and time periods. This paper investigates the handling of time as it has been applied to the fields of data modelling and artificial intelligence. Systems using the techniques are investigated. Significant features and properties are then extracted and examined where they are pertinent to systems capable of modelling temporal data.},
  doi      = {https://doi.org/10.1016/0306-4379(92)90016-G},
  keywords = {Temporal databases, temporal semantics, temporal reasoning, schema evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799290016G},
}

@Article{Dominguez2008,
  author   = {Eladio Domínguez and Jorge Lloret and Ángel L. Rubio and María A. Zapata},
  title    = {MeDEA: A database evolution architecture with traceability},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {65},
  number   = {3},
  pages    = {419 - 441},
  issn     = {0169-023X},
  abstract = {One of the most important challenges that software engineers (designers, developers) still have to face in their everyday work is the evolution of working database systems. As a step for the solution of this problem in this paper we propose MeDEA, which stands for Metamodel-based Database Evolution Architecture. MeDEA is a generic evolution architecture that allows us to maintain the traceability between the different artifacts involved in any database development process. MeDEA is generic in the sense that it is independent of the particular modeling techniques being used. In order to achieve this, a metamodeling approach has been followed for the development of MeDEA. The other basic characteristic of the architecture is the inclusion of a specific component devoted to storing the translation of conceptual schemas to logical ones. This component, which is one of the most noteworthy contributions of our approach, enables any modification (evolution) realized on a conceptual schema to be traced to the corresponding logical schema, without having to regenerate this schema from scratch, and furthermore to be propagated to the physical and extensional levels.},
  doi      = {https://doi.org/10.1016/j.datak.2007.12.001},
  keywords = {Conceptual modeling, Database schemas evolution and maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07002224},
}

@Article{Cleve2015,
  author   = {Anthony Cleve and Maxime Gobert and Loup Meurice and Jerome Maes and Jens Weber},
  title    = {Understanding database schema evolution: A case study},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {97},
  pages    = {113 - 121},
  issn     = {0167-6423},
  note     = {Special Issue on New Ideas and Emerging Results in Understanding Software},
  abstract = {Database reverse engineering (DRE) has traditionally been carried out by considering three main information sources: (1) the database schema, (2) the stored data, and (3) the application programs. Not all of these information sources are always available, or of sufficient quality to inform the DRE process. For example, getting access to real-world data is often extremely problematic for information systems that maintain private data. In recent years, the analysis of the evolution history of software programs have gained an increasing role in reverse engineering in general, but comparatively little such research has been carried out in the context of database reverse engineering. The goal of this paper is to contribute to narrowing this gap and exploring the use of the database evolution history as an additional information source to aid database schema reverse engineering. We present a tool-supported method for analyzing the evolution history of legacy databases, and we report on a large-scale case study of reverse engineering a complex information system and curate it as a benchmark for future research efforts within the community.},
  doi      = {https://doi.org/10.1016/j.scico.2013.11.025},
  keywords = {Database understanding, Schema evolution, Software repository mining},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313003092},
}

@Article{Paige2016,
  author   = {Richard F. Paige and Nicholas Matragkas and Louis M. Rose},
  title    = {Evolving models in Model-Driven Engineering: State-of-the-art and future challenges},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {111},
  pages    = {272 - 280},
  issn     = {0164-1212},
  abstract = {The artefacts used in Model-Driven Engineering (MDE) evolve as a matter of course: models are modified and updated as part of the engineering process; metamodels change as a result of domain analysis and standardisation efforts; and the operations applied to models change as engineering requirements change. MDE artefacts are inter-related, and simultaneously constrain each other, making evolution a challenge to manage. We discuss some of the key problems of evolution in MDE, summarise the key state-of-the-art, and look forward to new challenges in research in this area.},
  doi      = {https://doi.org/10.1016/j.jss.2015.08.047},
  keywords = {Evolution, Migration, Co-evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215001909},
}

@Article{Lee1998,
  author   = {Sang-Won Lee and Hyoung-Joo Kim},
  title    = {A model of schema versions for object-oriented databases based on the concept of rich base schema},
  journal  = {Information and Software Technology},
  year     = {1998},
  volume   = {40},
  number   = {3},
  pages    = {157 - 173},
  issn     = {0950-5849},
  abstract = {In this paper, we propose a model of schema versions for object-oriented databases called RiBS. At the heart of this model is the concept of the rich base schema called (RiBS). In our model, each schema version is in the form of a class hierarchy view over one base schema, called RiBS, which has richer schema information than any existing schema version in the database. Users are supposed to be concerned only with schema versions. Direct schema updates on schema versions are allowed, and their effects are, if necessary, automatically propagated to RiBS. We first describe the structural part of the model and then introduce a set of invariants that should always be satisfied by structural parts. As the third element of our model, we give a set of schema update operations, the semantics of which are defined, so as to preserve all the invariants. Another contribution of this paper is the work on schema-version-merging within the RiBS model. We identify several conflicts in schema-version-merging, and then provide a semi-automatic schema-version-merging algorithm to resolve these conflicts. This algorithm is semi-automatic in the sense that it requires minimal user involvement during schema-version-merging.},
  doi      = {https://doi.org/10.1016/S0950-5849(98)00037-8},
  keywords = {Schema version, Schema evolution, View, Schema integration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584998000378},
}

@Article{Enbley1998,
  author  = {David W. Enbley and Robert C. Goldstein},
  title   = {16th international conference on conceptual modeling (ER' 97)},
  journal = {Data \& Knowledge Engineering},
  year    = {1998},
  volume  = {28},
  number  = {1},
  pages   = {1 - 2},
  issn    = {0169-023X},
  note    = {16th International Conference on Conceptual Modelling},
  doi     = {https://doi.org/10.1016/S0169-023X(98)00010-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X9800010X},
}

@InCollection{Yue2018,
  author    = {Peng Yue and Zhenyu Tan},
  title     = {1.06 - GIS Databases and NoSQL Databases},
  booktitle = {Comprehensive Geographic Information Systems},
  publisher = {Elsevier},
  year      = {2018},
  editor    = {Bo Huang},
  pages     = {50 - 79},
  address   = {Oxford},
  isbn      = {978-0-12-804793-4},
  abstract  = {The 21st century has been an era of information explosion. Huge volumes of spatial data have been created, collected than ever before. The constant stream of spatial data poses a great challenge for traditional data storage systems. This article reviews some fundamental concepts of spatial databases and database models for geographic information systems and introduces several types of databases that are employed for geospatial data, including traditional data storage solutions and the most popular NoSQL approaches for big data. And an overview of the history and development of spatial databases is represented to better explain the relevant technologies of spatial databases. Finally, some possible solutions for spatiotemporal big data are suggested.},
  doi       = {https://doi.org/10.1016/B978-0-12-409548-9.09596-8},
  keywords  = {Array database, CAP theorem, Distributed file system, Document database, Geospatial data, Graph database, Key-value database, NoSQL database, Object database, Relational database management system, Relational-object database, Spatial database, Spatial index, Spatiotemporal big data, Wide-column database},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124095489095968},
}

@Article{1996,
  title   = {Subject index to volumes 11–20 (1993–1996)},
  journal = {Data \& Knowledge Engineering},
  year    = {1996},
  volume  = {20},
  number  = {3},
  pages   = {429 - 434},
  issn    = {0169-023X},
  note    = {Modeling Parts and Wholes},
  doi     = {https://doi.org/10.1016/S0169-023X(96)90015-4},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X96900154},
}

@InCollection{Galar2017,
  author    = {Diego Galar and Uday Kumar},
  title     = {Chapter 2 - Data Collection},
  booktitle = {eMaintenance},
  publisher = {Academic Press},
  year      = {2017},
  editor    = {Diego Galar and Uday Kumar},
  pages     = {73 - 128},
  isbn      = {978-0-12-811153-6},
  abstract  = {Industrial planners and managers need to understand the dynamics of complete supply chain, i.e., stocks, operations, infrastructure, communities, and individuals involved in the sector to set policy and manage industrial assets. Data collection and analysis, for example, can provide information on how industry is likely to respond to different policies. Constraints on production and development of new factories can be identified. Prices and cost changes in the manufacturing facilities can be assessed. Stocks likely to receive increased levels of exploitation may be identified before resource levels drop to a crisis point.},
  doi       = {https://doi.org/10.1016/B978-0-12-811153-6.00002-6},
  keywords  = {ETL process, Internet of things, Random data, Secure erase, SQL, Write zero},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780128111536000026},
}

@Article{Videau2019,
  author   = {M. Videau and D. Lebel and J.-F. Bussières},
  title    = {Drug shortages in Canada: Data for 2016–2017 and perspectives on the problem},
  journal  = {Annales Pharmaceutiques Françaises},
  year     = {2019},
  issn     = {0003-4509},
  abstract = {Summary
Objectives
Many signs point to the growing importance of drug shortages in Canada and around the world. Although drug shortages affect clinicians and patients every day, there is a paucity of literature describing the specific problems experienced and their clinical consequences. To describe the drug shortage situation in Canada in 2016–2017 and to discuss this issue in the Canadian context.
Methods
This retrospective study was based on data from one Canadian wholesaler (McKesson Canada) and the official Drug Shortages Canada website.
Results
From August 31, 2016, to September 4, 2017, the McKesson database showed 583 drug shortages, averaging 160 (standard deviation [SD] 180) days, and the drug shortage website showed 2,129 shortages, averaging 118 (SD 113) days. Of these shortages, 26% in the McKesson database and 14% at the official drug shortage website were for parenteral products. In both the McKesson database and the Canadian drug shortage database, the leading drug classes with shortages were central nervous system drugs (26.4% and 31.8%, respectively), cardiovascular drugs (12.0% and 21.9%), anti-infective agents (11.2% and 8.5%), gastrointestinal drugs (7.9% and 6.2%) and antineoplastic agents (7.4% and 5.1%).
Conclusions
This descriptive study highlights the high number of shortages in Canada in 2016–2017. The new federal regulation requiring declaration of drug shortages should lead to better monitoring of this problem at the national level. Although the causes of shortages are often identified, manufacturers and regulators are frequently unable to address or effectively prevent drug shortages.
Résumé
Objectifs
De nombreux signes témoignent de l’importance croissante des ruptures de médicaments au Canada et ailleurs dans le monde. Bien que les ruptures affectent quotidiennement les cliniciens et les patients, il existe encore trop peu de littérature décrivant cette problématique et leurs conséquences cliniques. L’objectif est de décrire la situation relative aux ruptures au Canada en 2016–2017 et de discuter de cette problématique dans le contexte canadien.
Méthodes
Il s’agit d’une étude descriptive rétrospective fondée sur les données d’un grossiste canadien (McKesson Canada) et du site officiel canadien de déclaration des ruptures.
Résultats
Du 31 août 2016 au 4 septembre 2017, on dénombre 583 médicaments en rupture chez McKesson (160±180jours en moyenne), et 2129 sur le site canadien (118±113jours en moyenne). Parmi ces ruptures, 26 % chez McKesson et 14 % sur le site canadien sont des médicaments administrés par voie parentérale. Les principales classes thérapeutiques de médicaments en rupture chez McKesson et sur le site canadien sont les médicaments du système nerveux central (26,4 % et 31,8 %), cardiovasculaires (12,0 % et 21,9 %), les anti-infectieux (11,2 % et 8,5 %), les médicaments gastro-intestinaux (7,9 % et 6,2 %) et les antinéoplasiques (7,4 % et 5,1 %).
Conclusion
Cette étude met en évidence un nombre élevé de ruptures au Canada en 2016–2017. La nouvelle réglementation sur l’obligation de déclaration des ruptures de médicaments permettra probablement un meilleur suivi de cette problématique à l’échelle nationale. Bien que les causes de ruptures soient souvent identifiées, les fabricants et les autorités réglementaires sont encore trop souvent impuissants pour lutter et prévenir efficacement les ruptures de médicaments.},
  doi      = {https://doi.org/10.1016/j.pharma.2018.11.007},
  keywords = {Drug shortages, Canada, Pharmacy practice, Short supply, Pharmacy, Logistics, Pénuries de médicaments, Canada, Pratique pharmaceutique, Pénurie, Pharmacie, Logistique},
  url      = {http://www.sciencedirect.com/science/article/pii/S0003450918301299},
}

@Article{Monk1996,
  author   = {Simon Monk and John A. Mariani and Beshir Elgalal and Helen Campbell},
  title    = {Migration from relational to object-oriented databases},
  journal  = {Information and Software Technology},
  year     = {1996},
  volume   = {38},
  number   = {7},
  pages    = {467 - 475},
  issn     = {0950-5849},
  abstract = {This paper examines the issues involved in the migration of database technology towards object-orientation. This includes issues of schema translation, data migration and the consideration of the whole database system including client programs. In addition, this paper reports some early results from a long-term project to provide support for the migration of data and meta-data from a relational to an object-oriented database. The initial tool processes the schema of a relational database and presents it to the user as the equivalent object-oriented schema. The result of this processing is an initial version of the object schema and requires user intervention in the naming of new abstract classes built by the system.},
  doi      = {https://doi.org/10.1016/0950-5849(95)01090-4},
  keywords = {Schema migration, Schema transformation, Database migration},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584995010904},
}

@Article{Laender2001,
  author  = {Alberto H.F Laender and Veda C Storey},
  title   = {19th International Conference on Conceptual Modeling (ER2000)},
  journal = {Data \& Knowledge Engineering},
  year    = {2001},
  volume  = {39},
  number  = {1},
  pages   = {1 - 2},
  issn    = {0169-023X},
  note    = {19th International Conference on Conceptual Modeling (ER2000)},
  doi     = {https://doi.org/10.1016/S0169-023X(01)00027-1},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X01000271},
}

@Article{Edmond2000,
  author   = {David Edmond and Arthur H.M. ter Hofstede},
  title    = {A reflective infrastructure for workflow adaptability},
  journal  = {Data \& Knowledge Engineering},
  year     = {2000},
  volume   = {34},
  number   = {3},
  pages    = {271 - 304},
  issn     = {0169-023X},
  abstract = {We present a flexible framework that enables workflow systems to adapt to changing conditions. The model is designed to reveal key aspects of the tasks involved in representing and enacting business processes. These fundamental characteristics are identified as state, behaviour, distribution, coordination and enactment. By isolating such core concepts in a way that allows them to be varied, we open up the general process of task coordination and execution, allowing for extensions in a planned way. By suitable manipulation of each of these aspects, at the appropriate level, a workflow system may be extensively modified in a way that minimises the effect of such change upon other aspects of the system.},
  doi      = {https://doi.org/10.1016/S0169-023X(00)00018-5},
  keywords = {Workflow, Reflection, Adaptation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X00000185},
}

@InCollection{BERTHOLD2003b,
  author    = {BERTHOLD DAUM},
  title     = {Introduction},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {xxi - xxv},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-155860816-0/50002-1},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500021},
}

@InCollection{Teorey2011,
  author    = {Toby Teorey and Sam Lightstone and Tom Nadeau and H.V. Jagadish},
  title     = {1 - Introduction},
  booktitle = {Database Modeling and Design (Fifth Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2011},
  editor    = {Toby Teorey and Sam Lightstone and Tom Nadeau and H.V. Jagadish},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {1 - 11},
  address   = {Boston},
  edition   = {Fifth Edition},
  isbn      = {978-0-12-382020-4},
  abstract  = {Publisher Summary
This chapter reviews the basic concepts of database management and introduce the role of data modeling and database design in the database life cycle. The basic component of a file in a file system is a data item, which is the smallest named unit of data that has meaning in the real world. A group of related data items treated as a unit by an application is called a record. Examples of types of records are order, salesperson, customer, product, and department. A file is a collection of records of a single type. Database systems have built upon and expanded these definitions: In a relational database, a data item is called a column or attribute, a record is called a row or tuple, and a file is called a table. A database is a more complex object; it is a collection of interrelated stored data that serves the needs of multiple users within one or more organizations—that is, an interrelated collection of many different types of tables. A database management system (DBMS) is a generalized software system for manipulating databases. A DBMS supports a logical view (schema, subschema); physical view (access methods, data clustering); data definition language; data manipulation language; and important utilities such as transaction management and concurrency control, data integrity, crash recovery, and security. Among the variety of data modeling approaches, the entity-relationship (ER) and Unified Modeling Language (UML) data models are arguably the most popular in use today because of their simplicity and readability. Knowledge of data modeling and database design techniques is important for database practitioners and application developers.},
  doi       = {https://doi.org/10.1016/B978-0-12-382020-4.00001-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012382020400001X},
}

@InCollection{Johnston2014a,
  author    = {Tom Johnston},
  title     = {Chapter 19 - Time, Types and the Future of Relational Databases},
  booktitle = {Bitemporal Data},
  publisher = {Morgan Kaufmann},
  year      = {2014},
  editor    = {Tom Johnston},
  pages     = {315 - 336},
  address   = {Boston},
  isbn      = {978-0-12-408067-6},
  abstract  = {This chapter describes how, in the future, relational databases might more completely and explicitly manage both time and types. It is a future in which people are associated with statements by means of different speech acts they perform with those statements, and in which speech acts, statements, and inscriptions are each associated with one of three distinct and orthogonal temporal dimensions. It is a future in which relational databases are linked to an extended ontology whose upper-level categories are common to all of them.},
  doi       = {https://doi.org/10.1016/B978-0-12-408067-6.00019-X},
  keywords  = {atomic statement, binary table, bridge ontology, descriptive ontology, extensible ontology, folk ontology, homonym, inscription, knowledge base, predicate, proposition, propositional attitude, referent, schema evolution, selective ontology, semantic interoperability, speech act, statement, statement provenance, structured ontology, subject, taxonomy, tritemporal},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012408067600019X},
}

@Article{Ardito2014,
  author   = {Carmelo Ardito and Paolo Bottoni and Maria Francesca Costabile and Giuseppe Desolda and Maristella Matera and Matteo Picozzi},
  title    = {Creation and use of service-based Distributed Interactive Workspaces},
  journal  = {Journal of Visual Languages \& Computing},
  year     = {2014},
  volume   = {25},
  number   = {6},
  pages    = {717 - 726},
  issn     = {1045-926X},
  note     = {Distributed Multimedia Systems DMS2014 Part I},
  abstract = {Distributed Interactive Workspaces (DIWs) are interactive environments, accessible through different devices, where end users create new content by exploring and aggregating data retrieved from distributed resources in the Web, tailor this content to their own personal needs, use it on different devices, and possibly share and co-create it with others. The need for collaborating with other people by means of DIWs is an important requirement that emerged in field studies conducted in different domains. This paper shows the extension of a platform for mashup composition to support collaboration through DIWs. In particular, it considers the possibility of producing annotated versions of DIWs, to add specific information and make it available to others without corrupting the original resources. It also investigates techniques for synchronous collaboration that enable a distributed creation and execution of the interactive workspaces on different devices and by different users.},
  doi      = {https://doi.org/10.1016/j.jvlc.2014.10.018},
  keywords = {Distributed interactive workspaces, Collaboration, Human-centric service composition},
  url      = {http://www.sciencedirect.com/science/article/pii/S1045926X14001116},
}

@Article{Breslin2013,
  author   = {Eamonn Breslin and Angelika Kaufmann and Siobhan Quenby},
  title    = {Bilirubin influences the clinical presentation of pre-eclampsia},
  journal  = {European Journal of Obstetrics \& Gynecology and Reproductive Biology},
  year     = {2013},
  volume   = {170},
  number   = {1},
  pages    = {111 - 113},
  issn     = {0301-2115},
  abstract = {Objectives
Pre-eclampsia is a placental, inflammatory disease modified by maternal anti-oxidant status to give a syndrome. In its most severe forms pre-eclampsia is followed by maternal and neonatal mortality and morbidity. Bilirubin is a known antioxidant and as such is associated with a reduced risk of cardiovascular and respiratory disease. Hence we aimed to find an association between maternal bilirubin levels and the clinical severity of the disease.
Study design
A retrospective observational study of 50,712 pregnancies, 925 of which had pre-eclampsia (1999–2010), to examine the association between bilirubin level and perinatal outcome.
Results
In women with pre-eclampsia, those with bilirubin levels in the lowest quintile were more likely to require caesarean section (p=0.001, aOR 2.59 (1.52–5.72)). The lowest quintile of bilirubin levels is associated with an increased risk of poor maternal (p=0.002, aOR 3.52 (95%CI 1.6–7.7)) and infant/fetal (p=0.001, OR=3.05 (95%CI=1.63–5.72)) outcome.
Conclusions
Low levels of bilirubin were associated with poor maternal and infant outcomes in women diagnosed with pre-eclampsia. Bilirubin may act as an anti-oxidant in this condition and thus modify the disease.},
  doi      = {https://doi.org/10.1016/j.ejogrb.2013.05.024},
  keywords = {Pre-eclampsia, Outcome, Prediction, Bilirubin},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301211513002558},
}

@Article{Aubry1996,
  author   = {F. Aubry and V. Chameroy and R. Di Paola},
  title    = {A medical image object-oriented database with image processing and automatic reorganization capabilities},
  journal  = {Computerized Medical Imaging and Graphics},
  year     = {1996},
  volume   = {20},
  number   = {4},
  pages    = {315 - 331},
  issn     = {0895-6111},
  note     = {Medical Image Databases},
  abstract = {The paper presents the medical image database developed for use in the methodological research environment and in the laboratory clinical environment, designed to be capable of being interfaced to an image processing system. This database is intended to solve the numerous problems due to the complexity—multidimensionality and multimodality—of medical images. These problems are posed in terms of management, archiving, structuring and accessing of this archive, and specification of the interfaces with users and with image processing systems. Solving these problems involves making a formal description of the image and the data associated with the image, while taking into account the specifics of medical imaging. The kernel which contains this description allows the physical architecture of the management and archiving system to be decoupled from its logical architecture. This decoupling is essential in order to automate the recording of new data, the automatic reorganization of the system schema in the event of change in the system environment, and to help in consulting the database.},
  doi      = {https://doi.org/10.1016/S0895-6111(96)00022-5},
  keywords = {Directory services, Image processing, Medical Image Database, Referential dependency graph, Meta-schema (of a database), Object oriented DBMS, Relational DBMS, Semantic model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895611196000225},
}

@Article{Burckhart1998,
  author   = {D. Burckhart and R. Jones and L. Mapelli and M. Michelotto and A. Patel and M. Skiadelli and I. Soloviev and P-Y. Duval and A. Le Van Suu and R. Nacasch and Z. Qian and F. Touchard and M. Caprini and S. Kolos and K. Nurdan and S. Wheeler},
  title    = {Software technologies for a prototype ATLAS DAQ},
  journal  = {Computer Physics Communications},
  year     = {1998},
  volume   = {110},
  number   = {1},
  pages    = {113 - 119},
  issn     = {0010-4655},
  abstract = {The ATLAS collaboration has defined a set of user requirements for the back-end software subsystem within the context of the data acquisition and event filter prototype “−1” project. Based on these requirements, a number of evaluations have been performed on candidate technologies and techniques in the areas of configuration data storage (Objectivity ODBMS; Rogue Wave Tools.h++ for C++ object persistence), inter-process communication (Corba; MPI), dynamic object behaviour (Harel StateChart generator), graphical user interfaces (cross-platform GUI builder; Java AWT) and software integration (ACE operating-system interface). This paper describes the important requirements which lead to the selection of these technologies, the results obtained from the evaluations and how we intend to apply them to the design and implementation phases of the project.},
  doi      = {https://doi.org/10.1016/S0010-4655(97)00163-X},
  url      = {http://www.sciencedirect.com/science/article/pii/S001046559700163X},
}

@Article{Mariani2001,
  author   = {J.A Mariani and A Kadyamatimba},
  title    = {OgDesk: an orthogonal graphical interface for object-oriented database systems that supports schema management, browsing and querying},
  journal  = {Information and Software Technology},
  year     = {2001},
  volume   = {43},
  number   = {7},
  pages    = {425 - 446},
  issn     = {0950-5849},
  abstract = {This paper reports on a graphical interface, OgDesk, for object-oriented database systems (OODBs). It orthogonally supports the activities of•Browsing types and instances•Creating and maintaining instances•Creating, using and programming with graphical representations of single type queries•Creation, maintenance and evolution of the schema.},
  doi      = {https://doi.org/10.1016/S0950-5849(01)00148-3},
  keywords = {Object oriented databases, Graphical user interface, Browsing, creating and maintaining types, instances, schemas and queries},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584901001483},
}

@Article{1991,
  title   = {Author index of volume 6 (1991)},
  journal = {Data \& Knowledge Engineering},
  year    = {1991},
  volume  = {6},
  number  = {6},
  pages   = {541 - 542},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(91)90027-U},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9190027U},
}

@Article{Goellue1996,
  author   = {Aleks Göllü and Akash Deshpande},
  title    = {Object Management Systems1},
  journal  = {IFAC Proceedings Volumes},
  year     = {1996},
  volume   = {29},
  number   = {1},
  pages    = {5494 - 5499},
  issn     = {1474-6670},
  note     = {13th World Congress of IFAC, 1996, San Francisco USA, 30 June - 5 July},
  abstract = {We describe a new approach for developing large-scale object-oriented software systems, which we call Object Management Systems (OMS). OMS are model-based distributed applications used for managing complex physical environments. The management functions supported by OMS are configuration, fault, performance, accounting, access and security, resource, and planning management. The OMS Tool Set consists of a semantic data and process model called the OMS object model, the SmartDB software platform, customized extensions of SmartDB-SmartNet, SmartPower, and SmartAHS-for specific application domains, and the OMS Software Engineering process. The OMS-based development process consists of three stages: domain customization, system architecture, and application programming. Each stage produces specifications and implementations in software integrated around the OMS Tool Set. This software development life-cycle reduces project risk, budget, and schedule.},
  doi      = {https://doi.org/10.1016/S1474-6670(17)58556-0},
  keywords = {object modeling, systems methodology, databases, hybrid, software tools, management systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667017585560},
}

@InCollection{McComb2003,
  author    = {Dave McComb},
  title     = {Chapter 6 - Metadata},
  booktitle = {Semantics in Business Systems},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Dave McComb},
  series    = {The Savvy Manager;#x0027;s Guides},
  pages     = {87 - 107},
  address   = {Burlington},
  isbn      = {978-1-55860-917-4},
  abstract  = {Publisher Summary
Metadata is data about data. Metadata has gone from being a documentation aid to being a central part of a development environment, and in some cases it has become the application itself. As “data about data,” it is almost pure semantics; that is, it stores the meaning of the data it describes. The business systems industry has changed metadata from being a data dictionary to being a style of development that recognizes that the definitions of the data are subject to change in the same way as the rest of the data in an application. Metadata-driven systems are more flexible than traditional designs. This chapter presents a case study of the powerful advantages that are available when one models metadata as data. Applications become both more flexible and more powerful.},
  doi       = {https://doi.org/10.1016/B978-155860917-4/50008-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558609174500088},
}

@Article{Monk1994,
  author   = {S.R. Monk},
  title    = {View definition in an object-oriented database},
  journal  = {Information and Software Technology},
  year     = {1994},
  volume   = {36},
  number   = {9},
  pages    = {549 - 554},
  issn     = {0950-5849},
  abstract = {The paper describes a novel method for creating views in an object-oriented database system by using a mechanism termed ‘dynamic instance conversion’. In this system, views are not defined as the result of a query expression but rather by reference to the class of which a view is to be made. It is argued that this approach is better suited to OODBs than the use of query expressions as it is more in keeping with the object-oriented paradigm. Instances of a class are automatically converted between views to appear in the format demanded by a query. This happens without loss of information by the instances, thus ensuring that changes to a view are ‘updated’ correctly. A Graphical User Interface (GUI) to the view system is also described. This GUI facilitates the creation of views and provides support for the definition of methods used to update view changes.},
  doi      = {https://doi.org/10.1016/0950-5849(94)90100-7},
  keywords = {database views, object-oriented database, schema browser},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584994901007},
}

@Article{Li1994,
  author   = {Qing Li and Guozhu Dong},
  title    = {A framework for object migration in object-oriented databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {1994},
  volume   = {13},
  number   = {3},
  pages    = {221 - 242},
  issn     = {0169-023X},
  abstract = {This article presents a framework developed for accomodating various object migrations in ‘statically-typed’ object databases. Requirements for supporting object migrations are stipulated, and a conceptual model for describing and facilitating different kinds of migrations is described. Associated issues of controlling such migrations are then addressed, along with an initial investigation on the interence of implied migration paths and the completeness of migration operators. Some guidelines are then given to help users conduct migrations more effectively. An implementation prototype on top of an object-oriented database system was built, which embodies full support of all migration types specified in the migration model.},
  doi      = {https://doi.org/10.1016/0169-023X(94)00015-8},
  keywords = {Object-oriented databases, Static classification, Object migrations, Migration control specification, Information-preserving completeness},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X94000158},
}

@Article{Demaid2006,
  author   = {A. Demaid and S. Ogden and J. Zucker},
  title    = {Access Enhancement Objects for data management in Smalltalk},
  journal  = {Computer Languages, Systems \& Structures},
  year     = {2006},
  volume   = {32},
  number   = {4},
  pages    = {185 - 202},
  issn     = {1477-8424},
  abstract = {This paper reports an approach to persistence in object-oriented languages, such as Smalltalk, that require a memory-resident image for computation. The architecture uses a selective persistence that provides sufficient data-handling, with support to evolutionary data description and data consultation, for an engineering domain application without any special syntactic extension to the application language. The approach is oriented to knowledge representation and has been tested on materials data information, from several sources, that informs engineering design modelling. The architecture described is an extension of Smalltalk that uses Access Enhancement Objects, which we term Persistence Enhancers, for data management, in conjunction with a storage model optimized for domain modelling.},
  doi      = {https://doi.org/10.1016/j.cl.2005.04.002},
  keywords = {Prototype, Object-oriented, Persistence, Smalltalk, Enhancer, Storage management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842405000217},
}

@Article{Libis2016,
  author   = {Vincent Libis and Baudoin Delépine and Jean-Loup Faulon},
  title    = {Sensing new chemicals with bacterial transcription factors},
  journal  = {Current Opinion in Microbiology},
  year     = {2016},
  volume   = {33},
  pages    = {105 - 112},
  issn     = {1369-5274},
  note     = {Antimicrobials • Microbial systems biology},
  abstract = {Bacteria rely on allosteric transcription factors (aTFs) to sense a wide range of chemicals. The variety of effectors has contributed in making aTFs the most used input system in synthetic biological circuits. Considering their enabling role in biotechnology, an important question concerns the size of the chemical space that can potentially be detected by these biosensors. From digging into the ever changing repertoire of natural regulatory circuits, to advances in aTF engineering, we review here different strategies that are pushing the boundaries of this chemical space. We also review natural and synthetic cases of indirect sensing, where aTFs work in combination with metabolism to enable detection of new molecules.},
  doi      = {https://doi.org/10.1016/j.mib.2016.07.006},
  url      = {http://www.sciencedirect.com/science/article/pii/S1369527416300947},
}

@Article{1993,
  title   = {Subject index to volume 11 (1993)},
  journal = {Data \& Knowledge Engineering},
  year    = {1993},
  volume  = {11},
  number  = {3},
  pages   = {319},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(93)90028-N},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9390028N},
}

@Article{Mehdi2017,
  author   = {Mohamad Mehdi and Chitu Okoli and Mostafa Mesgari and Finn Årup Nielsen and Arto Lanamäki},
  title    = {Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus},
  journal  = {Information Processing \& Management},
  year     = {2017},
  volume   = {53},
  number   = {2},
  pages    = {505 - 529},
  issn     = {0306-4573},
  abstract = {Although primarily an encyclopedia, Wikipedia’s expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia.},
  doi      = {https://doi.org/10.1016/j.ipm.2016.07.003},
  keywords = {Information retrieval, Information extraction, Natural language processing, Ontologies, Wikipedia, Literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306457316303004},
}

@InCollection{Gray2003,
  author    = {Jim Gray},
  title     = {Foreword},
  booktitle = {Advanced SQL:1999},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Jim Melton},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {vii - viii},
  address   = {San Francisco},
  isbn      = {978-1-55860-677-7},
  doi       = {https://doi.org/10.1016/B978-155860677-7/50000-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558606777500006},
}

@Article{Ramon-Carbonell2017,
  author   = {Marta de Ramón-Carbonell and Paloma Sánchez-Torres},
  title    = {Involvement of Penicillium digitatum PdSUT1 in fungicide sensitivity and virulence during citrus fruit infection},
  journal  = {Microbiological Research},
  year     = {2017},
  volume   = {203},
  pages    = {57 - 67},
  issn     = {0944-5013},
  abstract = {A putative sucrose transporter PdSUT1 included in the same clade that Sut1p from Schizosaccharomyces pombe was identified in Penicillium digitatum, the major citrus postharvest pathogen. PdSUT1 gene was characterized using target gene disruption and gene overexpression. The ΔPdSUT1 mutants generated by gene elimination showed reduction in fungal virulence during citrus fruit infection assayed in mature fruit at 20°C. However, the overexpression mutants did not increased disease severity neither in the mutants coming from a high virulent nor from a low virulent P. digitatum progenitor strains. Moreover, fungicide sensitivity was affected in the deletant mutants but not in the overexpression transformants. The expression analysis of several genes involved in fungicide resistance showed an intensification of MFS transporters and a decrease of sterol demethylases transcriptional abundance in the ΔPdSUT1 mutants compare to the parental wild type strain. PdSUT1 appear not to be directly involved in fungicide resistance although can affect the gene expression of fungicide related genes. These results indicate that PdSUT1 contribute to P. digitatum fungal virulence and influence fungicide sensitivity through carbohydrate uptake and MFS transporters gene activation.},
  doi      = {https://doi.org/10.1016/j.micres.2017.06.008},
  keywords = {Fungicide resistance, Host pathogen interaction, , Sucrose transporter, Virulence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0944501317302045},
}

@Article{1998,
  title   = {Subject index to volume 28 (1999)},
  journal = {Data \& Knowledge Engineering},
  year    = {1998},
  volume  = {28},
  number  = {3},
  pages   = {343},
  issn    = {0169-023X},
  note    = {Next Generation Information Technologies and Systems},
  doi     = {https://doi.org/10.1016/S0169-023X(98)00037-8},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X98000378},
}

@Article{Al-Amri2018,
  author   = {Ahmed H. Al-Amri and Abeer Al Saegh and Watfa Al-Mamari and Mohammed E. El-Asrag and Mohammed N. Al-Kindi and Mazin Al Khabouri and Nadia Al Wardy and Khalsa Al Lamki and Ahlam Gabr and Ahmed Idris and Chris F. Inglehearn and Steven J. Clapcote and Manir Ali},
  title    = {LHFPL5 mutation: A rare cause of non-syndromic autosomal recessive hearing loss},
  journal  = {European Journal of Medical Genetics},
  year     = {2018},
  issn     = {1769-7212},
  abstract = {Hearing loss is a debilitating disorder that impairs language acquisition, resulting in disability in children and potential isolation in adulthood. Its onset can have a genetic basis, though environmental factors, which are often preventable, can also cause the condition. The genetic forms are highly heterogeneous, and early detection is necessary to arrange appropriate patient support. Here we report the molecular basis of hereditary hearing loss in a consanguineous family with multiple affected members from Oman. Combining homozygosity mapping with whole exome sequencing identified a novel homozygous nucleotide substitution c.575T > C in the lipoma HMGIC fusion partner-like 5 gene (LHFPL5), that converted the 192nd amino acid residue in the protein from a leucine to a proline, p.(Leu192Pro). Sanger sequencing confirmed segregation with the disease phenotype as expected for a recessive condition and the variant was absent in 123,490 subjects from various disease-specific and population genetic studies as well as 150 unrelated individuals and 35 deaf patients of Omani ethnicity. This study, which describes a novel LHFPL5 mutation in a family of Omani origin with hereditary hearing loss, supports previous clinical descriptions of the condition and contributes to the genetic spectrum of mutations in this form of deafness.},
  doi      = {https://doi.org/10.1016/j.ejmg.2018.11.026},
  keywords = {Deafness, Homozygosity mapping, Exome sequencing, LHFPL5},
  url      = {http://www.sciencedirect.com/science/article/pii/S1769721218301022},
}

@Article{Goller2015,
  author   = {Mathias Goller and Stefan Berger},
  title    = {Handling measurement function changes with Slowly Changing Measures},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {53},
  pages    = {107 - 123},
  issn     = {0306-4379},
  abstract = {Data Warehouses (DWs) are historical databases on business events, organized as multi-dimensional hypercubes that support analytical decision making. Extract, Transform, and Load (ETL) processes apply measurement functions to compute parameterized scores from the business events, such as sales figures, customer reliability scores, churn likelihood, or sentiment indices. These scores, saved as measures in the DW, serve as basis for analytical reports and corporate performance analysis. Dimensions model subject-oriented data used as analysis perspectives when interpreting the measures. While measures and measurement functions are traditionally regarded as stable within the DW schema, its dimension values commonly change over time. In reality, measures are also subject to change if DW designers (i) tune a parameter of the underlying measurement function, or (ii) update the scoring algorithm as a whole. In both scenarios, the changes must be obvious to the business analysts. Otherwise the changed measure semantics leads to incomparable measure values, and thus unsound and worthless analysis results. To handle measure evolution properly, this paper proposes Slowly Changing Measures (SCMs) as an additional DW modeling concept. Its core idea is a valid time for measurement functions, analogous to dimensions. The paper proposes four increasingly rich SCM types, each specifying a set of options for the change history management of measure definitions. Most of the SCM types provide for probable changes of measurement functions at design time, reducing manual interference to the necessary minimum upon the actual change. Each SCM type is explained in detail, and illustrated using a practical scenario. Furthermore, the paper presents a proof-of-concept prototype based on the TPC-H business model, and implemented in a simple relational database system. The pros and cons of every SCM type are discussed, and recommendations given for their implementation in a practical DW system.},
  doi      = {https://doi.org/10.1016/j.is.2014.12.009},
  keywords = {Data Warehouse Modeling and Design, Evolving Measurement, Functions, OLAP Modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915000022},
}

@Article{1995,
  title   = {Index},
  journal = {Information and Software Technology},
  year    = {1995},
  volume  = {37},
  number  = {12},
  pages   = {715 - 719},
  issn    = {0950-5849},
  doi     = {https://doi.org/10.1016/0950-5849(95)90021-7},
  url     = {http://www.sciencedirect.com/science/article/pii/0950584995900217},
}

@InCollection{Shasha2003,
  author    = {Dennis Shasha and Philippe Bonnet},
  title     = {8 - Tuning e-commerce applications},
  booktitle = {Database Tuning},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Dennis Shasha and Philippe Bonnet},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {243 - 260},
  address   = {San Francisco},
  isbn      = {978-1-55860-753-8},
  abstract  = {Publisher Summary
This chapter describes the fundamental architecture underlying every e-commerce system and reviews the tuning considerations that apply to that architecture. E-commerce applications often have a three-tiered architecture consisting of Web servers, application servers, and database servers. Web servers are responsible for presentation. They call functions from the underlying application servers via server extensions such as servlets or dynamic HTML interpreters such as ASE. They deliver HTML or XML pages back to the client browsers. The application servers are responsible for the business logic. They implement the functions exposed to the clients. Typically, these functions include search, update shopping cart, pay, or create account. Each function uses data from a local cache or from the underlying database server and outputs HTML or XML pages. The database servers perform data access. The queries that the application servers submit to the database servers are characterized in the chapter; and capacity planning is also reviewed.},
  doi       = {https://doi.org/10.1016/B978-155860753-8/50009-3},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558607538500093},
}

@Article{Kamel1993,
  author   = {Nabil Kamel},
  title    = {Page-queries as a tool for organizing secondary memory auxiliary databases. II. Optimal selection of SADB contents},
  journal  = {Information Sciences},
  year     = {1993},
  volume   = {69},
  number   = {1},
  pages    = {127 - 156},
  issn     = {0020-0255},
  abstract = {In this paper, a specific aspect of a new approach to organizing secondary memory redundancy in centralized databases is investigated. The approach is based on optimal management of a special type of redundant temporaries, collectively called the auxiliary database. The specific aspect investigated is the problem of optimal selection of the contents of the auxilary database. The new approach is based on exploiting the irregularities in both the distribution of data values in the database and in its access patterns. In order to exploit the nonuniformity in the distribution of both queries and data, special atomic data constructs, called page-queries are used as the smallest indivisible information units in the auxiliary database. A page-query is roughly defined as the result of one query when it is processed against only one memory page. It is shown that use of these atomic data packets as the only building blocks in secondary memory auxilary databases allows us to realize optimization potential that could otherwise not be achieved. The problem of maintaining the auxiliary database in response to database evolution is approached in a self-adaptive fashion by periodically reorganizing its contents to suit the common usage and data distribution patterns. A mathematical programming formulation of the problem is given for which a fast solution algorithm is known. The approach taken to handle updates is to keep the main database always up to date and to adaptively select the contents of the temporaries based on their update history. Measurements obtained from a real implementation are presented and provide preliminary verification of the proposed approach.},
  doi      = {https://doi.org/10.1016/0020-0255(93)90042-K},
  url      = {http://www.sciencedirect.com/science/article/pii/002002559390042K},
}

@Article{1993a,
  title   = {Author index to volumes 1–10 (1985–1993)},
  journal = {Data \& Knowledge Engineering},
  year    = {1993},
  volume  = {10},
  number  = {3},
  pages   = {349 - 353},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(93)90039-R},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9390039R},
}

@Article{Solimando2014,
  author   = {Alessandro Solimando and Giorgio Delzanno and Giovanna Guerrini},
  title    = {Validating XML document adaptations via Hedge Automata transformations},
  journal  = {Theoretical Computer Science},
  year     = {2014},
  volume   = {560},
  pages    = {251 - 268},
  issn     = {0304-3975},
  note     = {Games, Automata, Logic and Formal Verification},
  abstract = {We present an automata-based method for the static analysis of user-defined XML document adaptations, expressed as sequences of update primitives of XQuery Update. The key feature of the method is the use of an automatic inference algorithm for extracting the type, expressed as a Hedge Automaton, of a sequence of document updates. The type is computed starting from the original schema S and from rewriting rules that formally define the operational semantics of a sequence of document updates. Type inclusion can then be used as a conformance test w.r.t. the type extracted from the target schema S′.},
  doi      = {https://doi.org/10.1016/j.tcs.2014.04.023},
  keywords = {Hedge Automata, XML Schema, XML, Hedge rewriting systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304397514003156},
}

@Article{1993b,
  title   = {Index},
  journal = {Information and Software Technology},
  year    = {1993},
  volume  = {35},
  number  = {11},
  pages   = {709 - 712},
  issn    = {0950-5849},
  doi     = {https://doi.org/10.1016/0950-5849(93)90099-O},
  url     = {http://www.sciencedirect.com/science/article/pii/095058499390099O},
}

@InCollection{Agrawal2002,
  author    = {Rakesh Agrawal and Ramakrishnan Srikant and Yirong Xu},
  title     = {Chapter 100 - Database technologies for electronic commerce},
  booktitle = {VLDB '02: Proceedings of the 28th International Conference on Very Large Databases},
  publisher = {Morgan Kaufmann},
  year      = {2002},
  editor    = {Philip A. Bernstein and Yannis E. Ioannidis and Raghu Ramakrishnan and Dimitris Papadias},
  pages     = {1055 - 1058},
  address   = {San Francisco},
  isbn      = {978-1-55860-869-6},
  abstract  = {Publisher Summary
Electronic commerce applications have posed new challenges for database systems. Electronic commerce applications such as portals, marketplaces, and online stores (Amazon.com, eBay.com), are often faced with the problem of quickly integrating new catalogs from different sources into their existing catalog (the “master” catalog). Electronic commerce applications have made it imperative for databases to support direct querying of database content from the web. However, the most popular Web interface is the Google-style search box, and queries submitted from such an interface may include neither attribute names nor units.},
  doi       = {https://doi.org/10.1016/B978-155860869-6/50107-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608696501074},
}

@Article{Cranmer2008,
  author   = {K.S. Cranmer},
  title    = {The ATLAS Analysis Architecture},
  journal  = {Nuclear Physics B - Proceedings Supplements},
  year     = {2008},
  volume   = {177-178},
  pages    = {126 - 130},
  issn     = {0920-5632},
  note     = {Proceedings of the Hadron Collider Physics Symposium 2007},
  abstract = {We present an overview of the ATLAS analysis architecture including the relevant aspects of the computing model and the major architectural aspects of the Athena framework. Emphasis will be given to the interplay between the analysis use cases and the technical aspects of the architecture including the design of the event data model, transient-persistent separation, data reduction strategies, analysis tools, and ROOT interoperability.},
  doi      = {https://doi.org/10.1016/j.nuclphysbps.2007.11.096},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920563207009255},
}

@Article{Shin2016,
  author   = {Yeseul Shin and Seok-Seong Kang and Jayoung Paek and Tae Eun Jin and Hong Seok Song and Hongik Kim and Hee-Moon Park and Young-Hyo Chang},
  title    = {Clostridium kogasensis sp. nov., a novel member of the genus Clostridium, isolated from soil under a corroded gas pipeline},
  journal  = {Anaerobe},
  year     = {2016},
  volume   = {39},
  pages    = {14 - 18},
  issn     = {1075-9964},
  abstract = {Two bacterial strains, YHK0403T and YHK0508, isolated from soil under a corroded gas pipe line, were revealed as Gram-negative, obligately anaerobic, spore-forming and mesophilic bacteria. The cells were rod-shaped and motile by means of peritrichous flagella. Phylogenetic analysis based on 16S rRNA gene sequences indicated that the isolates were members of the genus Clostridium and were the most closely related to Clostridiumscatologenes KCTC 5588T (95.8% sequence similarity), followed by Clostridiummagnum KCTC 15177T (95.8%), Clostridiumdrakei KCTC 5440T (95.7%) and Clostridiumtyrobutyricum KCTC 5387T (94.9%). The G + C contents of the isolates were 29.6 mol%. Peptidoglycan in the cell wall was of the A1γ type with meso-diaminopimelic acid. The major polar lipid was diphosphatidylglycerol (DPG), and other minor lipids were revealed as phosphatidylglycerol (PG), phosphatidylethanolamine (PE), two unknown glycolipids (GL1 and GL2), an unknown aminoglycolipid (NGL), two unknown aminophospholipids (PN1 and PN2) and four unknown phospholipids (PL1 to PL4). Predominant fatty acids were C16:0 and C16:1cis9 DMA. The major end products from glucose fermentation were identified as butyrate (12.2 mmol) and acetate (9.8 mmol). Collectively, the results from a wide range of phenotypic tests, chemotaxonomic tests, and phylogenetic analysis indicated that the two isolates represent novel species of the genus Clostridium, for which the name Clostridium kogasensis sp. nov. (type strain, YHK0403T = KCTC 15258T = JCM 18719T) is proposed.},
  doi      = {https://doi.org/10.1016/j.anaerobe.2016.02.006},
  keywords = {, Anaerobic, Phylogenetic, New species},
  url      = {http://www.sciencedirect.com/science/article/pii/S1075996416300099},
}

@Article{Kos2015,
  author   = {Jernej Kos and Mitar Milutinović and Luka Čehovin},
  title    = {nodewatcher: A substrate for growing your own community network},
  journal  = {Computer Networks},
  year     = {2015},
  volume   = {93},
  pages    = {279 - 296},
  issn     = {1389-1286},
  note     = {Community Networks},
  abstract = {Community networks differ from regular networks by their organic growth patterns—there is no central planning body that would decide how the network is built. Instead, the network grows in a bottom-up fashion as more people express interest in participating in the community and connect with their neighbors. People who participate in community networks are usually volunteers with limited free time. Due to these factors, making the management of community networks simpler and easier for all participants is the key component in boosting their growth. Specifics of individual networks often force communities to develop their own sets of tools and best practices which are hard to share and do not interoperate well with others. We propose a new general community network management platform nodewatcher that is built around the core principle of modularity and extensibility, making it suitable for reuse by different community networks. Devices are configured using a platform-independent configuration which nodewatcher can transform into deployable firmware images, eliminating any manual device configuration, reducing errors, and enabling participation of novice maintainers. An embedded monitoring system enables live overview and validation of the whole community network. We show how the system successfully operates in an actual community wireless network, wlan slovenija.},
  doi      = {https://doi.org/10.1016/j.comnet.2015.09.021},
  keywords = {Community networks, Management, Provisioning, Monitoring, Wireless, Mesh},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389128615003400},
}

@Article{Rezgui2011,
  author   = {Yacine Rezgui and Stefan Boddy and Matthew Wetherill and Grahame Cooper},
  title    = {Past, present and future of information and knowledge sharing in the construction industry: Towards semantic service-based e-construction?},
  journal  = {Computer-Aided Design},
  year     = {2011},
  volume   = {43},
  number   = {5},
  pages    = {502 - 515},
  issn     = {0010-4485},
  note     = {Emerging Industry Needs for Frameworks and Technologies for Exchanging and Sharing Product Lifecycle Knowledge},
  abstract = {The paper reviews product data technology initiatives in the construction sector and provides a synthesis of related ICT industry needs. A comparison between (a) the data centric characteristics of Product Data Technology (PDT) and (b) ontology with a focus on semantics, is given, highlighting the pros and cons of each approach. The paper advocates the migration from data-centric application integration to ontology-based business process support, and proposes inter-enterprise collaboration architectures and frameworks based on semantic services, underpinned by ontology-based knowledge structures. The paper discusses the main reasons behind the low industry take up of product data technology, and proposes a preliminary roadmap for the wide industry diffusion of the proposed approach. In this respect, the paper stresses the value of adopting alliance-based modes of operation.},
  doi      = {https://doi.org/10.1016/j.cad.2009.06.005},
  keywords = {Product data technology, STEP (ISO 10303), IFC, BIM, Ontology, CAD, Web service, e-Process},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010448509001766},
}

@Article{1998a,
  title   = {Author index to volume 28 (1999)},
  journal = {Data \& Knowledge Engineering},
  year    = {1998},
  volume  = {28},
  number  = {3},
  pages   = {341 - 342},
  issn    = {0169-023X},
  note    = {Next Generation Information Technologies and Systems},
  doi     = {https://doi.org/10.1016/S0169-023X(98)00036-6},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X98000366},
}

@Article{Shah1999,
  author   = {Abad Shah and Farshad Fotouhi and William Grosky},
  title    = {Share-kno: Knowledge Sharing Mechanism of the Temporal Object System},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  year     = {1999},
  volume   = {11},
  pages    = {25 - 43},
  issn     = {1319-1578},
  abstract = {Temporal Object System (TOS) handles the structural and stature changes to an object in a uniform and temporal fashion. The TOS takes a hybrid approach of the class-based and prototype-based approaches in modeling the real-word objects and knowledge sharing among the objects. This hybrid approach makes it flexible than the two approaches, i.e., the class-based approach and the prototype-based approach. In this paper, we propose a model of knowledge sharing mechanism, Share-kno (SK) for object hierarchy of the TOS. We also formally prove that the Share-kno (SK) model encapsulates more knowledge than the inheritance hierarchy and delegation hierarchy models proposed by Stein.},
  doi      = {https://doi.org/10.1016/S1319-1578(99)80002-0},
  keywords = {Object-oriented Databases, Temporal Database, Temporal Objects, Knowledge Sharing, Mechanism, Inheritance, Delegation, Modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S1319157899800020},
}

@Article{Shahian2019,
  author  = {David M. Shahian and Felix G. Fernandez and Vinay Badhwar},
  title   = {The Society of Thoracic Surgeons National Database at 30: Honoring Our Heritage, Celebrating the Present, Evolving for the Future},
  journal = {The Annals of Thoracic Surgery},
  year    = {2019},
  volume  = {107},
  number  = {4},
  pages   = {1259 - 1266},
  issn    = {0003-4975},
  doi     = {https://doi.org/10.1016/j.athoracsur.2019.02.002},
  url     = {http://www.sciencedirect.com/science/article/pii/S0003497519302322},
}

@Article{Marti2017,
  author   = {Jonathan Martí and Anna Queralt and Daniel Gasull and Alex Barceló and Juan José Costa and Toni Cortes},
  title    = {Dataclay: A distributed data store for effective inter-player data sharing},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {129 - 145},
  issn     = {0164-1212},
  abstract = {In the Big Data era, both the academic community and industry agree that a crucial point to obtain the maximum benefits from the explosive data growth is integrating information from different sources, and also combining methodologies to analyze and process it . For this reason, sharing data so that third parties can build new applications or services based on it is nowadays a trend . Although most data sharing initiatives are based on public data, the ability to reuse data generated by private companies is starting to gain importance as some of them (such as Google, Twitter, BBC or New York Times) are providing access to part of their data. However, current solutions for sharing data with third parties are not fully convenient to either or both data owners and data consumers. Therefore we present dataClay, a distributed data store designed to share data with external players in a secure and flexible way based on the concepts of identity and encapsulation. We also prove that dataClay is comparable in terms of performance with trendy NoSQL technologies while providing extra functionality, and resolves impedance mismatch issues based on the Object Oriented paradigm for data representation.},
  doi      = {https://doi.org/10.1016/j.jss.2017.05.080},
  keywords = {Data sharing, Distributed databases, NoSQL, Storage systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217301012},
}

@InCollection{Velegrakis2003,
  author    = {Yannis Velegrakis and Renée J. Miller and Lucian Popa},
  title     = {- Mapping Adaptation under Evolving Schemas},
  booktitle = {Proceedings 2003 VLDB Conference},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Johann-Christoph Freytag and Peter Lockemann and Serge Abiteboul and Michael Carey and Patricia Selinger and Andreas Heuer},
  pages     = {584 - 595},
  address   = {San Francisco},
  isbn      = {978-0-12-722442-8},
  abstract  = {Publisher Summary
This chapter identifies the problem of mapping adaptation in dynamic environments with evolving schemas. To achieve interoperability, modem information systems and e-commerce applications use mappings to translate data from one representation to another. In dynamic environments like the Web, data sources may change not only their data but also their schemas, their semantics, and their query capabilities. Such changes must be reflected in the mappings. The chapter motivates the need for an automated system to adapt mappings and describes several areas in which the solutions can be applied. This chapter presents a novel framework and a tool, Toronto Mapping Adaptation System (ToMAS), that automatically maintains the consistency of the mappings as schemas evolve. The approach is unique in many ways. It considers and manages a very general class of mappings including GLAV mappings. This chapter considers changes not only on the schema structure but also on the schema semantics either in the source or in the target. Further, it supports schema changes that involve multiple schema elements.},
  doi       = {https://doi.org/10.1016/B978-012722442-8/50058-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780127224428500586},
}

@Article{Wang1996a,
  author   = {Huaiqing Wang and Chen Wang},
  title    = {APACS: a multi-agent system with repository support},
  journal  = {Knowledge-Based Systems},
  year     = {1996},
  volume   = {9},
  number   = {5},
  pages    = {329 - 337},
  issn     = {0950-7051},
  abstract = {In this paper, we present APACS, Advanced Plant Analysis and Control System, a multi-agent system (MAS) with repository support. APACS has been designed and implemented for monitoring and diagnosing real-time nuclear power plant failures. Specifically, we demonstrate the importance of repository technology in achieving knowledge communication within a multi-agent system. In this paper, we will outline important design and conceptual problems such as the choice of data model, the portability problem, the transparency problem and the maintenance problem, encountered during our practical experience that will no doubt be encountered by other designers of MAS systems, as well as presenting our practical solutions and their benefits. It is hoped that this experience will be of interest to researchers due to its innovative solutions and their impacts in the conceptual frameworks of next generation MASs, and of interest to practitioners who may benefit from the demonstration of both the feasibility and the economical and technical benefits of knowledge sharing using repositories in the design and implementation of MASs.},
  doi      = {https://doi.org/10.1016/0950-7051(96)01043-X},
  keywords = {Multiple agents, Information repository, Knowledge sharing},
  url      = {http://www.sciencedirect.com/science/article/pii/095070519601043X},
}

@Article{McMahon1995,
  author   = {C.A. McMahon and D.J. Pitt},
  title    = {Hybrid computer database systems for materials engineering},
  journal  = {Materials \& Design},
  year     = {1995},
  volume   = {16},
  number   = {1},
  pages    = {3 - 13},
  issn     = {0261-3069},
  abstract = {Many approaches have been taken in the computer-based organization and delivery of engineering materials data and expertise, but relatively little attention has been given to the provision of information to designers working in adaptive or variant design areas in which only incremental design changes are made. The information required is characterized by the variety of its formats and sources, and by the complexity of the interrelationships between data. This paper describes a system designed to allow diverse computer-based information to be viewed, indexed and linked together. The system uses a combination of hypertext and database technology, with novel elements such as hypertext links that carry out database search, and a search feedback mechanism that gives continually updated information on the search terms that are valid for the current search state. Three materials data sets developed using the system are described. The first is a materials database combined with applications details and micrographs and with hypertext links to explanations of terms. The second is a database of test results from a large research programme, and the third a collection of good practice guidelines for fatigue design.},
  doi      = {https://doi.org/10.1016/0261-3069(95)00009-N},
  keywords = {materials information, databases, hypertext, fatigue design},
  url      = {http://www.sciencedirect.com/science/article/pii/026130699500009N},
}

@InCollection{Felfernig2014,
  title     = {Subject Index},
  booktitle = {Knowledge-Based Configuration},
  publisher = {Morgan Kaufmann},
  year      = {2014},
  editor    = {Alexander Felfernig and Lothar Hotz and Claire Bagley and Juha Tiihonen},
  pages     = {353 - 357},
  address   = {Boston},
  isbn      = {978-0-12-415817-7},
  doi       = {https://doi.org/10.1016/B978-0-12-415817-7.00038-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124158177000384},
}

@Article{Kefena2012,
  author   = {E. Kefena and Y. Mekasha and J.L. Han and S. Rosenbom and A. Haile and T. Dessie and A. Beja-Pereira},
  title    = {Discordances between morphological systematics and molecular taxonomy in the stem line of equids: A review of the case of taxonomy of genus Equus},
  journal  = {Livestock Science},
  year     = {2012},
  volume   = {143},
  number   = {2},
  pages    = {105 - 115},
  issn     = {1871-1413},
  abstract = {This paper revises the evolutionary history of the stem root of the genus Equus from Eocene period (54millionyears before present, MYBP) to present. It also assesses molecular taxonomy and evolutionary relationships of this line since the first appearance of fossil records in terrestrial deposits. Combining these two lines of evidences, we outline a more informative and consensus phylogeny in a more understandable context. We also compare and contrast evolutionary histories and phylogenetic relationships of equids inferred from paleontological as well as varieties of molecular data and their implications. Using pair-wise coalescence time estimates, we draw a consensus speciation order in the stem root of the genus Equus. With the help of molecular data, we suggest the reasons for enigmatic speciation events between asses and zebras as well as the backgrounds for genetic dissimilarities between hemiones of Asia and asses of Africa regardless their phenotypic similarities. Based on the evidences from molecular data and review of late Pleistocene megafauna extinction in the Americas, we believe that horses were certainly domesticated in the Eurasian Steppe or elsewhere that survived late Pleistocene megafauna's extinction than in the Americas. We discuss the true wild horse that was involved in horse domestication processes in line with recent evidences that unraveled multi-geographic origins and multi-maternal lineages in the present day domestic horses.},
  doi      = {https://doi.org/10.1016/j.livsci.2011.09.017},
  keywords = {Evolutionary history, Phylogenetic relationship, Speciation order, },
  url      = {http://www.sciencedirect.com/science/article/pii/S1871141311003362},
}

@Article{1991a,
  title   = {Subject index of volume 6 (1991)},
  journal = {Data \& Knowledge Engineering},
  year    = {1991},
  volume  = {6},
  number  = {6},
  pages   = {543 - 544},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(91)90028-V},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9190028V},
}

@Article{Vandewoude2005,
  author   = {Yves Vandewoude and Yolande Berbers},
  title    = {Fresco: Flexible and Reliable Evolution System for Components},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2005},
  volume   = {127},
  number   = {3},
  pages    = {197 - 205},
  issn     = {1571-0661},
  note     = {Proceedings of the Workshop on Software Evolution through Transformations: Model-based vs. Implementation-level Solutions (SETra 2004)},
  abstract = {Fresco is a methodology that allows for the dynamic adaptation of component-oriented applications. Fresco aims to support the developer in the realization of dynamic adaptation throughout the entire life cycle of a component. At design time, a tool (DeepCompare) assists the programmer in the preparation of a component with live update functionality. At runtime, a middleware environment called Draco guides the replacement process itself and ensures that a component replacement is executed correctly. In this position paper, the focus is on the design time support and the tool DeepCompare. After the functional development of a component, DeepCompare constructs a meta-model from both the old and the new component versions. These models are compared and equivalent data-structures are identified. This information is subsequently used to partially generate state transition functions. Possible benefits include the verification of the correctness of an update using component invariants and the estimation of the complexity of the upgrade in order to flag certain problem scenarios to the developer.},
  doi      = {https://doi.org/10.1016/j.entcs.2004.08.044},
  keywords = {Software Evolution, Change Detection, Live Updates, State Transfer},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066105001490},
}

@Article{Gray2000,
  author   = {J.P Gray and A Liu and L Scott},
  title    = {Issues in software engineering tool construction},
  journal  = {Information and Software Technology},
  year     = {2000},
  volume   = {42},
  number   = {2},
  pages    = {73 - 77},
  issn     = {0950-5849},
  abstract = {A brief introduction to software engineering tools is presented, and issues involved in the construction of these tools are discussed. Some of the current issues concerning tool developers are highlighted, which include: metaCASE technology, cognitive support, evaluation and validation of tools and data interchange. Some recent developments in tool construction techniques are examined, and opportunities for further research and development in tool building are identified.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00080-4},
  keywords = {MetaCASE, Cognitive support, Data interchange, Components, Process modelling, Tool architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000804},
}

@Article{1993c,
  title   = {Subject index to volumes 1–10 (1985–1993)},
  journal = {Data \& Knowledge Engineering},
  year    = {1993},
  volume  = {10},
  number  = {3},
  pages   = {355 - 361},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(93)90040-V},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9390040V},
}

@Article{Golfarelli2006,
  author   = {Matteo Golfarelli and Jens Lechtenbörger and Stefano Rizzi and Gottfried Vossen},
  title    = {Schema versioning in data warehouses: Enabling cross-version querying via schema augmentation},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {59},
  number   = {2},
  pages    = {435 - 459},
  issn     = {0169-023X},
  note     = {Including: Sixth ACM International Workshop on Web Information and Data Management},
  abstract = {As several mature implementations of data warehousing systems are fully operational, a crucial role in preserving their up-to-dateness is played by the ability to manage the changes that the data warehouse (DW) schema undergoes over time in response to evolving business requirements. In this paper we propose an approach to schema versioning in DWs, where the designer may decide to undertake some actions on old data aimed at increasing the flexibility in formulating cross-version queries, i.e., queries spanning multiple schema versions. First, we introduce a representation of DW schemata as graphs of simple functional dependencies, and discuss its properties. Then, after defining an algebra of schema graph modification operations aimed at creating new schema versions, we discuss how augmented schemata can be introduced to increase flexibility in cross-version querying. Next, we show how a history of versions for DW schemata is managed and discuss the relationship between the temporal horizon spanned by a query and the schema on which it can consistently be formulated.},
  doi      = {https://doi.org/10.1016/j.datak.2005.09.004},
  keywords = {Data warehousing, Schema versioning, Cross-version querying, Schema augmentation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05001485},
}

@Article{2012,
  title   = {Table of Contents},
  journal = {Current Problems in Cancer},
  year    = {2012},
  volume  = {36},
  number  = {4},
  pages   = {177 - 179},
  issn    = {0147-0272},
  note    = {Working with the SEER data: Opportunities and Cautions},
  doi     = {https://doi.org/10.1016/S0147-0272(12)00073-6},
  url     = {http://www.sciencedirect.com/science/article/pii/S0147027212000736},
}

@Article{Alam2015,
  author   = {Khubaib Amjad Alam and Rodina Ahmad and Adnan Akhunzada and Mohd Hairul Nizam Md Nasir and Samee U. Khan},
  title    = {Impact analysis and change propagation in service-oriented enterprises: A systematic review},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {54},
  pages    = {43 - 73},
  issn     = {0306-4379},
  abstract = {Context
The adoption of Service-oriented Architecture (SOA) and Business Process Management (BPM) is fairly recent. The major concern is now shifting towards the maintenance and evolution of service-based business information systems. Moreover, these systems are highly dynamic and frequent changes are anticipated across multiple levels of abstraction. Impact analysis and change propagation are identified as potential research areas in this regard.
Objective
The aim of this study is to systematically review extant research on impact analysis and propagation in the BPM and SOA domains. Identifying, categorizing and synthesizing relevant solutions are the main study objectives.
Method
Through careful review and screening, we identified 60 studies relevant to 4 research questions. Two classification schemes served to comprehend and analyze the anatomy of existing solutions. BPM is considered at the business level for business operations and processes, while SOA is considered at the service level as deployment architecture. We focused on both horizontal and vertical impacts of changes across multiple abstraction layers.
Results
Impact analysis solutions were mainly divided into dependency analysis, traceability analysis and history mining. Dependency analysis is the most frequently adopted technique followed by traceability analysis. Further categorization of dependency analysis indicates that graph-based techniques are extensively used, followed by formal dependency modeling. While considering hierarchical coverage, inter-process and inter-service change analyses have received considerable attention from the research community, whereas bottom-up analysis has been the most neglected research area. The majority of change propagation solutions are top-down and semi-automated.
Conclusions
This study concludes with new insight suggestions for future research. Although, the evolution of service-based systems is becoming of grave concern, existing solutions in this field are less mature. Studies on hierarchical change impact are scarce. Complex relationships of services with business processes and semantic dependencies are poorly understood and require more attention from the research community.},
  doi      = {https://doi.org/10.1016/j.is.2015.06.003},
  keywords = {SOA, BPM, Web service, CIA, Dependency analysis, Semantic annotation, MSR, Change propagation, SOC, Systematic literature review (SLR)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915001179},
}

@Article{Chiu1999,
  author   = {Dickson K.W. Chiu and Qing Li and Kamalakar Karlapalem},
  title    = {A meta modeling approach to workflow management systems supporting exception handling},
  journal  = {Information Systems},
  year     = {1999},
  volume   = {24},
  number   = {2},
  pages    = {159 - 184},
  issn     = {0306-4379},
  note     = {Meta-Modelling and Methodology Engineering},
  abstract = {Workflow Management Systems (WFMSs) facilitate the definition of structure and decomposition of business processes and assists in management of coordinating, scheduling, executing and monitoring of such activities. Most of the current WFMSs are built on traditional relational database systems and/or using an object-oriented database system for storing the definition and run time data about the workflows. However, a WFMS requires advanced modeling functionalities to support adaptive features, such as on-line exception handling. This article describes our advanced meta-modeling approach using various enabling technologies (such as object orientation, roles, rules, active capabilities) supported by an integrated environment, the ADOME, as a solid basis for a flexible WFMS involving dynamic match making, migrating workflows and exception handling.},
  doi      = {https://doi.org/10.1016/S0306-4379(99)00010-1},
  keywords = {Meta-modeling, Object-Orientation, Workflow Management, Match-Making, Exception Handling, Workflow Evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437999000101},
}

@Article{Nadal2017,
  author   = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
  title    = {A software reference architecture for semantic-aware Big Data systems},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {90},
  pages    = {75 - 92},
  issn     = {0950-5849},
  abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.06.001},
  keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917304287},
}

@Article{1994,
  title   = {Subject index to volume 12 (1994)},
  journal = {Data \& Knowledge Engineering},
  year    = {1994},
  volume  = {12},
  number  = {3},
  pages   = {365},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(94)90034-5},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X94900345},
}

@Article{Gosain2015a,
  author   = {Anjana Gosain and Sangeeta Sabharwal and Rolly Gupta},
  title    = {Architecture Based Materialized View Evolution: A Review},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {48},
  pages    = {256 - 262},
  issn     = {1877-0509},
  note     = {International Conference on Computer, Communication and Convergence (ICCC 2015)},
  abstract = {Data Warehouse evolution is a critical problem in present scenario due to perpetual transactions and change in their structure arising out of continual evolving users’ requirements. Handling properly all type of changes is a crucial process as it forms the core component of the modern DSS. Therefore DW has to be updated periodically according to different type of evolution of information sources. The problem of evolving an appropriate set of views is subjected to as the materialized view evolution problem. Many different materialized view evolution methods have been proposed in the literature to address this issue. This paper provides a survey of materialized view evolution methods. The paper aims at studying the materialized view evolution in relational databases and data warehouses as well as in a distributed setting. It defines an evolutionary approach for highlighting the materialized view evolution problem by identifying the three main dimensions that are the basis in the classification of materialized view evolution methods namely; (i) Framework, (ii) Architecture and (iii) Model/Design Model. This study reviews architecture based materialized view evolution methods, by identifying respective potentials and limits.},
  doi      = {https://doi.org/10.1016/j.procs.2015.04.179},
  keywords = {Architecture, View Maintenanc, Materialized view evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915006882},
}

@InCollection{BERTHOLD2003c,
  author    = {BERTHOLD DAUM},
  title     = {8 - From Conceptual Model to Schema},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {249 - 321},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-155860816-0/50010-0},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500100},
}

@Article{Casati1998,
  author   = {F Casati and S Ceri and B Pernici and G Pozzi},
  title    = {Workflow evolution},
  journal  = {Data \& Knowledge Engineering},
  year     = {1998},
  volume   = {24},
  number   = {3},
  pages    = {211 - 238},
  issn     = {0169-023X},
  note     = {ER '96},
  abstract = {A basic step towards flexibility in workflow systems is the consistent and effective management of workflow evolution, i.e. of changing existing workflows while they are operational. One of the most challenging issue is the handling of running instances when their schemata are modified: simple solutions can be devised, but they often imply losing all the work done or failing in capturing the advantages offered by workflow modifications; this is unacceptable for many applications. In this paper we address the problem of workflow evolution, from both a static and a dynamic point of view. We define a complete, minimal, and consistent set of modification primitives that allow modifications of workflow schemata and we introduce a taxonomy of policies to manage evolution of running instances when the corresponding workflow schema is modified. Formal criteria are introduced, based on a simple workflow conceptual model, in order to determine which running instances can be transparently migrated to the new version. A case study, relating the assembling of a desktop computer, will exemplify the introduced concepts.},
  doi      = {https://doi.org/10.1016/S0169-023X(97)00033-5},
  keywords = {Workflow, Evolution, Modification primitives, Running instances},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X97000335},
}

@Article{Spivak2012,
  author   = {David I. Spivak},
  title    = {Functorial data migration},
  journal  = {Information and Computation},
  year     = {2012},
  volume   = {217},
  pages    = {31 - 51},
  issn     = {0890-5401},
  abstract = {In this paper we present a simple database definition language: that of categories and functors. A database schema is a small category and an instance is a set-valued functor on it. We show that morphisms of schemas induce three “data migration functors”, which translate instances from one schema to the other in canonical ways. These functors parameterize projections, unions, and joins over all tables simultaneously and can be used in place of conjunctive and disjunctive queries. We also show how to connect a database and a functional programming language by introducing a functorial connection between the schema and the category of types for that language. We begin the paper with a multitude of examples to motivate the definitions, and near the end we provide a dictionary whereby one can translate database concepts into category-theoretic concepts and vice versa.},
  doi      = {https://doi.org/10.1016/j.ic.2012.05.001},
  keywords = {Category theory, Databases, Data migration, Adjoint functors, Queries},
  url      = {http://www.sciencedirect.com/science/article/pii/S0890540112001010},
}

@Article{Meyers2011,
  author   = {Bart Meyers and Hans Vangheluwe},
  title    = {A framework for evolution of modelling languages},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1223 - 1246},
  issn     = {0167-6423},
  note     = {Special Issue on Software Evolution, Adaptability and Variability},
  abstract = {In model-driven engineering, evolution is inevitable over the course of the complete life cycle of complex software-intensive systems and more importantly of entire product families. Not only instance models, but also entire modelling languages are subject to change. This is in particular true for domain-specific languages, whose language constructs are tightly coupled to an application domain. The most popular approach to evolution in the modelling domain is a manual process, with tedious and error-prone migration of artefacts such as instance models as a result. This paper provides a taxonomy for evolution of modelling languages and discusses the different evolution scenarios for various kinds of modelling artefacts, such as instance models, meta-models, and transformation models. Subsequently, the consequences of evolution and the required remedial actions are decomposed into primitive scenarios such that all possible evolutions can be covered exhaustively. These primitives are then used in a high-level framework for the evolution of modelling languages. We suggest that our structured approach enables the design of (semi-)automatic modelling language evolution solutions.},
  doi      = {https://doi.org/10.1016/j.scico.2011.01.002},
  keywords = {Evolution, Modelling languages, Language engineering, Model-driven engineering, Model transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311000141},
}

@Article{Breiteneder1995,
  author   = {Christian J. Breiteneder and Thomas A. Mück},
  title    = {Graph rewriting systems for the entity-relationship approach},
  journal  = {Data \& Knowledge Engineering},
  year     = {1995},
  volume   = {17},
  number   = {3},
  pages    = {215 - 243},
  issn     = {0169-023X},
  abstract = {Sequential graph rewriting systems are proposed as a meta-level formalism providing the concise and sound definition of different ER diagram languages. These rewriting systems can be used to define ER-based approaches for various DB modelling subtasks like schema design, evolution and integration. In addition, they are a natural choice for syntax directed ER CASE workbenches. In particular, by using specialized ER graph rewriting systems as meta input for CASE tools, the resulting tool behavior can be guided and controlled. Moreover, grammar driven modelling tools can be easily adapted for the needs of a particular enterprise or software factory without superimposing a particular ER dialect on the end users. Additional benefits result from the use of ER graph rewriting systems as a comparison framework for the continuously enlarging set of ER dialects. The presentation includes material on the ER graph rewriting formalism, i.e., the actual tool, as well as an introduction to some formal graph rewriting prerequisites. An exemplary application, in particular ER graph generation, is used to clarify the underlying formal concepts.},
  doi      = {https://doi.org/10.1016/0169-023X(95)00028-Q},
  keywords = {ER-approach, Graph rewriting systems, CASE},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9500028Q},
}

@Article{Zhang2018,
  author   = {Xueyan Zhang and Jing Chen and Ziying Ai and Zhewen Zhang and Li Lin and Hulai Wei},
  title    = {Targeting glycometabolic reprogramming to restore the sensitivity of leukemia drug-resistant K562/ADM cells to adriamycin},
  journal  = {Life Sciences},
  year     = {2018},
  volume   = {215},
  pages    = {1 - 10},
  issn     = {0024-3205},
  abstract = {Aims
Mounting studies have confirmed that cancer cells reprogram their metabolism during early carcinogenesis to develop many other hallmarks, and demonstrated a relationship between aerobic glycolysis and the occurrence of drug resistance. However, the molecular mechanisms and role in tumor drug resistance of aerobic glycolysis remain unclear.
Main methods
We analyzed differentially expressed genes (DEGs) at the RNA level between the multi-drug resistance (MDR) leukemia cell line K562/adriamycin (ADM) and its parental, drug-sensitive K562 cell line. Clustering and enrichment analysis of DEGs was performed. Oxamate, a lactic dehydrogenase inhibitor were used to assess the effect of glycolysis inhibition on ADM susceptibility and the expression of the enriched DEGs in K562/ADM cells.
Key findings
A total of 1742 DEGs were detected between the K562/ADM and K562 cell lines. The differential expression of unigenes encoding enzymes involved in glycometabolism signifies that there was a greater aerobic glycolysis flux in K562/ADM cells. The PI3K-AKT signaling pathway, which is related to glucose metabolism, showed representative differential enrichment and up-regulation in K562/ADM cells. Oxamate improved and re-sensitized the therapeutic effect of ADM in ADM-resistant cells by inhibiting aerobic glycolysis either directly or indirectly by down-regulation of the AKT-mTOR pathway.
Significance
Our findings suggest that ADM resistance mediated by the increase of aerobic glycolysis, which related to the over-activation of the AKT-mTOR-c-Myc pathway in MDR leukemia cells. Inhibition of aerobic glycolysis and down-regulation of signaling pathways involved in aerobic glycolysis represent a potential chemotherapeutic strategy for sensitizing leukemic cells and thereby overcoming MDR.},
  doi      = {https://doi.org/10.1016/j.lfs.2018.10.050},
  keywords = {Glycometabolic reprogramming, Aerobic glycolysis, Glycolysis inhibitor, Multi-drug resistance, Leukemia},
  url      = {http://www.sciencedirect.com/science/article/pii/S0024320518306751},
}

@Article{Gheyi2007,
  author   = {Rohit Gheyi and Tiago Massoni and Paulo Borba},
  title    = {A Static Semantics for Alloy and its Impact in Refactorings},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2007},
  volume   = {184},
  pages    = {209 - 233},
  issn     = {1571-0661},
  note     = {Proceedings of the Second Brazilian Symposium on Formal Methods (SBMF 2005)},
  abstract = {Refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to a formal semantics, not guaranteeing the absence of type errors or semantic changes. Consequently, developers using refactoring tools must rely on compilation and tests to ensure type-correctness and semantics preservation, respectively, which may not be satisfactory to critical software development. In this paper, we formalize a static semantics for Alloy, which is a formal object-oriented modeling language, and encode it in Prototype Verification System (PVS). The static semantics' formalization can be useful for specifying and proving that transformations in general (not only refactorings) do not introduce type errors, for instance, as we show here.},
  doi      = {https://doi.org/10.1016/j.entcs.2007.03.023},
  keywords = {refactoring, type system, theorem proving, object models},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066107004434},
}

@Article{Jacquinet-Husson2016,
  author  = {Nicole Jacquinet-Husson and Jean-Marie Flaud and Robert R. Gamache and Adriana Predoi-Cross and J. Vander Auwera},
  title   = {New visions of spectroscopic databases: An introduction to the special issue},
  journal = {Journal of Molecular Spectroscopy},
  year    = {2016},
  volume  = {326},
  pages   = {1 - 4},
  issn    = {0022-2852},
  note    = {New Visions of Spectroscopic Databases, Volume I},
  doi     = {https://doi.org/10.1016/j.jms.2016.07.006},
  url     = {http://www.sciencedirect.com/science/article/pii/S0022285216301242},
}

@Article{Gaede2006,
  author   = {F. Gaede},
  title    = {Marlin and LCCD—Software tools for the ILC},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2006},
  volume   = {559},
  number   = {1},
  pages    = {177 - 180},
  issn     = {0168-9002},
  note     = {Proceedings of the X International Workshop on Advanced Computing and Analysis Techniques in Physics Research},
  abstract = {The next big project proposed in particle physics is the International Linear Collider (ILC), an electron positron collider with an energy reach of around 1TeV. The ongoing optimization and development of a detector for the ILC is only possible through the extensive use of sophisticated simulation software. In this paper we give a brief review of the software tools that are available in the currently ongoing three international detector concept studies and present two new software packages that have been developed in the context of the Large Detector Concept (LDC) study. The first is a C++ application framework that provides a platform for the distributed development of reconstruction and analysis software and the second is a conditions data toolkit. The interoperability with other software packages is discussed.},
  doi      = {https://doi.org/10.1016/j.nima.2005.11.138},
  keywords = {Simulation, Monte Carlo, Software tools, Linear collider},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900205022643},
}

@Article{Vassiliadis2000,
  author   = {Panos Vassiliadis and Mokrane Bouzeghoub and Christoph Quix},
  title    = {Towards quality-oriented data warehouse usage and evolution},
  journal  = {Information Systems},
  year     = {2000},
  volume   = {25},
  number   = {2},
  pages    = {89 - 115},
  issn     = {0306-4379},
  note     = {The 11th International Conference on Advanced Information System Engineering},
  abstract = {As a decision support information system, a data warehouse must provide high level quality of data and services. In the DWQ project (Foundations of Data Warehouse Quality), we have proposed how semantically rich meta-information of a data warehouse can be stored in a metadata repository. This static representation of the various perspectives of data warehouse components and their linkage to quality factors is complemented by an operational methodology on how to use these quality factors and achieve the quality goals of the users. This approach is an extension of the Goal-Question-Metric (GQM) approach, based on the idea that a quality goal is operationally defined over a concrete set of questions, i.e., algorithmic steps. The proposed approach covers the full lifecycle of the data warehouse, allows capturing the interrelationships between different quality factors and helps the interested user to organize them in order to fulfill specific quality goals. Furthermore, we prove how the quality management of the data warehouse can guide the process of data warehouse evolution, by tracking the interrelationships between the components of the data warehouse. Finally, we present a case study, as a proof of concept for the proposed methodology.},
  doi      = {https://doi.org/10.1016/S0306-4379(00)00011-9},
  keywords = {Data Warehousing, Repositories, Evolution, Data Quality},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437900000119},
}

@Article{Baker1998,
  author   = {N. Baker and A. Bazan and F. Estrella and Z. Kovacs and T. Le Flour and J.-M. Le Goff and E. Leonardi and S. Lieunard and R. McClatchey and J.-P. Vialle},
  title    = {Workflow management in the assembly of CMS ECAL},
  journal  = {Computer Physics Communications},
  year     = {1998},
  volume   = {110},
  number   = {1},
  pages    = {170 - 176},
  issn     = {0010-4655},
  abstract = {As with all experiments in the LHC era, the Compact Muon Solenoid (CMS) detectors will be constituted of a very large number of constituent parts. Typically, each major detector may be constructed out of over a million precision parts and will be produced and assembled during the next decade by specialised centres distributed world-wide. Each constituent part of each detector must be accurately measured and tested locally prior to its ultimate assembly and integration in the experimental area at CERN. Much of the information collected during this phase will be needed not only to construct the detector, but for its calibration, to facilitate accurate simulation of its performance and to assist in its lifetime maintenance. The CRISTAL system is a prototype being developed to monitor and control the production and assembly process of the CMS Electromagnetic Calorimeter (ECAL). The software will be generic in design and hence reusable for other CMS detector groups. This paper discusses the distributed computing problems and design issues posed by this project. The overall software design architecture is described together with the main technology aspects of linking distributed object oriented databases via CORBA with WWW/Java-based query processing. The paper then concentrates on the design of the workflow management system of CRISTAL.},
  doi      = {https://doi.org/10.1016/S0010-4655(97)00173-2},
  keywords = {Distributed systems, Workflow management, Cooperative task management, Production and assembly system},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010465597001732},
}

@Article{By1991,
  author  = {Rolf de By and Roel Wieringa},
  title   = {Dynamic specification of temporal semantics of conceptual models},
  journal = {Data \& Knowledge Engineering},
  year    = {1991},
  volume  = {6},
  number  = {6},
  pages   = {449},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(91)90022-P},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9190022P},
}

@Article{Stoimenov1999,
  author   = {Leonid Stoimenov and Antonija Mitrovic and Slobodanka Djordjevic-Kajan and Dejan Mitrovic},
  title    = {Bridging objects and relations: a mediator for an OO front-end to RDBMSs},
  journal  = {Information and Software Technology},
  year     = {1999},
  volume   = {41},
  number   = {2},
  pages    = {57 - 66},
  issn     = {0950-5849},
  abstract = {An object-oriented paradigm is established as the leading approach for developing non-traditional applications, such as GIS or multimedia systems. On the other hand, relational databases have dominated the area of data processing in the past decade. These two trends motivate the research on integrating OO applications with relational databases. This paper presents our approach to the symbiosis of the OO and relational data models, which is built into GinisNT, a scalable OO GIS framework based on an OO-to-relational mapping algorithm. The mapping algorithm transforms classes and objects into relations and tuples, and vice versa, instantiates objects from relational databases. The methodology presented here is extremely efficient, as has been proved by a number of applications developed in GinisNT, and is at the same time cost efficient, as it builds upon existing platforms.},
  doi      = {https://doi.org/10.1016/S0950-5849(98)00112-8},
  keywords = {OO-to-relational mapping algorithm, Object-oriented model, Relational model, GIS},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584998001128},
}

@Article{1997,
  title   = {Subject index of volume 22 (1997)},
  journal = {Data \& Knowledge Engineering},
  year    = {1997},
  volume  = {22},
  number  = {3},
  pages   = {347},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/S0169-023X(97)89936-3},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X97899363},
}

@Article{1995a,
  title   = {List of contents and author index volume 9, 1995},
  journal = {Expert Systems with Applications},
  year    = {1995},
  volume  = {9},
  number  = {4},
  pages   = {III - VII},
  issn    = {0957-4174},
  note    = {Expert systems in accounting, auditing, and finance},
  doi     = {https://doi.org/10.1016/0957-4174(95)90029-2},
  url     = {http://www.sciencedirect.com/science/article/pii/0957417495900292},
}

@Article{Nativi2005,
  author   = {Stefano Nativi and John Caron and Ethan Davis and Ben Domenico},
  title    = {Design and implementation of netCDF markup language (NcML) and its GML-based extension (NcML-GML)},
  journal  = {Computers \& Geosciences},
  year     = {2005},
  volume   = {31},
  number   = {9},
  pages    = {1104 - 1118},
  issn     = {0098-3004},
  note     = {Application of XML in the Geosciences},
  abstract = {The Network Common Data Form (netCDF) is one of the primary methods of self-documenting data storage and access in the international geosciences research and education community and beyond. NetCDF was designed for use in a networked environment. The recent evolution toward web services approaches to data exchange has focused attention on communication via messages in the defacto standard XML language. XML is a text-based language while netCDF is based on a binary file storage mechanism; thus NcML is a natural augmentation of netCDF with extensions encapsulating descriptions of the structure and content of netCDF objects in an XML form. Since netCDF was designed to be self-documenting, the XML representation of internal netCDF documentation is a natural augmentation of the original netCDF concept. In fact, the netCDF Markup Language (NcML) and NcML-G (NcML-Geography) extensions described in this article have applications beyond merely representing the internal netCDF documentation in the XML language. The NcML coordinate system makes it possible to describe the coordinate system used to represent the netCDF dataset. Furthermore the NcML dataset is a tool for describing “virtual netCDF” files that may be aggregations of data from several existing netCDF files, or it can represent a target dataset to be created by transforming existing netCDF files into a new form described in the NcML language. The NcML-G extension provides a means for fusing the data models of the traditional netCDF atmospheric science community with those of the GIS community which is of the utmost importance. Bringing the data models and data systems of those communities together will foster an era of interdisciplinary research and education within the geosciences subdisciplines. It will also encourage closer interactions between the geosciences and the societal impacts community. The design and software implementation of the core NcML specification and its extensions are presented and discussed.},
  doi      = {https://doi.org/10.1016/j.cageo.2004.12.006},
  keywords = {Scientific data markup language, NetCDF, GML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0098300405001019},
}

@Article{Mezghanni2017,
  author   = {Imen Bouaziz Mezghanni and Faiez Gargouri},
  title    = {CrimAr: A Criminal Arabic Ontology for a Benchmark Based Evaluation},
  journal  = {Procedia Computer Science},
  year     = {2017},
  volume   = {112},
  pages    = {653 - 662},
  issn     = {1877-0509},
  note     = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
  abstract = {Recently, ontologies have become more important in modern Semantic Web as they capture knowledge in a particular domain of interest. Indeed, they emphasize interoperability and establish a common shared understanding among the involved actors of web-based applications. Nevertheless, in parallel with the abundance of the proposed approaches for ontology learning, a related problem of the evaluation of such automatically generated ontologies is emerging in different domains. In the Arabic legal domain, a benchmark golden ontology is so necessary in order to assess the good quality of the (semi-)automatic learned ontologies. In this paper, we introduce CrimAr, a handcrafted ontology based on the top-levels of LRI-Core, to represent all relevant knowledge in the Arabic legal domain, especially the criminal matter. The use of CrimAr is also demonstrated in a real case evaluation.},
  doi      = {https://doi.org/10.1016/j.procs.2017.08.113},
  keywords = {Legal ontology, Ontology evaluation, Arabic legal texts, LRI-Core},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050917314692},
}

@Article{Luckey2012,
  author   = {Markus Luckey and Martin Erwig and Gregor Engels},
  title    = {Systematic evolution of model-based spreadsheet applications},
  journal  = {Journal of Visual Languages \& Computing},
  year     = {2012},
  volume   = {23},
  number   = {5},
  pages    = {267 - 286},
  issn     = {1045-926X},
  abstract = {Using spreadsheets is the preferred method to calculate, display or store anything that fits into a table-like structure. They are often used by end users to create applications, although they have one critical drawback—spreadsheets are very error-prone. Recent research has developed methods to reduce this error-proneness by introducing a new way of object-oriented modeling of spreadsheets before using them. These spreadsheet models, termed ClassSheets, are used to generate concrete spreadsheets on the instance level. By this approach sources of errors are reduced and spreadsheet applications become easier to understand. As usual for almost every other application, requirements on spreadsheets change due to the changing environment. Thus, the problem of evolution of spreadsheets arises. The update and evolution of spreadsheets is the uttermost source of errors that may have severe impact. In this paper, we will introduce a model-based approach to spreadsheet evolution by propagating updates on spreadsheet models (i.e. ClassSheets) to spreadsheets. To this end, update commands for the ClassSheet layer are automatically transformed to those for the spreadsheet layer. We describe spreadsheet model update propagation using a formal framework and present an integrated tool suite that allows the easy creation and safe update of spreadsheet models. The presented approach greatly contributes to the problem of software evolution and maintenance for spreadsheets and thus avoids many errors that might have severe impacts.},
  doi      = {https://doi.org/10.1016/j.jvlc.2011.11.009},
  keywords = {Model-based, Spreadsheet, Evolution, Update, Propagation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1045926X12000389},
}

@Article{Zhang2002,
  author   = {Xin Zhang and Elke A. Rundensteiner},
  title    = {Integrating the maintenance and synchronization of data warehouses using a cooperative framework},
  journal  = {Information Systems},
  year     = {2002},
  volume   = {27},
  number   = {4},
  pages    = {219 - 243},
  issn     = {0306-4379},
  abstract = {Data warehouses (DW) are built by gathering information from several information sources and integrating it into one repository customized to users’ needs. Recently proposed view maintenance algorithms tackle the problem of (concurrent) data updates happening at different autonomous ISs, whereas the EVE system addresses the maintenance of a data warehouse after schema changes of ISs. The concurrency of schema changes and data updates performed by different ISs remains an unexplored problem however. This paper provides a solution to this problem that guarantees the concurrent view definition evolution and view extent maintenance of a DW defined over distributed ISs. To solve that problem, we introduce a framework called SDCC (Schema change and Data update Concurrency Control) system. SDCC integrates existing algorithms designed to address view maintenance subproblems, such as view extent maintenance after IS data updates, view definition evolution after IS schema changes, and view extent adaptation after view definition changes, into one system by providing protocols that enable them to correctly co-exist and collaborate. SDCC tracks any potential faulty updates of the DW caused by conflicting concurrent IS changes using a global message labeling scheme. An algorithm that is able to compensate for such conflicting updates by a local correction strategy, called local compensation (LC), is incorporated into SDCC. The correctness of LC is proven. The overhead of the SDCC solution beyond the costs of the known view maintenance algorithms it incorporates is shown to be negligible. Lastly, a refined hierarchy of consistency levels for the state of a data warehouse with respect to its underlying dynamic environment is presented, now incorporating both dynamicity of the data and the schema. The SDCC solution is shown to reach a semi-concurrency level of consistency, not reached by any prior DW system.},
  doi      = {https://doi.org/10.1016/S0306-4379(01)00049-7},
  keywords = {Data warehousing, View maintenance, Data updates and schema changes, View consistency, Distributed information sources},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437901000497},
}

@Article{Tagg2012,
  author   = {N. Tagg and J. Brangham and J. Chvojka and M. Clairemont and M. Day and B. Eberly and J. Felix and L. Fields and A.M. Gago and R. Gran and D.A. Harris and M. Kordosky and H. Lee and G. Maggi and E. Maher and W.A. Mann and C.M. Marshall and K.S. McFarland and A.M. McGowan and A. Mislivec and J. Mousseau and B. Osmanov and J. Osta and V. Paolone and G. Perdue and R.D. Ransome and H. Ray and H. Schellman and D.W. Schmitz and C. Simon and C.J. Solano Salinas and B.G. Tice and J. Walding and T. Walton and J. Wolcott and D. Zhang and B.P. Ziemer},
  title    = {Arachne—A web-based event viewer for MINERνA},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2012},
  volume   = {676},
  pages    = {44 - 49},
  issn     = {0168-9002},
  abstract = {Neutrino interaction events in the MINERνA detector are visually represented with a web-based tool called Arachne. Data are retrieved from a central server via AJAX, and client-side JavaScript draws images into the user's browser window using the draft HTML 5 standard. These technologies allow neutrino interactions to be viewed by anyone with a web browser, allowing for easy hand-scanning of particle interactions. Arachne has been used in MINERνA to evaluate neutrino data in a prototype detector, to tune reconstruction algorithms, and for public outreach and education.},
  doi      = {https://doi.org/10.1016/j.nima.2012.01.059},
  keywords = {Event viewer, Data visualization, XML, AJAX, HTML 5, experiment, NuMI beam},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900212001167},
}

@Article{1997a,
  title   = {Subject index to volume 23 (1997)},
  journal = {Data \& Knowledge Engineering},
  year    = {1997},
  volume  = {23},
  number  = {3},
  pages   = {319},
  issn    = {0169-023X},
  note    = {Distributed expertise},
  doi     = {https://doi.org/10.1016/S0169-023X(97)89664-4},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X97896644},
}

@Article{Medeiros2000,
  author   = {Claudia Bauzer Medeiros and Marie-Jo Bellosta and Geneviève Jomier},
  title    = {Multiversion views: Constructing views in a multiversion database},
  journal  = {Data \& Knowledge Engineering},
  year     = {2000},
  volume   = {33},
  number   = {3},
  pages    = {277 - 306},
  issn     = {0169-023X},
  abstract = {Commercial DBMS offer mechanisms for views and for versions. Research and development efforts in these directions are, however, characterized by concentration on either the one or the other mechanism, very seldom trying to take advantage of their complementary properties. This paper presents the multiversion view mechanism, which allows these orthogonal concepts to be managed together, taking advantage of their combined characteristics. Unlike previous efforts to combine views and versions, multiversion views create views over versions of data, thereby offering users coherent logical units of the versioned world. They allow a wide range of (virtual) data reorganization possibilities, which encompass, among others, operations found in temporal databases and OLAP. Multiversion views are illustrated and motivated by needs from a real life large case study of complex configuration management, described at the end of the paper.},
  doi      = {https://doi.org/10.1016/S0169-023X(00)00004-5},
  keywords = {Multiversion databases, Views, OLAP},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X00000045},
}

@InCollection{Teorey2006,
  author    = {Toby Teorey and Sam Lightstone and Tom Nadeau},
  title     = {1 - Introduction},
  booktitle = {Database Modeling and Design (Fourth Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2006},
  editor    = {Toby Teorey and Sam Lightstone and Tom Nadeau},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {1 - 11},
  address   = {San Francisco},
  edition   = {Fourth Edition},
  isbn      = {978-0-12-685352-0},
  abstract  = {Publisher Summary
This chapter deals with logical design methodologies and tools most popular for relational databases today. Database technology has evolved rapidly in the three decades since the rise and eventual dominance of relational database systems. While many specialized database systems (object-oriented, spatial, multimedia, etc.) have found substantial user communities in the science and engineering fields, relational systems remain the dominant database technology for business enterprises. Relational database design has evolved from an art to a science that has made partially implementable as a set of software design aids. Many of these design aids have appeared as the database component of computer-aided software engineering (CASE) tools, and many of them offer an interactive modeling capability using a simplified data modeling approach. Furthermore, the basic component of a file in a file system is a data item, which is the smallest named unit of data that has meaning in the real world. A file is a collection of records of a single type. Database systems have built upon and expanded these definitions: In a relational database, a data item is called a column or attribute; a record is called a row or tuple; and a file is called a table.},
  doi       = {https://doi.org/10.1016/B978-012685352-0/50001-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012685352050001X},
}

@Article{1997b,
  title   = {Author index of volume 22 (1997)},
  journal = {Data \& Knowledge Engineering},
  year    = {1997},
  volume  = {22},
  number  = {3},
  pages   = {345 - 346},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/S0169-023X(97)89935-1},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X97899351},
}

@Article{Liu2014,
  author   = {Lei Liu and Peng Zhang and Rong Fan and Rui Zhang and Hongji Yang},
  title    = {Modeling ontology evolution with SetPi},
  journal  = {Information Sciences},
  year     = {2014},
  volume   = {255},
  pages    = {155 - 169},
  issn     = {0020-0255},
  abstract = {Ontology is doomed to evolve due to the changes of the domain it models. It has always been the key issue how to represent the changes during ontology evolution, because it forms the basis to detect, evaluate, implement and propagate those changes. However, prevalent ontology languages (OWL, DL, RDF, etc.) are not adept in description of dynamic semantics such as evolution. Therefore in this paper, we propose a new calculus named SetPi to model ontology evolution. It extends the classical Pi calculus with the idea of ‘set’ which offers the ability to create new channels in the system. SetPi has a new feature, Name Set as an extension of the semantics of names. It is proposed with formal syntax, semantics, structure congruence and reduction rules. Then, we describe how to model elementary changes via SetPi. And we provide algorithms to model composite changes and evaluate the formal semantics of the changes. A case study is employed to verify the feasibility of the method. SetPi provides not only a new way to formalize operations on changes but also offers the foundation for consistency checking and effect propagation after the changes.},
  doi      = {https://doi.org/10.1016/j.ins.2013.07.017},
  keywords = {Formalization, Modeling, Ontology evolution, Pi-calculus},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025513005288},
}

@Article{Qin2009,
  author   = {Li Qin and Vijayalakshmi Atluri},
  title    = {Evaluating the validity of data instances against ontology evolution over the Semantic Web},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {1},
  pages    = {83 - 97},
  issn     = {0950-5849},
  note     = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
  abstract = {It is natural for ontologies to evolve over time. These changes could be at structural and semantic levels. Due to changes to an ontology, its data instances may become invalid, and as a result, may become non-interpretable. In this paper, we address precisely this problem, validity of data instances due to ontological evolution. Towards this end, we make the following three novel contributions to the area of Semantic Web. First, we propose formal notions of structural validity and semantic validity of data instances, and then present approaches to ensure them. Second, we propose semantic view as part of an ontology, and demonstrate that it is sufficient to validate a data instance against the semantic view rather than the entire ontology. We discuss how the semantic view can be generated through an implication analysis, i.e., how semantic changes to one component imply semantic changes to other components in the ontology. Third, we propose a validity identification approach that employs locally maintaining a hash value of the semantic view at the data instance.},
  doi      = {https://doi.org/10.1016/j.infsof.2008.01.004},
  keywords = {Ontology evolution, Data validity, Semantic Web},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584908000244},
}

@InCollection{BERTHOLD2003d,
  title     = {Index},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {507 - 534},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-155860816-0/50020-3},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500203},
}

@Article{Thirugnanasambandam2019,
  author   = {R. Thirugnanasambandam and D. Inbakandan and C. Kumar and B. Subashni and R. Vasantharaja and L. Stanley Abraham and N. Ayyadurai and P. Sriyutha Murthy and R. Kirubagaran and S. Ajmal Khan and T. Balasubramanian},
  title    = {Genomic insights of Vibrio harveyi RT-6 strain, from infected “Whiteleg shrimp” (Litopenaeus vannamei) using Illumina platform},
  journal  = {Molecular Phylogenetics and Evolution},
  year     = {2019},
  volume   = {130},
  pages    = {35 - 44},
  issn     = {1055-7903},
  abstract = {The pathogenicity of “Vibriosis” in shrimps imposes prominent menace to the sustainable growth of mariculture economy. Often the disease outbreak is associated speciously with Vibrio harveyi and its closely related species. The present study investigated the complete genome of the strain V. harveyi RT-6 to explore the molecular mechanism of pathogenesis. The genome of V. harveyi possesses a single chromosome of 6,374,398 bp in size, G + C content (44.7%) and 5730 protein coding genes. The reads of 1.3 Gb were retained from Illumina Hiseq 2500 sequencing method, assembled into 5912 predicted genes, 114 tRNAs genes, and 11 rRNAs genes. Unigenes were annotated by matching against Clusters of Orthologous Groups of proteins (COG)-5730, Gene ontology (GO)-1088, and Kyoto Encyclopedia of Genes and Genomes (KEGG) databases-3401. Furthermore, 13 insertion sequences-(IS), virulence factors and prophage regions were also identified. A total of 94 pathogenic genes and 36 virulence factor genes were mainly identified using Virulence Factors Database (VFDB). Out of the 36 virulence factors, 23 genes responsible for encoding flagella-based motility protein were exclusively predicted to take part in pathogenic mechanism. The Whole Genome Sequencing (WGS) of the strain RT-6 (accession number: SRR5410471) highlighted the underlying genes and specifically accountable functional genes that were responsible for pathogenic infections in shrimps.},
  doi      = {https://doi.org/10.1016/j.ympev.2018.09.015},
  keywords = {, Illumina sequencing, Cluster of orthologous, Gene ontology, KEGG, Virulence genes, Prophage regions},
  url      = {http://www.sciencedirect.com/science/article/pii/S1055790318301076},
}

@InCollection{Conradi1993,
  author    = {Reidar Conradi and Maria Letizia Jaccheri},
  title     = {Customization and Evolution of Process Models in EPOS},
  booktitle = {Information System Development Process},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {N. PRAKASH and C. ROLLAND and B. PERNICI},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {23 - 39},
  address   = {Amsterdam},
  abstract  = {EPOS is a kernel software engineering environment, offering integrated software configuration and process management. The EPOS process modeling (PM) support system runs on top of the versioned EPOSDB, operating in client-server mode. EPOSDB offers cooperating subdatabases/workspaces for each subproject. The originality of the EPOS approach to PM flexibility lies in a common, reflexive, object-oriented data model to describe persistent and versioned products, activities, tools, organizational contexts, and their meta-processes. A process model or schema is a set of data types for such entities and their relationships. A PM Manager is capable of defining and changing process models. A Planner will instantiate them into software processes. Work is going on to use roles and access rights to better control process change, and to support the full process model life-cycle by a “CASE tool” for PM.},
  doi       = {https://doi.org/10.1016/B978-0-444-81594-1.50008-8},
  issn      = {09265473},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444815941500088},
}

@Article{Khattak2013,
  author   = {Asad Masood Khattak and Khalid Latif and Sungyoung Lee},
  title    = {Change management in evolving web ontologies},
  journal  = {Knowledge-Based Systems},
  year     = {2013},
  volume   = {37},
  pages    = {1 - 18},
  issn     = {0950-7051},
  abstract = {Knowledge constantly grows in scientific discourse and is revised over time by different stakeholders, either collaboratively or through institutionalized efforts. The body of knowledge gets structured and refined as the Communities of Practice concerned with a field of knowledge develop a deeper understanding of the issues. As a result, the knowledge model moves from a loosely clustered terminology to a semi-formal or even formal ontology. Change history management in such evolving knowledge models is an important and challenging task. Different techniques have been introduced in the research literature to solve the issue. A comprehensive solution must address various multi-faceted issues, such as ontology recovery, visualization of change effects, and keeping the evolving ontology in a consistent state. More so because the semantics of changes and evolution behavior of the ontology are hard to comprehend. This paper introduces a change history management framework for evolving ontologies; developed over the last couple of years. It is a comprehensive and methodological framework for managing issues related to change management in evolving ontologies, such as versioning, provenance, consistency, recovery, change representation and visualization. The Change history log is central to our framework and is supported by a semantically rich and formally sound change representation scheme known as change history ontology. Changes are captured and then stored in the log in conformance with the change history ontology. The log entries are later used to revert ontology to a previous consistent state, and to visualize the effects of change on ontology during its evolution. The framework is implemented to work as a plug-in for ontology repositories, such as Joseki and ontology editors, such as Protege. The change detection accuracy of the proposed system Change Tracer has been compared with that of Changes Tab, Version Log Generator in Protege; Change Detection, and Change Capturing of NeOn Toolkit. The proposed system has shown better accuracy against the existing systems. A comprehensive evaluation of the methodology was designed to validate the recovery operations. The accuracy of Roll-Back and Roll-Forward algorithms was conducted using different versions of SWETO Ontology, CIDOC CRM Ontology, OMV Ontology, and SWRC Ontology. Experimental results and comparison with other approaches shows that the change management process of the proposed system is accurate, consistent, and comprehensive in its coverage.},
  doi      = {https://doi.org/10.1016/j.knosys.2012.05.005},
  keywords = {Ontology Change Management, Ontology Evolution, Ontology Recovery, Change History Ontology, Change Navigation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950705112001323},
}

@Article{Cioranu2015,
  author   = {Cosmin Cioranu and Marius Cioca and Carmen Novac},
  title    = {Database Versioning 2.0, a Transparent SQL Approach Used in Quantitative Management and Decision Making},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {55},
  pages    = {523 - 528},
  issn     = {1877-0509},
  note     = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
  abstract = {Managerial decisions are based on accurate information and in today's time raw data is produced even with a stroke of a key. Regardless of the data creating process one needs to know how the information was extracted and which pool of data was used. One important factor is time therefore we need to structure it in layers of data history in such a way that it can be analyzed, (post)process, in order to be able to retrieve valuable information. The simplest way is to use a Database Management System (DBMS), but even with such a management system we face the issue of making it a self-contained database on each version of data added. Our proposed system, a continuation of previous work, aims toward creating a database versioning system which keeps the natural dependency between data on each internal revision, a basis of security and alteration control mechanism, trend analytics, without sacrificing(within acceptable levels) speed, flexibility and the cost of implementation be kept as minimal as possible.},
  doi      = {https://doi.org/10.1016/j.procs.2015.07.030},
  keywords = {DBMS, SQL, Database Logical Version Control, Decision Making ;},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915015057},
}

@Article{Argiro2007,
  author   = {S. Argirò and S.L.C. Barroso and J. Gonzalez and L. Nellen and T. Paul and T.A. Porter and L. Prado Jr. and M. Roth and R. Ulrich and D. Veberič},
  title    = {The offline software framework of the Pierre Auger Observatory},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2007},
  volume   = {580},
  number   = {3},
  pages    = {1485 - 1496},
  issn     = {0168-9002},
  abstract = {The Pierre Auger Observatory is designed to unveil the nature and the origins of the highest energy cosmic rays. The large and geographically dispersed collaboration of physicists and the wide-ranging collection of simulation and reconstruction tasks pose some special challenges for the offline analysis software. We have designed and implemented a general purpose framework which allows collaborators to contribute algorithms and sequencing instructions to build up the variety of applications they require. The framework includes machinery to manage these user codes, to organize the abundance of user-contributed configuration files, to facilitate multi-format file handling, and to provide access to event and time-dependent detector information which can reside in various data sources. A number of utilities are also provided, including a novel geometry package which allows manipulation of abstract geometrical objects independent of coordinate system choice. The framework is implemented in C++, and takes advantage of object oriented design and common open source tools, while keeping the user side simple enough for C++ novices to learn in a reasonable time. The distribution system incorporates unit and acceptance testing in order to support rapid development of both the core framework and contributed user code.},
  doi      = {https://doi.org/10.1016/j.nima.2007.07.010},
  keywords = {Offline software, Framework, Object oriented, Simulation, Cosmic rays},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900207014106},
}

@Article{Ma2017,
  author   = {Xinxin Ma and Qi Wang and Hongmei Li and Chuantian Xu and Ning Cui and Xiaomin Zhao},
  title    = {16S rRNA genes Illumina sequencing revealed differential cecal microbiome in specific pathogen free chickens infected with different subgroup of avian leukosis viruses},
  journal  = {Veterinary Microbiology},
  year     = {2017},
  volume   = {207},
  pages    = {195 - 204},
  issn     = {0378-1135},
  abstract = {Intestinal flora play important roles in the pathogenisis of many pathogens. This study examined the cecal microbiome of chickens infected with avian leukosis virus (ALV) using 16S rRNA genes Illumina sequencing. One-day-old specific pathogen free chicks were inoculated in the abdomen with subgroup J or K of ALV. At 21-day-old, chickens positive for ALV viremia were selected and their cecal contents were extracted and examined for the composition of gut microflora by illumina sequencing of the V3+V4 region of the 16S rRNA genes. The results showed that there is a clear association with loss of important bacterial populations in concert with an enrichment of potentially pathogenic populations and ALV infections, despite of the virus subgroups. In addition, ALV-K infected chickens revealed a preference for opportunistic pathogens in Firmicutes such as Staphylococcus and Weissella and some genus from Bacillales. Whereas, ALV-J infected chickens were characterized by a larger number of notable pathogens like Escherichia-Shigella from Proteobacteria, and other condition pathogens including Enterococcus and members of Erysipelotrichaceae from Firmicutes, and members of Helicobacteraceae from Bacteroidetes. Collectively, our results suggest that relative abundance data from the cecal microbiome differentiates healthy chickens from those infected with ALVs. Most importantly, there was a significant difference in the gut microbiome of chickens infected with ALV-K compared to those with ALV-J infected ones. This strongly suggests that ALV infection may be associated with the microbiome and there may be multiple underlying mechanisms by which the microbiome is involved in the pathogenisis of different subgroup of ALV infections.},
  doi      = {https://doi.org/10.1016/j.vetmic.2017.05.016},
  keywords = {Avian leukosis virus, Subgroup, 16S rRNA, Cecal microbiome, Secondary infection},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378113517301906},
}

@Article{Riet2002,
  author  = {R.P. van de Riet},
  title   = {Notes on a classification for DKE},
  journal = {Data \& Knowledge Engineering},
  year    = {2002},
  volume  = {41},
  number  = {2},
  pages   = {133 - 139},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/S0169-023X(02)00037-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X0200037X},
}

@Article{Oueslati2015,
  author   = {Wided Oueslati and Hazar Hamdi and Zeineb Dhouioui and Jalel Akaichi},
  title    = {A Mobile Agent view Synchronization System to Uphold a Trajectory data Warehouse},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {60},
  pages    = {276 - 283},
  issn     = {1877-0509},
  note     = {Knowledge-Based and Intelligent Information \& Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings},
  abstract = {The trajectory data warehouse (TDW) view definitions are constructed from heterogeneous mobile information sources schema that are more and more independent. In fact, they frequently change their content due to perpetual transactions (data changes) and may change their structure due to continual users’ requirements evolving. Managing appropriately the view definition synchronization is a necessity since, the TDW is considered as the core component of the modern decision support systems. The aim of this paper is to propose a mobile agent view synchronization system to uphold a trajectory data warehouse under schema changes.},
  doi      = {https://doi.org/10.1016/j.procs.2015.08.127},
  keywords = {schema changes, mobile agent, view definition, view adaptation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915022541},
}

@Article{Khattak2015,
  author   = {A.M. Khattak and Z. Pervez and W.A. Khan and A.M. Khan and K. Latif and S.Y. Lee},
  title    = {Mapping evolution of dynamic web ontologies},
  journal  = {Information Sciences},
  year     = {2015},
  volume   = {303},
  pages    = {101 - 119},
  issn     = {0020-0255},
  abstract = {Information on the web and web services that are revised by stakeholders is growing incredibly. The presentation of this information has shifted from a representational model of web information with loosely clustered terminology to semi-formal terminology and even to formal ontology. Mediation (i.e., mapping) is required for systems and services to share information. Mappings are established between ontologies in order to resolve terminological and conceptual incompatibilities. Due to new discoveries in the field of information sharing, the body of knowledge has become more structured and refined. The domain ontologies that represent bodies of knowledge need to be able to accommodate new information. This allows for the ontology to evolve from one consistent state to another. Changes in resources cause existing mappings between ontologies to be unreliable and stale. This highlights the need for mapping evolution (regeneration) as it would eliminate the discrepancies from the existing mappings. In order to re-establish the mappings between dynamic ontologies, the existing systems require a complete mapping process to be restructured, and this process is time consuming. This paper proposes a mapping reconciliation approach between the updated ontologies that has been found to take less time to process compared to the time of existing systems when only the changed resources are considered and also eliminates the staleness of the existing mappings. The proposed approach employs the change history of ontology in order to store the ontology change information, which helps to drastically reduce the reconciliation time of the mappings between dynamic ontologies. A comprehensive evaluation of the performance of the proposed system on standard data sets has been conducted. The experimental results of the proposed system in comparison with six existing mapping systems are provided in this paper using 13 different data sets, which support our claims.},
  doi      = {https://doi.org/10.1016/j.ins.2014.12.040},
  keywords = {Ontology change, Change management, Change history, Mapping reconciliation},
  url      = {http://www.sciencedirect.com/science/article/pii/S002002551401192X},
}

@Article{1989,
  title   = {Author index to volume 4 (1989)},
  journal = {Data \& Knowledge Engineering},
  year    = {1989},
  volume  = {4},
  number  = {4},
  pages   = {357},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(89)90033-5},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X89900335},
}

@Article{Seng2009,
  author   = {Jia-Lang Seng and I.L. Kong},
  title    = {A schema and ontology-aided intelligent information integration},
  journal  = {Expert Systems with Applications},
  year     = {2009},
  volume   = {36},
  number   = {7},
  pages    = {10538 - 10550},
  issn     = {0957-4174},
  abstract = {The research issues of intelligent information integration have become ubiquitous and critically important in e-business (EB) with the increasing dependence on Internet/Intranet and information technology (IT). Accessing the intelligent information sources separately without integration may lead to the chaos of information requested. It is also not cost-effective in EB settings. A common general way to deal with heterogeneity problems in traditional III is to create a common data model. The eXtensible Markup Language (XML) has been the standard data document format for exchanging information on the Web. XML only deals with the structural heterogeneity; it can barely handle the semantic heterogeneity. Ontologies are regarded as an important and natural means to represent the implicit semantics and relationships in the real world. And they are used to assist to reach semantic interoperability in III in this research. In this paper, we provide a generic construct orientation no ad hoc method to generate the global schema to enable the web-based alternative to traditional III. We provide a wiser query method over multiple intelligent information sources by applying global-as-view (GAV) and local-as-view (LAV) approach with the use of ontology to enhance both structural and semantic interoperability of the underlying intelligent information sources. We construct a prototype implementing the method to provide a proof on the validity and feasibility.},
  doi      = {https://doi.org/10.1016/j.eswa.2009.02.067},
  keywords = {Intelligent information integration, XML, Ontology, Syntactic and semantic interoperability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417409002176},
}

@Article{Gibaud1998,
  author   = {B Gibaud and S Garlatti and C Barillot and E Faure},
  title    = {Computerized brain atlases as decision support systems: a methodological approach},
  journal  = {Artificial Intelligence in Medicine},
  year     = {1998},
  volume   = {14},
  number   = {1},
  pages    = {83 - 100},
  issn     = {0933-3657},
  note     = {Selected Papers from AIME '97},
  abstract = {This paper deals with the development of computerized brain atlases addressing both research and clinical needs. The authors analyze in detail the potential of these systems and discuss the capabilities and limitations of the digital atlases currently being developed around the world. The authors propose to reconsider the concept of a brain atlas, regarding both its content, and the way it has to be used and managed in order to set up more effective cooperation between the user and the system. Particular emphasis is placed on extensibility and reuse issues, which are critical in this rapidly evolving field. These orientations result from both the authors' experience and the analysis of current trends in the field of neuroimaging. The general methodology is illustrated with examples related to computer aided surgical planning.},
  doi      = {https://doi.org/10.1016/S0933-3657(98)00017-7},
  keywords = {Brain atlases, Decision support systems, Image interpretation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0933365798000177},
}

@Article{Jarke2000,
  author   = {Matthias Jarke},
  title    = {Experience-Based Knowledge Management},
  journal  = {IFAC Proceedings Volumes},
  year     = {2000},
  volume   = {33},
  number   = {12},
  pages    = {109 - 116},
  issn     = {1474-6670},
  note     = {7th IFAC Symposium on Automated Systems based on Human Skill 2000, Aachen, Germany, 15-17 June 2000},
  abstract = {Experience-based knowledge management is the art of capitalizing on failures and missed opportunities. We study three possible approaches within a cooperative information systems framework, building on a number of interdisciplinary research projects.},
  doi      = {https://doi.org/10.1016/S1474-6670(17)37287-7},
  keywords = {Knowledge Management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667017372877},
}

@Article{Ma2010,
  author   = {Qing-Hu Ma and Bing Tian and Yun-Liang Li},
  title    = {Overexpression of a wheat jasmonate-regulated lectin increases pathogen resistance},
  journal  = {Biochimie},
  year     = {2010},
  volume   = {92},
  number   = {2},
  pages    = {187 - 193},
  issn     = {0300-9084},
  abstract = {Jasmonates are known to induce the transcriptional activation of plant defense genes, which leads to the production of jasmonate-regulated proteins (JRP). We previously cloned and characterized a novel jacalin-like lectin gene (Ta-JA1) from wheat (Triticum aestivum L.), which codes a modular JRP with disease response and jacalin-related lectin (JRL) domains and is present only in the Gramineae family. The function of this protein is still unclear. Phylogenetic analysis indicated that Ta-JA1 and related proteins from cereals grouped together, which diverged from JRL with an additional N-terminal disease response domain. The recombinant Ta-JA1 proteins agglutinated rabbit erythrocytes, and this hemagglutination activity was preferentially inhibited by mannose. The Ta-JA1 protein was able to inhibit E. coli cell growth. Overexpression of Ta-JA1 in transgenic tobacco plants increased their resistance to infection by tobacco bacterial, fungal and viral pathogens. Our results suggest that Ta-JA1 belongs to a mannose-specific lectin, which may confer a basal but broad-spectrum resistance to plant pathogens. Ta-JA1 and its homologues in maize, rice, sorghum and creeping bentgrass may represent a new type of monocot lectin with a modular structure and diversity of physiological functions in biotic and abiotic stress responses.},
  doi      = {https://doi.org/10.1016/j.biochi.2009.11.008},
  keywords = {Broad-spectrum resistance, Jasmonates, Jasmonate-regulated proteins (JRP), Monocot jacalin-related lectins, L.},
  url      = {http://www.sciencedirect.com/science/article/pii/S0300908409003125},
}

@Article{Gouy1985,
  author   = {M. Gouy and C. Gautier and F. Milleret},
  title    = {System analysis and nucleic acid sequence banks},
  journal  = {Biochimie},
  year     = {1985},
  volume   = {67},
  number   = {5},
  pages    = {433 - 436},
  issn     = {0300-9084},
  abstract = {Résumé
Les séquences d'acides nucléiques connues représentent plus de 5 millions de bases. Ce volume et la complexité de ces données a justifié la création de plusieurs bases de données informatisées. Nous présentons ici les relations entre la construction de telles bases et la méthodologie de l'Analyse de Systèmes. La base ACNUC que nous avons construite et que nous maintenons est présentée en exemple.
Summary
The mass of published nucleic acid sequence data has required the design of several computerized data bases. We show that this activity is related to the methodology of System Analysis and that data bases are a means of modeling biological knowledge. As an example, the ACNUC data base we have created is presented.},
  doi      = {https://doi.org/10.1016/S0300-9084(85)80260-1},
  keywords = {séquences d'acides nucléiques, banque de données, analyse de systèmes, nucleic acid sequences, database, system analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0300908485802601},
}

@Article{Feng2009,
  author   = {Guoqi Feng and Dongliang Cui and Chengen Wang and Jiapeng Yu},
  title    = {Integrated data management in complex product collaborative design},
  journal  = {Computers in Industry},
  year     = {2009},
  volume   = {60},
  number   = {1},
  pages    = {48 - 63},
  issn     = {0166-3615},
  abstract = {Scientific computing in complex product engineering design process has the characteristics of being tentative and iterative, which produces large scale intermediate data with heterogeneous formats and complex relationships, and the efficient organization and management of data becomes a bottleneck of product design performance. The concept of business component is introduced in this paper based on the modular analysis of the engineering process, and an integrated data management and storage framework is proposed for efficient heterogeneous design data management, in which XML schema, relational database and design files are synthesized. A network version management model is designed to enhance the design process and design object data management, which can describe both the individual object's evolution process and the inheritance relationships among multiple objects. Furthermore, key operations of version merge, submission and visualization are discussed. Finally the methods and technologies are implemented in an integrated management system for aerodynamic design of aero turbine engines (IMS-DATE).},
  doi      = {https://doi.org/10.1016/j.compind.2008.09.006},
  keywords = {Complex product, Data management, Version control, Data model, XML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361508001036},
}

@Article{Chebotko2010,
  author   = {Artem Chebotko and Shiyong Lu and Xubo Fei and Farshad Fotouhi},
  title    = {RDFProv: A relational RDF store for querying and managing scientific workflow provenance},
  journal  = {Data \& Knowledge Engineering},
  year     = {2010},
  volume   = {69},
  number   = {8},
  pages    = {836 - 865},
  issn     = {0169-023X},
  abstract = {Provenance metadata has become increasingly important to support scientific discovery reproducibility, result interpretation, and problem diagnosis in scientific workflow environments. The provenance management problem concerns the efficiency and effectiveness of the modeling, recording, representation, integration, storage, and querying of provenance metadata. Our approach to provenance management seamlessly integrates the interoperability, extensibility, and inference advantages of Semantic Web technologies with the storage and querying power of an RDBMS to meet the emerging requirements of scientific workflow provenance management. In this paper, we elaborate on the design of a relational RDF store, called RDFProv, which is optimized for scientific workflow provenance querying and management. Specifically, we propose: i) two schema mapping algorithms to map an OWL provenance ontology to a relational database schema that is optimized for common provenance queries; ii) three efficient data mapping algorithms to map provenance RDF metadata to relational data according to the generated relational database schema, and iii) a schema-independent SPARQL-to-SQL translation algorithm that is optimized on-the-fly by using the type information of an instance available from the input provenance ontology and the statistics of the sizes of the tables in the database. Experimental results are presented to show that our algorithms are efficient and scalable. The comparison with two popular relational RDF stores, Jena and Sesame, and two commercial native RDF stores, AllegroGraph and BigOWLIM, showed that our optimizations result in improved performance and scalability for provenance metadata management. Finally, our case study for provenance management in a real-life biological simulation workflow showed the production quality and capability of the RDFProv system. Although presented in the context of scientific workflow provenance management, many of our proposed techniques apply to general RDF data management as well.},
  doi      = {https://doi.org/10.1016/j.datak.2010.03.005},
  keywords = {Provenance, Scientific workflow, Metadata management, Ontology, RDF, OWL, SPARQL-to-SQL translation, Query optimization, RDF store, RDBMS},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X10000455},
}

@Article{Storey2015,
  author   = {Veda C. Storey and Juan C. Trujillo and Stephen W. Liddle},
  title    = {Research on conceptual modeling: Themes, topics, and introduction to the special issue},
  journal  = {Data \& Knowledge Engineering},
  year     = {2015},
  volume   = {98},
  pages    = {1 - 7},
  issn     = {0169-023X},
  note     = {Research on conceptual modeling},
  abstract = {Conceptual modeling continues to evolve as researchers and practitioners reflect on the challenges of modeling and implementing data-intensive problems that appear in business and in science. These challenges of data modeling and representation are well-recognized in contemporary applications of big data, ontologies, and semantics, along with traditional efforts associated with methodologies, tools, and theory development. This introduction contains a review of some current research in conceptual modeling and identifies emerging themes. It also introduces the articles that comprise this special issue of papers from the 32nd International Conference on Conceptual Modeling (ER 2013).},
  doi      = {https://doi.org/10.1016/j.datak.2015.07.002},
  keywords = {Conceptual modeling, Big data, Business process modeling, Ontology, Modeling tools, Modeling techniques},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X15000476},
}

@Article{Joseph1991,
  author   = {John Joseph and Mark Shadowens and John Chen and Craig Thompson},
  title    = {Strawman reference model for change management of objects},
  journal  = {Computer Standards \& Interfaces},
  year     = {1991},
  volume   = {13},
  number   = {1},
  pages    = {249 - 269},
  issn     = {0920-5489},
  abstract = {This article provides a strawman11This effort is intended as a good start towards a change management reference model; its treatment of change management is not complete. We wrote it to be consistent with OODBTG's OODB Reference Model. One may view it as a mini-reference model refining the Change. Management section of that reference model. Alternatively, it can stand alone as a separate reference model since it identifies and isolates an orthogonal abstraction. reference model which can be used for comparing and reasoning about change management systems. It begins with a glossary of change management terms, providing common ground for people to communicate about change management. A descriptive reference model consisting of a collection of characteristics that can be used, again by people, for comparing existing and future change management systems is then described. Finally, based on the descriptive reference model, we define a functional reference model. The functional reference model provides a much more precise description of how machines can realize generic change management operations. While the glossary and descriptive reference model are steps on the path to forming standards, the more precise model is closer to what is needed to provide interoperability (standards) for machines. We hope that the step-wise refinement exposition used in this paper, from terms through a descriptive reference model to an operational reference model, will provide a good roadmap for OODBTG or similar groups to reach consensus leading to standards in the area of change management.},
  doi      = {https://doi.org/10.1016/0920-5489(91)90033-V},
  keywords = {Change management, comparisons, object-oriented, reference model},
  url      = {http://www.sciencedirect.com/science/article/pii/092054899190033V},
}

@Article{Murphy1993,
  author   = {J Murphy and J Grimson},
  title    = {Formal specification of a persistent object management system},
  journal  = {Information and Software Technology},
  year     = {1993},
  volume   = {35},
  number   = {5},
  pages    = {277 - 286},
  issn     = {0950-5849},
  abstract = {The goal of the research on which the paper is based was to specify formally a persistent object management system and to implement a part of the specification in the form of a prototype. The prototype is called the persistent object storage manager (POSM). The prototype was implemented in C++ using the IBM OS/2 operating system. POSM is formally specified in the paper. The data model used in POSM is rigorously defined using the Z notation. The operations of POSM and its state space are also specified in Z notation. Example schemas for the operations of the node management component of the prototype are presented. Z notation is justified in the paper and the benefits of using Z in the research are discussed.},
  doi      = {https://doi.org/10.1016/0950-5849(93)90061-7},
  keywords = {object management, object-oriented database systems, formal techniques, Z notation, mathematics in software design},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584993900617},
}

@Article{Vora2015,
  author   = {Urjaswala Vora},
  title    = {Precepts and Evolvability of Complex Systems},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {62},
  pages    = {565 - 574},
  issn     = {1877-0509},
  note     = {Proceedings of the 2015 International Conference on Soft Computing and Software Engineering (SCSE'15)},
  abstract = {In complex enterprise systems that undergo continual evolutions, the change impact tends to get cumulative and adverse and have high probability of degradation of software quality. Functional evolutions are the most frequent and the most impacting evolutions. The functional components in a software architecture that are aligned with them are the activities. Activity is a business process that fulfils an operation contract of an application. An application is a set of activities that get invoked by different users. In the software evolution process, the change impact analysis techniques generate the model(s) of the data-flows and/or control-flows within the source code for the activity to be evolved. They capture the propagation of changes and derive the change impact sets which are then used for the planning, estimation, development and verification phases of the evolution process. The taxonomical surveys of evolutions indicate that the control structures embedded within the source code undergo changes much more rapidly than the other elements and are the major culprits in the evolvability degradation. We define an architectural approach of modeling the control structures within the activities of an application as precepts, the control-flow rule-sets, that mitigates the adverse impact of evolution. Precepts also facilitate the definition of Evolvability Metrics that measure the evolvability index of an application. The existing metrics that indicate evolvability, measure the complexity as well as modularity at a low level and cannot be aggregated trivially. To validate these metrics, we define Efforts Deviation Index that captures the difficulty level of the change implementation process.},
  doi      = {https://doi.org/10.1016/j.procs.2015.08.533},
  keywords = {Evolvability, Complex Systems, Control Flows, Software Evolution, Evolvability Metrics, Precepts.},
  url      = {http://www.sciencedirect.com/science/article/pii/S187705091502668X},
}

@Article{1990,
  title   = {Volume contents and author index volume 15, 1990},
  journal = {Information Systems},
  year    = {1990},
  volume  = {15},
  number  = {6},
  pages   = {v - ix},
  issn    = {0306-4379},
  doi     = {https://doi.org/10.1016/0306-4379(90)90070-6},
  url     = {http://www.sciencedirect.com/science/article/pii/0306437990900706},
}

@Article{Shen2010,
  author   = {Terry H. Shen and Peter Tarczy-Hornoch and Landon T. Detwiler and Eithon Cadag and Christopher S. Carlson},
  title    = {Evaluation of probabilistic and logical inference for a SNP annotation system},
  journal  = {Journal of Biomedical Informatics},
  year     = {2010},
  volume   = {43},
  number   = {3},
  pages    = {407 - 418},
  issn     = {1532-0464},
  note     = {Translational Bioinformatics},
  abstract = {Genome wide association studies (GWAS) are an important approach to understanding the genetic mechanisms behind human diseases. Single nucleotide polymorphisms (SNPs) are the predominant markers used in genome wide association studies, and the ability to predict which SNPs are likely to be functional is important for both a priori and a posteriori analyses of GWA studies. This article describes the design, implementation and evaluation of a family of systems for the purpose of identifying SNPs that may cause a change in phenotypic outcomes. The methods described in this article characterize the feasibility of combinations of logical and probabilistic inference with federated data integration for both point and regional SNP annotation and analysis. Evaluations of the methods demonstrate the overall strong predictive value of logical, and logical with probabilistic, inference applied to the domain of SNP annotation.},
  doi      = {https://doi.org/10.1016/j.jbi.2009.12.002},
  keywords = {Single nucleotide polymorphisms (SNPs), Federated data integration, SNP annotation system, Logical inference, Probabilistic inference, SNP evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1532046409001579},
}

@Article{Shah2001,
  author   = {Abad Shah},
  title    = {A Framework for the Prototype-based Software Development Methodologies},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  year     = {2001},
  volume   = {13},
  pages    = {111 - 131},
  issn     = {1319-1578},
  abstract = {In the object-oriented paradigm, two techniques, class-based technique and prototype-based technique are available for modeling the real-world objects. In this paper, we first study and analyze both object-modeling techniques, then using this study and analysis we identify a class of applications, and argue that the class-based methodologies that use the class-based technique as the object-modeling technique, are inappropriate to use for the development of this class of applications. Considering the requirements of the identified class of applications, we argue for need of a class of software development methodologies which are referred to as the prototype-based methodologies. In this paper, we also propose modifications in the classical Water-Fall life-cycle software development model, which make it consistent with the requirements of the prototype-based methodologies. The modified life-cycle model provides a framework and basic guidelines for proposing the prototype-based methodologies.},
  doi      = {https://doi.org/10.1016/S1319-1578(01)80006-9},
  keywords = {object-oriented paradigm, class-based technique, prototype-based technique, object modeling, software development methodologies, framework},
  url      = {http://www.sciencedirect.com/science/article/pii/S1319157801800069},
}

@Article{Jorng-Tzong1994,
  author   = {Horng Jorng-Tzong and Liu Baw-Jhiune},
  title    = {Some aspects of operations in an object-oriented data base based on graphs},
  journal  = {Journal of Systems and Software},
  year     = {1994},
  volume   = {24},
  number   = {2},
  pages    = {155 - 179},
  issn     = {0164-1212},
  note     = {Object-orientation in Info. Systems},
  abstract = {Object-oriented data base operators have been extensively studied in recent years. In this article we attempt to enhance the set of operators and define them in a more formal way based on graphs. We adopt directed acyclic graphs to model object-oriented data bases. The operators are defined by graph transformations. Based on this graph-theoretic approach, a family of operators on graphs are defined as the basic operators for object-oriented data bases. Moreover, many applications of graphs, such as matching problems, are used to enhance the set of operators on object-oriented data bases. We can use this kind of operator to implement certain queries that are adequate for new application domains such as decision support systems. We also develop a set of schema-restructuring operators which can be used to integrate individual schemas. The integration presents users with a logically integrated global view of the data stored in the individual schemas without requiring that the schemas be physically integrated. We use a query language based on SMALLTALK-like messages. Queries will be implemented by translating the queries into our defined operators, which are then interpreted.},
  doi      = {https://doi.org/10.1016/0164-1212(94)90077-9},
  url      = {http://www.sciencedirect.com/science/article/pii/0164121294900779},
}

@InCollection{Cohen2007,
  author    = {Frank Cohen},
  title     = {Chapter 1 - The Problem with Service-Oriented Architecture},
  booktitle = {FastSOA},
  publisher = {Morgan Kaufmann},
  year      = {2007},
  editor    = {Frank Cohen},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {1 - 40},
  address   = {Burlington},
  isbn      = {978-0-12-369513-0},
  abstract  = {Publisher Summary
This chapter discusses the problems with the service-oriented architecture (SOA). The chapter reveals that business managers relish the idea of SOA. They widely believe SOA will get them to composite applications immediately. The wonderful part of SOA is its ability to pull together adopted and well-understood protocols, software development coding techniques, and a governance model. These lead to common use of SOA message patterns to exchange standards-based business documents in a business process. The problematic issues in SOA are driven by the gap between management's buy-in to SOA and the ability of software architects and developers to deliver production-worthy software code that achieves user satisfaction. This chapter introduces SOA from a performance and scalability perspective. Bosworth's issue is reflected in industry and institutional emphasis on document definitions (schema) and workflow message exchange patterns. Many enterprises began their SOA efforts by forming industry associations to define the document schemas they had in common.},
  doi       = {https://doi.org/10.1016/B978-012369513-0/50002-1},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123695130500021},
}

@Article{Rahayu2000,
  author   = {J.W. Rahayu and E. Chang and T.S. Dillon and D. Taniar},
  title    = {A methodology for transforming inheritance relationships in an object-oriented conceptual model to relational tables},
  journal  = {Information and Software Technology},
  year     = {2000},
  volume   = {42},
  number   = {8},
  pages    = {571 - 592},
  issn     = {0950-5849},
  abstract = {With the increasing popularity of Object-Relational technology, it becomes necessary to have a methodology that allows database designers to exploit the great modeling power of object-oriented conceptual model, and still facilitate implementation on relational database systems. This paper presents a transformation methodology from inheritance relationships to relational tables. This includes transformation of different types of inheritance, such as union inheritance, mutual exclusion inheritance, partition inheritance and multiple inheritance. Performance comparison between the proposed transformation methodology and existing methods is also carried out. From the evaluation, we conclude that the proposed transformation methodology is more efficient than the others.},
  doi      = {https://doi.org/10.1016/S0950-5849(00)00103-8},
  keywords = {Object conceptual modeling, Inheritance relationships, Object-Relational transformation methodology, Relational databases, Performance evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584900001038},
}

@Article{Magdalena2012,
  author   = {Ciobanu (Iacob) Nicoleta - Magdalena and Ciobanu (Defta) Costinela Luminiţa},
  title    = {Synchronous Partial Replication – Case Study: Implementing e-Learning Platform in an Academic Environment},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2012},
  volume   = {46},
  pages    = {1522 - 1526},
  issn     = {1877-0428},
  note     = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
  abstract = {Replication is the key factor in improving availability, fault-tolerance and accessibility of data in all types of distributed systems. Replicated data is stored at multiple sites so that it can be used even when some copies are not available due to site failures and databases are kept synchronized to maintain consistency. The scope of distributing these replicas is local data processing. This paper describes the importance of synchronous partial replication, implementation and query optimization steps using Oracle technology for an E-learning portal of a university with geographically distributed locations.},
  doi      = {https://doi.org/10.1016/j.sbspro.2012.05.333},
  keywords = {Distributed databases, replication, methods, strategies},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877042812014620},
}

@Article{Wollschlaeger2005,
  author   = {Martin Wollschlaeger and Henry Kulzer and Daniel Nübling and Peter Wenzel},
  title    = {A COMMON MODEL FOR XML DESCRIPTIONS IN AUTOMATION},
  journal  = {IFAC Proceedings Volumes},
  year     = {2005},
  volume   = {38},
  number   = {1},
  pages    = {106 - 111},
  issn     = {1474-6670},
  note     = {16th IFAC World Congress},
  abstract = {The use of XML as a description language has become state of the art within the automation and control domain. Use cases and application scenarios are manifold, resulting in heterogeneous XML document structures. However, a common basic model for XML Applications is still lacking. This paper describes requirements and principle structures for such a common model, which has been developed for use within PROFIBUS and PROFINET systems.},
  doi      = {https://doi.org/10.3182/20050703-6-CZ-1902.01536},
  keywords = {Automation, Description, XML, Internet, Software Tool},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667016375486},
}

@Article{1985,
  title   = {Author index to volumes 1–10 (1985–1993)},
  journal = {Data \& Knowledge Engineering},
  year    = {1985},
  volume  = {1-10},
  pages   = {i - viii},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(85)90001-1},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X85900011},
}

@Article{Cox2012,
  author   = {G.A. Cox and E. Armengaud and C. Augier and A. Benoît and L. Bergé and T. Bergmann and J. Blümer and G. Bres and A. Broniatowski and V. Brudanin and B. Censier and M. Chapellier and G. Chardin and F. Charlieux and S. Collin and P. Coulter and O. Crauste and M. De Jésus and J. Domange and L. Dumoulin and K. Eitel and D. Filosofov and N. Fourches and J. Gascon and G. Gerbier and J. Gironnet and M. Gros and S. Henry and S. Hervé and S. Jokisch and A. Juillard and M. Kleifges and H. Kluck and V. Kozlov and H. Kraus and V.A. Kudryavtsev and P. Loaiza and S. Marnieros and A. Menshikov and X.-F. Navick and C. Nones and E. Olivieri and P. Pari and L. Pattavina and B. Paul and M. Robinson and H. Rodenas and S. Rozov and V. Sanglard and B. Schmidt and S. Semikh and D. Tcherniakhovski and A.S. Torrento-Coello and M. Unrau and L. Vagneron and M.-A. Verdier and R.J. Walker and M. Weber and E. Yakushev and X. Zhang},
  title    = {A multi-tiered data structure and process management system based on ROOT and CouchDB},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2012},
  volume   = {684},
  pages    = {63 - 72},
  issn     = {0168-9002},
  abstract = {A multi-tiered data structure, analysis toolkit and data processing management system has been constructed using ROOT and CouchDB. This system is well suited for experiments that acquire many computer files of raw data over the course of months or years, that are distributed to different computing centers and further reduced in size by several steps of data processing. Data handling for experiments searching for rare events extracted from digitized pulse traces typically fit this description. An implementation of this system has been constructed for the EDELWEISS-III experiment and is described here in some detail. This solution may also serve as a prototype system for the proposed EURECA experiment.},
  doi      = {https://doi.org/10.1016/j.nima.2012.04.049},
  keywords = {Data structure, Data management, ROOT, CouchDB, Dark Matter, Multi-tier},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900212004044},
}

@Article{Jarke2002,
  author   = {Matthias Jarke},
  title    = {Experience-based knowledge management: a cooperative information systems perspective},
  journal  = {Control Engineering Practice},
  year     = {2002},
  volume   = {10},
  number   = {5},
  pages    = {561 - 569},
  issn     = {0967-0661},
  note     = {AHBHS00},
  abstract = {Experience-based knowledge management is the art of capitalizing on failures and missed opportunities. Building on a number of interdisciplinary research projects, we study three possible approaches within a cooperative information systems framework, focussing on the facets of pragmatic technology usage, model-based management control, and social work practice and learning, respectively.},
  doi      = {https://doi.org/10.1016/S0967-0661(01)00161-7},
  keywords = {Knowledge management, CSCW, Traceability, Document mining, Knowledge communities, Awareness},
  url      = {http://www.sciencedirect.com/science/article/pii/S0967066101001617},
}

@Article{Zicari1991,
  author   = {Roberto Zicari},
  title    = {Primitives for schema updates in an object-oriented database system: a proposal},
  journal  = {Computer Standards \& Interfaces},
  year     = {1991},
  volume   = {13},
  number   = {1},
  pages    = {271 - 284},
  issn     = {0920-5489},
  abstract = {Updating the schema is an important facility for object-oriented databases. However, updates should not result in inconsistencies either in the schema or in the database. We propose a classification of basic schema updates and define a set of parametrized primitives to perform schema updates which the designer will use to define his/her own update semantics.},
  doi      = {https://doi.org/10.1016/0920-5489(91)90034-W},
  keywords = {Database systems, language primitives, object-oriented, schemes, updates},
  url      = {http://www.sciencedirect.com/science/article/pii/092054899190034W},
}

@Article{Ma2011,
  author   = {Qing-Hu Ma and Cui Wang and Hai-Hao Zhu},
  title    = {TaMYB4 cloned from wheat regulates lignin biosynthesis through negatively controlling the transcripts of both cinnamyl alcohol dehydrogenase and cinnamoyl-CoA reductase genes},
  journal  = {Biochimie},
  year     = {2011},
  volume   = {93},
  number   = {7},
  pages    = {1179 - 1186},
  issn     = {0300-9084},
  abstract = {The subgroup 4 of R2R3-MYB transcription factors has been proposed as repressors regulating the phenylpropanoid pathway. Here, we report a cDNA encoding a subgroup 4 R2R3-MYB factor from wheat, designated as TaMYB4. A phylogenetic analysis showed that TaMYB4 is in a subclade that is specific to monocot plants. The TaMYB4 gene was highly expressed in stem and root tissues. In vitro binding analysis in yeast cells showed TaMYB4 interacted with OsCAD2 promoter characterized by an AC-II element that has been considered as the MYB-binding site in lignin biosynthetic genes. The overexpression of TaMYB4 in transgenic tobacco led to transcriptional reduction of both cinnamyl alcohol dehydrogenase (CAD) and cinnamoyl-CoA reductase (CCR) genes involved in the lignin biosynthesis. Enzymatic assay showed reduction of CAD and CCR activities in the transgenic tobacco plants that substantially decreased the levels of total lignin but increased it’s ratio of S/G. In addition, the total flavonoid content was increased in transgenic tobacco leaves, suggesting that the overexpression of TaMYB4 likely led to a redirection of the metabolic flux from the lignin pathway to the flavonoid pathway. These data suggest that TaMYB4 negatively regulates the lignin biosynthesis in wheat.},
  doi      = {https://doi.org/10.1016/j.biochi.2011.04.012},
  keywords = {Cinnamyl alcohol dehydrogenase (CAD), Cinnamoyl-CoA reductase (CCR), Lignin biosynthesis, R2R3-MYB transcriptional factors, TaMYB4, L.},
  url      = {http://www.sciencedirect.com/science/article/pii/S0300908411001313},
}

@Article{Kumar2014,
  author   = {Arun Kumar and Anish Kaachra and Shruti Bhardwaj and Sanjay Kumar},
  title    = {Copper, zinc superoxide dismutase of Curcuma aromatica is a kinetically stable protein},
  journal  = {Process Biochemistry},
  year     = {2014},
  volume   = {49},
  number   = {8},
  pages    = {1288 - 1296},
  issn     = {1359-5113},
  abstract = {This study details on cloning and characterization of Cu,Zn superoxide dismutase (Ca–Cu,Zn SOD) from a medicinally important plant species Curcuma aromatica. Ca–Cu,Zn SOD was 692bp with an open reading frame of 459bp. Expression of the gene in Escherichia coli cells followed by purification yielded the enzyme with Km of 0.047±0.008μM and Vmax of 1250±24units/mg of protein. The enzyme functioned (i) across a temperature range of −10 to +80°C with temperature optima at 20°C; and (ii) at pH range of 6–9 with optimum activity at pH 7.8. Ca–Cu,Zn SOD retained 50% of the maximum activity after autoclaving, and was stable at a wide storage pH ranging from 3 to 10. The enzyme tolerated varying concentrations of denaturating agent, reductants, inhibitors, trypsin, was fairly resistant to inactivation at 80°C for 180min (kd, 6.54±0.17×10−3 min−1; t1/2, 106.07±2.68min), and had midpoint of thermal transition (Tm) of 70.45°C. The results suggested Ca–Cu,Zn SOD to be a kinetically stable protein that could be used for various industrial applications.},
  doi      = {https://doi.org/10.1016/j.procbio.2014.04.010},
  keywords = {Superoxide dismutase, Autoclave stable, Thermal inactivation, Trypsin, Sodium dodecyl sulfate, Kinetic stability},
  url      = {http://www.sciencedirect.com/science/article/pii/S1359511314002347},
}

@InCollection{BERTHOLD2003e,
  author    = {BERTHOLD DAUM},
  title     = {7 - Relax NG},
  booktitle = {Modeling Business Objects with XML Schema},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {BERTHOLD DAUM},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {215 - 248},
  address   = {San Francisco},
  isbn      = {978-1-55860-816-0},
  doi       = {https://doi.org/10.1016/B978-155860816-0/50009-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608160500094},
}

@Article{Sawyer1995,
  author   = {Peter Sawyer and John A. Mariani},
  title    = {Database systems: challenges and opportunities for graphical HCI},
  journal  = {Interacting with Computers},
  year     = {1995},
  volume   = {7},
  number   = {3},
  pages    = {273 - 303},
  issn     = {0953-5438},
  abstract = {Databases and their applications form one of the most important classes of computer systems yet they have received relatively little attention from the HCI community. They have nevertheless spawned some notably innovative user interfaces and it is interesting to examine these in the light of contemporary HCI issues. The paper addresses the relationship between HCI and database systems, reviews some of the major themes running through existing database user interfaces and postulates some issues which are likely to be important to database usability in the future. The underlying argument is that databases are sufficiently different from other classes of application to necessitate a raft of user interface techniques specifically for the needs of database users which would reward increased attention by the HCI community.},
  doi      = {https://doi.org/10.1016/0953-5438(95)93605-5},
  keywords = {human-computer interaction, graphical user interfaces, databases},
  url      = {http://www.sciencedirect.com/science/article/pii/0953543895936055},
}

@Article{Selma2012,
  author   = {Khouri Selma and Boukhari Ilyès and Bellatreche Ladjel and Sardet Eric and Jean Stéphane and Baron Michael},
  title    = {Ontology-based structured web data warehouses for sustainable interoperability: requirement modeling, design methodology and tool},
  journal  = {Computers in Industry},
  year     = {2012},
  volume   = {63},
  number   = {8},
  pages    = {799 - 812},
  issn     = {0166-3615},
  note     = {Special Issue on Sustainable Interoperability: The Future of Internet Based Industrial Enterprises},
  abstract = {The spectacular growth of the Internet and its widespread adoption by worldwide corporations lead to an enormous quantity of heterogeneous, distributed and autonomous data sources. To facilitate the access to these huge amounts of data and make these sources interoperable, two technologies may be combined: data warehousing and ontologies. Data warehouses are designed to aggregate data and allow decision makers in these companies to obtain accurate, complete and up to date information. In the past decade, Data Warehouse Technology (DWT) has been successfully applied in several domains such as telecommunication, retail, finance and many other industries. It supports a wide range of applications throughout the enterprise. The DWT has been largely used to offer sustainable solutions for enterprises. On the other hand, ontologies are models for specifying the semantics of concepts used by various heterogeneous sources in a well defined and unambiguous way. Ontologies exist in various domains (E-commerce, Engineering, Tourism, etc.) and are used to increase interoperability between sources. They may be used to improve communication between decision makers and users collaborating together, by specifying the semantics of the used concepts. In this paper, we propose a methodology for designing data warehousing applications from various sources. Each source has its local ontology referencing a global one. The presence of ontologies has three main contributions: (i) each owner of each source may use it to define his/her requirements, (ii) it reduces most important types of conflicts that may exist in sources and requirements (schematic and semantic) and (iii) it facilitates the sustainable urbanization of the target data warehouse. Our methodology is supported by a case tool facilitating the tasks of data warehouse designers.},
  doi      = {https://doi.org/10.1016/j.compind.2012.08.001},
  keywords = {Structured web data, Data warehouses, Ontology-based databases, Requirements engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361512001200},
}

@Article{Ceusters2007,
  author   = {Werner Ceusters and Peter Elkin and Barry Smith},
  title    = {Negative findings in electronic health records and biomedical ontologies: A realist approach},
  journal  = {International Journal of Medical Informatics},
  year     = {2007},
  volume   = {76},
  pages    = {S326 - S333},
  issn     = {1386-5056},
  note     = {Ubiquity: Technologies for Better Health in Aging Societies - MIE 2006},
  abstract = {Purpose
A substantial fraction of the observations made by clinicians and entered into patient records are expressed by means of negation or by using terms which contain negative qualifiers (as in “absence of pulse” or “surgical procedure not performed”). This seems at first sight to present problems for ontologies, terminologies and data repositories that adhere to a realist view and thus reject any reference to putative non-existing entities. Basic Formal Ontology (BFO) and Referent Tracking (RT) are examples of such paradigms. The purpose of the research here described was to test a proposal to capture negative findings in electronic health record systems based on BFO and RT.
Methods
We analysed a series of negative findings encountered in 748 sentences taken from 41 patient charts. We classified the phenomena described in terms of the various top-level categories and relations defined in BFO, taking into account the role of negation in the corresponding descriptions. We also studied terms from SNOMED-CT containing one or other form of negation. We then explored ways to represent the described phenomena by means of the types of representational units available to realist ontologies such as BFO.
Results
We introduced a new family of ‘lacks’ relations into the OBO Relation Ontology. The relation lacks_part, for example, defined in terms of the positive relation part_of, holds between a particular p and a universal u when p has no instance of u as part. Since p and u both exist, assertions involving ‘lacks_part’ and its cognates meet the requirements of positivity.
Conclusion
By expanding the OBO Relation Ontology, we were able to accommodate nearly all occurrences of negative findings in the sample studied.},
  doi      = {https://doi.org/10.1016/j.ijmedinf.2007.02.003},
  keywords = {Referent Tracking, Negation, Negative findings, Ontology, Electronic health record},
  url      = {http://www.sciencedirect.com/science/article/pii/S1386505607000408},
}

@InCollection{Bouguettaya1993,
  author    = {Athman Bouguettaya and Roger King},
  title     = {Large Multidatabases: Issues and Directions},
  booktitle = {Interoperable Database Systems (Ds-5)},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {DAVID K. HSIAO and ERICH J. NEUHOLD and RON SACKS-DAVIS},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {55 - 68},
  address   = {Amsterdam},
  isbn      = {978-0-444-89879-1},
  abstract  = {The level of complexity in achieving interoperability in a multidatabase system largely depends on the number of component databases. If the size of the multidatabase system is small, it is reasonable to assume that current technology is adequate to address the interoperability problem. However, as the size increases, it becomes more and more difficult (if not impossible) to apply existing technology. The size of the multidatabase system is the single most important criterion that shapes the complexity of the problem. As the size becomes larger, new problems related to database autonomy and heterogeneity, as well as user education about available information, surface.},
  doi       = {https://doi.org/10.1016/B978-0-444-89879-1.50009-9},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444898791500099},
}

@InCollection{Celko2006,
  title     = {Index},
  booktitle = {Joe Celko's Analytics and OLAP in SQL},
  publisher = {Morgan Kaufmann},
  year      = {2006},
  editor    = {Joe Celko},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {173 - 181},
  address   = {Burlington},
  isbn      = {978-0-12-369512-3},
  doi       = {https://doi.org/10.1016/B978-012369512-3/50041-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012369512350041X},
}

@Article{Ortin2009,
  author   = {Francisco Ortin and Jose Manuel Redondo and J. Baltasar García Perez-Schofield},
  title    = {Efficient virtual machine support of runtime structural reflection},
  journal  = {Science of Computer Programming},
  year     = {2009},
  volume   = {74},
  number   = {10},
  pages    = {836 - 860},
  issn     = {0167-6423},
  abstract = {Increasing trends towards adaptive, distributed, generative and pervasive software have made object-oriented dynamically typed languages become increasingly popular. These languages offer dynamic software evolution by means of reflection, facilitating the development of dynamic systems. Unfortunately, this dynamism commonly imposes a runtime performance penalty. In this paper, we describe how to extend a production JIT-compiler virtual machine to support runtime object-oriented structural reflection offered by many dynamic languages. Our approach improves runtime performance of dynamic languages running on statically typed virtual machines. At the same time, existing statically typed languages are still supported by the virtual machine. We have extended the .Net platform with runtime structural reflection adding prototype-based object-oriented semantics to the statically typed class-based model of .Net, supporting both kinds of programming languages. The assessment of runtime performance and memory consumption has revealed that a direct support of structural reflection in a production JIT-based virtual machine designed for statically typed languages provides a significant performance improvement for dynamically typed languages.},
  doi      = {https://doi.org/10.1016/j.scico.2009.04.001},
  keywords = {Structural reflection, Dynamically typed languages, JIT compilation, SSCLI, Virtual machine, Prototype-based object-oriented model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642309000689},
}

@Article{Arias2003,
  author   = {P. Arias and M. Orlich and M. Prieto and S. Cedillo Rosales and H.-J. Thiel and M. Álvarez and P. Becher},
  title    = {Genetic heterogeneity of bovine viral diarrhoea viruses from Spain},
  journal  = {Veterinary Microbiology},
  year     = {2003},
  volume   = {96},
  number   = {4},
  pages    = {327 - 336},
  issn     = {0378-1135},
  note     = {Pestiviruses ii: Contributions from the 5th Pestivirus Symposium of the European Society for Veterinary Virology},
  abstract = {Forty-four bovine viral diarrhoea virus (BVDV) isolates collected from the north of Spain between 1989 and 2000 were characterised at the molecular level. For 24 of these isolates the entire Npro gene sequence was determined. Phylogenetic analysis revealed that 21 isolates belong to subgroup BVDV-1b, while two BVDV-1c isolates and one BVDV-1h isolate were found. Furthermore, 20 additional virus isolates were analysed by differential reverse transcription-polymerase chain reaction (RT-PCR) and classified as BVDV-1. The results from our study demonstrate that BVDV-1b is the most prevalent subgroup present in bovine dairy herds of Spain, while there is no evidence for the presence of BVDV-2.},
  doi      = {https://doi.org/10.1016/j.vetmic.2003.09.009},
  keywords = {Bovine viral diarrhoea virus, Genotyping, Cattle, Viruses, Diagnosis, Viruses},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378113503002529},
}

@Article{Lenz2007,
  author   = {Richard Lenz and Manfred Reichert},
  title    = {IT support for healthcare processes – premises, challenges, perspectives},
  journal  = {Data \& Knowledge Engineering},
  year     = {2007},
  volume   = {61},
  number   = {1},
  pages    = {39 - 58},
  issn     = {0169-023X},
  note     = {Business Process Management},
  abstract = {Healthcare processes require the cooperation of different organizational units and medical disciplines. In such an environment optimal process support becomes crucial. Though healthcare processes frequently change, and therefore the separation of the flow logic from the application code seems to be promising, workflow technology has not yet been broadly used in healthcare environments. In this paper we elaborate both the potential and the essential limitations of IT support for healthcare processes. We identify different levels of process support in healthcare, and distinguish between organizational processes and the medical treatment process. To recognize the limitations of IT support we adopt a broad socio-technical perspective based on scientific literature and personal experience. Despite of the limitations we identified, undeniably, IT has a huge potential to improve healthcare quality which has not been explored by current IT solutions. In particular, we indicate how advanced process management technology can improve IT support for healthcare processes.},
  doi      = {https://doi.org/10.1016/j.datak.2006.04.007},
  keywords = {Healthcare processes, Medical guidelines, Medical decision support, Workflow management, Adaptive information systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X06000784},
}

@InCollection{Bargmeyer2007,
  author    = {B. Bargmeyer and S. Chance},
  title     = {Chapter 15 - Environmental Data: Edge Issues and the Path Forward},
  booktitle = {Environmental Data Exchange Network for Inland Water},
  publisher = {Elsevier},
  year      = {2007},
  editor    = {Palle Haastrup and Jørgen Würtz},
  pages     = {245 - 257},
  address   = {Amsterdam},
  isbn      = {978-0-444-52973-2},
  doi       = {https://doi.org/10.1016/B978-044452973-2/50018-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978044452973250018X},
}

@InCollection{Gudivada2018,
  author    = {Venkat N. Gudivada and Srini Ramaswamy and Seshadri Srinivasan},
  title     = {7 - Data Management Issues in Cyber-Physical Systems},
  booktitle = {Transportation Cyber-Physical Systems},
  publisher = {Elsevier},
  year      = {2018},
  editor    = {Lipika Deka and Mashrur Chowdhury},
  pages     = {173 - 200},
  isbn      = {978-0-12-814295-0},
  abstract  = {The overarching goal of this chapter is to describe data management and security-related issues in cyber-physical systems (CPS). First, we discuss CPS as a synergistic confluence several domains. Next, we analyse their data management and security needs. The recent emergence of NoSQL systems and numerous choices for data management in CPS are discussed. Elasticsearch, a NoSQL system for unstructured data processing, analytics and management, is described. The chapter concludes by indicating emerging trends and data-related research issues in CPS.},
  doi       = {https://doi.org/10.1016/B978-0-12-814295-0.00007-1},
  keywords  = {Cyber-physical systems, Cybersecurity, Data management, Embedded systems, NoSQL systems},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780128142950000071},
}

@Article{1993d,
  title   = {Author index to volume 1–10 (1985–1993)},
  journal = {Data \& Knowledge Engineering},
  year    = {1993},
  volume  = {11},
  number  = {3},
  pages   = {i - viii},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(93)90030-S},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9390030S},
}

@Article{Melin2018,
  author   = {Patricia Melin and Daniela Sánchez},
  title    = {Multi-objective optimization for modular granular neural networks applied to pattern recognition},
  journal  = {Information Sciences},
  year     = {2018},
  volume   = {460-461},
  pages    = {594 - 610},
  issn     = {0020-0255},
  abstract = {A new method for Modular Neural Network optimization based on a Multi-objective Hierarchical Genetic Algorithm is proposed in this paper. The modular neural network using a granular approach and its optimization using a multi-objective hierarchical genetic algorithm provides better results than when the modular neural network is applied without a granular approach and optimization of parameters. The optimization of different parameters of the modular granular neural network architecture, such as the number of modules (sub-granules), size of the dataset for the training phase, goal error, learning algorithm, number of hidden layers and their respective number of neurons are performed in the proposed method. The fitness functions aim at minimizing the size of the dataset for the training phase and the error using a multi-objective approach. This method can be used in different areas of application, such as human recognition, classification problems or time series prediction. In this case the proposed method is tested with human recognition based on the face and ear biometric measures, where the proposed method aims at finding non-dominated solutions based on the number of data points for training and the recognition error. Benchmark face and ear databases are used to illustrate the advantages of the proposed approach.},
  doi      = {https://doi.org/10.1016/j.ins.2017.09.031},
  keywords = {Modular neural networks, Granular computing, Hierarchical genetic algorithm, Human recognition, Multi-objective optimization, Pattern recognition, Face recognition, Ear recognition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025517309593},
}

@Article{Antonellis1979,
  author   = {V. de Antonellis and F. de Cindio and G.Degli Antoni and G. Mauri},
  title    = {Use of bipartite graphs as a notation for data bases},
  journal  = {Information Systems},
  year     = {1979},
  volume   = {4},
  number   = {2},
  pages    = {137 - 141},
  issn     = {0306-4379},
  abstract = {Studies on computing systems making use of data bases have produced various notations to represent “schemata” of relations between data. These notations use graphs both as a tool to describe features relevant to applications and as an effective method of teaching. However, a critical limit may be the fact that they handle in different ways the concepts relevant to data and those relevant to programs. As a matter of fact, the ways of handling concepts relevant to programs are often unsuitable. For example, note that an answer to easy requests may require fairly complex programs. This paper introduces a uniform notation both for data-base schemata and for a class of application programs. This is accomplished by associating a suitable interpretation with bipartite graphs.},
  doi      = {https://doi.org/10.1016/0306-4379(79)90015-2},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437979900152},
}

@Article{Goutas1991,
  author   = {S Goutas and P Soupos and D Christodoulakis},
  title    = {Formalization of object-oriented database model with rules},
  journal  = {Information and Software Technology},
  year     = {1991},
  volume   = {33},
  number   = {10},
  pages    = {741 - 757},
  issn     = {0950-5849},
  abstract = {The paper investigates knowledge representation in an object-oriented database management system first within the data model with rules and second in the computational model by using logic. Issues of structure, integrity, and retrieval are focused on. The proposed system provides object-oriented concepts for describing complex structured data, rules for expressing object-dependent constraints and object associations, and, finally, logic for inference and retrieval.},
  doi      = {https://doi.org/10.1016/0950-5849(91)90048-G},
  keywords = {databases, database management, object-oriented databases, data models, knowledge representation, production rules, query languages, logic programming},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499190048G},
}

@Article{Eastman1994,
  author   = {Charles M. Eastman and Nirva Fereshetian},
  title    = {Information models for use in product design: a comparison},
  journal  = {Computer-Aided Design},
  year     = {1994},
  volume   = {26},
  number   = {7},
  pages    = {551 - 572},
  issn     = {0010-4485},
  abstract = {The paper reviews the reasons for the growing interest in product modelling and the application of data-modelling concepts to cad/cam development. The information needs for the design of products, as distinct from those for manufacturing, business and other applications, are articulated. Given these needs, five information models used in product modelling are reviewed: the entity-relationship model, niam, idef1x, express and EDM.},
  doi      = {https://doi.org/10.1016/0010-4485(94)90087-6},
  keywords = {databases, data models, standards},
  url      = {http://www.sciencedirect.com/science/article/pii/0010448594900876},
}

@Article{Theodoulidis1991,
  author   = {Charalampos I Theodoulidis and Pericles Loucopoulos},
  title    = {The time dimension in conceptual modelling},
  journal  = {Information Systems},
  year     = {1991},
  volume   = {16},
  number   = {3},
  pages    = {273 - 300},
  issn     = {0306-4379},
  abstract = {In recent years there has been a growing interest in the explicit introduction of time modelling in a conceptual schema. This has come about as a result of a realization that the development of large information systems is becoming increasingly more difficult as user requirements become broader and more sophisticated. Arguably the most critical activity in the development of a large data-intensive information system is that of requirements capture and specification. The effectiveness of such a specification depends largely on the ability of the chosen conceptual model to represent the problem domain in such a way so as to permit natural and rigorous descriptions within a methodological framework. The explicit representation of time in a conceptual model plays a major role in achieving this effectiveness. This paper examines the ontology and properties of time in the context of information systems and conceptual modelling. In particular, a critical set of ontological assumptions about time are examined which in turn give rise to the way different paradigms deal with time modelling. The paper describes and critically evaluates nine contemporary approaches to the specification and use of time in information systems.},
  doi      = {https://doi.org/10.1016/0306-4379(91)90002-Q},
  keywords = {Time modelling, conceptual modelling, requirements engineering, temporal aspects of information systems},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799190002Q},
}

@Article{Brown1995,
  author  = {Carol E. Brown and Anthony Wensley},
  title   = {Guest editors' introduction expert systems—one set of views of the state of the art},
  journal = {Expert Systems with Applications},
  year    = {1995},
  volume  = {9},
  number  = {4},
  pages   = {433 - 439},
  issn    = {0957-4174},
  note    = {Expert systems in accounting, auditing, and finance},
  doi     = {https://doi.org/10.1016/0957-4174(95)00014-3},
  url     = {http://www.sciencedirect.com/science/article/pii/0957417495000143},
}

@InCollection{Reiter1992,
  author    = {Raymond Reiter},
  title     = {The Projection Problem in the Situation Calculus: A Soundness and Completeness Result, with an Application to Database Updates},
  booktitle = {Artificial Intelligence Planning Systems},
  publisher = {Morgan Kaufmann},
  year      = {1992},
  editor    = {James Hendler},
  pages     = {198 - 203},
  address   = {San Francisco (CA)},
  isbn      = {978-0-08-049944-4},
  abstract  = {We describe a novel application of planning in the situation calculus to formalize the evolution of a database under update transactions. In the resulting theory, query evaluation becomes identical to the temporal projection problem. We next define a class of axioms for which the classical AI planning technique of goal regression provides a sound and complete method for solving the projection problem, hence for querying evolving databases. Finally, we briefly discuss several issues which naturally arise in the settings of databases and planning, namely, proofs by mathematical induction of properties of world states, logic programming implementations of the projection problem, and historical queries.},
  doi       = {https://doi.org/10.1016/B978-0-08-049944-4.50028-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080499444500288},
}

@Article{Riet2004,
  author  = {R.P. van de Riet},
  title   = {Editorial Introduction by the Editor for Europe: Reind van de Riet},
  journal = {Data \& Knowledge Engineering},
  year    = {2004},
  volume  = {50},
  number  = {3},
  pages   = {247 - 260},
  issn    = {0169-023X},
  note    = {Special jubilee issue: DKE 50},
  doi     = {https://doi.org/10.1016/j.datak.2004.03.001},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X04000278},
}

@Article{Alhajj1998,
  author   = {Reda Alhajj and Faruk Polat},
  title    = {Proper handling of query results towards maximizing reusability in object-oriented databases},
  journal  = {Information Sciences},
  year     = {1998},
  volume   = {107},
  number   = {1},
  pages    = {247 - 272},
  issn     = {0020-0255},
  abstract = {Reusability is one of the vital and distinguishing features that lead to the wide acceptance of the object-oriented technology. In this paper, we present a method for placing a given class in its intended position within the hierarchy to maximize reusability. We group classes to be added to the hierarchy into four categories; the classes whose both subclasses and superclasses are known, only subclasses are known, only superclasses are known, or neither subclasses nor superclasses are known. We concentrate on query results to have the object model maintain closure property because reusability is achieved only when the result of a query is properly placed in the hierarchy in an automated way. To enforce closure, we differentiate between operands Object Algebra Expressions (OAEs) hierarchy and class hierarchy and define the mapping from the class hierarchy to the OAEs hierarchy and vice versa. The first mapping shows how a class is used as an operand and the second mapping shows how to reflect a query result into the class hierarchy. We classify classes as base classes and brother classes. Each brother class shares the definition of a base class because a brother class holds the result of a selection. We introduce two algorithms that help in reusability maximization. The first algorithm adjusts the list of superclasses of a given class to increase the inherited facilities and hence decrease locally defined facilities. The second algorithm maximizes reusability by considering the subclasses of each of the superclasses of a given class to have the latter class properly placed in the hierarchy.},
  doi      = {https://doi.org/10.1016/S0020-0255(97)10051-2},
  keywords = {Base class, Brother class, Closure, Class hierarchy, OAEs hierarchy, Reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025597100512},
}

@Article{Miura1992,
  author   = {Takao Miura and Kunihiko Moriya},
  title    = {On the completeness of visual operations for a semantic data model},
  journal  = {Data \& Knowledge Engineering},
  year     = {1992},
  volume   = {9},
  number   = {1},
  pages    = {19 - 44},
  issn     = {0169-023X},
  abstract = {In this paper we propose a data processing model called PIM towards management of personal information. This model is essentially based on a semantic data model AIS, and we define two sets of data manipulation languages, one is for visual operations, another for logic-based operations. Visual languages allow users to construct database queries utilizing iconic pictures over database diagrams while logic-based languages provide declarative semantics. The main result of this paper is that these two languages have the equivalent expressive power, that is to say, our visual language is complete. This means that the proposed visual language can be characterized by a logical framework.},
  doi      = {https://doi.org/10.1016/0169-023X(92)90016-5},
  keywords = {Databases, data models, data manipulation languages, visual languages, expressive power},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X92900165},
}

@Article{Kryza2007,
  author   = {Bartosz Kryza and Renata Słota and Marta Majewska and Jan Pieczykolan and Jacek Kitowski},
  title    = {Grid organizational memory—provision of a high-level Grid abstraction layer supported by ontology alignment},
  journal  = {Future Generation Computer Systems},
  year     = {2007},
  volume   = {23},
  number   = {3},
  pages    = {348 - 358},
  issn     = {0167-739X},
  abstract = {In this paper the problem of managing ontological descriptions for a dynamically-changing Grid environment is addressed. The focus of the research is on unification of semantic descriptions for Grid services and resources through ontologies. The Grid Organizational Memory (GOM) has been designed and implemented to enable storing and accessing Grid metadata, kept in the form of ontologies. In GOM, ontology storage and management are designed to support the natural evolution process both in the knowledge structure and knowledge management services. An important element of the GOM framework is the ontology separation schema, specifying the internal vertical and horizontal dependencies according to the Virtual Organization thematic domains, efficiency of knowledge retrieval and ontology development support. The separation schema is applied in the knowledge base distribution model on the Grid. An ontology alignment-based approach is proposed to minimize user commitment on the Grid ontological metadata, to support ontology usage and development. In particular the ontology similarity based approach is presented as support for ontology updates, e.g. extension with new facts from external ontologies and environments, as well as a more efficient and complete, less implementation-bounded querying process. This paper presents research on semantic description of the Grid environment within the scope of the K-Wf Grid project that addresses knowledge-based semiautomatic workflow construction for applications on the Grid.},
  doi      = {https://doi.org/10.1016/j.future.2006.07.001},
  keywords = {Semantic grid, Metadata management, Ontology similarity, Ontology store},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X06001385},
}

@Article{Gorla1999,
  author   = {N. Gorla and C.-T. Liu},
  title    = {FHIN: an efficient storage structure and access method for object-oriented databases},
  journal  = {Information and Software Technology},
  year     = {1999},
  volume   = {41},
  number   = {10},
  pages    = {673 - 688},
  issn     = {0950-5849},
  abstract = {While relational database technology has dominated the database field for more than a decade, object-oriented database (OODB) technology has recently gained a lot of attention in the database community. Many researchers are concerned about the performance of OODBs. This paper proposes an OODB design methodology called fragmented hash-indexed (FHIN) that is aimed at improving the operating performance of OODBs. The FHIN model's storage structure contains an Instances–Classes Table (ICT) with a two-segment data design. Query processing is done by accessing data segments through ICT with an algorithm introduced here. The FHIN model uses three access methods: hashing, indexing, or hash-indexing. The database performance of FHIN is compared to two previous access methods using 1050 simulation runs. Results indicate that the FHIN model is 43% better than either of the other models in smaller databases, 65% better in larger databases, 50% better under conditions of high updating, and 72% better under conditions of low updating. These results suggest that FHIN methodology has promise and is worthy of exploration and OODB software development.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00028-2},
  keywords = {Database, Object-oriented, Data structure, Database performance, Database management systems, Access methods},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000282},
}

@InCollection{Agrawal2004,
  author    = {Rakesh Agrawal and Roberto Bayardo and Christos Faloutsos and Jerry Kiernan and Ralf Rantzau and Ramakrishnan Srikant},
  title     = {- Auditing Compliance with a Hippocratic Database},
  booktitle = {Proceedings 2004 VLDB Conference},
  publisher = {Morgan Kaufmann},
  year      = {2004},
  editor    = {Mario A. Nascimento and M. Tamer Özsu and Donald Kossmann and Renée J. Miller and José A. Blakeley and Berni Schiefer},
  pages     = {516 - 527},
  address   = {St Louis},
  isbn      = {978-0-12-088469-8},
  abstract  = {Publisher Summary
This chapter introduces an auditing framework for determining whether a database system is adhering to its data disclosure policies. Users formulate audit expressions to specify the data subjected to disclosure review. An audit component accepts audit expressions and returns all the queries that accessed the specified data during their execution. The overhead of the approach on query processing is small, involving primarily the logging of each query string along with other minor annotations. Database triggers are used to capture updates in a backlog database. At the time of audit, a static analysis phase selects a subset of logged queries for further analysis. These queries are combined and transformed into an SQL audit query, which when run against the backlog database, identifies the suspicious queries efficiently and precisely. The chapter describes the algorithms and data structures used in a DB2-based implementation of this framework. Investigational results reinforce the design choices and emphasize the practicality of the approach.},
  doi       = {https://doi.org/10.1016/B978-012088469-8.50047-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780120884698500474},
}

@Article{Kim1993,
  author   = {Won Kim and Yongmoo Suh and Andrew B Whinston},
  title    = {An IBIS and object-oriented approach to scientific research data management},
  journal  = {Journal of Systems and Software},
  year     = {1993},
  volume   = {23},
  number   = {2},
  pages    = {183 - 197},
  issn     = {0164-1212},
  note     = {Object-Oriented Software},
  abstract = {One of the major challenges in information systems technology is to build a system of scientific data bases and associated tools to assist collaborative research among scientists worldwide across a broad range of scientific disciplines. Two of the many important issues that need to be addressed in meeting this challenge are the selection of a collaboration paradigm for the scientists and a technology for the management of a large-volume scientific data base. We propose the use of an issue-based collaborative exploration paradigm and an object-oriented data base technology for managing a large scientific data base.},
  doi      = {https://doi.org/10.1016/0164-1212(93)90083-A},
  url      = {http://www.sciencedirect.com/science/article/pii/016412129390083A},
}

@Article{1993e,
  title   = {Subject index to volume 9 (1992/93)},
  journal = {Data \& Knowledge Engineering},
  year    = {1993},
  volume  = {9},
  number  = {3},
  pages   = {355},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/0169-023X(93)90014-G},
  url     = {http://www.sciencedirect.com/science/article/pii/0169023X9390014G},
}

@Article{Kirchberg2007,
  author   = {Markus Kirchberg and Klaus-Dieter Schewe and Alexei Tretiakov and Baide (Richard) Wang},
  title    = {A multi-level architecture for distributed object bases},
  journal  = {Data \& Knowledge Engineering},
  year     = {2007},
  volume   = {60},
  number   = {1},
  pages    = {150 - 184},
  issn     = {0169-023X},
  note     = {Intelligent Data Mining},
  abstract = {The work described in this article arises from two needs. First, there is still a need for providing more sophisticated database systems than just relational ones. Secondly, there is a growing need for distributed databases. These needs are addressed by fragmenting schemata of a generic object data model and providing an architecture for its implementation. Key features of the architecture are the use of abstract communicating agents to realise database transactions and queries, the use of a remote object call mechanism to enable remote agents to communicate with one another, and the use of multi-level transactions. Linguistic reflection is used to map database schemata to the level of the agents. Transparency for the users is achieved by using dialogue objects, which are extended views on the database.},
  doi      = {https://doi.org/10.1016/j.datak.2005.11.007},
  keywords = {Distributed database architecture, Object-oriented database system, Integration of programming and query language constructs, Multi-level transaction management, Persistent object store},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05001801},
}

@InCollection{Berkel1988,
  author    = {T. Berkel and P. Klahold and G. Schlageter and W. Wilkes},
  title     = {Modelling CAD-Objects by Abstraction},
  booktitle = {Proceedings of the Third International Conference on Data and Knowledge Bases},
  publisher = {Morgan Kaufmann},
  year      = {1988},
  editor    = {C. BEERI and J.W. SCHMIDT and U. DAYAL},
  pages     = {227 - 240},
  isbn      = {978-1-4832-1313-2},
  abstract  = {In the field of technical applications (CAD/CAM) the representation and administration of complex objects and versions is of great importance. This paper presents a universal model for these aspects, and outlines a corresponding language. The model is based on an abstraction mechanism which relates objects on different levels of abstraction. Each object represents a certain view of a real-world object, tailored to the demands of the user (person or program) of the database. The mechanism of abstraction allows to create new objects by specializing or generalizing existing objects. Furthermore, the abstraction relationship can be used to model the concept of versions: The common properties of versions are expressed by a more abstract object. By weakening the separation of types and objects this model allows to represent individually structured objects (in particular CAD-objects) as well as homogeneously structured data (as handeled by today's database systems) in an integrated way.},
  doi       = {https://doi.org/10.1016/B978-1-4832-1313-2.50024-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978148321313250024X},
}

@Article{Casati1999,
  author   = {Fabio Casati and Mariagrazia Fugini and Isabelle Mirbel},
  title    = {An environment for designing exceptions in workflows},
  journal  = {Information Systems},
  year     = {1999},
  volume   = {24},
  number   = {3},
  pages    = {255 - 273},
  issn     = {0306-4379},
  note     = {10th International Conference on Advanced Information Systems Engineering},
  abstract = {When designing a workflow schema, the workflow designer must often explicitly deal with exceptional situations, such as abnormal process termination or suspension of task execution. This paper shows how the designer can be supported by tools allowing him to capture exceptional behavior within a workflow schema, by reusing an available set of pre-configured exceptions skeletons. Exceptions are expressed by means of triggers, to be executed on the top of an active database environment. In particular, the paper deals with the handling of typical workflow exceptional situations which are modeled as generic exception skeletons to be included in a new workflow schema by simply specializing or instantiating them. Such skeletons, called patterns, are stored in a catalog; the paper describes the catalog structure and its management tools constituting an integrated environment for pattern-based exception design and reuse.},
  doi      = {https://doi.org/10.1016/S0306-4379(99)00018-6},
  keywords = {Workflows, Exceptions, Design Patterns},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437999000186},
}

@Article{Antcheva2009,
  author   = {I. Antcheva and M. Ballintijn and B. Bellenot and M. Biskup and R. Brun and N. Buncic and Ph. Canal and D. Casadei and O. Couet and V. Fine and L. Franco and G. Ganis and A. Gheata and D. Gonzalez Maline and M. Goto and J. Iwaszkiewicz and A. Kreshuk and D. Marcos Segura and R. Maunder and L. Moneta and A. Naumann and E. Offermann and V. Onuchin and S. Panacek and F. Rademakers and P. Russo and M. Tadel},
  title    = {ROOT — A C++ framework for petabyte data storage, statistical analysis and visualization},
  journal  = {Computer Physics Communications},
  year     = {2009},
  volume   = {180},
  number   = {12},
  pages    = {2499 - 2512},
  issn     = {0010-4655},
  note     = {40 YEARS OF CPC: A celebratory issue focused on quality software for high performance, grid and novel computing architectures},
  abstract = {ROOT is an object-oriented C++ framework conceived in the high-energy physics (HEP) community, designed for storing and analyzing petabytes of data in an efficient way. Any instance of a C++ class can be stored into a ROOT file in a machine-independent compressed binary format. In ROOT the TTree object container is optimized for statistical data analysis over very large data sets by using vertical data storage techniques. These containers can span a large number of files on local disks, the web, or a number of different shared file systems. In order to analyze this data, the user can chose out of a wide set of mathematical and statistical functions, including linear algebra classes, numerical algorithms such as integration and minimization, and various methods for performing regression analysis (fitting). In particular, the RooFit package allows the user to perform complex data modeling and fitting while the RooStats library provides abstractions and implementations for advanced statistical tools. Multivariate classification methods based on machine learning techniques are available via the TMVA package. A central piece in these analysis tools are the histogram classes which provide binning of one- and multi-dimensional data. Results can be saved in high-quality graphical formats like Postscript and PDF or in bitmap formats like JPG or GIF. The result can also be stored into ROOT macros that allow a full recreation and rework of the graphics. Users typically create their analysis macros step by step, making use of the interactive C++ interpreter CINT, while running over small data samples. Once the development is finished, they can run these macros at full compiled speed over large data sets, using on-the-fly compilation, or by creating a stand-alone batch program. Finally, if processing farms are available, the user can reduce the execution time of intrinsically parallel tasks — e.g. data mining in HEP — by using PROOF, which will take care of optimally distributing the work over the available resources in a transparent way.
Program summary
Program title: ROOT Catalogue identifier: AEFA_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEFA_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: LGPL No. of lines in distributed program, including test data, etc.: 3 044 581 No. of bytes in distributed program, including test data, etc.: 36 325 133 Distribution format: tar.gz Programming language: C++ Computer: Intel i386, Intel x86-64, Motorola PPC, Sun Sparc, HP PA-RISC Operating system: GNU/Linux, Windows XP/Vista, Mac OS X, FreeBSD, OpenBSD, Solaris, HP-UX, AIX Has the code been vectorized or parallelized?: Yes RAM:>55 Mbytes Classification: 4, 9, 11.9, 14 Nature of problem: Storage, analysis and visualization of scientific data Solution method: Object store, wide range of analysis algorithms and visualization methods Additional comments: For an up-to-date author list see: http://root.cern.ch/drupal/content/root-development-team and http://root.cern.ch/drupal/content/former-root-developers Running time: Depending on the data size and complexity of analysis algorithms References:[1]http://root.cern.ch.},
  doi      = {https://doi.org/10.1016/j.cpc.2009.08.005},
  keywords = {C++, Object-oriented, Framework, Interpreter, Data storage, Data analysis, Visualization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010465509002550},
}

@Article{Flemming2001,
  author   = {Ulrich Flemming and Zeyno Aygen},
  title    = {A hybrid representation of architectural precedents},
  journal  = {Automation in Construction},
  year     = {2001},
  volume   = {10},
  number   = {6},
  pages    = {687 - 699},
  issn     = {0926-5805},
  note     = {Design Representation},
  abstract = {We present a hybrid representation of architectural precedents that separates precedent instances from the concepts they embody, where the concepts are defined in terms of multiple classification taxonomies. The representation allows us to combine the classical view of concept acquisition with aspects of the probabilistic and exemplar views and to organize the database into the equivalents of “episodic” and “semantic” memory. A first application and test context is provided by the Software Environment to Support Early Building Design (SEED), where precedents are used as prototypes and cases. But the representation is flexible enough to support the use of precedents in other application contexts.},
  doi      = {https://doi.org/10.1016/S0926-5805(00)00086-8},
  keywords = {Architectural design, Precedents, Concept formation, Design databases, Design prototypes, Case-based design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0926580500000868},
}

@InCollection{Bean2010,
  author    = {James Bean},
  title     = {Chapter 13 - The Interface and Change},
  booktitle = {SOA and Web Services Interface Design},
  publisher = {Morgan Kaufmann},
  year      = {2010},
  editor    = {James Bean},
  series    = {The MK/OMG Press},
  pages     = {259 - 277},
  address   = {Boston},
  isbn      = {978-0-12-374891-1},
  abstract  = {Publisher Summary
All services go through change at some point. Some changes are less significant than others and, if properly designed, the service interface can help to mitigate the impact of those changes. More significant change, such as adding a new mandatory element or deleting an element from the service interface, may require deeper refactoring of the service and its consumers. The service interface plays a significant role in not only allowing for change but also anticipating change. For Web services, the XML schema definitions for the service interface allow a “wildcard” capability that can be exploited to advantage. Versioning does not resolve all change management complexities. It describes the “change state” of a service. There are several different approaches to version identification, each with advantages and disadvantages. The version should in some manner be defined by the external file name for the schema artifact or by the internal element content of the message. One important design consideration is to distinguish between significant and less significant types of change. Significant changes to a service and its interface should be reflected by a “major” version. Less significant changes should be identified by a “minor” version. The enterprise service bus can implement forms of message transformation between versions. Web services management can apply indirection policies to an endpoint and then redirect messages to the appropriate service versions. All change and all versions that reflect change must be implemented in a consistent manner with minimal impact to consumers, must be optimized to avoid proliferation of one-off services, and must be tested to ensure the framework is operational and effective.},
  doi       = {https://doi.org/10.1016/B978-0-12-374891-1.00013-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123748911000137},
}

@Article{Mantakas2010,
  author   = {Aristidis Mantakas and Tom Farrell},
  title    = {The influence of increasing BMI in nulliparous women on pregnancy outcome},
  journal  = {European Journal of Obstetrics \& Gynecology and Reproductive Biology},
  year     = {2010},
  volume   = {153},
  number   = {1},
  pages    = {43 - 46},
  issn     = {0301-2115},
  abstract = {Objective
The aim of the study was to demonstrate the influence of BMI in pregnancy on rates of adverse pregnancy outcome in overweight nulliparous women.
Study design
The study was a retrospective review of data from the local hospital database held at the Jessop Wing of the Royal Hallamshire Hospital in Sheffield. We reviewed all nulliparous women with recorded BMI at booking between January 2001 and November 2008 who delivered singleton babies. All the women were stratified into five groups (underweight, normal, overweight, obese, and morbidly obese). The different BMI range groups were compared with the group of women with a normal BMI (20–25). SPSS v15 was used for statistical analysis.
Results
The caesarean section rate rose from 18.2% in women of normal BMI to 40.6% in the morbidly obese women (RR 2.2 – CI 1.7–2.8). Morbidly obese women had three times that risk of macrosomia compared with normal BMI women (RR 3.1 – CI 2.1–4.8). The stillbirth rate was associated with increasing obesity with RR 16.7 (CI 4.9–56) for the morbidly obese women.
Conclusions
Increasing degrees of obesity are associated with increases in the incidence of caesarean section, fetal birth weight and adverse pregnancy outcomes. The increased risk shows an increment in a stepwise fashion among the different BMI groups.},
  doi      = {https://doi.org/10.1016/j.ejogrb.2010.06.021},
  keywords = {Maternal obesity, Raised maternal Body Mass Index, Stillbirth, Caesarean section rate, Macrosomia},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301211510003106},
}

@Article{Hu2003,
  author   = {Jinmin Hu and Paul Grefen},
  title    = {Conceptual framework and architecture for service mediating workflow management},
  journal  = {Information and Software Technology},
  year     = {2003},
  volume   = {45},
  number   = {13},
  pages    = {929 - 939},
  issn     = {0950-5849},
  abstract = {This paper proposes a three-layer workflow concept framework to realize workflow enactment flexibility by dynamically binding activities to their implementations at run time. A service mediating layer is added to bridge business process definition and its implementation. Based on this framework, we propose an extended workflow management systems architecture, in which service contracting layer, service binding layer and service invocation layer are extensions to support the proposed service mediating concept. According to an enactment specification, different instances of the same activity can be bound to different services to achieve enactment flexibility. The conceptual framework and architecture together provide a comprehensive approach to flexible service enactment in B2B collaborative settings.},
  doi      = {https://doi.org/10.1016/S0950-5849(03)00092-2},
  keywords = {Workflow management, Service mediation, Enactment flexibility, Dynamic binding, Business-to-Business collaboration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584903000922},
}

@Article{Ma2008,
  author   = {Qing-Hu Ma and Yang Xu},
  title    = {Characterization of a caffeic acid 3-O-methyltransferase from wheat and its function in lignin biosynthesis},
  journal  = {Biochimie},
  year     = {2008},
  volume   = {90},
  number   = {3},
  pages    = {515 - 524},
  issn     = {0300-9084},
  abstract = {Caffeic acid 3-O-methyltransferase (COMT) catalyzes the multi-step methylation reactions of hydroxylated monomeric lignin precursors, and is believed to occupy a pivotal position in the lignin biosynthetic pathway. A cDNA (TaCM) was identified from wheat and it was found to be expressed constitutively in stem, leaf and root tissues. The deduced amino acid sequence of TaCM showed a high degree of identity with COMT from other plants, particularly in SAM binding motif and the residues responsible for catalytic and substrate specificity. The predicted TaCM three-dimensional structure is very similar with a COMT from alfalfa (MsCOMT), and TaCM protein had high immunoreactive activity with MsCOMT antibody. Kinetic analysis indicated that the recombinant TaCM protein exhibited the highest catalyzing efficiency towards caffeoyl aldehyde and 5-hydroxyconiferaldehyde as substrates, suggesting a pathway leads to S lignin via aldehyde precursors. Authority of TaCM encoding a COMT was confirmed by the expression of antisense TaCM gene in transgenic tobacco which specifically down-regulated the COMT enzyme activity. Lignin analysis showed that the reduction in COMT activity resulted in a marginal decrease in lignin content but sharp reduction in the syringl lignin. Furthermore, the TaCM protein exhibited a strong activity towards ester precursors including caffeoyl-CoA and 5-hydroxyferuloyl-CoA. Our results demonstrate that TaCM is a typical COMT involved in lignin biosynthesis. It also supports the notion, in agreement with a structural analysis, that COMT has a broad substrate preference.},
  doi      = {https://doi.org/10.1016/j.biochi.2007.09.016},
  keywords = { L., Caffeic acid 3--methyltransferase, 5-Hydroxyferulic acid -methyltransferase, Caffeoyl CoA 3-methyltranferase, Lignin biosynthesis},
  url      = {http://www.sciencedirect.com/science/article/pii/S030090840700260X},
}

@Article{Leonardi2007,
  author   = {Erwin Leonardi and Tran T. Hoai and Sourav S. Bhowmick and Sanjay Madria},
  title    = {DTD-Diff: A change detection algorithm for DTDs},
  journal  = {Data \& Knowledge Engineering},
  year     = {2007},
  volume   = {61},
  number   = {2},
  pages    = {384 - 402},
  issn     = {0169-023X},
  abstract = {The DTD of a set of XML documents may change due to many reasons such as changes to the real-world events, changes to the user’s requirements, and mistakes in the initial design. In this paper, we present a novel algorithm called DTD-Diff to detect the changes to DTDs that defines the structure of a set of XML documents. Such change detection tool can be useful in several ways such as maintenance of XML documents, incremental maintenance of relational schema for storing XML data, and XML schema integration. We compare DTD-Diff with existing XML change detection approaches and show that converting DTD to XML schema (XSD) (which is in XML document format) and detecting the changes using existing XML change detection algorithms is not a feasible option. Our experimental results show that DTD-Diff is 5–325 times faster than X-Diff when it detects the changes to the XSD files. Compared to XyDiff, DTD-Diff is up to 38 times faster. We also study the result quality of detected deltas.},
  doi      = {https://doi.org/10.1016/j.datak.2006.06.003},
  keywords = {Change detection, DTD, XML, Algorithm, Performance},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X0600098X},
}

@Article{Proper1995,
  author   = {H.A. Proper and Th.P. van der Weide},
  title    = {Information disclosure in evolving information systems: taking a shot at a moving target},
  journal  = {Data \& Knowledge Engineering},
  year     = {1995},
  volume   = {15},
  number   = {2},
  pages    = {135 - 168},
  issn     = {0169-023X},
  abstract = {In this paper, we introduce a query language for evolving information systems. Evolving information systems go beyond the capacity of conventional database systems, not only as they incorporate a time dimension, but also since they allow all aspects of the system to evolve. The introduced language is related to the philosophy underlying NIAM (Natural language Information Analysis Method). This method investigates the grammar of the communication in the Universe of Discourse. Usually this grammar is depicted as an information structure diagram (NIAM or ER schema). This paper describes the language Elisa-D, which is based on this grammar. As a result, expressions in this language have a direct meaning in the universe of discourse, while natural language expressions are easily formalised in this language.},
  doi      = {https://doi.org/10.1016/0169-023X(94)00035-D},
  keywords = {Evolving information system, Conceptual query language EVORM ER, PSM, Elisa-D},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9400035D},
}

@Article{Nagai2004,
  author   = {Makoto Nagai and Michiko Hayashi and Shigeo Sugita and Yoshihiro Sakoda and Masashi Mori and Toshiaki Murakami and Tadashi Ozawa and Naoki Yamada and Hiroomi Akashi},
  title    = {Phylogenetic analysis of bovine viral diarrhea viruses using five different genetic regions},
  journal  = {Virus Research},
  year     = {2004},
  volume   = {99},
  number   = {2},
  pages    = {103 - 113},
  issn     = {0168-1702},
  abstract = {Phylogenetic analysis of the five different regions (5′ non-coding region (5′NCR), Npro, E2, NS3 and NS5B–3′NCR) of 48 Japanese and reported bovine viral diarrhea virus (BVDV) genomes was performed. Japanese BVDVs were segregated into BVDV1 subdivided into six subgroups and BVDV2. One isolate, So CP/75, isolated in 1975 and previously proposed as subgroup 1e according to its 5′NCR sequence, was quite unique and formed an independent lineage in the tree of any region. Another isolate, 190CP, obtained from an experimental mucosal disease case was classified as subgroup 1e, defined by Becher et al. in the 5′NCR, Npro and E2 regions, whereas it was classified as subgroup 1a in the NS5B–3′NCR region. The genomic sequences of the American isolates ILLC and ILLNC obtained from the GenBank database were assigned into subgroup 1b in the 5′NCR, Npro, E2 and NS5B–3′NCR regions, whereas they were assigned into subgroup 1a in the NS3 region, suggesting that recombination between the virus strains classified into different subgroups had occurred in an animal. These findings suggest that phylogenetic analysis of several genetic regions is useful for the further characterization of field BVDV isolates.},
  doi      = {https://doi.org/10.1016/j.virusres.2003.10.006},
  keywords = {Bovine viral diarrhea virus, Phylogenetic tree, Sequence diversity, Recombinant genome},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168170203003174},
}

@Article{Kim1997,
  author   = {Duk Hyun Kim and Sung Joo Park},
  title    = {FORM: A flexible data model for integrated CASE environments},
  journal  = {Data \& Knowledge Engineering},
  year     = {1997},
  volume   = {22},
  number   = {2},
  pages    = {133 - 158},
  issn     = {0169-023X},
  abstract = {An Integrated Computer-Aided Software Engineering Environment (ICASE) is a software system that can support the development, maintenance, and use of enterprise-wide information resources. An ICASE requires a data model that can represent heterogeneous types of entities and relationships evolving in time and context. This paper describes a data model called FORM (Flexible Object-Relationship Model) for the object management system of ICASEs. FORM is an extended object-oriented data model adopting concepts in semantic data modeling and meta-modeling. Relationship types as well as entity types, both of which can be defined as objects in an object schema help integrate heterogeneous concepts; and meta-types that hold adaptable semantics of object manipulation help raise the automation level of ICASEs. An object-modeling approach for version control is illustrated to show the effectiveness of the framework.},
  doi      = {https://doi.org/10.1016/S0169-023X(96)00042-0},
  keywords = {Integrated CASE, Meta-modeling, Object-oriented data model, Primitive relationship, Semantic data model, Version control},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X96000420},
}

@Article{Do2014,
  author   = {Thi Huyen Do and Thi Thao Nguyen and Thanh Ngoc Nguyen and Quynh Giang Le and Cuong Nguyen and Keitarou Kimura and Nam Hai Truong},
  title    = {Mining biomass-degrading genes through Illumina-based de novo sequencing and metagenomic analysis of free-living bacteria in the gut of the lower termite Coptotermes gestroi harvested in Vietnam},
  journal  = {Journal of Bioscience and Bioengineering},
  year     = {2014},
  volume   = {118},
  number   = {6},
  pages    = {665 - 671},
  issn     = {1389-1723},
  abstract = {The 5.6 Gb metagenome of free-living microbial flora in the gut of the lower termite Coptotermes gestroi, harvested in Vietnam, was sequenced using Illumina technology. Genes related to biomass degradation were mined for a better understanding of biomass digestion in the termite gut and to identify lignocellulolytic enzymes applicable to biofuel production. The sequencing generated 5.4 Gb of useful reads, containing 125,431 ORFs spanning 78,271,365 bp, 80% of which was derived from bacteria. The 12 most abundant bacterial orders were Spirochaetales, Lactobacillales, Bacteroidales, Clostridiales, Enterobacteriales, Pseudomonades, Synergistales, Desulfovibrionales, Xanthomonadales, Burkholderiales, Bacillales, and Actinomycetales, and 1460 species were estimated. Of more than 12,000 ORFs with predicted functions related to carbohydrate metabolism, 587 encoding hydrolytic enzymes for cellulose, hemicellulose, and pectin were identified. Among them, 316 ORFs were related to cellulose degradation, and included β-glucosidases, 6-phospho-β-glucosidases, licheninases, glucan endo-1,3-β-d-glucosidases, endoglucanases, cellulose 1,4-β-cellobiosidases, glucan 1,3-β-glucosidases, and cellobiose phosphorylases. In addition, 259 ORFs were related to hemicellulose degradation, encoding endo-1,4-β-xylanases, α-galactosidases, α-N-arabinofuranosidases, xylan 1,4-β-xylosidases, arabinan endo-1,5-α-l-arabinosidases, endo-1,4-β-mannanases, and α-glucuronidases. Twelve ORFs encoding pectinesterases and pectate lyases were also obtained. To our knowledge, this is the first successful application of Illumina-based de novo sequencing for the analysis of a free-living bacterial community in the gut of a lower termite C. gestroi and for mining genes related to lignocellulose degradation from the gut bacteria.},
  doi      = {https://doi.org/10.1016/j.jbiosc.2014.05.010},
  keywords = {Free-living gut bacterial community, , Cellulase, Hemicellulase, Illumina sequencing, Pectinesterase, Metagenome},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389172314001807},
}

@Article{Yu2008,
  author   = {Lean Yu and Wei Huang and Shouyang Wang and Kin Keung Lai},
  title    = {Web warehouse – a new web information fusion tool for web mining},
  journal  = {Information Fusion},
  year     = {2008},
  volume   = {9},
  number   = {4},
  pages    = {501 - 511},
  issn     = {1566-2535},
  note     = {Special Issue on Web Information Fusion},
  abstract = {In this study, we introduce a web information fusion tool – web warehouse, which is suitable for web mining and knowledge discovery. To formulate a web warehouse, a four-layer web warehouse architecture for decision support is firstly proposed. According to the layered web warehouse framework architecture, an extraction–fusion–mapping–loading (EFML) process model for web warehouse construction is then constructed. In the web warehouse process model, a series of web services including wrapper service, mediation service, ontology service and mapping service are used. Particularly, two kinds of mediators are introduced to fuse the heterogeneous web information. Finally, a simple case study is presented to illustrate the construction process of web warehouse.},
  doi      = {https://doi.org/10.1016/j.inffus.2006.10.007},
  keywords = {Web warehouse, Web mining, Extraction–fusion–mapping–loading process, Wrapper service, Mediation service, Mapping service, Semi-structured data},
  url      = {http://www.sciencedirect.com/science/article/pii/S1566253506000807},
}

@Article{Bassiliades1995,
  author   = {N. Bassiliades and I. Vlahavas},
  title    = {PRACTIC: A concurrent object data model for a parallel object-oriented database system},
  journal  = {Information Sciences},
  year     = {1995},
  volume   = {86},
  number   = {1},
  pages    = {149 - 178},
  issn     = {0020-0255},
  abstract = {A concurrent object data model for a parallel object-oriented database system, named PRACTIC, and its abstract machine are presented. PRACTIC means PaRallel ACTIve Classes and is based on the vertical partitioning and concurrent management of the database schema classes and metaclasses, which are collectively called active objects. Active objects are permanent processes in memory that encapsulate their definitions, methods, and management procedures. Semiactive and passive objects exist to realize abstract classes and instances (the actual data), respectively. The object model gives rise to a query/method execution model that provides parallelism on all levels of the instantiation hierarchy. The abstract PRACTIC machine directly maps the model to a MIMD machine. The performance of one of the proposed parallel query/method execution schemes is measured by simulation on the abstract machine.},
  doi      = {https://doi.org/10.1016/0020-0255(95)00092-4},
  url      = {http://www.sciencedirect.com/science/article/pii/0020025595000924},
}

@Article{Kudrass2002,
  author   = {Thomas Kudrass},
  title    = {Management of XML documents without schema in relational database systems},
  journal  = {Information and Software Technology},
  year     = {2002},
  volume   = {44},
  number   = {4},
  pages    = {269 - 275},
  issn     = {0950-5849},
  abstract = {Many applications deal with highly flexible XML documents from different sources, which makes it difficult to define their structure by a fixed schema or a DTD. Therefore, it is necessary to explore ways to cope with such XML documents. The paper analyzes different storage and retrieval methods for schemaless XML documents using the capabilities of relational systems. We compare our experiences with the implementation of an XML-to-relational mapping with an LOB implementation in a commercial RDBMS. The paper concludes with a vision of how different storage methods could converge towards a common high-level XML-API for databases.},
  doi      = {https://doi.org/10.1016/S0950-5849(02)00017-4},
  keywords = {Storing XML data, Relational database, Large object, XML query},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584902000174},
}

@Article{Garlatti1998,
  author   = {Serge Garlatti and Mike Sharples},
  title    = {The use of a computerized brain atlas to support knowledge-based training in radiology},
  journal  = {Artificial Intelligence in Medicine},
  year     = {1998},
  volume   = {13},
  number   = {3},
  pages    = {181 - 205},
  issn     = {0933-3657},
  abstract = {Trainers of radiologists face the particular challenges of teaching normal and abnormal appearance for a variety of imaging modalities, providing access to a large appropriately-indexed case library, and teaching a consistent approach to the reporting of cases. The computer has the potential to address these issues, to supplement conventional teaching of radiology by providing case-based tutoring and diagnostic support based on a large library of images of normal and abnormal anatomy, described in a consistent terminology. The paper presents a new approach to computer-based training in radiology that combines a knowledge-based tutor with an on-line medical atlas. It describes two existing computer systems, the MR Tutor and ATLAS, and discusses the medical, computational, epistemic, and pedagogic issues involved in developing a combined Atlas–Tutor. Integrating an atlas with a training system could significantly improve the teaching and support offered, but practical difficulties include the need to merge knowledge representations and to incorporate techniques for registering atlas plates on images that exhibit abnormalities. The paper addresses these problems, and concludes by indicating how the Atlas–Tutor might be employed in practical radiology training.},
  doi      = {https://doi.org/10.1016/S0933-3657(98)00030-X},
  keywords = {Knowledge-based training, Computer-based atlas, Knowledge representation, Magnetic resonance imaging, Neuroradiology},
  url      = {http://www.sciencedirect.com/science/article/pii/S093336579800030X},
}

@Article{March2007,
  author   = {Salvatore T. March and Alan R. Hevner},
  title    = {Integrated decision support systems: A data warehousing perspective},
  journal  = {Decision Support Systems},
  year     = {2007},
  volume   = {43},
  number   = {3},
  pages    = {1031 - 1043},
  issn     = {0167-9236},
  note     = {Integrated Decision Support},
  abstract = {Successfully supporting managerial decision-making is critically dependent upon the availability of integrated, high quality information organized and presented in a timely and easily understood manner. Data warehouses have emerged to meet this need. They serve as an integrated repository for internal and external data—intelligence critical to understanding and evaluating the business within its environmental context. With the addition of models, analytic tools, and user interfaces, they have the potential to provide actionable information resources—business intelligence that supports effective problem and opportunity identification, critical decision-making, and strategy formulation, implementation, and evaluation. Four themes frame our analysis: integration, implementation, intelligence, and innovation.},
  doi      = {https://doi.org/10.1016/j.dss.2005.05.029},
  keywords = {Data warehouse, Data warehousing architecture, Integrated decision support, Intelligence, Business intelligence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923605000941},
}

@Article{Englebert1999,
  author   = {Vincent Englebert and Jean-Luc Hainaut},
  title    = {DB-MAIN: A next generation meta-CASE},
  journal  = {Information Systems},
  year     = {1999},
  volume   = {24},
  number   = {2},
  pages    = {99 - 112},
  issn     = {0306-4379},
  note     = {Meta-Modelling and Methodology Engineering},
  abstract = {This paper describes the DB-MAIN meta-CASE architecture that attempts to conciliate seemingly contradictory goals concerning efficiency, ergonomics, evolution, reuse and ontologies integration. The architecture is based on two layers, the first of which includes built-in basic concepts and functions that should be part of every Information System oriented CASE tool. The second layer comprises four meta-mechanisms intended to reuse, extend and specialize the basic constructs: ontology definition (through a dynamic repository), graphical interface definition, functional definition and methodology definition. These mechanisms are activated through four cooperating languages that allows the CASE engineer to customize the DB-MAIN environment from merely adapting the graphical interface to defining new modeling domains. The paper shows how these mechanisms meet a hierarchy of requirements and how they compare with the current state of the art.},
  doi      = {https://doi.org/10.1016/S0306-4379(99)00007-1},
  keywords = {Meta-CASE, CASE Tool, Graphical Visualization, Dynamic Functionalities, Meta-Modelisation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437999000071},
}

@Article{Gallinucci2019,
  author   = {Enrico Gallinucci and Matteo Golfarelli and Stefano Rizzi},
  title    = {Approximate OLAP of document-oriented databases: A variety-aware approach},
  journal  = {Information Systems},
  year     = {2019},
  issn     = {0306-4379},
  abstract = {Schemaless databases, and document-oriented databases in particular, are preferred to relational ones for storing heterogeneous data with variable schemas and structural forms. However, the absence of a unique schema adds complexity to analytical applications, in which a single analysis often involves large sets of data with different schemas. In this paper we propose an original approach to OLAP on collections stored in document-oriented databases. The basic idea is to stop fighting against schema variety and welcome it as an inherent source of information wealth in schemaless sources. Our approach builds on four stages: schema extraction, schema integration, FD enrichment, and querying; these stages are discussed in detail in the paper. To make users aware of the impact of schema variety, we propose a set of indicators inspired by the definition of attribute density. Finally, we experimentally evaluate our approach in terms of efficiency and effectiveness.},
  doi      = {https://doi.org/10.1016/j.is.2019.02.004},
  keywords = {NoSQL, Document-oriented databases, Multidimensional modeling, OLAP},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437918303235},
}

@Article{Leeuwen2006,
  author   = {J.P. van Leeuwen and S. Fridqvist},
  title    = {An information model for collaboration in the construction Industry},
  journal  = {Computers in Industry},
  year     = {2006},
  volume   = {57},
  number   = {8},
  pages    = {809 - 816},
  issn     = {0166-3615},
  note     = {Collaborative Environments for Concurrent Engineering Special Issue},
  abstract = {Collaborative work is an essential ingredient for success in the construction industry. With the advancements of capabilities of information technologies and communication infrastructures, the effective utilisation of these technologies has become very important and strongly affects business processes that have long followed traditional paths. This article describes the main characteristics of the concept-modelling framework that is developed in the DesKs project. Concept modelling gives end-users access to the schema of design models and provides a high level of flexibility for modelling. To support collaborative work, it provides remote data access and allows users to share resources that, instead of being exchanged or stored centrally, remain active at their source in tight relation with business processes. The main technical aspects of the concept-modelling framework are discussed. Object version control and timeline management of revisions of objects are used to increase the integrity between objects that are accessed and edited by multiple users across a network.},
  doi      = {https://doi.org/10.1016/j.compind.2006.04.011},
  keywords = {Collaborative design, Concept modelling, Object version management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361506000881},
}

@Article{Oren2008,
  author   = {Eyal Oren and Benjamin Heitmann and Stefan Decker},
  title    = {ActiveRDF: Embedding Semantic Web data into object-oriented languages},
  journal  = {Journal of Web Semantics},
  year     = {2008},
  volume   = {6},
  number   = {3},
  pages    = {191 - 202},
  issn     = {1570-8268},
  note     = {World Wide Web Conference 2007Semantic Web Track},
  abstract = {Semantic Web applications share a large portion of development effort with database-driven Web applications. Existing approaches for development of these database-driven applications cannot be directly applied to Semantic Web data due to differences in the underlying data model. We develop a mapping approach that embeds Semantic Web data into object-oriented languages and thereby enables reuse of existing Web application frameworks. We analyse the relation between the Semantic Web and the Web, and survey the typical data access patterns in Semantic Web applications. We discuss the mismatch between object-oriented programming languages and Semantic Web data, for example in the semantics of class membership, inheritance relations, and object conformance to schemas. We present ActiveRDF, an object-oriented API for managing RDF data that offers full manipulation and querying of RDF data, does not rely on a schema and fully conforms to RDF(S) semantics. ActiveRDF can be used with different RDF data stores: adapters have been implemented to generic SPARQL endpoints, Sesame, Jena, Redland and YARS and new adapters can be added easily. We demonstrate the usage of ActiveRDF and its integration with the popular Ruby on Rails framework which enables rapid development of Semantic Web applications.},
  doi      = {https://doi.org/10.1016/j.websem.2008.04.003},
  keywords = {Semantic Web, RDF(S), Embedded data access, Object-oriented programming, Scripting languages},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826808000401},
}

@Article{Shah1996,
  author   = {Jami J Shah and Dae K Jeon and Susan D Urban and Plamen Bliznakov and Mary Rogers},
  title    = {Database infrastructure for supporting engineering design histories},
  journal  = {Computer-Aided Design},
  year     = {1996},
  volume   = {28},
  number   = {5},
  pages    = {347 - 360},
  issn     = {0010-4485},
  note     = {Computer-Aided Concurrent Design},
  abstract = {An engineering design history is a step-by-step account of the events and the states through which a design artifact evolved. Database technology has not yet provided adequate mechanisms for capturing and reusing design history information. This paper presents an infrastructure for computer-aided archiving and interrogation of engineering design histories. This framework combines research from software engineering, data engineering, and knowledge engineering to develop an environment for the capture and reuse of design histories. A Design Process Representation Language (DRL), is combined with ‘enhanced’ step models to capture design steps and design parameters. This high-level representation is translated into Data Definition Language (DDL) for operations on an object-oriented database. Advances in database technology that are required to fully support intelligent design history systems are discussed.},
  doi      = {https://doi.org/10.1016/0010-4485(95)00054-2},
  keywords = {distributed databases, product data management, , SDAI, design process, world wide web, heterogeneous database},
  url      = {http://www.sciencedirect.com/science/article/pii/0010448595000542},
}

@Article{Melo1995,
  author   = {Walcélio L Melo and Noureddine Belkhatir and Jacky Estublier},
  title    = {A software engineering environment driven by event-condition-action rules and its trigger mechanism},
  journal  = {Information and Software Technology},
  year     = {1995},
  volume   = {37},
  number   = {10},
  pages    = {580 - 587},
  issn     = {0950-5849},
  abstract = {Recently, PSEEs (Process-Centred Software Engineering Environments) have been investigated as a new architecture of SEEs in which the software processes are explicitly described and drive the user interactions. A typical PSEE is composed of two components: a resource manager and a process manager. The resource manager is responsible for the management and control of all objects manipulated during the software processes. The process manager is the component supporting an explicit formalism to describe software processes. ADELE/TEMPO is a sample of this new tendency. This paper presents the main components of the kernel of the ADELE/ TEMPO system, i.e. its resource manager and process manager. Special attention is given to how these different basic components are integrated into a platform where software process models can be explicitly described by event-condition-action rules (E-C-A) and supported by an active software engineering database.},
  doi      = {https://doi.org/10.1016/0950-5849(95)90934-M},
  keywords = {trigger, active software engineering database, role concept, process modelling, event-condition-action rules, knowledge-based software engineering environments},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499590934M},
}

@InCollection{McComb2003a,
  author    = {Dave McComb},
  title     = {Chapter 8 - Business Rules and Creating Meaning},
  booktitle = {Semantics in Business Systems},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Dave McComb},
  series    = {The Savvy Manager;#x0027;s Guides},
  pages     = {129 - 156},
  address   = {Burlington},
  isbn      = {978-1-55860-917-4},
  abstract  = {Publisher Summary
This chapter discusses the business rules. The business rules movement is a significant step forward in building and maintaining business applications. Business rules and semantics go hand in hand. Business rules are much more powerful when they are built on a solid semantic foundation, and the business rules themselves form a semantic ontology of what it is possible to express in these nonprocedural forms. The key issues covered in this chapter: applications built with current technology are expensive to build and maintain; their complexity makes it difficult to change the system as the environment changes; some of that complexity is in the rules that express the logic of the application, although they are buried in a great deal of code that procedurally determines when those rules can fire; the business rules approach is a major breakthrough in flexibility and expressiveness of application logic; basing business rules on semantic-based ontologies and concentrating on expressing rules at the highest level of those ontologies will yield a great reduction in complexity of the rule base.},
  doi       = {https://doi.org/10.1016/B978-155860917-4/50010-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558609174500106},
}

@Article{Abiteboul1990,
  author   = {Serge Abiteboul},
  title    = {Towards a deductive object-oriented database language},
  journal  = {Data \& Knowledge Engineering},
  year     = {1990},
  volume   = {5},
  number   = {4},
  pages    = {263 - 287},
  issn     = {0169-023X},
  abstract = {A language for databases with sets, tuples, lists, object identity and structural inheritance is proposed. The core language is logic-based with a fixpoint semantics. Methods with overloading and methods evaluated externally providing extensibility of the language are considered. Other important issues such as updates and the introduction of explicit control are discussed.},
  doi      = {https://doi.org/10.1016/0169-023X(90)90016-7},
  keywords = {Databases, Knowledge bases, Object-oriented, Deductive databases},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X90900167},
}

@InCollection{Doan2012,
  author    = {AnHai Doan and Alon Halevy and Zachary Ives},
  title     = {6 - General Schema Manipulation Operators},
  booktitle = {Principles of Data Integration},
  publisher = {Morgan Kaufmann},
  year      = {2012},
  editor    = {AnHai Doan and Alon Halevy and Zachary Ives},
  pages     = {161 - 171},
  address   = {Boston},
  isbn      = {978-0-12-416044-6},
  doi       = {https://doi.org/10.1016/B978-0-12-416044-6.00006-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124160446000065},
}

@InCollection{Teorey2011a,
  author    = {Toby Teorey and Sam Lightstone and Tom Nadeau and H.V. Jagadish},
  title     = {11 - CASE Tools for Logical Database Design},
  booktitle = {Database Modeling and Design (Fifth Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2011},
  editor    = {Toby Teorey and Sam Lightstone and Tom Nadeau and H.V. Jagadish},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {233 - 259},
  address   = {Boston},
  edition   = {Fifth Edition},
  isbn      = {978-0-12-382020-4},
  abstract  = {Publisher Summary
There are several good CASE tools available for computer- assisted database design. Database design is just one part of the analysis and design phase of creating effective business application software, but it is often the part that is the most challenging and the most critical to perform. This chapter focuses on commercially available tools that simplify these design processes. These computer-aided system engineering, or CASE, tools provide functions that assist in system design. CASE tools are widely used in numerous industries and domains, such as circuit design, manufacturing, and architecture. Logical database design is another area where CASE tools have proven effective. This chapter explores the offerings of the major vendors in this space: IBM, Computer Associates, and Sybase. Each of these companies offers feature-rich technology for developing logical database designs and transitioning them into physical databases you can use. The aim of this chapter is to survey these products to give the reader a taste for the capabilities they provide.},
  doi       = {https://doi.org/10.1016/B978-0-12-382020-4.00013-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123820204000136},
}

@Article{Boddy2007,
  author   = {S. Boddy and Y. Rezgui and M. Wetherill and G. Cooper},
  title    = {Knowledge informed decision making in the building lifecycle: An application to the design of a water drainage system},
  journal  = {Automation in Construction},
  year     = {2007},
  volume   = {16},
  number   = {5},
  pages    = {596 - 606},
  issn     = {0926-5805},
  abstract = {Critical decisions which influence the sustainability of a construction project are made in a pressurised, time-critical environment. These decisions must be supported and informed by knowledge resources, with the reasons for these decisions feeding back into the body of knowledge. This paper reports on a research initiative which brings together a knowledge management environment and a decision support tool, bringing sustainability knowledge management into the applications used in architectural design. Validation of the resulting knowledge informed decision making environment emphasised the potential benefits that such environments can bring in assisting decision making by capitalizing on the wealth of existing related knowledge.},
  doi      = {https://doi.org/10.1016/j.autcon.2006.10.001},
  keywords = {Architectural design, Decision making, Design knowledge, Problem solving, Sustainable construction},
  url      = {http://www.sciencedirect.com/science/article/pii/S0926580506001087},
}

@Article{Kong2019,
  author   = {Hyun Gi Kong and Hyun Ho Kim and Joon-hui Chung and JeHoon Jun and Soohyun Lee and Hak-Min Kim and Sungwon Jeon and Seung Gu Park and Jong Bhak and Choong-Min Ryu},
  title    = {The Galleria mellonella Hologenome Supports Microbiota-Independent Metabolism of Long-Chain Hydrocarbon Beeswax},
  journal  = {Cell Reports},
  year     = {2019},
  volume   = {26},
  number   = {9},
  pages    = {2451 - 2464.e5},
  issn     = {2211-1247},
  abstract = {Summary
The greater wax moth, Galleria mellonella, degrades wax and plastic molecules. Despite much interest, the genetic basis of these hallmark traits remains poorly understood. Herein, we assembled high-quality genome and transcriptome data from G. mellonella to investigate long-chain hydrocarbon wax metabolism strategies. Specific carboxylesterase and lipase and fatty-acid-metabolism-related enzymes in the G. mellonella genome are transcriptionally regulated during feeding on beeswax. Strikingly, G. mellonella lacking intestinal microbiota successfully decomposes long-chain fatty acids following wax metabolism, although the intestinal microbiome performs a supplementary role in short-chain fatty acid degradation. Notably, final wax derivatives were detected by gas chromatography even in the absence of gut microbiota. Our findings provide insight into wax moth adaptation and may assist in the development of unique wax-degradation strategies with a similar metabolic approach for a plastic molecule polyethylene biodegradation using organisms without intestinal microbiota.},
  doi      = {https://doi.org/10.1016/j.celrep.2019.02.018},
  keywords = {, polyethylene, wax degradation, genome assembly, transcriptome, intestinal microbiota},
  url      = {http://www.sciencedirect.com/science/article/pii/S2211124719301809},
}

@Article{Zwoelf2016,
  author   = {Carlo Maria Zwölf and Nicolas Moreau and Marie-Lise Dubernet},
  title    = {New model for datasets citation and extraction reproducibility in VAMDC},
  journal  = {Journal of Molecular Spectroscopy},
  year     = {2016},
  volume   = {327},
  pages    = {122 - 137},
  issn     = {0022-2852},
  note     = {New Visions of Spectroscopic Databases, Volume II},
  abstract = {In this paper we present a new paradigm for the identification of datasets extracted from the Virtual Atomic and Molecular Data Centre (VAMDC) e-science infrastructure. Such identification includes information on the origin and version of the datasets, references associated to individual data in the datasets, as well as timestamps linked to the extraction procedure. This paradigm is described through the modifications of the language used to exchange data within the VAMDC and through the services that will implement those modifications. This new paradigm should enforce traceability of datasets, favor reproducibility of datasets extraction, and facilitate the systematic citation of the authors having originally measured and/or calculated the extracted atomic and molecular data.},
  doi      = {https://doi.org/10.1016/j.jms.2016.04.009},
  keywords = {Database, Data citation, Atomic data, Molecular data},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022285216300613},
}

@Article{Chouder2019,
  author   = {Mohamed L. Chouder and Stefano Rizzi and Rachid Chalal},
  title    = {EXODuS: Exploratory OLAP over Document Stores},
  journal  = {Information Systems},
  year     = {2019},
  volume   = {79},
  pages    = {44 - 57},
  issn     = {0306-4379},
  note     = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
  abstract = {OLAP has been extensively used for a couple of decades as a data analysis approach to support decision making on enterprise structured data. Now, with the wide diffusion of NoSQL databases holding semi-structured data, there is a growing need for enabling OLAP on document stores as well, to allow non-expert users to get new insights and make better decisions. Unfortunately, due to their schemaless nature, document stores are hardly accessible via direct OLAP querying. In this paper we propose EXODuS, an interactive, schema-on-read approach to enable OLAP querying of document stores in the context of self-service BI and exploratory OLAP. To discover multidimensional hierarchies in document stores we adopt a data-driven approach based on the mining of approximate functional dependencies; to ensure good performances, we incrementally build local portions of hierarchies for the levels involved in the current user query. Users execute an analysis session by expressing well-formed multidimensional queries related by OLAP operations; these queries are then translated into the native query language of MongoDB, one of the most popular document-based DBMS. An experimental evaluation on real-world datasets shows the efficiency of our approach and its compatibility with a real-time setting.},
  doi      = {https://doi.org/10.1016/j.is.2017.11.004},
  keywords = {Document stores, JSON, Exploratory OLAP, Self-service BI, Multidimensional modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437917304507},
}

@Article{Brun2003,
  author   = {R. Brun and P. Buncic and F. Carminati and A. Morsch and F. Rademakers and K. Safarik},
  title    = {Computing in ALICE},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2003},
  volume   = {502},
  number   = {2},
  pages    = {339 - 346},
  issn     = {0168-9002},
  note     = {Proceedings of the VIII International Workshop on Advanced Computing and Analysis Techniques in Physics Research},
  abstract = {The objective of the offline framework is to reconstruct and analyse the data coming from real interactions. The ALICE Offline framework, AliRoot, has already been used during the production of the Technical Design Reports of each detector to optimise their design and it is currently used to evaluate the physics performance of the full ALICE detector. This paper describes the AliRoot software environment. We wish to put into perspective the main decisions and the organisation of the offline project. First a general description of the ALICE offline framework (AliRoot) is given, starting with a short historical background followed by a description of the simulation, reconstruction and analysis architecture and the organisation of the ALICE offline project. Finally we briefly indicate the main conclusions of our work on AliRoot.},
  doi      = {https://doi.org/10.1016/S0168-9002(03)00440-6},
  keywords = {Object models, Frameworks, Large distributed data bases, Simulation, Reconstruction, Visualisation, Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900203004406},
}

@Article{Al-Jadir2010,
  author   = {Lina Al-Jadir and Christine Parent and Stefano Spaccapietra},
  title    = {Reasoning with large ontologies stored in relational databases: The OntoMinD approach},
  journal  = {Data \& Knowledge Engineering},
  year     = {2010},
  volume   = {69},
  number   = {11},
  pages    = {1158 - 1180},
  issn     = {0169-023X},
  note     = {Special issue on contribution of ontologies in designing advanced information systems},
  abstract = {A major obstacle to the development of ontologies in support of the Semantic Web is the poor capability of current ontology techniques to handle very large ontologies, in particular regarding scalability of reasoners. This paper builds on the assumption that very large ontologies can be efficiently handled using database management systems (DBMS), designed to provide best performance in storing, updating, and managing large volumes of data. To enhance DBMS with the reasoning functionality that characterizes ontology management, we propose to implement reasoning into the DBMS via a set of PL/SQL stored procedures. These procedures support all usual reasoning tasks: Class subsumption, property subsumption, class satisfiability, ABox consistency, and ABox realization. They perform these tasks at update time and materialize all inferred knowledge (facts and axioms) in the database. Contrarily to the inferencing at query time in most of existing works, our approach is designed to speed up ontology querying, which is supposed to represent the most frequent and therefore critical usage of ontologies. The paper discusses querying patterns and reports on benchmarking (with the LUBM benchmark) the performance of our prototype, called OntoMinD, compared to Oracle with Semantic Technologies. Benchmark results demonstrate the appropriateness of our approach.},
  doi      = {https://doi.org/10.1016/j.datak.2010.07.006},
  keywords = {Ontology storage, Ontology reasoning, Ontology querying, Benchmark, Database},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X10000881},
}

@Article{Lim1998,
  author  = {Hwa A. Lim and Tauseef R. Butt},
  title   = {Bioinformatics takes charge},
  journal = {Trends in Biotechnology},
  year    = {1998},
  volume  = {16},
  number  = {3},
  pages   = {104 - 107},
  issn    = {0167-7799},
  doi     = {https://doi.org/10.1016/S0167-7799(97)01152-9},
  url     = {http://www.sciencedirect.com/science/article/pii/S0167779997011529},
}

@Article{Didelot2009,
  author   = {Xavier Didelot and Margaret Barker and Daniel Falush and Fergus G. Priest},
  title    = {Evolution of pathogenicity in the Bacillus cereus group},
  journal  = {Systematic and Applied Microbiology},
  year     = {2009},
  volume   = {32},
  number   = {2},
  pages    = {81 - 90},
  issn     = {0723-2020},
  abstract = {The Bacillus cereus group of bacteria comprises soil-dwelling saprophytes but on occasion these bacteria can cause a wide range of diseases in humans, including food poisoning, systemic infections and highly lethal forms of anthrax. While anthrax is almost invariably caused by strains from a single evolutionary lineage, Bacillus anthracis, variation in the virulence properties of strains from other lineages has not been fully addressed. Using multi-locus sequence data from 667 strains, we reconstructed the evolutionary history of the B. cereus group in terms of both clonal inheritance and recombination. The strains included 155 clinical isolates representing B. anthracis, and isolates from emetic and diarrhoeal food poisoning, septicaemia and related infections, wound, and lung infections. We confirmed the existence of three major clades and found that clinical isolates of B. cereus (with the exception of emetic toxin-producing strains) are evenly distributed between and within clades 1 and 2. B. anthracis in particular and emetic toxin-producing B. cereus show more clonal structure and are restricted to clade 1. Our characterization of the patterns of genetic exchange showed that there exist partial barriers to gene flow between the three clades. The pathogenic strains do not exhibit atypically high or low rates of recombination, consistent with the opportunistic nature of most pathogenic infections. However, there have been a large number of recent imports in clade 1 of strains from external origins, which is indicative of an on-going shift in gene-flow boundaries for this clade.},
  doi      = {https://doi.org/10.1016/j.syapm.2009.01.001},
  keywords = { group, Multi-locus sequence typing, Evolution of pathogenicity, Homologous recombination, Opportunistic pathogen},
  url      = {http://www.sciencedirect.com/science/article/pii/S072320200900006X},
}

@InCollection{Abiteboul1990a,
  author    = {Serge Abiteboul},
  title     = {TOWARDS A DEDUCTIVE OBJECT-ORIENTED DATABASE LANGUAGE},
  booktitle = {Deductive and Object-Oriented Databases},
  publisher = {North-Holland},
  year      = {1990},
  editor    = {Won KIM and Jean-Marie NICOLAS and Shojiro NISHIO},
  pages     = {453 - 472},
  address   = {Amsterdam},
  isbn      = {978-0-444-88433-6},
  abstract  = {A language for databases with sets, tuples, lists, object identity and structural inheritance is proposed. The core language is logic-based with a fixpoint semantics. Methods with overloading and methods evaluated externally providing extensibility of the language are considered. Other important issues such as updates and the introduction of explicit control are discussed.},
  doi       = {https://doi.org/10.1016/B978-0-444-88433-6.50032-0},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444884336500320},
}

@Article{Barabucci2016,
  author   = {Gioele Barabucci and Paolo Ciancarini and Angelo Di Iorio and Fabio Vitali},
  title    = {Measuring the quality of diff algorithms: a formalization},
  journal  = {Computer Standards \& Interfaces},
  year     = {2016},
  volume   = {46},
  pages    = {52 - 65},
  issn     = {0920-5489},
  abstract = {The automatic detection of differences between documents is a very common task in several domains. This paper introduces a formal way to compare diff algorithms and to analyze the deltas they produce. There is no one-fits-all definition for the quality of a delta, because it is strongly related to the application domain and the final use of the detected changes. Researchers have historically focused on minimality: reducing the size of the produced edit scripts and/or taming the computational complexity of the algorithms. Recently they started giving more relevance to the human interpretability of the deltas, designing tools that produce more readable, usable and domain-oriented results. We propose a universal delta model and a set of metrics to characterize and compare effectively deltas produced by different algorithms, in order to highlight what are the most suitable ones for use in a given task and domain.},
  doi      = {https://doi.org/10.1016/j.csi.2015.12.005},
  keywords = {Diff algorithms, Output quality, Metrics, Delta model, XML diff},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548915001464},
}

@Article{Chen1998,
  author  = {Peter P.-S. Chen and Reind P. van de Riet},
  title   = {Introduction to the special issue celebrating the 25th volume of Data \& Knowledge Engineering: DKE},
  journal = {Data \& Knowledge Engineering},
  year    = {1998},
  volume  = {25},
  number  = {1},
  pages   = {1 - 9},
  issn    = {0169-023X},
  doi     = {https://doi.org/10.1016/S0169-023X(97)00059-1},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X97000591},
}

@Article{OLeary2004,
  author   = {Daniel E O'Leary},
  title    = {On the relationship between REA and SAP},
  journal  = {International Journal of Accounting Information Systems},
  year     = {2004},
  volume   = {5},
  number   = {1},
  pages    = {65 - 81},
  issn     = {1467-0895},
  abstract = {Resources–Events–Agents (REA) is the best known theoretical accounting enterprise database model, while SAP is the dominant enterprise resource-planning system. The purpose of this paper is to investigate some of the relationships between the underlying data models in REA and SAP. The criteria of Dunn and McCarthy [J. Inf. Syst. (1997) 31] for differentiating accounting systems (database orientation, semantic orientation, and structuring orientation) are used to structure a summary of those relationships. Although it is found that there are substantial similarities, SAP does have some implementation compromises that generally keep it from being fully REA. These compromises are based on the use of accounting artifacts and other, often, implementation-specific compromises. In addition, there are emerging differences between the two. A comparison between REA and SAP is important for a number of reasons. First, the tie between SAP and REA provides SAP with a theoretic basis. Second, the existence of a relationship between REA and SAP provides an important basis for the coverage of REA in the curriculum. Third, the opening of a dialogue between SAP and REA could spur further development of both.},
  doi      = {https://doi.org/10.1016/j.accinf.2004.02.004},
  keywords = {Theoretical accounting enterprise database model, REA, SAP},
  url      = {http://www.sciencedirect.com/science/article/pii/S1467089504000041},
}

@InCollection{Montgomery1994,
  author    = {Stephen Montgomery},
  title     = {16 - Database Environments},
  booktitle = {Object-Oriented Information Engineering},
  publisher = {Academic Press},
  year      = {1994},
  editor    = {Stephen Montgomery},
  pages     = {225 - 250},
  isbn      = {978-0-12-505040-1},
  doi       = {https://doi.org/10.1016/B978-0-12-505040-1.50019-1},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780125050401500191},
}

@Article{Chiu2001,
  author   = {Dickson K.W. Chiu and Qing Li and Kamalakar Karlapalem},
  title    = {Web interface-driven cooperative exception handling in ADOME workflow management system},
  journal  = {Information Systems},
  year     = {2001},
  volume   = {26},
  number   = {2},
  pages    = {93 - 120},
  issn     = {0306-4379},
  abstract = {Exception handling in workflow management systems (WFMSs) is a very important problem since it is not possible to specify all possible outcomes and alternatives. Effective reuse of existing exception handlers can greatly help in dealing with workflow exceptions. On the other hand, cooperative support for user-driven computer supported resolution of unexpected exceptions and workflow evolution at run-time is vital for an adaptive WFMS. We have been developing ADOME-WFMS as a comprehensive framework in which the problem of workflow exception handling can be adequately addressed. In this article, we present an adaptive exception manager and its web-based interface for ADOME-WFMS with procedures for supporting the following: reuse of exception handlers, thorough and automated resolution of expected exceptions, effective management of Problem Solving Agents, cooperative exception handling, user-driven computer supported resolution of unexpected exceptions, and workflow evolution.},
  doi      = {https://doi.org/10.1016/S0306-4379(01)00012-6},
  keywords = {Workflow Management, Exception Handling, Reuse, Workflow Evolution, ECA Rules, Meta-Modeling, Object-Orientation, Workflow Recovery, Web Interface},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437901000126},
}

@Article{Murphy1995,
  author   = {John Murphy and Jane Grimson},
  title    = {Multidatabase interoperability in the Jupiter system},
  journal  = {Information and Software Technology},
  year     = {1995},
  volume   = {37},
  number   = {9},
  pages    = {503 - 513},
  issn     = {0950-5849},
  abstract = {This article proposes a model for multidatabase interoperability whereby a formal model and multidatabase language of Jupiter∗∗Partially funded under the ESPRIT Programme of the Commission of the European Union as part of the EDITH Project 7507, European Distributed Information Technology for Healthcare. are introduced and the formal semantics of the multi-database language is defined in terms of the formal model. The development of the model is incremental in approach, starting with relational model semantics and extending this semantics to a multirelational model. Due to space requirements we only outline the multirelational part of our work and take the single relational part as given. The contribution of this article is that it defines a uniform formal framework for the investigation of properties of multidatabase systems and their languages. An architecture and implementation language are defined and a CORBA-based prototype: the Jupiter Interoperator, is discussed. Legacy systems are characterized in this article along three related dimensions: the legacy applications themselves; the subsystems which support the legacy applications and interoperability; and the models and languages of the legacy applications and subsystems.},
  doi      = {https://doi.org/10.1016/0950-5849(95)97294-I},
  keywords = {legacy systems, interoperability, formal models, multidatabases},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499597294I},
}

@Article{Lee2007,
  author   = {Sunjae Lee and Wonchul Seo and Dongwoo Kang and Kwangsoo Kim and Jae Yeol Lee},
  title    = {A framework for supporting bottom-up ontology evolution for discovery and description of Grid services},
  journal  = {Expert Systems with Applications},
  year     = {2007},
  volume   = {32},
  number   = {2},
  pages    = {376 - 385},
  issn     = {0957-4174},
  abstract = {The problem of service sharing in a Grid environment arises from the heterogeneity of ontologies. The discovery and description of Grid services based on the heterogeneous ontologies might give rise to misunderstanding of the contents. In order to reduce and ultimately to remove the misunderstanding, a domain-specific ontology should be shared among concerned parties, and the abilities of Grid services should be discovered and described based on that shared ontology. However, since the ontology evolves as time goes by, the shared ontology for the Grid services should have a flexible infrastructure that has an ability to reflect the changes in ontologies. This paper proposes a flexible ontology management approach for discovery and description of Grid service capabilities supporting ontology evolution whose goal is to enhance the interoperability among Grid services. In this approach, concepts and descriptions in an ontology are defined independently, and they are connected by relationships. In addition, the relationships are updated based on real-time evaluations of ontology users in order to flexibly support ontology evolution. A bottom-up ontology evolution means such environment that allows ontology users to evaluate impact factors of concepts in an ontology and that results of the evaluation are reflected to the modification of the ontology. The contribution of this paper is to suggest the ontology management framework that not only enables semantic discovery and description of a Grid service capability but also supports a bottom-up ontology evolution based on the users’ evaluations.},
  doi      = {https://doi.org/10.1016/j.eswa.2005.11.036},
  keywords = {Semantic discovery and description of Grid services, Ontology evolution, Ontology management framework},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417405003477},
}

@InCollection{King2002,
  author    = {Roger (Buzz) King},
  title     = {Chapter 92 - The Rubicon of Smart Data},
  booktitle = {VLDB '02: Proceedings of the 28th International Conference on Very Large Databases},
  publisher = {Morgan Kaufmann},
  year      = {2002},
  editor    = {Philip A. Bernstein and Yannis E. Ioannidis and Raghu Ramakrishnan and Dimitris Papadias},
  pages     = {1002 - 1005},
  address   = {San Francisco},
  isbn      = {978-1-55860-869-6},
  abstract  = {Publisher Summary
Each person, group, or program will have its own information space that is always quietly evolving behind the scenes and according to the owner's wishes. These information spaces will be simple to share, thus allowing us to easily trade data and build new information spaces out of old ones. Information spaces will even become predictive by learning owners' data habits, and will thus offer highly valuable data that was not even requested. The software layering and the promises go on. Agents, armed with user profiles and declarative user requests, will find just the right stuff, and then use mediators to extract, integrate, and reformat data. Agents will often animate data, making it sing and dance on the screens. This is all part of a widespread phenomenon that can be seen throughout computer science. Consider the database world and the problem of extracting data semantics. The spectrum of elegant-to-engineering solutions is decades old and is based on the creation of metadata—schemas, annotations, structured terminologies, etc., and ways of manipulating that metadata. The elegant solutions started coming out in the '70s and '80s. When these proved ineffective, far less aggressive goals were laid out, and less functional but far more useful solutions emerged and have found their way into the commercial world.},
  doi       = {https://doi.org/10.1016/B978-155860869-6/50099-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558608696500998},
}

@Article{1991b,
  title   = {Contents},
  journal = {Tectonophysics},
  year    = {1991},
  volume  = {192},
  number  = {3},
  pages   = {403 - 404},
  issn    = {0040-1951},
  doi     = {https://doi.org/10.1016/0040-1951(91)90113-7},
  url     = {http://www.sciencedirect.com/science/article/pii/0040195191901137},
}

@Article{Oliveira2018,
  author   = {Alessandreia Oliveira and Gabriel Tessarolli and Gleiph Ghiotto and Bruno Pinto and Fernando Campello and Matheus Marques and Carlos Oliveira and Igor Rodrigues and Marcos Kalinowski and Uéverton Souza and Leonardo Murta and Vanessa Braganholo},
  title    = {An efficient similarity-based approach for comparing XML documents},
  journal  = {Information Systems},
  year     = {2018},
  volume   = {78},
  pages    = {40 - 57},
  issn     = {0306-4379},
  abstract = {XML documents are widely used to interchange information among heterogeneous systems, ranging from office applications to scientific experiments. Independently of the domain, XML documents may evolve, so identifying and understanding the changes they undergo becomes crucial. Some syntactic diff approaches have been proposed to address this problem. They are mainly designed to compare revisions of XML documents using explicit IDs to match elements. However, elements in different revisions may not share IDs due to tool incompatibility or even divergent or missing schemas. In this paper, we present Phoenix, a similarity-based approach for comparing revisions of XML documents that does not rely on explicit IDs. Phoenix uses dynamic programming and optimization algorithms to compare different features (e.g., element name, content, attributes, and sub-elements) of XML documents and calculate the similarity degree between them. We compared Phoenix with X-Diff and XyDiff, two state-of-the-art XML diff algorithms. XyDiff was the fastest approach but failed in providing precise matching results. X-Diff presented higher efficacy in 30 of the 56 scenarios but was slow. Phoenix executed in a fraction of the running time required by X-Diff and achieved the best results in terms of efficacy in 26 of 56 tested scenarios. In our evaluations, Phoenix was by far the most efficient approach to match elements across revisions of the same XML document.},
  doi      = {https://doi.org/10.1016/j.is.2018.07.001},
  keywords = {XML, Diff, Match, Similarity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437916304926},
}

@InCollection{Dong1993,
  author    = {Guozhu Dong and Kotagiri Ramamohanarao},
  title     = {Representation and Translation of Queries in Heterogeneous Databases with Schematic Discrepancies},
  booktitle = {Interoperable Database Systems (Ds-5)},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {DAVID K. HSIAO and ERICH J. NEUHOLD and RON SACKS-DAVIS},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {177 - 189},
  address   = {Amsterdam},
  isbn      = {978-0-444-89879-1},
  abstract  = {To make heterogeneous databases with schematic discrepancies interoperable, user queries on all databases (as one unified pool of data) must be representable in some global way and then translated to (or re-represented as) multiple queries on individual databases. This paper examines this query representation/translation problem, with a focus on schematic discrepancies. The parameters include the formats of the global reference and the languages. Two kinds of translation, static and dynamic, are considered. For the static case a query is translated to a fixed (static) query for all possible schemas of a database. For the dynamic case a query is translated to different queries depending on the schema of the database. Even with very restrictive styles (evolution constraints) on individual databases, relational calculus on the individual databases is not as powerful as it is on a global reference relation (gRR). In contrast, the IDL language is always as powerful on the individual databases as it is on gRR. We discuss a problem associated with fixed IDL queries. This work also has implication on first order normal form.},
  doi       = {https://doi.org/10.1016/B978-0-444-89879-1.50016-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444898791500166},
}

@Article{Daly2018,
  author   = {J.P. Daly},
  title    = {Audacious Psyche: Visualizing Evolution in John Pringle Nichol’s Romantic Universe},
  journal  = {Endeavour},
  year     = {2018},
  volume   = {42},
  number   = {2},
  pages    = {133 - 144},
  issn     = {0160-9327},
  note     = {Tools of Reason: The Practice of Scientific Diagramming from Antiquity to the Present},
  abstract = {John Pringle Nichol (1804–1859), a Scottish Romantic astronomer, educator, and social reformer, used visual representations to develop and communicate key elements of his theory of evolution as a universal principle. Examining four of the diverse representations that appeared in Nichol’s popular science books between 1846 and 1850 reveals the rich possibilities of evolutionary imagery prior to the emergence of more dominant forms of representation in the wake of Charles Darwin’s On the Origin of Species (1859). The abstract and schematic nature of many of Nichol’s visual representations—which included line diagrams and imaginative, mythic imagery (the latter developed in collaboration with the Scottish Romantic artist David Scott)—made them apt vessels for his Romantic evolutionary concepts, because a single image could simultaneously represent features of evolution across multiple domains, reflecting the Romantic concept of the unity of nature and the myriad analogies between its constituent parts. All of the images embodied narrative in one form or another and required use of the imagination in the act of interpretation. Many of the images facilitated the viewer’s ability to conceptualize unobservable or only partially observable features of evolutionary change. Even as these visual representations acted as tools of perspective, insight, and clarity, they also helped to generate new ambiguities, such as a fundamental tension between teleology and contingency.},
  doi      = {https://doi.org/10.1016/j.endeavour.2018.07.002},
  keywords = {John Pringle Nichol, David Scott, Diagram, Scientific illustration, Evolution, Romanticism, Scottish science, Narrative, Cosmology, Astronomy, Nebular hypothesis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0160932718300516},
}

@Article{Cipriani2011,
  author   = {N. Cipriani and M. Wieland and M. Großmann and D. Nicklas},
  title    = {Tool support for the design and management of context models},
  journal  = {Information Systems},
  year     = {2011},
  volume   = {36},
  number   = {1},
  pages    = {99 - 114},
  issn     = {0306-4379},
  note     = {Selected Papers from the 13th East-European Conference on Advances in Databases and Information Systems (ADBIS 2009)},
  abstract = {A central task in the development of context-aware applications is the modeling and management of complex context information. In this paper, we present the NexusEditor, which can ease this task by providing a graphical user interface to design schemas for spatial and technical context models, interactively create queries, send them to a server and visualize the results. One main contribution is to show how schema awareness can improve such a tool: The NexusEditor dynamically parses the underlying data model and provides additional syntactic checks, semantic checks, and short-cuts based on the schema information. Furthermore, the tool helps to design new schema definitions based on the existing ones, which is crucial for an iterative and user-centric development of context-aware applications. Finally, it provides interfaces to existing information spaces and visualization tools for spatial data like GoogleEarth.},
  doi      = {https://doi.org/10.1016/j.is.2010.07.004},
  keywords = {Data modeling and database design, Advanced database applications, XML and databases, Personalization in databases and information systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437910000669},
}

@InCollection{Borzemski1986,
  author    = {L. Borzemski and S. Lebiediewa},
  title     = {DIPOS: A DATABASE FOR COMPUTER AIDED DESIGN IN ELECTRIC POWER SYSTEMS},
  booktitle = {Computer Aided Design in Control and Engineering Systems},
  publisher = {Pergamon},
  year      = {1986},
  editor    = {P. MARTIN LARSEN and N.E. HANSEN},
  pages     = {261 - 265},
  isbn      = {978-0-08-032557-6},
  abstract  = {The database system named DIPOS intended for use in computer-aided electric power system expansion planning is presented. The GAD system containing the DIPOS database is briefly explained in the context of the transmission network expansion planning. There are many reasons that in CAD applications we should rather think about specialized or problem-oriented databases, which would give unique solutions in particular situations, than general purpose databases with maintenance and operation overheads. In the paper the user's view of data, user interface, database data model and physical database organization are discussed. The system is the relational database from the user's point of view but it employs the network database data model at the system level. This allows the user to have well known natural relational data model with its correctness and ability to perform profitable relational operations, whereas the DIPOS database network data model gives additional advantages in the pre-definition of possible use patterns of the data. An example of database application is also presented.},
  doi       = {https://doi.org/10.1016/B978-0-08-032557-6.50052-2},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080325576500522},
}

@InCollection{Mortimer1993,
  author    = {Andrew J. Mortimer},
  title     = {CHAPTER 15 - INFORMATION RESOURCE MANAGEMENT},
  booktitle = {Information Structure Design for Databases},
  publisher = {Butterworth-Heinemann},
  year      = {1993},
  editor    = {Andrew J. Mortimer},
  pages     = {183 - 203},
  isbn      = {978-0-7506-0683-7},
  abstract  = {Publisher Summary
This chapter discusses the aspects of the data analyst's role that impact on the management of information as a resource and continue after the initial analysis and design process is complete. Once the database has been designed and built, once the data is in place and the processes that act upon this data are operating smoothly, the data analyst becomes involved in the evolution of the database and the system that makes use of it. The analyst is responsible for maintaining and managing the database definition. Aspects of the data analysts' tasks relate to the security and the integrity of the information held in the database. Fourth generation environment (4GE) is a software set of tools that allow data analysts in the fast development of information systems. A4GE operates within a narrow domain of applications and has an underlying engine that determines the style of the system being produced. It is composed of a database including a powerful dictionary facility, a data definition language (DDL), a data manipulation language (DML), a powerful report generator, a screen and menu generator, and a graphical output generator. These tools, used in conjunction with prototyping, would produce a system that is in tune with the user expectations and needs.},
  doi       = {https://doi.org/10.1016/B978-0-7506-0683-7.50022-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780750606837500226},
}

@Article{1988,
  title   = {Imputing missing values under order restrictions: Bruce Thompson, Carol Handry, and Michael Terrin Maryland Medical Research Institute, Baltimore, Maryland(68)},
  journal = {Controlled Clinical Trials},
  year    = {1988},
  volume  = {9},
  number  = {3},
  pages   = {262},
  issn    = {0197-2456},
  doi     = {https://doi.org/10.1016/0197-2456(88)90132-8},
  url     = {http://www.sciencedirect.com/science/article/pii/0197245688901328},
}

@Article{Blinov2003,
  author   = {Mikhail Blinov and Ahmed Patel},
  title    = {An application of the Reference Model for Open Distributed Processing to electronic brokerage},
  journal  = {Computer Standards \& Interfaces},
  year     = {2003},
  volume   = {25},
  number   = {4},
  pages    = {411 - 425},
  issn     = {0920-5489},
  note     = {CORBA: protocols, applications, process models and standards},
  abstract = {This paper presents the design of an Architecture for Electronic Brokerage Systems (AEBS). This architecture is based on the Reference Model for Open Distributed Processing (RM-ODP) model. It specifies a brokerage system from five RM-ODP viewpoints. AEBS include a generic model of electronic brokerage. It defines protocols and interfaces between actors. It enables communication between actors via an advanced object-oriented interface (defined in terms of the Common Object Request Broker Architecture (CORBA) technology) as well as using traditional trading protocols. These interfaces allow a broker to operate in both the rapidly developing object-oriented CORBA environment and the electronic markets based on customary protocols. CORBA was also used for the design of the internal object model and internal interfaces of a brokerage system. This paper focuses primarily on the specification of the AEBS architecture from the enterprise and computational viewpoints of the RM-ODP model.},
  doi      = {https://doi.org/10.1016/S0920-5489(03)00011-4},
  keywords = {Electronic commerce, Brokerage, CORBA, Software architecture, RM-ODP},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548903000114},
}

@Article{1993f,
  title   = {Calendar of meetings},
  journal = {Computational Statistics \& Data Analysis},
  year    = {1993},
  volume  = {16},
  number  = {3},
  pages   = {373 - 377},
  issn    = {0167-9473},
  doi     = {https://doi.org/10.1016/0167-9473(93)90142-G},
  url     = {http://www.sciencedirect.com/science/article/pii/016794739390142G},
}

@Article{Thompson1993,
  author   = {Craig Thompson},
  title    = {A reference model for object data management},
  journal  = {Computer Standards \& Interfaces},
  year     = {1993},
  volume   = {15},
  number   = {2},
  pages    = {121 - 147},
  issn     = {0920-5489},
  note     = {Object-Oriented Reference Models},
  abstract = {This article contains a reprint of the ‘Reference Model on Object Data Management’ taken from the Final Report of the Object-Oriented Database Task Group (OODBTG). OODBTG was a subcommittee under ANSI ASCX3 that met between 1989 and 1991 to develop a characterization of OODB systems that could be used to guide future ANSI efforts to develop more concrete standards in the area of object-oriented software technology. The article begins with a guest introduction on the history of OODB standards efforts that describes how the work of the OODBTG is continuing to affect standards activities that have followed it. As such, it represents the personal view of the author and not necessarily the views of others who worked on the OODB Task Group. Of course, the reprinted Reference Model on Object Data Management, which follows this Introduction, does represent the work of the entire OODBTG.},
  doi      = {https://doi.org/10.1016/0920-5489(93)90011-F},
  keywords = {Database, object-oriented, reference model, ANSI, standards},
  url      = {http://www.sciencedirect.com/science/article/pii/092054899390011F},
}

@Article{Saake1991,
  author   = {Gunter Saake},
  title    = {Descriptive specification of database object behaviour},
  journal  = {Data \& Knowledge Engineering},
  year     = {1991},
  volume   = {6},
  number   = {1},
  pages    = {47 - 73},
  issn     = {0169-023X},
  abstract = {Traditional database design methodologies are not appropriate for the specific requirements of object-oriented database systems and new database application areas. Apart from semantic complications arising from object-oriented database structures with complex objects, arbitrary data types as attribute domains, or generalization hierarchies, specification and semantics of dynamic database behaviour has to be of main interest for typical object-oriented applications, too. We propose the use of a temporal logic as a specification language for dynamic object behaviour and point out the formal semantics of such database dynamics specifications. A layered conceptual database design methodology is presented together with a discussion on design support techniques for behaviour specifications. Finally, implementation aspects are treated.},
  doi      = {https://doi.org/10.1016/0169-023X(91)90015-P},
  keywords = {Database specification, Object-oriented databases, Conceptual schema, Database integrity, Temporal constraints, Object behaviour, Temporal logic},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9190015P},
}

@Article{Zhou2004,
  author   = {Feng Zhou and Chao Jin and Yinghui Wu and Weimin Zheng},
  title    = {TODS: cluster object storage platform designed for scalable services},
  journal  = {Future Generation Computer Systems},
  year     = {2004},
  volume   = {20},
  number   = {4},
  pages    = {549 - 563},
  issn     = {0167-739X},
  note     = {Advanced services for Clusters and Internet computing},
  abstract = {In this paper we present the design and implementation of Tsinghua Object Data Store (TODS), a cluster object storage system to support the building of scalable Internet services. TODS provides a unified, transparent and object-oriented view of the storage devices of the whole cluster, which greatly simplifies cluster service development. In the meantime, it is designed to be scalable and efficient. Services built on it can simply inherit these properties in a lot of cases. TODS supports ACID transactions, which facilitates the building of complex transactional services. TODS abstracts away from service logic most complexities of data management, which have often become major obstacles in developing high quality Internet services. The design principles, architecture and implementation of TODS are discussed. In our performance experiments, the system scales smoothly to a 36-node server cluster and achieves 11,160 in-memory reads per second and 396 transactions per second. We also demonstrate that the programming interface is significantly easier to use than that of RDBMS with a head-to-head comparative experiment.},
  doi      = {https://doi.org/10.1016/S0167-739X(03)00173-0},
  keywords = {Object store, Cluster storage, Transparent persistence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X03001730},
}

@Article{Tekli2012,
  author   = {Joe Tekli and Richard Chbeir},
  title    = {Minimizing user effort in XML grammar matching},
  journal  = {Information Sciences},
  year     = {2012},
  volume   = {210},
  pages    = {1 - 40},
  issn     = {0020-0255},
  abstract = {XML grammar matching has found considerable interest recently, due to the growing number of heterogeneous XML documents on the Web, and the need to integrate, search and retrieve XML documents originated from different data sources. In this study, we provide an approach for automatic XML grammar matching and comparison aiming to minimize the amount of user effort required to perform the match task. This requires (i) considering the various characteristics and constraints of XML grammars (in comparison with ‘grammar simplifying’ approaches), (ii) allowing a flexible combination of different matching criteria (in comparison with static approaches), and (iii) effectively considering the semi-structured nature of XML (in contrast with heuristic methods). To achieve this, we propose an extensible framework based on the concept of tree edit distance as an optimal technique to consider XML structure, integrating different matching criteria to capture all basic XML grammar characteristics, ranging over element semantic and syntactic similarities, cardinality and alternativeness constraints, as well as data-type correspondences and relative ordering. In addition, our framework is flexible, enabling the user to choose mapping cardinality (i.e., 1:1,1:n,n:1,n:n), in comparison with exiting static methods (usually constrained to 1:1). User constraints and feedback are equally considered in order to adjust matching results to the user’s perception of correct matches. Experiments on real and synthetic XML grammars demonstrate the effectiveness and efficiency of our matching strategy in identifying mappings, in comparison with alternative methods.},
  doi      = {https://doi.org/10.1016/j.ins.2012.04.026},
  keywords = {XML, Semi-structured data, XML grammar, Schema matching, Structural similarity, Semantic similarity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025512002903},
}

@Article{Grimson2001,
  author   = {Jane Grimson},
  title    = {Delivering the electronic healthcare record for the 21st century},
  journal  = {International Journal of Medical Informatics},
  year     = {2001},
  volume   = {64},
  number   = {2},
  pages    = {111 - 127},
  issn     = {1386-5056},
  abstract = {In spite of over four decades of research into Electronic Healthcare Record Systems, the penetration of records which incorporate more than simply basic information, into the working life of healthcare organisations is relatively small. This paper discusses some of the key impediments to progress including in particular, the lack of application of software engineering methodologies, the absence of usable standards, and the failure to acknowledge the impact of record systems on the healthcare system itself. However, Health Informatics researchers need to be preparing for the next generation of systems which will be triggered by the twin revolutions of the Internet and Genetic Medicine. This next generation of EHCR will be a longitudinal cradle-to-the-grave active record readily accessible and available via the Internet, and which will be linked to clinical protocols and guidelines to drive the delivery of healthcare to the individual citizen. Post-genomic research will unravel the link between genes, disease, treatment and the environment and this information will be used to promote health and individualise care. A number of key research issues are identified which need to be addressed in order to realise the delivery of the next generation of EHCR Systems.},
  doi      = {https://doi.org/10.1016/S1386-5056(01)00205-2},
  keywords = {Electronic healthcare record systems, Internet and genetic medicine, Clinical protocols},
  url      = {http://www.sciencedirect.com/science/article/pii/S1386505601002052},
}

@Article{Snodgrass2008,
  author   = {Richard T. Snodgrass and Curtis Dyreson and Faiz Currim and Sabah Currim and Shailesh Joshi},
  title    = {Validating quicksand: Temporal schema versioning in τXSchema},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {65},
  number   = {2},
  pages    = {223 - 242},
  issn     = {0169-023X},
  note     = {Including Special Section: 3rd XML Schema and Data Management Workshop (XSDM 2006) – Five selected and extended papers},
  abstract = {The W3C XML Schema recommendation defines the structure and data types for XML documents, but lacks explicit support for time-varying XML documents or for a time-varying schema. In previous work we introduced τXSchema, which is an infrastructure and suite of tools to support the creation and validation of time-varying documents, without requiring any changes to XML Schema. In this paper we extend τXSchema to support versioning of the schema itself. We introduce the concept of a bundle, which is an XML document that references a base (non-temporal) schema, temporal annotations describing how the document can change, and physical annotations describing where timestamps are placed. When the schema is versioned, the base schema and temporal and physical schemas can themselves be time-varying documents, each with their own (possibly versioned) schemas. We describe how the validator can be extended to validate documents in this seeming precarious situation of data that changes over time, while its schema and even its representation are also changing.},
  doi      = {https://doi.org/10.1016/j.datak.2007.09.003},
  keywords = {Database, Temporal, Schema},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001589},
}

@InCollection{Cure2015,
  title     = {Chapter Two - Database Management Systems},
  booktitle = {RDF Database Systems},
  publisher = {Morgan Kaufmann},
  year      = {2015},
  editor    = {Olivier Curé and Guillaume Blin},
  pages     = {9 - 40},
  address   = {Boston},
  isbn      = {978-0-12-799957-9},
  abstract  = {In this chapter, we present the main aspects and solutions of database management systems that have inspired or are currently influencing RDF stores. This ranges from systems based on the relational model to NoSQL and the recent NewSQL stores. It covers aspects such as storage solutions, efficient query processing through indexation, data and processing distribution and parallelism.},
  doi       = {https://doi.org/10.1016/B978-0-12-799957-9.00002-X},
  keywords  = {database management system, relational model, NoSQL, Structured Query Language, Index, ACID, Distribution},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012799957900002X},
}

@Article{Medeiros1994,
  author   = {Claudia Bauzer Medeiros and Márcia Jacobina Andrade},
  title    = {Implementing integrity control in active data bases},
  journal  = {Journal of Systems and Software},
  year     = {1994},
  volume   = {27},
  number   = {3},
  pages    = {171 - 181},
  issn     = {0164-1212},
  note     = {Information and Knowledge Management},
  abstract = {This article presents a system for maintaining static integrity constraints in data bases that uses the active data base paradigm. This system has been added to an active version of the O2 object-oriented data base system, and it is fully functional. Constraints, specified by the user in a fist-order logic language, are transformed into production rules, which are stored in the data base. These rules are used to maintain the corresponding set of constraints for all applications that use the data base. We extend previous work on constraint maintenance in two ways: our system can be used as a constraint maintenance layer on top of object-oriented, relational, and nested relational data bases, and for object-oriented systems, we provide constraint support not only in the case of object composition, but also consider inheritance and methods.},
  doi      = {https://doi.org/10.1016/0164-1212(94)90040-X},
  url      = {http://www.sciencedirect.com/science/article/pii/016412129490040X},
}

@Article{Tan1999,
  author   = {Ching-Wai Tan and Angela Goh},
  title    = {Composite event support in an active database},
  journal  = {Computers \& Industrial Engineering},
  year     = {1999},
  volume   = {37},
  number   = {4},
  pages    = {731 - 744},
  issn     = {0360-8352},
  abstract = {An intelligent or active database system is able to provide support to engineering applications, which require both data and rules to be managed. CLOSE is an active database, which integrates an object-oriented database, ObjectStore with an expert system, CLIPS. Engineering applications require a broad range of composite events, which are combinations of primitive events such as time events, method events and absolute events. This paper focuses on the design and implementation of composite events. Detection of these composite events is supported by a Rule Manager. The data structures used and the mechanism for composite event detection is described. Example applications are also presented in order to highlight the usefulness of composite events.},
  doi      = {https://doi.org/10.1016/S0360-8352(00)00008-5},
  keywords = {Active database, Object-oriented database, Expert systems, Composite events},
  url      = {http://www.sciencedirect.com/science/article/pii/S0360835200000085},
}

@Article{Wang2005,
  author   = {Ji-Rui Wang and Yu-Ming Wei and Ze-Hong Yan and You-Liang Zheng},
  title    = {Detection of single nucleotide polymorphisms in 24 kDa dimeric α-amylase inhibitors from cultivated wheat and its diploid putative progenitors},
  journal  = {Biochimica et Biophysica Acta (BBA) - General Subjects},
  year     = {2005},
  volume   = {1723},
  number   = {1},
  pages    = {309 - 320},
  issn     = {0304-4165},
  abstract = {Seventeen new genes encoding 24 kDa family dimeric α-amylase inhibitors had been characterized from cultivated wheat and its diploid putative progenitors. And the different α-amylase inhibitors in this family, which were determined by coding regions single nucleotide polymorphisms (cSNPs) of their genes, were investigated. The amino acid sequences of 24 kDa α-amylase inhibitors shared very high coherence (91.2%). It indicated that the dimeric α-amylase inhibitors in the 24 kDa family were derived from common ancestral genes by phylogenetic analysis. Eight α-amylase inhibitor genes were characterized from one hexaploid wheat variety, and clustered into four subgroups, indicating that the 24 kDa dimeric α-amylase inhibitors in cultivated wheat were encoded by multi-gene. Forty-five cSNPs, including 35 transitions and 10 transversions, were found, and resulted in a total of ten amino acid changes. The cSNPs at the first site of a codon cause much more nonsynonymous (92.9%) than synonymous mutations, while nonsynonymous and synonymous mutations were almost equal when the cSNPs were at the third site. It was observed that there was Ile105 instead of Val105 at the active region Val104-Val105-Asp106-Ala107 of the α-amylase inhibitor by cSNPs in some inhibitors from Aegilops speltoides, diploid and hexaploid wheats.},
  doi      = {https://doi.org/10.1016/j.bbagen.2005.03.002},
  keywords = {Single nucleotide polymorphism, 24 kDa dimeric α-amylase inhibitor, , },
  url      = {http://www.sciencedirect.com/science/article/pii/S0304416505000759},
}

@Article{Li1999,
  author   = {Qing Li and Raymond K. Wong},
  title    = {Multifaceted object modeling with roles: A comprehensive approach},
  journal  = {Information Sciences},
  year     = {1999},
  volume   = {117},
  number   = {3},
  pages    = {243 - 266},
  issn     = {0020-0255},
  abstract = {In conventional object-oriented database systems, it is assumed silently that fundamental object types and inter-object relationships can be classified statically, prescribing basic structural and behavioral properties for all the objects in the database. Such a classification-based approach falls short of supporting emerging data-intensive applications requiring more advanced dynamic capabilities. One of such advanced capabilities is the support of modeling subjectivity – the ability to (among others) provide multiple perspectives of application objects or to model so-called `multifaceted objects' which, on different stages, can exhibit different forms and behavior. This article describes an extended object-oriented approach that we have been investigating for this purpose. Advanced features embodied by dynamic and versatile role facilities have been introduced into a conventional object-oriented model, which facilitates specifying and modeling such applications in a natural, incremental and systematic way. Examples from automated manufacturing systems have been used to examine the necessity and utility of such advanced role facilities, and experimental prototype systems of the role mechanism have also been developed by exploiting different techniques and approaches of implementing roles.},
  doi      = {https://doi.org/10.1016/S0020-0255(99)00025-0},
  keywords = {Object databases, Modeling subjectivity, Automated manufacturing applications, Dynamic object roles and constraints},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025599000250},
}

@InCollection{Ichiko1990,
  author    = {Takao Ichiko},
  title     = {An Approach for High Quality Software},
  booktitle = {An Analysis of the Information Technology Standardization Process},
  publisher = {Elsevier},
  year      = {1990},
  editor    = {John L. Berg and Harald Schumny},
  pages     = {177 - 188},
  address   = {Amsterdam},
  isbn      = {978-0-444-87390-3},
  doi       = {https://doi.org/10.1016/B978-0-444-87390-3.50029-3},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444873903500293},
}

@Article{Charousset2016,
  author   = {Dominik Charousset and Raphael Hiesgen and Thomas C. Schmidt},
  title    = {Revisiting actor programming in C++},
  journal  = {Computer Languages, Systems \& Structures},
  year     = {2016},
  volume   = {45},
  pages    = {105 - 131},
  issn     = {1477-8424},
  abstract = {The actor model of computation has gained significant popularity over the last decade. Its high level of abstraction makes it appealing for concurrent applications in parallel and distributed systems. However, designing a real-world actor framework that subsumes full scalability, strong reliability, and high resource efficiency requires many conceptual and algorithmic additives to the original model. In this paper, we report on designing and building CAF, the C++ Actor Framework. CAF targets at providing a concurrent and distributed native environment for scaling up to very large, high-performance applications, and equally well down to small constrained systems. We present the key specifications and design concepts—in particular a message-transparent architecture, type-safe message interfaces, and pattern matching facilities—that make native actors a viable approach for many robust, elastic, and highly distributed developments. We demonstrate the feasibility of CAF in three scenarios: first for elastic, upscaling environments, second for including heterogeneous hardware like GPUs, and third for distributed runtime systems. Extensive performance evaluations indicate ideal runtime at very low memory footprint for up to 64 CPU cores, or when offloading work to a GPU. In these tests, CAF continuously outperforms the competing actor environments Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the raw message passing framework OpenMPI.},
  doi      = {https://doi.org/10.1016/j.cl.2016.01.002},
  keywords = {C++ Actor framework, Concurrent programming, Message-oriented middleware, Distributed software architecture, GPU computing, Performance analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842416000038},
}

@InCollection{Li2014,
  title     = {Index},
  booktitle = {Scholarly Information Discovery in the Networked Academic Learning Environment},
  publisher = {Chandos Publishing},
  year      = {2014},
  editor    = {LiLi Li},
  series    = {Chandos Information Professional Series},
  pages     = {421 - 429},
  address   = {Oxford},
  isbn      = {978-1-84334-763-7},
  doi       = {https://doi.org/10.1016/B978-1-84334-763-7.50024-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978184334763750024X},
}

@Article{Maurer1994,
  author   = {H Maurer and N Scherbakov and K Andrews and P Srinivasan},
  title    = {Object-oriented modelling of hyperstructure: overcoming the static link deficiency},
  journal  = {Information and Software Technology},
  year     = {1994},
  volume   = {36},
  number   = {6},
  pages    = {315 - 322},
  issn     = {0950-5849},
  abstract = {Although the object-oriented paradigm is well suited for modelling self-contained independent objects, it is not suited for modelling persistent relations (static links) between abstract data objects. At the same time, the concept of computer-navigable links is an integral part of the hypermedia paradigm. In contrast to multimedia, where the object-oriented paradigm plays a leading role, this ‘static link’ deficiency considerably reduces the application of object-oriented methods in hypermedia. In this paper, we present a new logical data model (the HM Data Model) which incorporates the well-known principles of object-oriented data modelling into the management of large-scale, multi-user hypermedia databases. The model is based on the notion of abstract hypermedia data objects called S-collections. Computer-navigable links are encapsulated within a particular S-collection and are also bound between S-collections. This approach not only overcomes the static link deficiency of the object-oriented paradigm, but also supports modularity, incremental development, and flexible versioning, and provides a solid logical basis for semantic modelling.},
  doi      = {https://doi.org/10.1016/0950-5849(94)90031-0},
  keywords = {hypermedia, HM Data Model, object-orientation, abstract data objects, structured authoring},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584994900310},
}

@InCollection{Li2014a,
  author    = {LiLi Li},
  title     = {1 - Information in the digital age},
  booktitle = {Scholarly Information Discovery in the Networked Academic Learning Environment},
  publisher = {Chandos Publishing},
  year      = {2014},
  editor    = {LiLi Li},
  series    = {Chandos Information Professional Series},
  pages     = {3 - 27},
  address   = {Oxford},
  isbn      = {978-1-84334-763-7},
  abstract  = {Abstract:
An essential feature of today’s information society is the high speed of information delivery and dissemination. With the prosperity of the Internet and the World Wide Web (WWW), cutting-edge and emerging technologies are providing us with more and more innovative media to access, display, transform, transmit, and store information across heterogeneous applications, databases, networks, platforms, and systems. This is the reason faculty and scholars have expressed concern about information overload in the age of the information explosion. To help students learn more about information and the information explosion, this chapter explores the definition of information and its primary features in the digital age. Also discussed are some differences and associations among information, data, knowledge, and experience. Finally, this chapter highlights the information explosion and its implications for students in dynamic and interactive academic learning environments.},
  doi       = {https://doi.org/10.1533/9781780634449.1.3},
  keywords  = {data, databases, experience, format, information, information explosion, information overload, knowledge, media},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781843347637500019},
}

@Article{Ma2005,
  author   = {Qing-Hu Ma and Bin Tian},
  title    = {Characterization of a wheat histidine-containing phosphotransfer protein (HP) that is regulated by cytokinin-mediated inhibition of leaf senescence},
  journal  = {Plant Science},
  year     = {2005},
  volume   = {168},
  number   = {6},
  pages    = {1507 - 1514},
  issn     = {0168-9452},
  abstract = {A wheat histidine-containing phosphotransfer (HP) cDNA (TaHP1) was isolated from a wheat seedling cDNA library using a RT-PCR product as a probe. The predicted amino acid sequence of TaHP1 is homologous to the histidine-containing phosphotransfer module of the multistep His-Asp phosphorelay from maize and Arabidopsis. The occurrence of HP in wheat was extensively inspected, two homologues were retrieved from wheat EST database and were named as TaHP2 and TaHP3, respectively. TaHP1, 2 and 3 are high homology and form as TaHP1 gene family. Phylogenetic analysis suggests that two classes of HP existed in plants. The secondary and three-dimensional structure analysis by molecular modeling revealed that the basic structure of plant HP proteins is similar with that of YDP1, a yeast HP protein. However, a slight difference in helix arrangement was found between TaHP1 and AHP1, the representative of class I and class II HP proteins, respectively. TaHP1 is present as a single copy gene in the wheat genome as demonstrated by DNA gel blot analysis. RNA gel blot hybridization demonstrated that TaHP1 family was actively expressed in seedlings, but could not be detected in root tissues. Treatment of leaf segments with BA induced TaHP1 family transcript accumulation in a dose-dependent manner, while treatment with trans-zeatin did not. These effects paralleled BA-mediated retardation leaf senescence. These results suggest that HP, like response regulator (RR), is also involved in the resetting or fine-tuning of cytokinin signal transduction and participation in cytokinin-mediated signaling during specific physiological processes, such as leaf senescence.},
  doi      = {https://doi.org/10.1016/j.plantsci.2005.02.022},
  keywords = {Histidine-containing phosphotransfer, Structural analysis, Cytokinin signal transduction, Leaf senescence, L.},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168945205000543},
}

@InCollection{Vazirgiannis2003,
  author    = {Michalis Vazirgiannis},
  title     = {Data Modeling: Object-Oriented Data Model},
  booktitle = {Encyclopedia of Information Systems},
  publisher = {Elsevier},
  year      = {2003},
  editor    = {Hossein Bidgoli},
  pages     = {505 - 520},
  address   = {New York},
  isbn      = {978-0-12-227240-0},
  doi       = {https://doi.org/10.1016/B0-12-227240-4/00035-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B0122272404000356},
}

@Article{1988a,
  title   = {experience with distributed data analyses in the cardia study: Gary Cutter, Laure Perkins, Lynne Wagenknecht, David Martin, and sam Shannon for the CARDIA Study University of Alabama at Brimingham, Birmingham, Alabama(70)},
  journal = {Controlled Clinical Trials},
  year    = {1988},
  volume  = {9},
  number  = {3},
  pages   = {262},
  issn    = {0197-2456},
  doi     = {https://doi.org/10.1016/0197-2456(88)90134-1},
  url     = {http://www.sciencedirect.com/science/article/pii/0197245688901341},
}

@InCollection{Halpin2001,
  author    = {Terry Halpin},
  title     = {5 - Mandatory Roles},
  booktitle = {Information Modeling and Relational Databases},
  publisher = {Academic Press},
  year      = {2001},
  editor    = {Terry Halpin},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {163 - 213},
  address   = {San Diego},
  isbn      = {978-1-55860-672-2},
  abstract  = {Publisher Summary
One of the steps of conceptual schema design procedure requires adding mandatory role constraints and check for logical derivations. Once mandatory role constraints are added, a logical derivation check should be performed to see if some fact types are derivable using logical rather than arithmetic operations. Constraints on a derived fact type should normally be derivable. If there is a chain of two or more functional fact types with uniqueness constraints on all the first roles, a functional binary from the start to the end of this chain is derivable if it is defined by projecting on the joint of these fact types—its uniqueness constraint is transitively implied. Derivation rules should be biconditionals. A role is mandatory if and only if, for all states of the database, the role must be played by every member of the population of its object type; otherwise the role is optional. A mandatory role is also called a total role, since it is played by the total population of its object type.},
  doi       = {https://doi.org/10.1016/B978-155860672-2/50008-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558606722500087},
}

@Article{Ebenau2005,
  author   = {Carsten Ebenau and Jens Rottschäfer and Georg Thierauf},
  title    = {An advanced evolutionary strategy with an adaptive penalty function for mixed-discrete structural optimisation},
  journal  = {Advances in Engineering Software},
  year     = {2005},
  volume   = {36},
  number   = {1},
  pages    = {29 - 38},
  issn     = {0965-9978},
  note     = {Evolutionary Optimization of Engineering Problems},
  abstract = {Modular three-dimensional steel frames are prefabricated structures with slender elements and dominating compressive forces. Their analysis includes different non-linearities and the optimisation variables are often mixed-discrete or topological. The optimisation of these systems requires the most advanced solution techniques but also a systematic preprocessing, a sophisticated non-linear structural analysis and an automated handling of stress- and displacement-constraints as well as further side-constraints resulting from the underlying codes of practice. In this contribution, a (μ+1)-evolutionary strategy combined with an adaptive penalty function and a special selection scheme is presented to solve this optimisation problem.},
  doi      = {https://doi.org/10.1016/j.advengsoft.2003.10.008},
  keywords = {Steel structures, Optimisation, Mixed-discrete, Evolution strategy, Penalty function},
  url      = {http://www.sciencedirect.com/science/article/pii/S0965997804000791},
}

@InCollection{Teorey2006a,
  author    = {Toby Teorey and Sam Lightstone and Tom Nadeau},
  title     = {9 - CASE Tools for Logical Database Design},
  booktitle = {Database Modeling and Design (Fourth Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2006},
  editor    = {Toby Teorey and Sam Lightstone and Tom Nadeau},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {187 - 211},
  address   = {San Francisco},
  edition   = {Fourth Edition},
  isbn      = {978-0-12-685352-0},
  abstract  = {Publisher Summary
This chapter focuses on commercially available tools to simplify these design processes. These computer-aided system engineering, or CASE, tools provide functions that assist in system design. CASE tools are widely used in numerous industries and domains, such as circuit design, manufacturing, and architecture. Logical database design is another area where CASE tools have proven effective. There are several good CASE tools available for computer-assisted database design. This chapter highlights some of the features of three of the leading products: IBM Rational Data Architect, Computer Associates AllFusion ERwin Data Modeler, and Sybase PowerDesigner. Each provides powerful capabilities to assist in developing ER models and transforming those models to logical database designs and physical implementations. All of these products support a wide range of database vendors, including DB2 UDB, DB2 zOS, Informix Data Server (IDS), Oracle, SQL Server, and many others through ODBC support. Each product has different advantages and strengths.},
  doi       = {https://doi.org/10.1016/B978-012685352-0/50009-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780126853520500094},
}

@Article{Kwan1999,
  author   = {Irene Kwan and Qing Li},
  title    = {A hybrid approach to convert relational schema to object-oriented schema},
  journal  = {Information Sciences},
  year     = {1999},
  volume   = {117},
  number   = {3},
  pages    = {201 - 241},
  issn     = {0020-0255},
  abstract = {In the paradigm of DataBase Re-Engineering (DBRE), reverse-engineering data semantics by schema translation from lower level of abstraction such as relational schema to a higher level of abstraction such as Extended Entity Relational (EER) model has been, in the past, extensively studied with due to its relative simplicity in matching. However, schema transformation from EER model to Object-Oriented DataBase (OODB) schema is not straightforward due to its missing of dynamic semantic representation, since an Object-Oriented (OO) schema should contain both the structure and operations of the data objects. In this paper, we describe a hybrid approach which applies both heuristic learning techniques in discovering the behavioural semantics from relational schema and knowledge-based approach in recovering static and structural semantics, to reach a complete conversion. A practical case is applied through an implemented prototype to validate the effectiveness of our methodology.},
  doi      = {https://doi.org/10.1016/S0020-0255(99)00009-2},
  keywords = {Object-oriented, Relational database, DataBase Re-Engineering, Knowledge-based technique, Heuristic learning technique},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025599000092},
}

@Article{Ding2004,
  author   = {Yu Ding and Qing-Hu Ma},
  title    = {Characterization of a cytosolic malate dehydrogenase cDNA which encodes an isozyme toward oxaloacetate reduction in wheat},
  journal  = {Biochimie},
  year     = {2004},
  volume   = {86},
  number   = {8},
  pages    = {509 - 518},
  issn     = {0300-9084},
  abstract = {Malate dehydrogenase (MDH), which is ubiquitous in nature, catalyzes the interconversion of oxaloacetate and malate. Higher plants contain multiple forms of MDH that differ in co-enzyme specificity, subcellular localization and physiological function. Cytosolic NAD-dependent MDH (cyMDH) is one class of MDH that has not been extensively characterized in plants. Here we report the cloning of a cDNA from wheat by RT-PCR and cDNA library screening, which is designated as TaMDH. Sequence analysis indicated that TaMDH exhibits a highly similarity to other plant cyMDHs. Immunological analysis confirmed that TaMDH encoded a cytosolic NAD-dependent MDH. The secondary and three-dimensional structures of TaMDH were analyzed by molecular modeling. DNA gel-blot analyses demonstrated that TaMDH gene exists as two copies in the wheat genome. RNA and protein gel-blot hybridization indicated that both TaMDH mRNA and protein were constitutively expressed in vegetative tissues of wheat, with slightly lower levels in roots than in leaves and stems. In silico analysis indicated that TaMDH was also expressed in various reproductive tissues and tissues under many different stress conditions. Kinetic analysis of bacterially expressed and purified protein confirmed that TaMDH catalyzed a reaction driven towards malate synthesis, which is consistent with other cyMDHs. Evolutionary analysis showed that this class of genes evolved from a very ancestral gene. The cyMDH represents an ancestral form of MDH, which is highly conserved in plants, animals and bacteria. This implies that cyMDHs are housekeeping genes and may have very essential functions in plant metabolism.},
  doi      = {https://doi.org/10.1016/j.biochi.2004.07.011},
  keywords = {Cytosolic malate dehydrogenase, cDNA cloning, Biochemical analysis, L},
  url      = {http://www.sciencedirect.com/science/article/pii/S0300908404001269},
}

@Article{1993g,
  title   = {IASC news},
  journal = {Computational Statistics \& Data Analysis},
  year    = {1993},
  volume  = {15},
  number  = {4},
  pages   = {470 - 471},
  issn    = {0167-9473},
  doi     = {https://doi.org/10.1016/0167-9473(93)90184-U},
  url     = {http://www.sciencedirect.com/science/article/pii/016794739390184U},
}

@Article{Qutaishat1997,
  author   = {M.A. Qutaishat and N.J. Fiddian and W.A. Gray},
  title    = {Extending OMT to support bottom-up design modelling in a heterogeneous distributed database environment},
  journal  = {Data \& Knowledge Engineering},
  year     = {1997},
  volume   = {22},
  number   = {2},
  pages    = {191 - 205},
  issn     = {0169-023X},
  abstract = {We present an extension to the object model of OMT to cope with bottom-up database design. By investigation, it was discovered that OMT as it stands is inadequate to capture some real world semantic and structural information needed to perform schema integration for pre-existing databases in a heterogeneous distributed database environment. Therefore the proposed extension, called Integrated OMT (IOMT), was formally defined and an appropriate extended graphical notation was produced as a part of our work. This extended form was implemented and applied effectively using a tool which we call the schema meta-integration/visualisation system (SMIS/SMVS).},
  doi      = {https://doi.org/10.1016/S0169-023X(97)81410-3},
  keywords = {Heterogeneous distributed databases, Bottom-up design, Object-oriented data modelling, Schema integration, Schema visualisation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X97814103},
}

@Article{Bellahsene2000,
  author   = {Zohra Bellahsene},
  title    = {Updates and object-generating views in ODBS},
  journal  = {Data \& Knowledge Engineering},
  year     = {2000},
  volume   = {34},
  number   = {2},
  pages    = {125 - 163},
  issn     = {0169-023X},
  abstract = {In object-oriented database systems, a view mechanism can extend the object paradigm by adapting object structure and object behavior to the needs of the different applications or users. In this paper we consider views with object-generating semantics which is an important construct in database environment and mediator systems (Y. Papakonstantinou, S. Abiteboul, H. Garcia-Molina, Object Fusion in Mediator Systems, in: Proceedings of the International Conference on Very Large DataBases, New York, July 1998). We reexamine the problem of updating views according to the aspects of object-orientation, namely: object identity, inheritance and aggregation path. Furthermore, we propose a taxonomy of virtual classes in order to define the update semantics with respect to the virtual class semantics. We present also a cost model for our update algorithms to quantify the impact of our materialization method upon updates propagation times.},
  doi      = {https://doi.org/10.1016/S0169-023X(00)00011-2},
  keywords = {Object-oriented databases, Object identity, View updating, Materialized views, Object-generated views},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X00000112},
}

@Article{Torres2017,
  author   = {Alexandre Torres and Renata Galante and Marcelo S. Pimenta and Alexandre Jonatan B. Martins},
  title    = {Twenty years of object-relational mapping: A survey on patterns, solutions, and their implications on application design},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {82},
  pages    = {1 - 18},
  issn     = {0950-5849},
  abstract = {Context
Almost twenty years after the first release of TopLink for Java, Object-Relational Mapping Solutions (ORMSs) are available at every popular development platform, providing useful tools for developers to deal with the impedance mismatch problem. However, no matter how ubiquitous these solutions are, this essential problem remains as challenging as ever. Different solutions, each with a particular vocabulary, are difficult to learn, and make the impedance problem looks deceptively simpler than it really is.
Objective
The objective of this paper is to identify, discuss, and organize the knowledge concerning ORMSs, helping designers towards making better informed decisions about designing and implementing their models, focusing at the static view of persistence mapping.
Method
This paper presents a survey with nine ORMSs, selected from the top ten development platforms in popularity. Each ORMS was assessed, by documentation review and experience, in relation to architectural and structural patterns, selected from literature, and its characteristics and implementation options, including platform specific particularities.
Results
We found out that all studied ORMSs followed architectural and structural patterns in the literature, but often with distinct nomenclature, and some singularities. Many decisions, depending on how patterns are implemented and configured, affect how class models should be adapted, in order to create practical mappings to the database.
Conclusion
This survey identified what structural patterns each ORMS followed, highlighting major structural decisions a designer must take, and its consequences, in order to turn analysis models into object oriented systems. It also offers a pattern based set of characteristics that developers can use as a baseline to make their own assessments of ORMSs.},
  doi      = {https://doi.org/10.1016/j.infsof.2016.09.009},
  keywords = {Object-relational mapping, Design patterns, Impedance mismatch problem, Enterprise patterns, Class models},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916301859},
}

@Article{Roantree1999,
  author   = {M. Roantree and J.B. Kennedy and P.J. Barclay},
  title    = {Providing views and closure for the object data management group object model},
  journal  = {Information and Software Technology},
  year     = {1999},
  volume   = {41},
  number   = {15},
  pages    = {1037 - 1044},
  issn     = {0950-5849},
  abstract = {The object data management group (ODMG) object model offers a standard for object-oriented database designers, while attempting to address some issues of interoperability. This research is focused on the viability of using the ODMG data model as a canonical data model in a multidatabase environment, and where weaknesses are identified we have proposed amendments to enable the model to suit the specific needs of this type of distributed database system. This paper describes our efforts to extend its relational style algebra, and to provide query closure and a viewing mechanism for object query language to construct multidatabase schemas.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00042-7},
  keywords = {Object model, Multidatabases, Object data management group, Views, Object query language},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000427},
}

@Article{Thieme1995,
  author   = {Christiaan Thieme and Arno Siebes},
  title    = {Guiding schema integration by behavioural information},
  journal  = {Information Systems},
  year     = {1995},
  volume   = {20},
  number   = {4},
  pages    = {305 - 316},
  issn     = {0306-4379},
  note     = {Sixth International Conference on Advanced Information Systems Engineering},
  abstract = {This paper presents an approach to schema integration, where schemas are restructured using schema transformations and schemas are merged using join operators. The novelty of the approach is that behavioural information is used to guide schema restructuring as well as schema merging.},
  doi      = {https://doi.org/10.1016/0306-4379(95)00016-W},
  keywords = {Schema Integration, Behaviour, Object Oriented Databases},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799500016W},
}

@Article{Chebotko2009,
  author   = {Artem Chebotko and Shiyong Lu and Farshad Fotouhi},
  title    = {Semantics preserving SPARQL-to-SQL translation},
  journal  = {Data \& Knowledge Engineering},
  year     = {2009},
  volume   = {68},
  number   = {10},
  pages    = {973 - 1000},
  issn     = {0169-023X},
  abstract = {Most existing RDF stores, which serve as metadata repositories on the Semantic Web, use an RDBMS as a backend to manage RDF data. This motivates us to study the problem of translating SPARQL queries into equivalent SQL queries, which further can be optimized and evaluated by the relational query engine and their results can be returned as SPARQL query solutions. The main contributions of our research are: (i) We formalize a relational algebra based semantics of SPARQL, which bridges the gap between SPARQL and SQL query languages, and prove that our semantics is equivalent to the mapping-based semantics of SPARQL; (ii) Based on this semantics, we propose the first provably semantics preserving SPARQL-to-SQL translation for SPARQL triple patterns, basic graph patterns, optional graph patterns, alternative graph patterns, and value constraints; (iii) Our translation algorithm is generic and can be directly applied to existing RDBMS-based RDF stores; and (iv) We outline a number of simplifications for the SPARQL-to-SQL translation to generate simpler and more efficient SQL queries and extend our defined semantics and translation to support the bag semantics of a SPARQL query solution. The experimental study showed that our proposed generic translation can serve as a good alternative to existing schema dependent translations in terms of efficient query evaluation and/or ensured query result correctness.},
  doi      = {https://doi.org/10.1016/j.datak.2009.04.001},
  keywords = {SPARQL-to-SQL translation, SPARQL semantics, SPARQL, SQL, RDF, query, RDF store, RDBMS},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X09000469},
}

@Article{Goyal1993,
  author   = {P. Goyal and T.S. Narayanan and F. Sadri},
  title    = {Concurrency control for object bases},
  journal  = {Information Systems},
  year     = {1993},
  volume   = {18},
  number   = {3},
  pages    = {167 - 180},
  issn     = {0306-4379},
  abstract = {This paper presents concurrency control techniques suitable for engineering design environments. Design environments are quite different from conventional business applications for which databases have been traditionally used. Transactions in a design application are usually very long compared with those in traditional applications, necessitating new concurrency control techniques. We present the Dynamic Validation concurrency control scheme, an optimistic concurrency control scheme that avoids locking data items for extended periods of time. Dynamic validation is designed specifically for low-contention object-oriented database applications with long-duration transactions, such as engineering design applications. We prove that dynamic validation enforces executions that are “serializable with respect to the database state”. This correctness criterion relaxes the classical serializability criterion, and allows schedules that are equivalent, i.e. produce the same effect, to a serial execution of transactions starting from the same initial database state.},
  doi      = {https://doi.org/10.1016/0306-4379(93)90035-Y},
  keywords = {Database systems, concurrency control, object-oriented databases, optimistic techniques, dynamic validation, non-traditional applications},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799390035Y},
}

@Article{Fung2009,
  author   = {Kam Hay Fung and Graham Cedric Low},
  title    = {Methodology evaluation framework for dynamic evolution in composition-based distributed applications},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {12},
  pages    = {1950 - 1965},
  issn     = {0164-1212},
  abstract = {Dynamic evolution can be used to upgrade distributed applications without shutdown and restart as a way of improving service levels while minimising the loss of business revenue caused by the downtime. An evaluation framework assessing the level of support offered by existing methodologies in composition-based application (e.g. component-based and service-oriented) development is proposed. It was developed by an analysis of the literature and existing methodologies together with a refinement based on a survey of experienced practitioners and researchers. The use of the framework is demonstrated by applying it to twelve methodologies to assess their support for dynamic evolution.},
  doi      = {https://doi.org/10.1016/j.jss.2009.06.032},
  keywords = {Dynamic evolution, Composition-based applications, Service-oriented computing, Feature analysis, Evaluation framework, Method engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209001393},
}

@Article{Amor1995,
  author   = {Robert Amor and Godfried Augenbroe and John Hosking and Wouter Rombouts and john Grundy},
  title    = {Directions in modelling environments},
  journal  = {Automation in Construction},
  year     = {1995},
  volume   = {4},
  number   = {3},
  pages    = {173 - 187},
  issn     = {0926-5805},
  abstract = {Schema definition is a vital component in the development of computerised A/E/C projects, but existing tools to manage this task are limited both in terms of the scope of problems they can tackle and their integration with each other. This paper describes a global modelling and development environment for large modelling projects. This environment provides a total solution from initial design of schemas to validation, manipulation and navigation through final models. A major benefit of the described system is the ability to provide multiple views of evolving schemas (or models) in both graphical and textual forms. This allows modellers to visualise their schemas and instance models either textually or graphically as desired. The system automatically maintains the consistency of the information in these views even when modifications are made in other views. Simple and intuitive view navigation methods allow required information to be rapidly accessed. The environment supports strict checking of model instances and schemas in one of the major ISO-standardised modelling languages now used in product data technology. In this paper we show how such a modelling environment has been constructed for evaluation in the JOULE funded COMBINE project.},
  doi      = {https://doi.org/10.1016/0926-5805(95)00003-J},
  keywords = {Modelling environment, Consistency, Multiple views, Views, Building models, Information management, Integrated system, Product modelling},
  url      = {http://www.sciencedirect.com/science/article/pii/092658059500003J},
}

@Article{Schulz2011,
  author   = {Christoph Schulz and Michael Löwe and Harald König},
  title    = {A categorical framework for the transformation of object-oriented systems: Models and data},
  journal  = {Journal of Symbolic Computation},
  year     = {2011},
  volume   = {46},
  number   = {3},
  pages    = {316 - 337},
  issn     = {0747-7171},
  note     = {Applied and Computational Category Theory},
  abstract = {Refactoring of information systems is hard, for two reasons. On the one hand, large databases exist which have to be adjusted. On the other hand, many programs access those data. Data and programs all have to be migrated in a consistent manner such that their semantics does not change. This paper addresses the data part of the problem and introduces a model for object-oriented structures, describing the schema level with classes, associations, and inheritance as well as the instance level with objects and links. Positive Horn formulas based on predicates are used to formulate constraints to be obeyed by the schema and instance level, in order to reflect object-oriented structures. Homomorphisms are used for the typing of the instance level as well as for the description of refactorings which specify the addition, folding, and unfolding of schema elements. A categorial framework is presented which allows us to derive instance migrations from schema transformations in such a way that instances of the old schema are automatically migrated into instances of the new schema. The natural use of the pullback functor for unfolding is followed by an initial semantics approach: Instance migration is completed with the help of a co-adjoint functor on arrow categories.},
  doi      = {https://doi.org/10.1016/j.jsc.2010.09.010},
  keywords = {Refactoring, Evolution, Transformation, Migration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0747717110001690},
}

@Article{Dong1998,
  author   = {Yue Dong and A. Goh},
  title    = {An intelligent database for engineering applications},
  journal  = {Artificial Intelligence in Engineering},
  year     = {1998},
  volume   = {12},
  number   = {1},
  pages    = {1 - 14},
  issn     = {0954-1810},
  abstract = {Many engineering applications need to response to unpredictable events in a timely manner. Active database systems provide an event-driven rule processing capability to meet this requirement. In this paper, we present an intelligent database which integrates an object-oriented database (OODB) with an expert system, CLIPS. The paper describes the design and implementation of the rule manager of this intelligent database. In the rule manager, event-condition-action (ECA) rules are represented as first class objects of the OODB. A rule definition language (RDL) has been developed to manipulate ECA rules in a declarative way. A graphical user interface (GUI) also supplies a template to interactively define, delete, update and check ECA rules. Detection of time events, method events, absolute events and composite events is supported by the rule manager. The CLIPS inference engine is used to control condition evaluation and action execution after an ECA rule is triggered. Finally, a typical workflow application is used to illustrate the functionality of the system.},
  doi      = {https://doi.org/10.1016/S0954-1810(96)00033-7},
  keywords = {object-oriented database, active database, expert systems, rules},
  url      = {http://www.sciencedirect.com/science/article/pii/S0954181096000337},
}

@InCollection{Lecordix2007,
  author    = {François Lecordix and Cécile Lemarié},
  title     = {Chapter 15 - Managing Generalisation Updates in IGN Map Production},
  booktitle = {Generalisation of Geographic Information},
  publisher = {Elsevier Science B.V.},
  year      = {2007},
  editor    = {William A. Mackaness and Anne Ruas and L. Tiina Sarjakoski},
  series    = {International Cartographic Association},
  pages     = {285 - 300},
  address   = {Amsterdam},
  isbn      = {978-0-08-045374-3},
  abstract  = {Publisher Summary
This chapter reviews practical experience gained from the application of generalization technologies in the production environment of the French National Mapping Agency (IGN). Two main stages in production that require generalization solutions are presented. The first step concerns the process of creating from a geographic database a first version of a cartographic database in which generalization conflicts have been solved. This complex derivation step is made only once for a map which will be published over many years after updating. The second step focuses on the repetitive process to propagate updating in a cartographic database in order to provide new map editions with a less expensive solution than via a new derivation process. In this chapter, generalization solutions to produce a 1:100,000 topographic map series are presented in detail and illustrated with a number of examples illustrating road generalization.},
  doi       = {https://doi.org/10.1016/B978-008045374-3/50017-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978008045374350017X},
}

@InCollection{Jarke1990,
  author    = {Matthias Jarke and Manfred Jeusfeld and Thomas Rose},
  title     = {Software Process Modeling as a Strategy for KBMS Implementation},
  booktitle = {Deductive and Object-Oriented Databases},
  publisher = {North-Holland},
  year      = {1990},
  editor    = {Won KIM and Jean-Marie NICOLAS and Shojiro NISHIO},
  pages     = {531 - 550},
  address   = {Amsterdam},
  isbn      = {978-0-444-88433-6},
  abstract  = {Abstract
Deductive and object-oriented databases should not be viewed as competitors but as two layers of abstraction (specification and implementation) within an overall knowledge base management systems (KBMS) architecture. Software process modeling is proposed as an efficient means to maintain the relationships between the two layers. A detailed account of experiences with implementing a deductive and structurally object-oriented system called ConceptBase gives preliminary evidence of the value of our proposal; ConceptBase may also serve as a basis for bootstrapping an environment for fully object-oriented databases.},
  doi       = {https://doi.org/10.1016/B978-0-444-88433-6.50036-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444884336500368},
}

@Article{Kim1997a,
  author   = {Inhan Kim and Thomas Liebich and Tom Maver},
  title    = {Managing design data in an integrated CAAD environment: a product model approach},
  journal  = {Automation in Construction},
  year     = {1997},
  volume   = {7},
  number   = {1},
  pages    = {35 - 53},
  issn     = {0926-5805},
  abstract = {This paper proposes a prototype architectural design environment which aims to integrate various applications for designing a building. Within an object-oriented design environment, a core data model and a data management system have been implemented to seamlessly connect all applications. The process of design has been investigated with the purpose of characterising the role that a system of this kind may have. In defining the system, an approach has been used that privileges the relationships with the existing computer-aided design (CAD) tools based on data exchange standards in course of definition today.},
  doi      = {https://doi.org/10.1016/S0926-5805(97)00036-8},
  keywords = {Product model, Design databases, Object-oriented design system, Computer-aided architectural design (CAAD)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0926580597000368},
}

@InCollection{Halpin2008,
  author    = {Terry Halpin and Tony Morgan},
  title     = {5 - Mandatory Roles},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {159 - 209},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50009-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123735683500096},
}

@Article{1991c,
  title   = {Editorial Board},
  journal = {Tectonophysics},
  year    = {1991},
  volume  = {192},
  number  = {1},
  pages   = {II - III},
  issn    = {0040-1951},
  note    = {Magnetic anomalies\3-Land and sea},
  doi     = {https://doi.org/10.1016/0040-1951(91)90240-S},
  url     = {http://www.sciencedirect.com/science/article/pii/004019519190240S},
}

@Article{Doke1990,
  author   = {E. Reed Doke},
  title    = {An industry survey of emerging prototyping methodologies},
  journal  = {Information \& Management},
  year     = {1990},
  volume   = {18},
  number   = {4},
  pages    = {169 - 176},
  issn     = {0378-7206},
  abstract = {The literature indicates that although the phased traditional systems development approach continues to be used, prototyping techniques are becoming popular. Instead of a single prototyping methodology, however, studies suggest there are several, each with its unique processes and characteristics. These developments raise important questions: which specific methodologies are being used, to what extent are they being applied, and how important are they? A mail survey of the MIS managers in Fortune 1000 firms was conducted to attempt to answer these questions. Four separate methodologies are identified and investigated. Results indicate that over 60% of the respondent organizations are prototyping; all four methodologies are very popular and seen as important to the development of information systems.},
  doi      = {https://doi.org/10.1016/0378-7206(90)90037-I},
  keywords = {Prototype, Prototyping Methodologies, Prototyping Methodology Taxonomy, System Design},
  url      = {http://www.sciencedirect.com/science/article/pii/037872069090037I},
}

@Article{Badard2001,
  author   = {T. Badard and D. Richard},
  title    = {Using XML for the exchange of updating information between geographical information systems},
  journal  = {Computers, Environment and Urban Systems},
  year     = {2001},
  volume   = {25},
  number   = {1},
  pages    = {17 - 31},
  issn     = {0198-9715},
  note     = {GISRUK 2000},
  abstract = {The growth of institutional investment in Geographic Information Systems (GISs) testifies to their increasing importance as analytical decision-making tools utilising highly specialised data. However, a simple system still does not exist for the easy and consistent integration of evolutions into GIS. At present institutions either retrieve the updates themselves, often failing entirely results in information loss and major inconsistencies or they purchase reference data sets from geographic information producers. The problems and expense associated with the integration of updates for geographic databases are well documented. However, it is the lack of choice available in the method of delivering these updates that results in inefficiency, as a whole new database is often delivered even when there is relatively little real change in its data. This paper details the implementation of a new solution for the exchange of updating information for geographical information systems. Developed at the COGIT laboratory of IGN (the French National Mapping Agency) it is based on XML (eXtensible Markup Language) and aims to providing users with structured and detailed information concerning evolutions in order to make their integration into systems easier and more consistent. We highlight the challenges and benefits involved in implementing this transfer method, such as the interoperability between GIS software and the problems of real time updating of databases within client/server architectures. This paper aims to demonstrate that the use of XML in data exchanges is likely to become commonplace between GISs, especially with the aid of Java technology and the web-based client/server data access. It may even be possible to supply consumers with adapted updates on demand and enable real time maintenance of databases to be carried out.},
  doi      = {https://doi.org/10.1016/S0198-9715(00)00038-7},
  keywords = {Graphical information systems, Metadata, Database evolution, Geometric modification, HTML, XML (Extensible Markup Language), SFXML (Simple feature XML), SGML (Standard generalised Markup Language), GML (Geography Markup Language), SOTF (Spatial Object Transfer format), DTD (Document type definition)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0198971500000387},
}

@Article{Al-Haddad2010,
  author   = {Mohammad A. Al-Haddad and Jeff Friedlin and Joe Kesterson and Joshua A. Waters and Juan R. Aguilar-Saavedra and C. Max Schmidt},
  title    = {Natural language processing for the development of a clinical registry: a validation study in intraductal papillary mucinous neoplasms},
  journal  = {HPB},
  year     = {2010},
  volume   = {12},
  number   = {10},
  pages    = {688 - 695},
  issn     = {1365-182X},
  abstract = {Background
Medical natural language processing (NLP) systems have been developed to identify, extract and encode information within clinical narrative text. However, the role of NLP in clinical research and patient care remains limited. Pancreatic cysts are common. Some pancreatic cysts, such as intraductal papillary mucinous neoplasms (IPMNs), have malignant potential and require extended periods of surveillance. We seek to develop a novel NLP system that could be applied in our clinical network to develop a functional registry of IPMN patients.
Objectives
This study aims to validate the accuracy of our novel NLP system in the identification of surgical patients with pathologically confirmed IPMN in comparison with our pre-existing manually created surgical database (standard reference).
Methods
The Regenstrief EXtraction Tool (REX) was used to extract pancreatic cyst patient data from medical text files from Indiana University Health. The system was assessed periodically by direct sampling and review of medical records. Results were compared with the standard reference.
Results
Natural language processing detected 5694 unique patients with pancreas cysts, in 215 of whom surgical pathology had confirmed IPMN. The NLP software identified all but seven patients present in the surgical database and identified an additional 37 IPMN patients not previously included in the surgical database. Using the standard reference, the sensitivity of the NLP program was 97.5% (95% confidence interval [CI] 94.8–98.9%) and its positive predictive value was 95.5% (95% CI 92.3–97.5%).
Conclusions
Natural language processing is a reliable and accurate method for identifying selected patient cohorts and may facilitate the identification and follow-up of patients with IPMN.},
  doi      = {https://doi.org/10.1111/j.1477-2574.2010.00235.x},
  keywords = {Intraductal papillary mucinous neoplasm, pancreatic cancer, prevention, cystic neoplasm, precancerous, natural language processing, data mining},
  url      = {http://www.sciencedirect.com/science/article/pii/S1365182X15302574},
}

@Article{Dickinson2004,
  author   = {Danielle N Dickinson and Myron T La Duc and Masataka Satomi and James D Winefordner and David H Powell and Kasthuri Venkateswaran},
  title    = {MALDI-TOFMS compared with other polyphasic taxonomy approaches for the identification and classification of Bacillus pumilus spores},
  journal  = {Journal of Microbiological Methods},
  year     = {2004},
  volume   = {58},
  number   = {1},
  pages    = {1 - 12},
  issn     = {0167-7012},
  abstract = {To verify the efficacy of matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOFMS) protein profiling for identifying and differentiating bacterial species, several strains of Bacillus pumilus were examined in a thorough taxonomic study incorporating a polyphasic approach. Sixteen isolates of putative B. pumilus isolated from spacecraft assembly facilities, the Mars Odyssey spacecraft, and the International Space Station, were characterized for their biochemical and molecular profiles using the Biolog system, DNA techniques, and MALDI-TOFMS protein profiling. MALDI-TOFMS protein profiling was more accurate than Biolog metabolic profiling, more discriminating than 16S rDNA sequence analysis, and complemented the results of gyrB sequence analysis and DNA–DNA hybridization for the identification of the B. pumilus spores. This is the first report whereby MALDI-TOFMS generated protein profiles from a set of microbes is compared directly with DNA–DNA hybridization yielding a positive correlation. Unique, cluster-specific biomarker peaks have been identified in the spores of the B. pumilus examined in this study. MALDI-TOFMS protein profiling is a rapid and simple analysis and has been demonstrated as a useful taxonomic tool for differentiating spores of the genus Bacillus. For practical purposes, it would be ideal (and necessary) to have a publicly available, standardized MALDI profile database, to facilitate the use of the technique as a diagnostic method to differentiate bacterial species.},
  doi      = {https://doi.org/10.1016/j.mimet.2004.02.011},
  keywords = {, Spores, Mass spectrometry, Protein profiling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167701204000569},
}

@Article{Reiter1993,
  author   = {Raymond Reiter},
  title    = {Proving properties of states in the situation calculus},
  journal  = {Artificial Intelligence},
  year     = {1993},
  volume   = {64},
  number   = {2},
  pages    = {337 - 351},
  issn     = {0004-3702},
  abstract = {In the situation calculus, it is sometimes necessary to prove that certain properties are true in all world states accessible from the initial state. This is the case for some forms of reasoning about the physical world, for certain planning applications, and for verifying integrity constraints in databases. Not surprisingly, this requires a suitable form of mathematical induction. This paper motivates the need for proving properties of states in the situation calculus, proposes appropriate induction principles for this task, and gives examples of their use in databases and for reasoning about the physical world.},
  doi      = {https://doi.org/10.1016/0004-3702(93)90109-O},
  url      = {http://www.sciencedirect.com/science/article/pii/000437029390109O},
}

@Article{Daley2008,
  author   = {Peter Daley and Astrid Petrich and Kevin May and Kathy Luinstra and Candy Rutherford and Pamela Chedore and Frances Jamieson and Marek Smieja},
  title    = {Comparison of in-house and commercial 16S rRNA sequencing with high-performance liquid chromatography and genotype AS and CM for identification of nontuberculous mycobacteria},
  journal  = {Diagnostic Microbiology and Infectious Disease},
  year     = {2008},
  volume   = {61},
  number   = {3},
  pages    = {284 - 293},
  issn     = {0732-8893},
  abstract = {Sequencing of the 16S gene or other targets and line probe assay are in wide use for the identification of nontuberculous mycobacteria. We compared in-house and commercial sequencing with 3 sequence databases against high-performance liquid chromatography (HPLC) and line probe assay (HAIN Genotype AS and CM) for the identification of 84 reference, clinical, and unique strains representing 41 species. Consensus of methods was used as reference standard. Sequencing identification was more specific and flexible than HPLC, but it was limited by database content and quality as well as fragment length. No one database satisfied all requirements. In-house sequencing was lower in cost than commercial sequencing or line probe assay.},
  doi      = {https://doi.org/10.1016/j.diagmicrobio.2008.02.018},
  keywords = {Nontuberculous mycobacteria, 16S sequencing, Line probe assay, High-performance liquid chromatography},
  url      = {http://www.sciencedirect.com/science/article/pii/S0732889308001569},
}

@Article{Novelli2001,
  author   = {Noël Novelli and Rosine Cicchetti},
  title    = {Functional and embedded dependency inference: a data mining point of view},
  journal  = {Information Systems},
  year     = {2001},
  volume   = {26},
  number   = {7},
  pages    = {477 - 506},
  issn     = {0306-4379},
  abstract = {The issue of discovering functional dependencies from populated databases has received a great deal of attention because it is a key concern in database analysis. Such a capability is strongly required in database administration and design while being of great interest in other application fields such as query folding. Investigated for long years, the issue has been recently addressed in a novel and more efficient way by applying principles of data mining algorithms. The two algorithms fitting in such a trend are TANE and Dep-Miner. They strongly improve previous proposals. In this paper, we propose a new approach adopting a data mining point of view. We define a novel characterization of minimal functional dependencies. This formal framework is sound and simpler than related work. We introduce the new concept of free set for capturing source of functional dependencies. By using the concepts of closure and quasi-closure of attribute sets, targets of such dependencies are characterized. Our approach is enforced through the algorithm FUN which is particularly efficient since it is comparable or improves the two best operational solutions (according to our knowledge): TANE and Dep-Miner. It makes use of various optimization techniques and it can work on very large databases. Applying on real life or synthetic data more or less correlated, comparative experiments are performed in order to assess performance of FUN against TANE and Dep-Miner. Moreover, our approach also exhibits (without significant additional execution time) embedded functional dependencies, i.e. dependencies captured in any subset of the attribute set originally considered. Embedded dependencies capture a knowledge specially relevant in all fields where materialized data sets are managed (e.g. materialized views widely used in data warehouses).},
  doi      = {https://doi.org/10.1016/S0306-4379(01)00032-1},
  keywords = {Data mining, Database design, Functional dependency, Lattices, Algorithms},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437901000321},
}

@InCollection{Betz2007,
  booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance},
  publisher = {Morgan Kaufmann},
  year      = {2007},
  editor    = {Charles T. Betz},
  pages     = {397 - 406},
  address   = {Burlington},
  isbn      = {978-0-12-370593-8},
  doi       = {https://doi.org/10.1016/B978-012370593-8/50039-3},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123705938500393},
}

@Article{Hall2010,
  author   = {Eric R. Hall and Alessandro Monti and William W. Mohn},
  title    = {A comparison of bacterial populations in enhanced biological phosphorus removal processes using membrane filtration or gravity sedimentation for solids–liquid separation},
  journal  = {Water Research},
  year     = {2010},
  volume   = {44},
  number   = {9},
  pages    = {2703 - 2714},
  issn     = {0043-1354},
  abstract = {In an earlier phase of this study, we compared the performances of pilot scale treatment systems operated in either a conventional enhanced biological phosphorus removal (CEBPR) mode, or a membrane enhanced biological phosphorus removal (MEBPR) mode. In the present investigation, we characterized the bacterial community populations in these processes during parallel operation with the same municipal wastewater feed. The objectives of the study were (1) to assess the similarity of the bacterial communities supported in the two systems over time, (2) to determine if distinct bacterial populations are associated with the MEBPR and CEBPR processes, and (3) to relate the dynamics of the community composition to changes in treatment process configuration and to treatment process performance. The characteristics of the bacterial populations were first investigated with ribosomal intergenic spacer analysis, or RISA. To further understand the bacterial population dynamics, important RISA phylotypes were isolated and identified through 16S RNA gene sequencing. The parallel MEBPR and CEBPR systems developed bacterial communities that were distinct. The CEBPR community appeared to exhibit greater diversity, and this may have been the primary reason why the CEBPR treatment train demonstrated superior functional stability relative to the MEBPR counterpart. Moreover, the more diverse bacterial population apparent in the CEBPR system was observed to be more dynamic than that of the MEBPR process. Several RISA bands were found to be characteristic of either the membrane or conventional biological system. In particular, the MEBPR configuration appeared to be selective for the slow-growing organism Magnospira bakii and for the foam-associated Microthrix parvicella and Gordonia sp., while gravity separation led to the washout of M. parvicella. In both pilot trains, sequence analysis confirmed the presence of EBPR-related organisms such as Accumulibacter phosphatis. The survey of the CEBPR system also revealed many uncultured organisms that have not been well characterized. The study demonstrated that a simple replacement of a secondary clarifier with membrane solids-liquid separation is sufficient to shift the composition of an activated sludge microbial community significantly.},
  doi      = {https://doi.org/10.1016/j.watres.2010.01.024},
  keywords = {Biological phosphorus removal, Membrane bioreactor, Bacterial populations, Dynamics, UCT process, Wastewater treatment},
  url      = {http://www.sciencedirect.com/science/article/pii/S0043135410000588},
}

@Article{Ling1996,
  author   = {Tok Wang Ling and Cheng Hian Goh and Mong Li Lee},
  title    = {Extending classical functional dependencies for physical database design},
  journal  = {Information and Software Technology},
  year     = {1996},
  volume   = {38},
  number   = {9},
  pages    = {601 - 608},
  issn     = {0950-5849},
  abstract = {Traditionally, database design activities are partitioned into distinct phases in which a logical design phase precedes physical database design. The objective of the logical design step is to eliminate redundancies and updating anomalies using the notion of data dependencies, while leaving the physical design step to consider how the database schema may be restructured to provide more efficient access. We argue in this paper that the separation of these two steps often results in physical database design not being able to benefit from knowledge of the semantics of data captured in the earlier phases of the database design life cycle. As a step towards overcoming this problem, we demonstrate how classical functional dependencies can be extended to capture data semantics relevant to the design of database schemas which are more desirable from the efficiency point of view. This is accomplished via the introduction of strong and weak functional dependencies. Strong functional dependencies indicate that the relationship between two attributes almost never change. This concept allows us to have controlled redundancies which is beneficial as it can reduce dramatically the effort needed to access frequently needed information. Weak functional dependencies capture the common situations in real life where classical functional dependencies between two attributes hold in general but may be violated in rare cases. Three new normal forms — the relaxed 3NF, replicated 3NF and relax-replicated 3NF, induced by the strong and weak functional dependencies, provide a theoretical framework for designing database schemas which are more efficient and practical, while not compromising the integrity of the underlying database. That is, relations in these new normal forms will not suffer from undesirable updating anomalies.},
  doi      = {https://doi.org/10.1016/0950-5849(96)01097-X},
  keywords = {Database design, Functional dependencies, Normal forms},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499601097X},
}

@InCollection{Chiacchiera2009,
  author    = {Fulvio Chiacchiera and Cristiano Simone},
  title     = {Chapter 15 Signal-Dependent Control of Autophagy-Related Gene Expression},
  booktitle = {Autophagy in Disease and Clinical Applications, Part C},
  publisher = {Academic Press},
  year      = {2009},
  volume    = {453},
  series    = {Methods in Enzymology},
  pages     = {305 - 324},
  abstract  = {Several tumors arise from deregulated signaling pathways leading to increased proliferation and impairment of differentiation. To bypass endogenous control mechanisms and to survive the environmental stress associated with increased growth, tumor cells acquire a plethora of modifications that ultimately tend to down-regulate the ability to undergo apoptosis and exacerbate prosurvival mechanisms. Autophagy is an evolutionarily conserved mechanism through which cells recycle essential molecular constituents or eliminate damaged organelles under stress conditions imposed by nutrients or growth factors deprivation. As such, autophagy acts as a prosurvival mechanism for cancer cells. However, when overactivated, autophagy could also represent a cell death mechanism acting through self-cannibalization. Therefore, understanding the various signaling pathways that regulate autophagy could be of extreme importance. Indeed, the identification of specific molecular targets amenable to pharmacological manipulation to induce cancer cell self-cannibalization could represent a promising approach to treat apoptosis-resistant tumors.},
  doi       = {https://doi.org/10.1016/S0076-6879(08)04015-9},
  issn      = {0076-6879},
  url       = {http://www.sciencedirect.com/science/article/pii/S0076687908040159},
}

@Article{Carugo2002,
  author   = {Oliviero Carugo and Sándor Pongor},
  title    = {The evolution of structural databases},
  journal  = {Trends in Biotechnology},
  year     = {2002},
  volume   = {20},
  number   = {12},
  pages    = {498 - 501},
  issn     = {0167-7799},
  abstract = {Starting with the Protein Data Bank (PDB) as a common ancestor, the evolution of structural databases has been driven by the rapprochement of the structural world and the practical applications. The result is an impressive number of secondary structural databases that is welcomed by structural biologists and bioinformaticians but runs the risk of producing an embarrassment of riches among non-specialist users. Given that any profit depends on the number of customers, efficient interfaces between many structural data banks must be available to make their contents easily accessible. Increasing the information content of central structural repositories might be the best way to guide users through the many, sometimes overlapping databases.},
  doi      = {https://doi.org/10.1016/S0167-7799(02)02082-6},
  keywords = {structural databases, PDB, Protein Data Bank, 3D structural database},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167779902020826},
}

@Article{Buehrer1996,
  author   = {Daniel J. Buehrer and Yang-Wen Fan},
  title    = {SL-trees: An indexing structure for object-oriented data bases},
  journal  = {Journal of Systems and Software},
  year     = {1996},
  volume   = {32},
  number   = {3},
  pages    = {237 - 249},
  issn     = {0164-1212},
  abstract = {In an object-oriented data base, the scope of a query is often a class hierarchy rooted at a particular class. That is, a query may either request instances chosen only from a root class, or it may request instances which are chosen from any subclass of the root class. So, the data base indexing method must support efficient retrieval of instances from either a single class or from a hierarchy of classes. In this paper, we propose a new hierarchical indexing structure called the SL-tree. Like previous indexing methods for data bases, SL-trees are based on B+ trees. Like H-trees, SL-trees have a B+-like tree for each class of a class hierarchy, and these trees are nested according to their superclass-subclass relationships. However, the SL-trees have far fewer links than H-trees, leading to better performance because of lower storage overhead and fewer link updates. The simulation results indicate that SL-trees need very few nesting pointers and that SL-trees are a simple and efficient indexing structure for object-oriented data bases.},
  doi      = {https://doi.org/10.1016/0164-1212(95)00127-1},
  url      = {http://www.sciencedirect.com/science/article/pii/0164121295001271},
}

@Article{Seng2012,
  author   = {Jia-Lang Seng and Zon Wong},
  title    = {An intelligent XML-based multidimensional data cube exchange},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {8},
  pages    = {7371 - 7390},
  issn     = {0957-4174},
  abstract = {Motivated by the globalization trend and Internet speed competition, enterprise nowadays often divides into many departments or organizations or even merges with other companies that located in different regions to bring up the competency and reaction ability. As a result, there are a number of data warehouse systems in a geographically-distributed enterprise. To meet the distributed decision-making requirements, the data in different data warehouses is addressed to enable data exchange and integration. Therefore, an open, vendor-independent, and efficient data exchange standard to transfer data between data warehouses over the Internet is an important issue. However, current solutions for cross-warehouse data exchange employ only approaches either based on records or transferring plain-text files, which are neither adequate nor efficient. In this research, issues on multidimensional data exchange are studied and an Intelligent XML-based multidimensional data exchange model is developed. In addition, a generic-construct-based approach is proposed to enable many-to-many systematic mapping between distributed data warehouses, introducing a consistent and unique standard exchange format. Based on the transformation model we develop between multidimensional data model and XML data model, and enhanced by the multidimensional metadata management mechanism proposed in this research, a general-purpose intelligent XML-based multidimensional data exchange process over web is facilitated efficiently and improved in quality. Moreover, we develop an intelligent XML-based prototype system to exchange multidimensional data, which shows that the proposed multidimensional data exchange model is feasible, and the multidimensional data exchange process is more systematic and efficient using metadata.},
  doi      = {https://doi.org/10.1016/j.eswa.2012.01.069},
  keywords = {Data warehouse, Multidimensional data cube, Intelligent metadata management, XML, Data exchange, Generic construct, Mapping and transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412000826},
}

@InCollection{Martin1990,
  author    = {J. Aguilar Martin and S. Sawadogo},
  title     = {FUZZY LINGUISTIC VARIABLES IN THE EXPERT SUPERVISION OF CONTROL SYSTEMS},
  booktitle = {Advanced Information Processing in Automatic Control (AIPAC '89)},
  publisher = {Pergamon},
  year      = {1990},
  editor    = {R. HUSSON},
  series    = {IFAC Symposia Series},
  pages     = {177 - 179},
  address   = {Oxford},
  isbn      = {978-0-08-037034-7},
  abstract  = {This is a general view of the role that should play Artificial Intelligence techniques as Expert Systems, in Supervision of Automatic Control. The efficiency of this approach is dependent of the possibilities of numerical symbolic translation in order to take advantage of the abilities of declarative programms of I.A. The use of trapezoidal fuzzy membership function connected with linguistic variables is a way of solution for this problem. An example of supervision of a closed loop system performing the so called Adaptive Control scheme is described in order to illustrate the above statements.},
  doi       = {https://doi.org/10.1016/B978-0-08-037034-7.50032-8},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080370347500328},
}

@Article{Breunig1999,
  author   = {M. Breunig and A.B. Cremers and H.-J. Götze and S. Schmidt and R. Seidemann and S. Shumilov and A. Siehl},
  title    = {First steps towards an interoperable GIS — an example from Southern Lower Saxony —},
  journal  = {Physics and Chemistry of the Earth, Part A: Solid Earth and Geodesy},
  year     = {1999},
  volume   = {24},
  number   = {3},
  pages    = {179 - 189},
  issn     = {1464-1895},
  abstract = {In geosciences the necessity of combining geological and geophysical information as well as applying tools for designing 3D geological and geophysical models is well accepted. Nevertheless, in most cases this demand is only put into practice by file transfer between the applications. The creation of a common 3D model is complicated by the inflexible handling of new and reinterpreted data and by changing applications, heterogeneous operating systems and/or hardware platforms. We present a new approach for a component based GIS which is coupled with an object oriented database management system. The original data, as well as the derived data and the 3D models, are stored in the extensible database. Geological and geophysical 3D modeling tools have direct access to the database via the Common Object Request Broker Architecture (COREA). By this means, we obtained both the integration of the software components and independence from changing software applications and changing platforms, finally resulting in an interoperable 3D GIS.},
  doi      = {https://doi.org/10.1016/S1464-1895(99)00016-2},
  url      = {http://www.sciencedirect.com/science/article/pii/S1464189599000162},
}

@Article{al-Qaimari1994,
  author   = {Ghassan al-Qaimari and Norman W Paton and Alistair C Kilgour},
  title    = {Visualizing advanced data modelling constructs},
  journal  = {Information and Software Technology},
  year     = {1994},
  volume   = {36},
  number   = {10},
  pages    = {597 - 605},
  issn     = {0950-5849},
  abstract = {Semantic data modelling constructs, such as relationships, composite objects and versions, are used to represent knowledge explicitly in object-oriented databases. Such constructs increase the ability of the database to capture not only the structural aspects of data, as in traditional databases, but also the meaning of data. The modelling process can be further enhanced by supporting expressive visual representations of the constructs in a direct-manipulation interface. This paper focuses upon the development of effective visualizations for such constructs by presenting a range of sample visualizations, outlining how they can be prototyped rapidly in an integrated interface development environment, and describing how evaluation techniques can be used to assess alternative proposals.},
  doi      = {https://doi.org/10.1016/0950-5849(94)90019-1},
  keywords = {data modelling, visualization, interface evaluation, object-oriented databases},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584994900191},
}

@Article{Missaoui1998,
  author   = {R. Missaoui and R. Godin and H. Sahraoui},
  title    = {Migrating to an object-oriented database using semantic clustering and transformation rules},
  journal  = {Data \& Knowledge Engineering},
  year     = {1998},
  volume   = {27},
  number   = {1},
  pages    = {97 - 113},
  issn     = {0169-023X},
  abstract = {This paper presents a methodology for handling an important step of database migration. The methodology is based on a set of techniques: (i) semantic clustering, (ii) metamodeling, and (iii) knowledge-based schema transformation. Semantic clustering (i.e., grouping based on semantic cohesion) is mainly used to facilitate the process of translating an extended entity relationship schema into a schema of complex objects. Meta modeling is used to define the data models involved in the process of schema transformation. Finally, transformation rules are defined and used for mapping a schema from a source model into a schema expressed in a target model. In this paper, we limit ourselves to the mapping of an extended entity relationship diagram into an object-oriented database schema using the object model supported by the ODMG standard.},
  doi      = {https://doi.org/10.1016/S0169-023X(98)00004-4},
  keywords = {Metamodeling, Database migration, Semantic clustering, Object-oriented databases, Schema transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X98000044},
}

@Article{Ottaviano2007,
  author   = {Gianmarco I.P. Ottaviano and G. Alfredo Minerva},
  title    = {Thirty-five years of R(S)UE: A retrospective},
  journal  = {Regional Science and Urban Economics},
  year     = {2007},
  volume   = {37},
  number   = {4},
  pages    = {434 - 449},
  issn     = {0166-0462},
  note     = {Regional Science and Urban Economics at 35. A Retrospective/Prospective Special Issue},
  abstract = {The aim of this paper is to provide a quantitative retrospective of thirty-five years of Regional Science and Urban Economics, from Volume 1 (1971) to Volume 35 (2005). In doing so, the paper follows the intellectual development of the journal through its different editors by comparing their editorial statements with the actual changes in the characteristics of articles they selected for publication.},
  doi      = {https://doi.org/10.1016/j.regsciurbeco.2007.01.007},
  keywords = {Regional economics, Spatial econometrics, Urban economics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166046207000282},
}

@InCollection{Johnston2010,
  author    = {Tom Johnston and Randall Weis},
  title     = {5 - The Core Concepts of Asserted Versioning},
  booktitle = {Managing Time in Relational Databases},
  publisher = {Morgan Kaufmann},
  year      = {2010},
  editor    = {Tom Johnston and Randall Weis},
  pages     = {95 - 118},
  address   = {Boston},
  isbn      = {978-0-12-375041-9},
  abstract  = {Publisher Summary
This chapter presents the conceptual foundations of Asserted Versioning. It defines the core concepts of objects, episodes, versions, and assertions. Every row in an asserted version table is the assertion of a version of an episode of an object. With temporal tables an object may be represented by any number of rows; for these tables, then, the entity integrity constraint must be modified. That modification results in temporal entity integrity. For temporal tables, the corresponding constraint is temporal entity integrity (TEI). In enforcing the rule that no two versions of the same object may conflict, TEI is analogous to conventional entity integrity. The only physical tables managed by Asserted Versioning are bitemporal tables. But on the basis of those bitemporal tables, Asserted Versioning also manages two unitemporal views, and one nontemporal view. Asserted Versioning defines one bitemporal schema, and manages only that one kind of physical table. Those tables support unitemporal versions and unitemporal assertions, but it supports them as views. By means of views, those tables also support conventional, nontemporal tables.},
  doi       = {https://doi.org/10.1016/B978-0-12-375041-9.00005-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123750419000054},
}

@Article{Bouguettaya1995,
  author   = {Athman Bouguettaya and Mike Papazoglou and Roger King},
  title    = {On building a hyperdistributed database},
  journal  = {Information Systems},
  year     = {1995},
  volume   = {20},
  number   = {7},
  pages    = {557 - 577},
  issn     = {0306-4379},
  abstract = {Sharing data among disparate databases has so far mostly been achieved through some form of ad-hoc schema integration. This approach becomes less tractable as the number of participating database increases. Therefore, the complexity of making autonomous heterogeneous databases interoperate is dependent on adequately addressing the autonomy and heterogeneity issues. In this paper, we describe a prototype that implements an approach which addresses these issues in the context of large multidatabase systems. In particular, we describe a scheme that builds a Hyperdistributed Database using a two-staged approach. We also describe how conglomerations of databases are formed, modified, and evolved.},
  doi      = {https://doi.org/10.1016/0306-4379(95)00030-8},
  keywords = {Multidatabases, Federated Databases, Interoperable Heterogeneous and Autonomous Databases, Query Languages},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437995000308},
}

@Article{Palopoli1994,
  author   = {Luigi Palopoli and Riccardo Torlone},
  title    = {A rule-based update language for complex objects with identity},
  journal  = {Data \& Knowledge Engineering},
  year     = {1994},
  volume   = {13},
  number   = {1},
  pages    = {67 - 96},
  issn     = {0169-023X},
  abstract = {This paper deals with the definition of a rule-based update language for complex object databases. Several update operators are introduced, which correspond to basic manipulations of data entities with object identity. These operators can be used in rule bodies for specifying complex update transactions. The syntax and the semantics of the language is given in the framework of CO, a simple data model for complex objects with identity. An interpreter of the language is defined and its correctness is discussed. Finally, the issue of maintaining database consistency with respect to several kinds of integrity constraints is illustrated.},
  doi      = {https://doi.org/10.1016/0169-023X(94)90047-7},
  keywords = {Complex objects, Update operations, Logic-based languages for databases, Innegrity constraint management},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X94900477},
}

@Article{Greenhill2000,
  author   = {Stewart Greenhill and Svetha Venkatesh},
  title    = {Semantic data modelling and visualisation using Noetica},
  journal  = {Data \& Knowledge Engineering},
  year     = {2000},
  volume   = {33},
  number   = {3},
  pages    = {241 - 276},
  issn     = {0169-023X},
  abstract = {Noetica is a tool for structuring knowledge about concepts and the relationships between them. It differs from typical information systems in that the knowledge it represents is abstract, highly connected, and includes meta-knowledge (knowledge about knowledge). Noetica represents knowledge using a strongly typed graph data model. By providing a rich type system it is possible to represent conceptual information using formalised structures. A class hierarchy provides a basic classification for all objects. This allows for a consistency of representation that is not often found in “free” semantic networks, and gives the ability to easily extend a knowledge model while retaining its semantics. Visualisation and query tools are provided for this data model. Visualisation can be used to explore complete sets of link-classes, show paths while navigating through the database, or visualise the results of queries. Noetica supports goal-directed queries (a series of user-supplied goals that the system attempts to satisfy in sequence) and pathfinding queries (where the system finds relationships between objects in the database by following links).},
  doi      = {https://doi.org/10.1016/S0169-023X(00)00003-3},
  keywords = {Knowledge representation, Graph traversal, Data visualisation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X00000033},
}

@Article{Hertogh2008,
  author   = {Benoît De Hertogh and Leïla Lahlimi and Christophe Lambert and Jean-Jacques Letesson and Eric Depiereux},
  title    = {Design and implementation of a database for Brucella melitensis genome annotation},
  journal  = {Veterinary Microbiology},
  year     = {2008},
  volume   = {127},
  number   = {3},
  pages    = {369 - 378},
  issn     = {0378-1135},
  abstract = {The genome sequences of three Brucella biovars and of some species close to Brucella sp. have become available, leading to new relationship analysis. Moreover, the automatic genome annotation of the pathogenic bacteria Brucella melitensis has been manually corrected by a consortium of experts, leading to 899 modifications of start sites predictions among the 3198 open reading frames (ORFs) examined. This new annotation, coupled with the results of automatic annotation tools of the complete genome sequences of the B. melitensis genome (including BLASTs to 9 genomes close to Brucella), provides numerous data sets related to predicted functions, biochemical properties and phylogenic comparisons. To made these results available, αPAGe, a functional auto-updatable database of the corrected sequence genome of B. melitensis, has been built, using the entity-relationship (ER) approach and a multi-purpose database structure. A friendly graphical user interface has been designed, and users can carry out different kinds of information by three levels of queries: (1) the basic search use the classical keywords or sequence identifiers; (2) the original advanced search engine allows to combine (by using logical operators) numerous criteria: (a) keywords (textual comparison) related to the pCDS's function, family domains and cellular localization; (b) physico-chemical characteristics (numerical comparison) such as isoelectric point or molecular weight and structural criteria such as the nucleic length or the number of transmembrane helix (TMH); (c) similarity scores with Escherichia coli and 10 species phylogenetically close to B. melitensis; (3) complex queries can be performed by using a SQL field, which allows all queries respecting the database's structure. The database is publicly available through a Web server at the following url: http://www.fundp.ac.be/urbm/bioinfo/aPAGe.},
  doi      = {https://doi.org/10.1016/j.vetmic.2007.09.010},
  keywords = {, Relational, Database, Annotation, DB-MAIN},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378113507004336},
}

@Article{Mea1996,
  author   = {Vincenzo Della Mea and Carlo Alberto Beltrami and Vito Roberto and Davide Brunato},
  title    = {HTML generation and semantic markup for telepathology},
  journal  = {Computer Networks and ISDN Systems},
  year     = {1996},
  volume   = {28},
  number   = {7},
  pages    = {1085 - 1094},
  issn     = {0169-7552},
  note     = {Proceedings of the Fifth International World Wide Web Conference 6-10 May 1996},
  abstract = {The paper presents a new strategy for the authoring of hypermedia documents; describes an HTML generator called HistMaker, and its application to the domain of Anatomic Pathology. A simple extension to HTML is presented, whose aim is introducing a general-purpose grouping construct to allow the semantic markup of hierarchically structured hypermedia documents. Such a structural information can be used for an effective authoring, browsing and searching of documents. The authoring tool HistMaker is introduced on the basis of a model of a pathologic case; its implementation and practical results are also discussed.},
  doi      = {https://doi.org/10.1016/0169-7552(96)00049-9},
  keywords = {Structured hypermedia},
  url      = {http://www.sciencedirect.com/science/article/pii/0169755296000499},
}

@Article{Wang1997,
  author   = {Huaiqing Wang},
  title    = {LearnOOP: An active agent-based educational system},
  journal  = {Expert Systems with Applications},
  year     = {1997},
  volume   = {12},
  number   = {2},
  pages    = {153 - 162},
  issn     = {0957-4174},
  abstract = {Computer-based learning systems have become increasingly important. However, such self-study systems still lack the aspect of a real-life classroom and suffer due to their passive nature. They are often dull and plodding. This paper describes the LearnOOP, an active distance learning system for learning object-oriented programming. LearnOOP has several features: (1) active, i.e. the system is like a human teacher, who monitors the student's activities and gives guidance when necessary, (2) interactive, i.e. the student can ask the system questions and the system returns intelligent answers and (3) collaborative and co-operative, i.e. there are facilities for students and teachers to collaborate and co-operate with each other. In this paper, a powerful conceptual framework has been presented using a knowledge representation language, Telos, to facilitate the formal thinking about the tools and techniques used in designing, developing and using co-operative computer-based learning systems. The architecture of LearnOOP consists of three layers, the educational agent layer, the knowledge server layer and the repository layer. By deploying repository technology, the architecture provides a means for multiple intelligent educational agents to conduct active interactions between students and the system, between students and teachers and among students. The LearnOOP prototype demonstrates the need for multiple intelligent agent co-operation and collaboration in future active and co-operative learning systems and illustrates the feasibility for practical future design and deployment of such systems.},
  doi      = {https://doi.org/10.1016/S0957-4174(96)00090-5},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417496000905},
}

@Article{Lee2000,
  author   = {Jae-Nam Lee and Ron Chi-Wai Kwok},
  title    = {A fuzzy GSS framework for organizational knowledge acquisition},
  journal  = {International Journal of Information Management},
  year     = {2000},
  volume   = {20},
  number   = {5},
  pages    = {383 - 398},
  issn     = {0268-4012},
  abstract = {Although the concept of viewing knowledge as a critical resource has been widely accepted in prior studies, it is not fully understood how to acquire available knowledge in order to improve organizational effectiveness. However, it is sure that organizational knowledge management should pursue the achievement of the business goal by delivering relevant and useful information to the right person at the right time. Group Support Systems (GSS) can play an important role in transfering scattered information into meaningful business knowledge for supporting strategic corporate decision-making. This study proposes a fuzzy GSS framework for acquiring workgroup knowledge from individual memory and aggregating workgroup knowledge into organizational knowledge. This study also proposes an architecture to support the fuzzy GSS framework. The architecture consists of user agents, information management agents, and a fuzzy model manager. To illustrate how the fuzzy GSS framework can be used to support the whole process of organizational knowledge acquisition, an Internet-based GSS was developed and applied in a marketing decision process. It showed that the framework was effective for acquiring organizational knowledge.},
  doi      = {https://doi.org/10.1016/S0268-4012(00)00030-X},
  keywords = {Group support systems, Knowledge acquisition, Knowledge management, Organizational learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S026840120000030X},
}

@Article{Skyt2003,
  author   = {Janne Skyt and Christian S. Jensen and Leo Mark},
  title    = {A foundation for vacuuming temporal databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {2003},
  volume   = {44},
  number   = {1},
  pages    = {1 - 29},
  issn     = {0169-023X},
  abstract = {A wide range of real-world database applications, including financial and medical applications, are faced with accountability and traceability requirements. These requirements lead to the replacement of the usual update-in-place policy by an append-only policy that retain all previous states in the database. This policy result in so-called transaction-time databases which are ever-growing. A variety of physical storage structures and indexing techniques as well as query languages have been proposed for transaction-time databases, but the support for physical removal of data, termed vacuuming, has only received little attention. Such vacuuming is called for by, e.g., the laws of many countries and the policies of many businesses. Although necessary, with vacuuming, the database’s perfect recollection of the past may be compromised via, e.g., selective removal of records pertaining to past states. This paper provides a semantic foundation for the vacuuming of transaction-time databases. The main focus is to establish a foundation for the correct processing of queries and updates against vacuumed databases. However, options for user, application, and database interactions in response to queries and updates against vacuumed data are also outlined.},
  doi      = {https://doi.org/10.1016/S0169-023X(02)00060-5},
  keywords = {Vacuuming, Logical deletion, Physical removal, Transaction-time databases},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X02000605},
}

@Article{Yu2015,
  author   = {Qi Yu and Ying Ding and Min Song and Sungjeon Song and Jianhua Liu and Bin Zhang},
  title    = {Tracing database usage: Detecting main paths in database link networks},
  journal  = {Journal of Informetrics},
  year     = {2015},
  volume   = {9},
  number   = {1},
  pages    = {1 - 15},
  issn     = {1751-1577},
  abstract = {This paper presents a database link network to measure the impact of databases on biological research. To this end, we used the 20,861 full-text articles from PubMed Central in the field of Bioinformatics. We then extracted databases from the methodology sections of these articles and their references. The list of databases was built with The 2013 Nucleic Acids Research Molecular Biology Database Collection (available online), which includes 1512 databases. The database link network was constructed from sets of pairs of databases mentioned in the methodology sections of full-text PubMed Central articles. The edges of the database link network represent the link relationships between two databases. The weight of each edge is determined either by the link frequency of the two databases (i.e., in the link-weighted database link network) or the topic similarity between two databases (i.e., in the similarity-weighted database link network). With the database link network, we analyzed the topological structure and main paths of the database link network to trace the usage, connection, and evolution of databases. We also conducted content analysis by comparing content similarities among the papers citing databases.},
  doi      = {https://doi.org/10.1016/j.joi.2014.10.002},
  keywords = {Database link network, Main path, Bibliometrics, Bioinformatrics},
  url      = {http://www.sciencedirect.com/science/article/pii/S1751157714000947},
}

@Article{Qiu2007,
  author   = {Z.M. Qiu and Y.S. Wong},
  title    = {Dynamic workflow change in PDM systems},
  journal  = {Computers in Industry},
  year     = {2007},
  volume   = {58},
  number   = {5},
  pages    = {453 - 463},
  issn     = {0166-3615},
  abstract = {Current manufacturing industry requires product data management (PDM) for efficient product development and production. As an important part of effective PDM solutions, workflow management facilitates creating and executing workflow so as to streamline business processes. Unfortunately, existing workflow management solutions are designed to handle static business processes; when a workflow change occurs, these solutions usually stop the affected workflow completely and start the modified one from scratch. This over-simplified approach leads to re-execution of nodes whose work have been lost due to the restart process. This paper proposes an approach to facilitate efficient dynamic workflow change by minimising repetitive execution of finished workflow nodes. This approach also address the data integrity issue by managing various workflow data such as node properties and scripts. A case study has been carried out in a PDM system to illustrate the potential application of the approach.},
  doi      = {https://doi.org/10.1016/j.compind.2006.09.014},
  keywords = {Workflow, Dynamic change, PDM, Data integrity, Case study, Implementation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361506001631},
}

@Article{Kempkens2000,
  author   = {R Kempkens and P Rösch and L Scott and J Zettel},
  title    = {A multi-layer multi-view architecture for software engineering environments},
  journal  = {Information and Software Technology},
  year     = {2000},
  volume   = {42},
  number   = {2},
  pages    = {141 - 149},
  issn     = {0950-5849},
  abstract = {This paper presents our experience with constructing a multi-view environment for software process modeling. The environment (Spearmint) is designed to support the capture, analysis and maintenance of large, complex software process models. The environment uses multiple views to handle the inherent complexity of real software processes and to model the fact that different people within organizations have different, sometimes conflicting, views of the same process. Spearmint also supports multiple display representations for process information and addresses requirements for good maintainability, extensibility and performance. Our experience has been that a layered architecture that makes a clear separation of concerns in the application is invaluable for implementing such a multi-view tool. In this paper, we describe some of the experiences we have had with designing and implementing such an architecture.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00086-5},
  keywords = {Project support environment, Software engineering environment, Process modeling, Multi-view architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000865},
}

@Article{Valencia2005,
  author   = {Alfonso Valencia},
  title    = {Automatic annotation of protein function},
  journal  = {Current Opinion in Structural Biology},
  year     = {2005},
  volume   = {15},
  number   = {3},
  pages    = {267 - 274},
  issn     = {0959-440X},
  note     = {Sequences and topology/Nucleic acids},
  abstract = {The annotation of protein function at genomic scale is essential for day-to-day work in biology and for any systematic approach to the modeling of biological systems. Currently, functional annotation is essentially based on the expansion of the relatively small number of experimentally determined functions to large collections of proteins. The task of systematic annotation faces formidable practical problems related to the accuracy of the input experimental information, the reliability of current systems for transferring information between related sequences, and the reproducibility of the links between database information and the original experiments reported in publications. These technical difficulties merely lie on the surface of the deeper problem of the evolution of protein function in the context of protein sequences and structures. Given the mixture of technical and scientific challenges, it is not surprising that errors are introduced, and expanded, in database annotations. In this situation, a more realistic option is the development of a reliability index for database annotations, instead of depending exclusively on efforts to correct databases. Several groups have attempted to compare the database annotations of similar proteins, which constitutes the first steps toward the calibration of the relationship between sequence and annotation space.},
  doi      = {https://doi.org/10.1016/j.sbi.2005.05.010},
  url      = {http://www.sciencedirect.com/science/article/pii/S0959440X05000941},
}

@Article{1988b,
  title   = {Analytic aspects of single-subject trials: Robin Roberts, Gordon Guyatt, and Jana Keller McMaster University, Hamilton, Ontario, Canada(69)},
  journal = {Controlled Clinical Trials},
  year    = {1988},
  volume  = {9},
  number  = {3},
  pages   = {262},
  issn    = {0197-2456},
  doi     = {https://doi.org/10.1016/0197-2456(88)90133-X},
  url     = {http://www.sciencedirect.com/science/article/pii/019724568890133X},
}

@Article{Ponesakki2017,
  author   = {Vasanthakumar Ponesakki and Sayan Paul and Dinesh Kumar Sudalai Mani and Veeraragavan Rajendiran and Paulkumar Kanniah and Sudhakar Sivasubramaniam},
  title    = {Annotation of nerve cord transcriptome in earthworm Eisenia fetida},
  journal  = {Genomics Data},
  year     = {2017},
  volume   = {14},
  pages    = {91 - 105},
  issn     = {2213-5960},
  abstract = {In annelid worms, the nerve cord serves as a crucial organ to control the sensory and behavioral physiology. The inadequate genome resource of earthworms has prioritized the comprehensive analysis of their transcriptome dataset to monitor the genes express in the nerve cord and predict their role in the neurotransmission and sensory perception of the species. The present study focuses on identifying the potential transcripts and predicting their functional features by annotating the transcriptome dataset of nerve cord tissues prepared by Gong et al., 2010 from the earthworm Eisenia fetida. Totally 9762 transcripts were successfully annotated against the NCBI nr database using the BLASTX algorithm and among them 7680 transcripts were assigned to a total of 44,354 GO terms. The conserve domain analysis indicated the over representation of P-loop NTPase domain and calcium binding EF-hand domain. The COG functional annotation classified 5860 transcript sequences into 25 functional categories. Further, 4502 contig sequences were found to map with 124 KEGG pathways. The annotated contig dataset exhibited 22 crucial neuropeptides having considerable matches to the marine annelid Platynereis dumerilii, suggesting their possible role in neurotransmission and neuromodulation. In addition, 108 human stem cell marker homologs were identified including the crucial epigenetic regulators, transcriptional repressors and cell cycle regulators, which may contribute to the neuronal and segmental regeneration. The complete functional annotation of this nerve cord transcriptome can be further utilized to interpret genetic and molecular mechanisms associated with neuronal development, nervous system regeneration and nerve cord function.},
  doi      = {https://doi.org/10.1016/j.gdata.2017.10.002},
  keywords = {Transcriptome, Nerve cord, , Annotation},
  url      = {http://www.sciencedirect.com/science/article/pii/S2213596017301216},
}

@InCollection{Eckman2003,
  author    = {Barbara A. Eckman},
  title     = {CHAPTER 3 - A Practitioner's Guide to Data Management and Data Integration in Bioinformatics},
  booktitle = {Bioinformatics},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Zoé Lacroix and Terence Critchlow},
  series    = {The Morgan Kaufmann Series in Multimedia Information and Systems},
  pages     = {35 - 73},
  address   = {Burlington},
  isbn      = {978-1-55860-829-0},
  doi       = {https://doi.org/10.1016/B978-155860829-0/50005-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978155860829050005X},
}

@Article{Hodges1993,
  author   = {Robert Hodges},
  title    = {The move of IRDS repository standards toward object orientation},
  journal  = {Computer Standards \& Interfaces},
  year     = {1993},
  volume   = {15},
  number   = {2},
  pages    = {307 - 318},
  issn     = {0920-5489},
  note     = {Object-Oriented Reference Models},
  abstract = {Standards for repository systems are in the midst of a move toward an object-oriented foundation. Repository systems need object management concepts like inheritance, encapsulating operations, and name overloading to function with the flexibility and extensibility required to control and integrate information at an enterprise level. Object orientation provides the necessary basis for future evolution of repository standards to knowledge management solutions that support a full and rigorous definition of the meaning of an enterprise's information and business policies.},
  doi      = {https://doi.org/10.1016/0920-5489(93)90017-L},
  keywords = {IRDS, repository, services architecture, object services},
  url      = {http://www.sciencedirect.com/science/article/pii/092054899390017L},
}

@Article{Ding2002,
  author   = {Ying Ding and Dieter Fensel and Michel Klein and Borys Omelayenko},
  title    = {The semantic web: yet another hip?},
  journal  = {Data \& Knowledge Engineering},
  year     = {2002},
  volume   = {41},
  number   = {2},
  pages    = {205 - 227},
  issn     = {0169-023X},
  abstract = {Currently, computers are changing from single, isolated devices into entry points to a worldwide network of information exchange and business transactions called the World Wide Web (WWW). For this reason, support in data, information, and knowledge exchange has become a key issue in current computer technology. The success of the WWW has made it increasingly difficult to find, access, present, and maintain the information required by a wide variety of users. In response to this problem, many new research initiatives and commercial enterprises have been set up to enrich available information with machine processable semantics. This semantic web will provide intelligent access to heterogeneous, distributed information, enabling software products (agents) to mediate between user needs and the information sources available. This paper summarizes ongoing research in the area of the semantic web, focusing especially on ontology technology.},
  doi      = {https://doi.org/10.1016/S0169-023X(02)00041-1},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X02000411},
}

@Article{Amor1999,
  author   = {Robert W. Amor and John G. Hosking and Warwick B. Mugridge},
  title    = {ICAtect-II: a framework for the integration of building design tools},
  journal  = {Automation in Construction},
  year     = {1999},
  volume   = {8},
  number   = {3},
  pages    = {277 - 289},
  issn     = {0926-5805},
  abstract = {The development of a system capable of integrating a range of building design tools poses many challenges. Our framework for integrating design tools provides a structured approach, allowing individual parts to be developed independently. In this paper, we describe the overall framework and suggest a method for modeling and implementing each portion of the framework. Furthermore, we illustrate how such a system can integrate several design tools and be realized as a functional design system.},
  doi      = {https://doi.org/10.1016/S0926-5805(98)00077-6},
  keywords = {Product modeling, Component modeling, Model mapping},
  url      = {http://www.sciencedirect.com/science/article/pii/S0926580598000776},
}

@Article{1986,
  title   = {List of contents},
  journal = {Computers \& Operations Research},
  year    = {1986},
  volume  = {13},
  number  = {6},
  pages   = {vii - xii},
  issn    = {0305-0548},
  doi     = {https://doi.org/10.1016/0305-0548(86)90081-X},
  url     = {http://www.sciencedirect.com/science/article/pii/030505488690081X},
}

@Article{Turk2001,
  author   = {Žiga Turk},
  title    = {Phenomenologial foundations of conceptual product modelling in architecture, engineering and construction},
  journal  = {Artificial Intelligence in Engineering},
  year     = {2001},
  volume   = {15},
  number   = {2},
  pages    = {83 - 92},
  issn     = {0954-1810},
  abstract = {The ultimate goal of conceptual modelling in architecture, engineering and construction (AEC) has been to define the data structures that could be used to describe the entire built environment through all its life cycle stages — from inception and design to demolition. In spite of the magnitude and complexity of this task, the theoretical foundations of modelling received little attention. In this paper, the theoretical foundations of the traditional modelling approaches are questioned using phenomenology and hermeneutics as philosophical base. The author exposes the difference between the remodelling of some existing models, the modelling of physical objects and the modelling of psychical, intentional objects. The author concludes that AEC or building product and process models do not model objective reality but the modeller's partial understanding of that reality. Therefore, several correct but different models may and should exist. Future software architectures in AEC should not be built on a unified, centralized model but, on a combination of models, which may not be standardised but whose schemas are encoded in a standard manner.},
  doi      = {https://doi.org/10.1016/S0954-1810(01)00008-5},
  keywords = {Product modelling, Conceptual modelling, Computer integrated construction, Phenomenology, Hermeneutics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0954181001000085},
}

@Article{Brun1997,
  author   = {Rene Brun and Fons Rademakers},
  title    = {ROOT — An object oriented data analysis framework},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {1997},
  volume   = {389},
  number   = {1},
  pages    = {81 - 86},
  issn     = {0168-9002},
  note     = {New Computing Techniques in Physics Research V},
  abstract = {The ROOT system in an Object Oriented framework for large scale data analysis. ROOT written in C++, contains, among others, an efficient hierarchical OO database, a C++ interpreter, advanced statistical analysis (multi-dimensional histogramming, fitting, minimization, cluster finding algorithms) and visualization tools. The user interacts with ROOT via a graphical user interface, the command line or batch scripts. The command and scripting language is C++ (using the interpreter) and large scripts can be compiled and dynamically linked in. The OO database design has been optimized for parallel access (reading as well as writing) by multiple processes.},
  doi      = {https://doi.org/10.1016/S0168-9002(97)00048-X},
  url      = {http://www.sciencedirect.com/science/article/pii/S016890029700048X},
}

@Article{1976,
  title   = {Reports and theses},
  journal = {Information Systems},
  year    = {1976},
  volume  = {2},
  number  = {1},
  pages   = {29 - 33},
  issn    = {0306-4379},
  doi     = {https://doi.org/10.1016/0306-4379(76)90005-3},
  url     = {http://www.sciencedirect.com/science/article/pii/0306437976900053},
}

@Article{Arenas2016,
  author   = {Marcelo Arenas and Elena Botoeva and Diego Calvanese and Vladislav Ryzhikov},
  title    = {Knowledge base exchange: The case of OWL 2 QL},
  journal  = {Artificial Intelligence},
  year     = {2016},
  volume   = {238},
  pages    = {11 - 62},
  issn     = {0004-3702},
  abstract = {In this article, we define and study the problem of exchanging knowledge between a source and a target knowledge base (KB), connected through mappings. Differently from the traditional database exchange setting, which considers only the exchange of data, we are interested in exchanging implicit knowledge. As representation formalism we use Description Logics (DLs), thus assuming that the source and target KBs are given as a DL TBox+ABox, while the mappings have the form of DL TBox assertions. We define a general framework of KB exchange, and study the problem of translating the knowledge in the source KB according to the mappings expressed in OWL 2 QL, the profile of the standard Web Ontology Language OWL 2 based on the description logic DL-LiteR. We develop novel game- and automata-theoretic techniques, and we provide complexity results that range from NLogSpace to ExpTime.},
  doi      = {https://doi.org/10.1016/j.artint.2016.05.002},
  keywords = {Description logic, Knowledge exchange, DL-Lite, Data exchange, Query inseparability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0004370216300522},
}

@Article{Goutas1989,
  author   = {S. Goutas and P. Soupos and D. Christodoulakis},
  title    = {A new approach towards an object-oriented database system},
  journal  = {Microprocessing and Microprogramming},
  year     = {1989},
  volume   = {27},
  number   = {1},
  pages    = {127 - 132},
  issn     = {0165-6074},
  note     = {Fifteenth EUROMICRO Symposium on Microprocessing and Microprogramming},
  abstract = {In this paper we describe an object-oriented (O-O) database system that supports databases consisting of an extensible set of self-sufficient modules loosely coupled with each other. The object-oriented model used provides semantic constructs that enhance its expressive power and ensure consistency of data. Retrieval is accommodated by a query language based on Horn Clauses. These properties make our system suitable for storage of data in areas such as CAD/CAM, office information systems and artificial intelligence.},
  doi      = {https://doi.org/10.1016/0165-6074(89)90033-1},
  url      = {http://www.sciencedirect.com/science/article/pii/0165607489900331},
}

@Article{Teskey1991,
  author   = {D.J. Teskey and P.J. Hood},
  title    = {The canadian aeromagnetic database:Evolution and applications to the definition of major crustal boundaries},
  journal  = {Tectonophysics},
  year     = {1991},
  volume   = {192},
  number   = {1},
  pages    = {41 - 56},
  issn     = {0040-1951},
  note     = {Magnetic anomalies\3-Land and sea},
  abstract = {The Aeromagnetic Survey Programme of the Geological Survey of Canada, much of which was carried out under Federal-Provincial agreements, has resulted in over 10,000,000 line kilometres of data flown at a mean elevation of 305 m and a mean line separation of 800 m except in mountainous terrain where constant barometric altitudes were maintained for given areas. These data have been digitized (if not originally digitally recorded) and used to produce the 1:1,000,000 “Magnetic Anomaly Map Series” of the Geological Survey of Canada and associated gridded data. These grids were then combined with private data, shipborne data from the east and west coasts and U.S. Navy data from the high Arctic to produce the first all-digital version of the “Magnetic Anomaly Map of Canada”. A study of the longer wavelength features on this anomaly map has resulted in greatly enhanced understanding of the major tectonic units and crustal boundaries within the Canadian landmass. Important features can be delineated by data enhancement and processing techniques such as upward continuation to emphasize deep sources, vertical gradients to sharply outline contacts (e.g. the boundaries of the economically important greenstone belts in the Canadian Shield), and shaded relief to highlight low-amplitude short-wavelength features. An additional technique which provides interesting insights is curvature analysis, which can be used to calculate the strike of an anomaly at each point. When plotted with a particular colour assigned according to the strike direction, the resulting patterns help to delineate trends over long distances.},
  doi      = {https://doi.org/10.1016/0040-1951(91)90245-N},
  url      = {http://www.sciencedirect.com/science/article/pii/004019519190245N},
}

@InCollection{ARBIB1994,
  author    = {MICHAEL A. ARBIB},
  title     = {2 - Models of Visuomotor Coordination in Frog and Monkey},
  booktitle = {Neural Modeling and Neural Networks},
  publisher = {Pergamon},
  year      = {1994},
  editor    = {F. Ventriglia},
  series    = {Pergamon Studies in Neuroscience},
  pages     = {23 - 40},
  address   = {Amsterdam},
  abstract  = {Publisher Summary
The escape direction in response to the approach of a large moving object may be briefly characterized as a compromise between the forward direction of the animal and the direction immediately away from the looming stimulus. Barriers can modify avoidance behavior, just as they modify approach behavior. If a barrier is interposed to block the preferred direction of escape for a stimulus coming from a particular direction, then the behavior of the animal changes and it tends to jump just to the left or just to the right of the barrier. When a limb moves, it needs a burst of agonist contraction to accelerate the limb in the desired direction, followed by an appropriately timed antagonist burst to decelerate it to rest at the desired position (with a possible small agonist correction thereafter). A new resting level of muscle contraction holds the limb in its new position. By contrast, the eye has little inertia and therefore, no antagonist burst is required; the eye has no changing load to require feedback.},
  doi       = {https://doi.org/10.1016/B978-0-08-042277-0.50007-1},
  issn      = {13522388},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080422770500071},
}

@Article{Berio2007,
  author   = {Giuseppe Berio and Mounira Harzallah},
  title    = {Towards an integrating architecture for competence management},
  journal  = {Computers in Industry},
  year     = {2007},
  volume   = {58},
  number   = {2},
  pages    = {199 - 209},
  issn     = {0166-3615},
  note     = {Competence Management in Industrial Processes},
  abstract = {In companies, competence management involves several heavy processes that we have categorised in four classes: competence identification, competence assessment, competence acquisition, and competence usage. Competence management, comprising the management of knowledge about competence, can take advantage from the knowledge engineering techniques to support the mentioned process categories. The paper on the one hand describes how the knowledge engineering techniques proposed in the literature can be used to support the various competence management processes. On the other hand, based on the authors’ previous work on competence management information systems (CRAI approach), the paper provides a critical discussion of the mentioned knowledge engineering techniques: i.e. their strengths, benefits and weaknesses in the context of the process categories are carried out. Then, it proposes an integrating architecture for competence management. A running example is used throughout the paper to better illustrate knowledge techniques and their applications to the competence management.},
  doi      = {https://doi.org/10.1016/j.compind.2006.09.007},
  keywords = {Knowledge engineering techniques, Individual competence, Competence management information systems, CRAI model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361506001588},
}

@Article{Ehlmann2000,
  author   = {B.K. Ehlmann and N. Rishe and J. Shi},
  title    = {The formal specification of ORN semantics},
  journal  = {Information and Software Technology},
  year     = {2000},
  volume   = {42},
  number   = {3},
  pages    = {159 - 170},
  issn     = {0950-5849},
  abstract = {Object Relationship Notation (ORN) is a declarative scheme that permits a variety of common types of relationships to be conveniently defined to a Database Management System (DBMS), thereby allowing the DBMS to automatically enforce their semantics. Though first proposed for object DBMSs, ORN is applicable to any data model that represents binary entity-relationships or to any DBMS that implements them. In this paper, we first describe ORN semantics informally as has been done in previous papers. We then provide a formal specification of these semantics using the Z-notation. Specifying ORN semantics via formal methods gives ORN a solid mathematical foundation. The semantics are defined in the context of an abstract database of sets and relations in a recursive manner that is precise, unambiguous, and noncircular.},
  doi      = {https://doi.org/10.1016/S0950-5849(99)00051-8},
  keywords = {Object relationship notation, Data modeling, Formal methods},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584999000518},
}

@Article{Alagic2002,
  author   = {Suad Alagić},
  title    = {Institutions: integrating objects, XML and databases},
  journal  = {Information and Software Technology},
  year     = {2002},
  volume   = {44},
  number   = {4},
  pages    = {207 - 216},
  issn     = {0950-5849},
  abstract = {A general model theory based on institutions is proposed as a formal framework for investigating typed object-oriented, XML and other data models equipped with integrity constraints. A major challenge in developing such a unified model theory is in the requirement that it must be able to handle major structural differences between the targeted models as well as the significant differences in the logic bases of their associated constraint languages. A distinctive feature of this model theory is that it is transformation-oriented. It is based on structural transformations within a particular category of models or across different categories with a fundamental requirement that the associated constraints are managed in such a way that the database integrity is preserved.},
  doi      = {https://doi.org/10.1016/S0950-5849(02)00010-1},
  keywords = {Object-oriented, XML, Data models, Institutions, Transformations, Integrity constraints},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584902000101},
}

@Article{Arnold1999,
  author   = {James Andrew Arnold and Paul Teicholz and John Kunz},
  title    = {An approach for the interoperation of web-distributed applications with a design model},
  journal  = {Automation in Construction},
  year     = {1999},
  volume   = {8},
  number   = {3},
  pages    = {291 - 303},
  issn     = {0926-5805},
  abstract = {This paper defines the data and inference requirements for the integration of analysis applications with a product model described by a CAD/CAE application. Application input conditions often require sets of complex data that may be considered views of a product model database. We introduce a method that is compatible with the STEP and PLIB product description standards to define an intermediate model that selects, extracts, and validates views of information from a product model to serve as input for an engineering CAD/CAE application. The intermediate model framework was built and tested in a software prototype, the Internet Broker for Engineering Services (IBES). The first research case for IBES integrates applications that specify certain components, for example pumps and valves, with a CAD/CAE application. This paper therefore explores a sub-set of the general problem of integrating product data semantics between various engineering applications. The IBES integration method provides support for a general set of services that effectively assist interpretation and validate information from a product model for an engineering purpose. Such methods can enable application interoperation for the automation of typical engineering tasks, such as component specification and procurement.},
  doi      = {https://doi.org/10.1016/S0926-5805(98)00078-8},
  keywords = {CAD/CAE, IBES, STEP, PLIB},
  url      = {http://www.sciencedirect.com/science/article/pii/S0926580598000788},
}

@Article{Fischer1998,
  author   = {Martin A Fischer and Lloyd M Waugh and Alan Axworthy},
  title    = {IT support of single project, multi-project and industry-wide integration},
  journal  = {Computers in Industry},
  year     = {1998},
  volume   = {35},
  number   = {1},
  pages    = {31 - 45},
  issn     = {0166-3615},
  abstract = {This paper discusses the social or organizational context in which integration in the architecture–engineering–construction industry takes place. We distinguish types and examples of integration at the project level, the multi-project level and the industry level. These types of integration motivate opportunities for the use of information technology. The paper then discusses work related to integration technology in support of integration, in particular in the area of information modeling. It addresses several research challenges with respect to purpose, representation, reasoning, user interface and testing of these models. The goal of this paper is to provide the background for the focus of this journal issue.},
  doi      = {https://doi.org/10.1016/S0166-3615(97)00082-1},
  keywords = {Integration, Architecture–engineering–construction industry, Information technology},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361597000821},
}

@InCollection{Bellazzi2014,
  author    = {Riccardo Bellazzi and Matteo Gabetta and Giorgio Leonardi},
  title     = {Chapter 10 - Engineering Principles in Biomedical Informatics},
  booktitle = {Methods in Biomedical Informatics},
  publisher = {Academic Press},
  year      = {2014},
  editor    = {Indra Neil Sarkar},
  pages     = {313 - 345},
  address   = {Oxford},
  isbn      = {978-0-12-401678-1},
  abstract  = {Engineering is one of the main pillars of biomedical informatics, providing design principles, methods, and tools for the effective implementation of computational solutions in health care. The basic engineering approach consists of a number of phases, comprising modeling, designing, testing, and verifying. Such an approach has become widely applied in biomedical informatics. In this chapter, we analyze three different engineering approaches crucial for biomedical informatics: (1) the design of computational solutions that use the Unified Modeling Language (UML); (2) the representation, simulation, and learning of careflow systems; and, finally, (3) the role that engineering has in data mining, with a specific focus on temporal data and dynamical systems, as well as on principles for engineering the data analysis process.},
  doi       = {https://doi.org/10.1016/B978-0-12-401678-1.00010-5},
  keywords  = {Engineering, UML, Workflows, Dynamic systems, Data mining},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124016781000105},
}

@Article{Thalheim2005,
  author   = {Bernhard Thalheim},
  title    = {Component development and construction for database design},
  journal  = {Data \& Knowledge Engineering},
  year     = {2005},
  volume   = {54},
  number   = {1},
  pages    = {77 - 95},
  issn     = {0169-023X},
  note     = {21st International Conference on Conceptual Modeling},
  abstract = {Principles for database modeling were developed and intensively investigated in the late 1970s and early 1980s. They are based on concepts such as supertypes and subtypes, restructuring through normalization, type construction by constructors, generic models, and associations with pre-specified semantic meaning, e.g., classified relationship types. Use of these principles can result in large and complex database schemas that are difficult for developers to understand, integrate, and extend. We observe that often such schemas consist of identifiable sub-schemas that are loosely coupled. We use this observation to develop a design theory aimed at component-based construction of schemata. Building blocks used for schema construction are termed kernel components. Kernel components are composed to form more complex components by the application of the component constructors. Our approach allows a developer to derive an understandable schema topography for very large and complex databases.},
  doi      = {https://doi.org/10.1016/j.datak.2004.10.002},
  keywords = {Database design, Component-based construction, Kernel components, Component constructors},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X04001570},
}

@Article{Demaid1992,
  author   = {Adrian Demaid and John Zucker},
  title    = {Prototype-oriented representation of engineering design knowledge},
  journal  = {Artificial Intelligence in Engineering},
  year     = {1992},
  volume   = {7},
  number   = {1},
  pages    = {47 - 61},
  issn     = {0954-1810},
  abstract = {In this paper we present arguments concerning the nature of inheritance hierarchies for representing the relationships between computing objects which encapsulate engineering design knowledge. We review and criticize some other options among object-oriented languages and knowledge representation systems for creating inheritance hierarchies. These are languages and systems which support the relatively rigid class/instance model of property sharing and use a support environment for modification and change. By contrast we provide coupling between objects which employs a scheme of property inheritance that uses ‘exemplary’ or prototypical objects. Our approach yields a class-free inheritance system, in the tradition presented in the literature or the Actor languages, that enables design concepts to emerge easily as the user describes and modifies them at the keyboard. Our prototype-oriented approach offers a means of continually replicating design concepts through object refinement: this principle of one object being specified as a refinement of another object is specialization. We show that treating a knowledge base as evolutionary encourages exploratory comparisons and supports its customization for design representation. We use single taxonomic inheritance to demonstrate the effectiveness of representing information which originates from different perspectives (distinguished structurally as separate inheritance subhierarchies which allow a design world of discourse consisting of many ad hoc groupings to evolve) and criticize the use of multiple inheritance for knowledge representation.},
  doi      = {https://doi.org/10.1016/0954-1810(92)80006-C},
  url      = {http://www.sciencedirect.com/science/article/pii/095418109280006C},
}

@Article{Ruiz2017,
  author   = {Francisco Javier Bermúdez Ruiz and Jesús García Molina and Oscar Díaz García},
  title    = {On the application of model-driven engineering in data reengineering},
  journal  = {Information Systems},
  year     = {2017},
  volume   = {72},
  pages    = {136 - 160},
  issn     = {0306-4379},
  abstract = {Model-Driven Engineering (MDE) emphasizes the systematic use of models to improve software productivity and some aspects of the software quality such as maintainability or interoperability. Model-driven techniques have proven useful not only as regards developing new software applications but also the reengineering of legacy systems. Models and metamodels provide a high-level formalism with which to represent artefacts commonly manipulated in the different stages of a software evolution process (e.g., a software migration) while model transformation allows the automation of the evolution tasks to be performed. Some approaches and experiences of model-driven software reengineering have recently been presented but they have been focused on the code while data reengineering aspects have been overlooked. The objective of this work is to assess to what extent data reengineering processes could also take advantage of MDE techniques. The article starts by characterising data-reengineering in terms of the tasks involved. It then goes on to state that MDE is particularly amenable as regards addressing the tasks previously identified. We present an MDE-based approach for the reengineering of data whose purpose is to improve the quality of the logical schema in a relational data migration scenario. As a proof of concept, the approach is illustrated for two common problems in data re-engineering: undeclared foreign keys and disabled constraints. This approach is organised following the three stages of a software reengineering process: reverse engineering, restructuring and forward engineering. We show how each stage is implemented by means of model transformation chains. A running example is used to illustrate each stage of the process throughout the article. The approach is validated with a real widely-used database. An assessment of the application of MDE in each stage is then presented, and we conclude by identifying the main benefits and drawbacks of using MDE in data reengineering.},
  doi      = {https://doi.org/10.1016/j.is.2017.10.004},
  keywords = {Model-driven engineering, Data modernisation, Data reengineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915300508},
}

@Article{Fernandes1992,
  author   = {AAA Fernandes and NW Paton and MH Williams and A Bowles},
  title    = {Approaches to deductive object-oriented databases},
  journal  = {Information and Software Technology},
  year     = {1992},
  volume   = {34},
  number   = {12},
  pages    = {787 - 803},
  issn     = {0950-5849},
  abstract = {The paper is concerned with the problem of combining deductive and object-oriented features to produce a deductive object-oriented database system which is comparable to those currently available under the relational view of data modelling not only in its functionality but also in the techniques employed in its construction and use. Under this assumption, the kinds of issues that have to be tackled for a similar research strategy to produce comparable results are highlighted. The authors motivate their terms of comparison, characterize three broad approaches to deductive object-oriented databases and introduce the notion of language convergence to help in the characterization of some shortcomings that have been perceived in them. Three proposals that have come to light in the past three years are looked into in some detail, in so far as they exemplify some of the positions in the space of choices defined. The main contribution of the paper is towards a characterization of the language convergence property of deductive database languages which has a key role in addressing critiques of the deductive and object-oriented database research enterprise. A basic familiarity with notions from deductive databases and from object-oriented databases is assumed.},
  doi      = {https://doi.org/10.1016/0950-5849(92)90121-5},
  keywords = {deductive databases, object-oriented databases, data modelling, datalog, prolog},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584992901215},
}

@Article{Zamboulis2010,
  author   = {Lucas Zamboulis and Nigel Martin and Alexandra Poulovassilis},
  title    = {Query performance evaluation of an architecture for fine-grained integration of heterogeneous grid data sources},
  journal  = {Future Generation Computer Systems},
  year     = {2010},
  volume   = {26},
  number   = {8},
  pages    = {1073 - 1091},
  issn     = {0167-739X},
  abstract = {Grid data sources may have schema- and data-level conflicts that need to be addressed using data transformation and integration technologies not supported by the current generation of Grid data access and querying middleware. We present an architecture that combines Grid data access and distributed querying with fine-grained data transformation/integration technologies, and the results of a query performance evaluation on this architecture. The performance evaluation indicates that it is indeed feasible to combine such technologies while achieving acceptable query performance. We also discuss the significance of our results for the further development of query performance over heterogeneous Grid data sources.},
  doi      = {https://doi.org/10.1016/j.future.2010.05.008},
  keywords = {Data integration, Query processing, Grid computing, Bioinformatics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X10000889},
}

@Article{Hsu1987,
  author   = {Cheng Hsu and Craig Skevington},
  title    = {Integration of data and knowledge in manufacturing enterprises: A conceptual framework},
  journal  = {Journal of Manufacturing Systems},
  year     = {1987},
  volume   = {6},
  number   = {4},
  pages    = {277 - 285},
  issn     = {0278-6125},
  abstract = {This paper formulates a fundamental approach to the information integration of manufacturing enterprises. The approach entails the use of an intelligent metadatabase to create an information environment established through ‘feature’ primitives for the enterprise as a whole. The functional interrelationships, information transformations, and knowledge representation are therefore designed directly into the schema of the database system. It is this concept that allows the database environment of a computerized manufacturing system to provide the true integration of manufacturing and the promised gains in productivity. This concept of metadatabase differs fundamentally from both the strategies of ‘interfacing’ and ‘super database management system’ prevailing in the field of manufacturing automation today.},
  doi      = {https://doi.org/10.1016/0278-6125(87)90004-5},
  keywords = {Computer Integrated Manufacturing, Metadatabase, Features, Intelligent Information Handling},
  url      = {http://www.sciencedirect.com/science/article/pii/0278612587900045},
}

@Article{Diederich1997,
  author   = {J. Diederich},
  title    = {Basic properties for biological databases: Character development and support},
  journal  = {Mathematical and Computer Modelling},
  year     = {1997},
  volume   = {25},
  number   = {10},
  pages    = {109 - 127},
  issn     = {0895-7177},
  abstract = {In this paper, we examine problems and solutions for building a large set of characters for descriptive data derived from published species descriptions. The ideas presented lead in the direction of creating a kind of BioDBMS that can be used to support large integrated biological databases.},
  doi      = {https://doi.org/10.1016/S0895-7177(97)00078-2},
  keywords = {Biological databases, Biological characters, Data modeling, Basic property, Schema design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895717797000782},
}

@Article{Sternberg1984,
  author   = {Robert J. Sternberg},
  title    = {A theory of knowledge acquisition in the development of verbal concepts},
  journal  = {Developmental Review},
  year     = {1984},
  volume   = {4},
  number   = {2},
  pages    = {113 - 138},
  issn     = {0273-2297},
  abstract = {A theory of knowledge acquisition in the development of verbal concepts is presented. The theory is divided into three subtheories. The first specifies the information processes asserted to be central to the development of verbal concepts. The second subtheory specifies the informational cues upon which these processes operate. The third subtheory specifies variables that moderate the use of the proposed processes on the proposed cues. Each of these subtheories is described, and it is shown how the subtheories apply to the acquisition of verbal concepts. Following this description, a model of mental representation is described and proposed as the model that is built up by the knowledge-acquisition processes posited by the theory. Then, the loci of development in the theory are discussed. Finally, the theory is briefly compared to other theories, and some conclusions about acquisition of verbal knowledge are drawn.},
  doi      = {https://doi.org/10.1016/0273-2297(84)90001-7},
  url      = {http://www.sciencedirect.com/science/article/pii/0273229784900017},
}

@Article{Melnik2003,
  author   = {Sergey Melnik and Erhard Rahm and Philip A. Bernstein},
  title    = {Developing metadata-intensive applications with Rondo},
  journal  = {Journal of Web Semantics},
  year     = {2003},
  volume   = {1},
  number   = {1},
  pages    = {47 - 74},
  issn     = {1570-8268},
  abstract = {The future of the Semantic Web depends on whether or not we succeed to integrate reliably thousands of online applications, services, and databases. These systems are tied together using mediators, mappings, database views, and transformation scripts. Model-management aims at reducing the amount of programming needed for the development of such integrated applications. We present a first complete prototype of a generic model-management system, in which high-level operators are used to manipulate models and mappings between models. We define the key operators and conceptual structures and describe their use and implementation. We examine the solutions for three model-management tasks: change propagation, view reuse, and reintegration.},
  doi      = {https://doi.org/10.1016/j.websem.2003.07.003},
  keywords = {Generic model management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826803000040},
}

@Article{Hines1998,
  author   = {M.L. Hines},
  title    = {Conceptual object-oriented database: A theoretical model},
  journal  = {Information Sciences},
  year     = {1998},
  volume   = {105},
  number   = {1},
  pages    = {31 - 68},
  issn     = {0020-0255},
  abstract = {The evolution of object-oriented databases (OODBs) has not been accompanied by an equivalent evolution of theoretical models, leaving the foundations for OODBs ill-defined. Design of OODBs has been hampered by the lack of design techniques/tools which correspond to the theoretical model. This paper defines a core conceptual object-oriented database (COODB) model providing a foundation and framework for theoretical research. Structural and behavioral definitions for an object, a class, and a class hierarchy are given. A specification hierarchy is introduced as a design tool, and a messaging component is defined which enables asynchronous and synchronous interaction.},
  doi      = {https://doi.org/10.1016/S0020-0255(97)10018-4},
  keywords = {Object-oriented databases, System specification, Metadata evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025597100184},
}

@InCollection{Castellani1993,
  author    = {X. Castellani},
  title     = {Mechanisms of Standardized Reusability of Objects (MCO methodology)},
  booktitle = {Information System Development Process},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {N. PRAKASH and C. ROLLAND and B. PERNICI},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {61 - 78},
  address   = {Amsterdam},
  abstract  = {This paper describes mechanisms of standardized reusability of object characteristics. These mechanisms can be used during the integration of new object types or of new instances in an object system. These mechanisms use object reusability gauges. Seven results of object reusability are presented and show how objects of an object system can be built in a standardized manner using these gauges. These mechanisms allow an incremental and automatizable design of object reusability. These mechanisms belong to MCO methodology. They are presented by using the concepts and the vocabulary of this object-oriented analysis and design methodology. But these mechanisms are general; they can be used with concepts of other object-oriented analysis and design methods, of object-oriented languages and of data base management systems.},
  doi       = {https://doi.org/10.1016/B978-0-444-81594-1.50010-6},
  issn      = {09265473},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444815941500106},
}

@Article{Tekli2016,
  author   = {Joe Tekli and Nathalie Charbel and Richard Chbeir},
  title    = {Building semantic trees from XML documents},
  journal  = {Journal of Web Semantics},
  year     = {2016},
  volume   = {37-38},
  pages    = {1 - 24},
  issn     = {1570-8268},
  abstract = {The distributed nature of the Web, as a decentralized system exchanging information between heterogeneous sources, has underlined the need to manage interoperability, i.e., the ability to automatically interpret information in Web documents exchanged between different sources, necessary for efficient information management and search applications. In this context, XML was introduced as a data representation standard that simplifies the tasks of interoperation and integration among heterogeneous data sources, allowing to represent data in (semi-) structured documents consisting of hierarchically nested elements and atomic attributes. However, while XML was shown most effective in exchanging data, i.e., in syntactic interoperability, it has been proven limited when it comes to handling semantics, i.e.,  semantic interoperability, since it only specifies the syntactic and structural properties of the data without any further semantic meaning. As a result, XML semantic-aware processing has become a motivating challenge in Web data management, requiring dedicated semantic analysis and disambiguation methods to assign well-defined meaning to XML elements and attributes. In this context, most existing approaches: (i) ignore the problem of identifying ambiguous XML elements/nodes, (ii) only partially consider their structural relationships/context, (iii) use syntactic information in processing XML data regardless of the semantics involved, and (iv) are static in adopting fixed disambiguation constraints thus limiting user involvement. In this paper, we provide a new XML Semantic Disambiguation Framework titled XSDFdesigned to address each of the above limitations, taking as input: an XML document, and then producing as output a semantically augmented XML tree made of unambiguous semantic concepts extracted from a reference machine-readable semantic network. XSDF consists of four main modules for: (i) linguistic pre-processing of simple/compound XML node labels and values, (ii) selecting ambiguous XML nodes as targets for disambiguation, (iii) representing target nodes as special sphere neighborhood vectors including all XML structural relationships within a (user-chosen) range, and (iv) running context vectors through a hybrid disambiguation process, combining two approaches: concept-basedand context-based disambiguation, allowing the user to tune disambiguation parameters following her needs. Conducted experiments demonstrate the effectiveness and efficiency of our approach in comparison with alternative methods. We also discuss some practical applications of our method, ranging over semantic-aware query rewriting, semantic document clustering and classification, Mobile and Web services search and discovery, as well as blog analysis and event detection in social networks and tweets.},
  doi      = {https://doi.org/10.1016/j.websem.2016.03.002},
  keywords = {XML and Semi-structured data, Word sense disambiguation, Semantic-aware processing, Semantic ambiguity, Context representation, Knowledge bases},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826816000202},
}

@Article{Breunig2016,
  author   = {Martin Breunig and Paul V. Kuper and Edgar Butwilowski and Andreas Thomsen and Markus Jahn and André Dittrich and Mulhim Al-Doori and Darya Golovko and Mathias Menninghaus},
  title    = {The story of DB4GeO – A service-based geo-database architecture to support multi-dimensional data analysis and visualization},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year     = {2016},
  volume   = {117},
  pages    = {187 - 205},
  issn     = {0924-2716},
  abstract = {Multi-dimensional data analysis and visualization need efficient data handling to archive original data, to reproduce results on large data sets, and to retrieve space and time partitions just in time. This article tells the story of more than twenty years research resulting in the development of DB4GeO, a web service-based geo-database architecture for geo-objects to support the data handling of 3D/4D geo-applications. Starting from the roots and lessons learned, the concepts and implementation of DB4GeO are described in detail. Furthermore, experiences and extensions to DB4GeO are presented. Finally, conclusions and an outlook on further research also considering 3D/4D geo-applications for DB4GeO in the context of Dubai 2020 are given.},
  doi      = {https://doi.org/10.1016/j.isprsjprs.2015.12.006},
  keywords = {Spatio-temporal data modelling, Geodatabase, Geoservices, Geovisualization, n-d topology},
  url      = {http://www.sciencedirect.com/science/article/pii/S0924271615002762},
}

@Article{Bernauer2004,
  author   = {Martin Bernauer and Michael Schrefl},
  title    = {Self-maintaining web pages: from theory to practice},
  journal  = {Data \& Knowledge Engineering},
  year     = {2004},
  volume   = {48},
  number   = {1},
  pages    = {39 - 73},
  issn     = {0169-023X},
  abstract = {The self-maintaining web pages (SMWP) approach employs concepts from distributed database design and active databases to keep pre-generated web pages in synchronization with database content. It maps fragments of relations to web pages and propagates modifications of relations incrementally to web pages. This paper shows how the SMWP approach can be put into practice using off-the-shelf relational database technology. It presents a declarative language for the definition of parameterized fragments and web pages, shows how fragments are stored, and describes how previously presented algorithms for propagating modifications of relations to web pages can be realized by database triggers.},
  doi      = {https://doi.org/10.1016/S0169-023X(03)00109-5},
  keywords = {Active databases, E-Commerce, Web applications/XML, Replication, caching and views},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X03001095},
}

@Article{Keefe1989,
  author   = {T.F. Keefe and W.T. Tsai and M.B. Thuraisingham},
  title    = {SODA: A secure object-oriented database system},
  journal  = {Computers \& Security},
  year     = {1989},
  volume   = {8},
  number   = {6},
  pages    = {517 - 533},
  issn     = {0167-4048},
  abstract = {This paper describes a security model for object-oriented systems. The model supports a flexible data classification policy based on inheritance. The classification poligy allows for a smooth transition between rigid classification rules and unlimited polyinstantiation. The security model treats the data model as well as the computational model of object-oriented systems allowing more flexibility. This model trades an increase in complexity for a more flexible security model.},
  doi      = {https://doi.org/10.1016/0167-4048(89)90081-3},
  keywords = {Secure database, Data classification, Object-oriented model, Security entities},
  url      = {http://www.sciencedirect.com/science/article/pii/0167404889900813},
}

@Article{Mueller2004,
  author   = {Robert Müller and Ulrike Greiner and Erhard Rahm},
  title    = {AgentWork: a workflow system supporting rule-based workflow adaptation},
  journal  = {Data \& Knowledge Engineering},
  year     = {2004},
  volume   = {51},
  number   = {2},
  pages    = {223 - 256},
  issn     = {0169-023X},
  abstract = {Current workflow management systems still lack support for dynamic and automatic workflow adaptations. However, this functionality is a major requirement for next–generation workflow systems to provide sufficient flexibility to cope with unexpected failure events. We present the concepts and implementation of AgentWork, a workflow management system supporting automated workflow adaptations in a comprehensive way. A rule-based approach is followed to specify exceptions and necessary workflow adaptations. AgentWork uses temporal estimates to determine which remaining parts of running workflows are affected by an exception and is able to predictively perform suitable adaptations. This helps to ensure that necessary adaptations are performed in time with minimal user interaction which is especially valuable in complex applications such as for medical treatments.},
  doi      = {https://doi.org/10.1016/j.datak.2004.03.010},
  keywords = {Workflow management, Adaptive systems, Active rules, Temporal logics, Agents},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X0400076X},
}

@Article{Bronts1995,
  author   = {G.H.W.M. Bronts and S.J. Brouwer and C.L.J. Martens and H.A. Proper},
  title    = {A unifying object role modelling theory},
  journal  = {Information Systems},
  year     = {1995},
  volume   = {20},
  number   = {3},
  pages    = {213 - 235},
  issn     = {0306-4379},
  abstract = {This article presents the idea of defining a kernel for object role modelling techniques, upon which different drawing styles can be based. We propose such a kernel (the ORM kernel) and define, as a case study, an ER and a NIAM drawing style on top of it. One of the prominent advantages of such a kernel is the possibility to build a CASE-tool supporting multiple methods. Such a CASE-tool would allow users with different methodological backgrounds to use it and view the modelled domains in terms of their favourite method. This is illustrated using a running example of a concrete domain in which we use the ORM kernel in combination with the NIAM and ER drawing style.},
  doi      = {https://doi.org/10.1016/0306-4379(95)00010-2},
  keywords = {Object-Role Modelling, Conceptual Modelling, ER, Information Systems},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437995000102},
}

@Article{Tuzhilin1995,
  author   = {Alexander Tuzhilin and James Clifford},
  title    = {On periodicity in temporal databases},
  journal  = {Information Systems},
  year     = {1995},
  volume   = {20},
  number   = {8},
  pages    = {619 - 639},
  issn     = {0306-4379},
  abstract = {The issue of periodicity is generally understood to be a desirable property of temporal data that should be supported by temporal database models and their query languages. Nevertheless, there has so far not been any systematic examination of how to incorporate this concept into a temporal DBMS. In this paper we describe two concepts of periodicity, which we call strong periodicity and near periodicity, and discuss how they capture formally two of the intuitive meanings of this term. We formally compare the expressive power of these two concepts, relate them to existing temporal query languages, and show how they can be incorporated into temporal relational database query languages, such as the proposed temporal extension to SQL, in a clean and straightforward manner.},
  doi      = {https://doi.org/10.1016/0306-4379(95)00034-8},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437995000348},
}

@InCollection{Murthy1993,
  author    = {Sunil K. Murthy and Shamkant B. Navathe and Aloysius Cornelio},
  title     = {Organizing Engineering Designs and Design Techniques**Work was done while the authors were at University of Florida},
  booktitle = {Computer-Aided Design/Engineering (CAD/CAE) Techniques and their Applications, Part 1 of 2},
  publisher = {Academic Press},
  year      = {1993},
  editor    = {C.T. LEONDES},
  volume    = {58},
  series    = {Control and Dynamic Systems},
  pages     = {27 - 60},
  doi       = {https://doi.org/10.1016/B978-0-12-012758-0.50008-8},
  issn      = {0090-5267},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780120127580500088},
}

@Article{Eastman1999,
  author   = {Charles Eastman and Tay Sheng Jeng},
  title    = {A database supporting evolutionary product model development for design},
  journal  = {Automation in Construction},
  year     = {1999},
  volume   = {8},
  number   = {3},
  pages    = {305 - 323},
  issn     = {0926-5805},
  abstract = {This paper presents the facilities in the EDM-2 product modeling and database language that support model evolution. It reviews the need for model evolution as a system and/or language requirement to support product modeling. Four types of model evolution are considered: (1) translation between distinct models, (2) deriving views from a central model, (3) modification of an existing model, and (4) model evolution based on writable views associated with each application. While the facilities described support all for types of evolution, the last type is emphasized. The language based modeling capabilities described in EDM-2 include: (a) mapping facilities for defining derivations and views within a single model or between different models; (b) procedural language capabilities supporting model addition, deletion and modification; (c) support for object instance migration so as to partition the set of class instances into multiple classes; (d) support for managing practical deletion of portions of a model; (e) explicit specification and automatic management of integrity between a building model and various views. The rationale and language features, and in some cases, the implementation strategy for the features, are presented.},
  doi      = {https://doi.org/10.1016/S0926-5805(98)00079-X},
  keywords = {Product modeling, Model mapping, Model evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S092658059800079X},
}

@Article{Liu1993,
  author   = {Ling Liu},
  title    = {A recursive object algebra based on aggregation abstraction for manipulating complex objects},
  journal  = {Data \& Knowledge Engineering},
  year     = {1993},
  volume   = {11},
  number   = {1},
  pages    = {21 - 60},
  issn     = {0169-023X},
  abstract = {We present an object algebra for manipulating complex objects in object-oriented database systems. All operators are recursively defined. Unlike most of the existing query languages, the design of this object algebra is based on aggregation abstraction. It allows to take complex objects collectively as a unit of high level queries and enables complex objects to be accessed at all levels of aggregation hierarchies without resorting to any kind of path expressions. Features of aggregation abstraction, such as acyclicity of aggregation hierarchies and aggregation inheritance, have played important roles in such a development. We also formally described the output type of each operator in order to support dynamic classification of query results in the IsA type/class semi-lattice. The algebraic-equivalence rewriting rules for query optimization of this algebra are developed, too.},
  doi      = {https://doi.org/10.1016/0169-023X(93)90044-P},
  keywords = {Object-oriented databases, object algebra, object-creating operators, object preserving operators, query optimization},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9390044P},
}

@Article{Abiteboul1986,
  author   = {Serge Abiteboul and Richard Hull},
  title    = {Restructuring hierarchical database objects},
  journal  = {Theoretical Computer Science},
  year     = {1986},
  volume   = {62},
  number   = {1},
  pages    = {3 - 38},
  issn     = {0304-3975},
  abstract = {A class of hierarchical structures arising in Database Systems (complex objects) and Office Information Systems (forms) is studied. Two formalisms for restructuring are presented. The first focuses on a class of algebraic operators based on rewrite rules, and the second on structural transformations which preserve or augment data capacity. These transformations are related to a subclass of the rewrite operations which is closed under composition.},
  doi      = {https://doi.org/10.1016/0304-3975(86)90010-1},
  url      = {http://www.sciencedirect.com/science/article/pii/0304397586900101},
}

@InCollection{Bellinzona1993,
  author    = {R. Bellinzona and M.G. Fugini and V. de Mey},
  title     = {Reuse of Specifications and Designs in a Development Information System},
  booktitle = {Information System Development Process},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {N. PRAKASH and C. ROLLAND and B. PERNICI},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {79 - 96},
  address   = {Amsterdam},
  abstract  = {This paper describes the approach to application specification and design via reuse at the basis of the Development Information System of the ITHACA1 environment. Requirements and detailed design of a specific application are incrementally composed by aggregating available reusable components stored in the Software Information Base repository. The paper reviews the Development Information System, then focuses on two tools of the system: RECAST (Requirements Composition And Specification Tool) and Visual ADL, which help the developer in selecting reusable artifacts from the Software Base and in composing and tailoring them according to the specific needs of the application. The paper illustrates the composition approach and describes how reuse is supported via meta classes incorporating suggestions for component reuse and tailoring, and for detailed design.},
  doi       = {https://doi.org/10.1016/B978-0-444-81594-1.50011-8},
  issn      = {09265473},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444815941500118},
}

@InCollection{Ishikawa2002,
  author    = {Hiroshi Ishikawa},
  title     = {3 - Object-Oriented Database Systems},
  booktitle = {Database and Data Communication Network Systems},
  publisher = {Academic Press},
  year      = {2002},
  editor    = {Cornelius T. Leondes},
  pages     = {77 - 122},
  address   = {San Diego},
  isbn      = {978-0-12-443895-8},
  abstract  = {Publisher Summary
This chapter describes a prototype object-oriented database management system (DBMS) called Jasmine, focusing on the implementation of its object-oriented features. Jasmine shares a lot of functionality with other object-oriented database systems. The chapter also focuses on the impact of the design of Jasmine's object-oriented model and the language on database implementation technology. Jasmine extends the relational database technology. It provides nested relations to efficiently manage complex objects and provides user-defined functions evaluated on page buffers to efficiently process method invocation in queries. The chapter also describes schema translation as an extension to Jasmine. Jasmine also provides a view facility for schema integration and a constraint management facility including integrity constraints, triggers, and rules.},
  doi       = {https://doi.org/10.1016/B978-012443895-8/50005-2},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124438958500052},
}

@Article{Jurgensen1987,
  author   = {Helmut Jurgensen and Dan A. Simovici},
  title    = {Towards an abstract theory of dependency constraints in relational databases},
  journal  = {Information Sciences},
  year     = {1987},
  volume   = {41},
  number   = {1},
  pages    = {43 - 60},
  issn     = {0020-0255},
  abstract = {We introduce the notion of an FD system on a semilattice as a generalization of the concept of a closed set of functional dependencies, and we study lattice-theoretical properties of those objects. The notion of a table as an abstraction of relations of relational databases is also considered, and a generalization of relational algebra is presented; this offers a better perspective on the role played by Armstrong's relations. Finally, an application of this algebraic approach to dynamic databases is included.},
  doi      = {https://doi.org/10.1016/0020-0255(87)90004-1},
  url      = {http://www.sciencedirect.com/science/article/pii/0020025587900041},
}

@Article{Narang2000,
  author   = {Rajesh Narang and K.D. Sharma},
  title    = {A Semantically Extended Views Integration Method and Its Application to Pavement Views Integration},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  year     = {2000},
  volume   = {12},
  pages    = {45 - 67},
  issn     = {1319-1578},
  abstract = {Due to increasing interest in distributed databases, the importance of schema integration techniques is significantly increasing. It has been realized database design is such a complex task that it can't be performed in a centralized way, therefore, a more reasonable approach is to first allow the different departments of an organization to build their own schema/view of the database and then integrate them to represent the global schema of the complete knowledge. Keeping this view in mind, an integration strategy based on the concept of structural comparison and semantic comparison of schema is proposed. The structural comparison finds out similar or near similar types to mainly ascertain subset relationship. It does not analyze the entities very deeply, therefore, it can't detect other relationships hidden in the structural specification of the types. Hence to circumvent these problems, semantic comparison of the types is also considered. The semantic comparison helps to detect other kinds of hidden relationships such as role relationship, identical relationship and compatible relationship between diverse schema components. After this process, all semantic related components are implicitly merged to get a universal view of the knowledge spread in distributed environment. The whole concept is summed up in the form of an integration algorithm.},
  doi      = {https://doi.org/10.1016/S1319-1578(00)80002-6},
  url      = {http://www.sciencedirect.com/science/article/pii/S1319157800800026},
}

@Article{Wang1997a,
  author   = {Huaiqing Wang},
  title    = {Intelligent agent-assisted decision support systems: Integration of knowledge discovery, knowledge analysis, and group decision support},
  journal  = {Expert Systems with Applications},
  year     = {1997},
  volume   = {12},
  number   = {3},
  pages    = {323 - 335},
  issn     = {0957-4174},
  abstract = {The integration of data mining techniques with decision support systems to assist in dealing with information overload has received increased attention and importance over recent years. However, challenges remain regarding practical deployment and implementation of such integration, due to the increased complexity of decision making, system co-ordination and knowledge communication. It is the purpose of this paper to outline the issues necessary to be addressed in a practical decision support system that integrates data mining techniques. The paper will describe a novel architecture to support the co-operative decision process by utilizing event-driven and task-driven data mining agents, along with user assistant agents and a knowledge manager agent. An internet-based prototype for supporting marketing planning decisions is also presented to demonstrate the practicality and feasibility of the proposed intelligent agent-based decision support system architecture.},
  doi      = {https://doi.org/10.1016/S0957-4174(96)00103-0},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417496001030},
}

@Article{Soutou2001,
  author   = {Christian Soutou},
  title    = {Modeling relationships in object-relational databases},
  journal  = {Data \& Knowledge Engineering},
  year     = {2001},
  volume   = {36},
  number   = {1},
  pages    = {79 - 107},
  issn     = {0169-023X},
  abstract = {We propose an approach for designing an object-relational database. We inspect each case of semantic relationships (one-to-one, one-to-many, many-to-many and n-ary). For each of these cases we list the different solutions for implementation and propose some solutions. We also make a comparison with the relational data model. Our results can be applied to current RDBMS including object extensions (Oracle8, DB2-IBM, Informix…).},
  doi      = {https://doi.org/10.1016/S0169-023X(00)00035-5},
  keywords = {Object-relational, Database modeling, Semantic relationships},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X00000355},
}

@Article{Miura1994,
  author   = {Takao Miura},
  title    = {Nesting quantification in a visual data manipulation language},
  journal  = {Data \& Knowledge Engineering},
  year     = {1994},
  volume   = {12},
  number   = {2},
  pages    = {167 - 196},
  issn     = {0169-023X},
  abstract = {In this paper we propose a new approach to provide us with (universal and existential) quantification in a visual data manipulation language. Overall ideas are two basic constructs, one for universal ∀ quantifier and another for existential ∃ quantifier. Then a nesting feature is considered to obtain general expressive power. The major contribution of this paper is that, for every query specification with any depth of nesting of quantification, there exists an equivalent simplified form. This means that database management systems can make plans for specific evaluation strategies to gain efficiency.},
  doi      = {https://doi.org/10.1016/0169-023X(94)90013-2},
  keywords = {Databases, Visual data manipulation languages, Quantifications, Expressive power},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X94900132},
}

@Article{Hoffer1980,
  author   = {Jeffrey A. Hoffer},
  title    = {Database design practices for inverted files},
  journal  = {Information \& Management},
  year     = {1980},
  volume   = {3},
  number   = {4},
  pages    = {149 - 161},
  issn     = {0378-7206},
  abstract = {Database research literature has proposed many procedures, both manual and automated, for database design; selection of secondary indexes for inverted file type database management systems (DBMS) has been repeatedly addressed. The empirical study reported here indicates that practical inverted file design has been relatively unaffected by this research. This paper characterizes the actual database design process used at inverted file DBMS installations along such dimension as: types of secondary keys constructed, the individuals who make index design decisions, the decisions that are changed (and when) after the initial database implementation, the factors that are considered in indexing decisions, and the literature which is used in the process. The study shows that key selection (as one example of a design decision) is addressed by ad hoc procedures and well conceived procedures are not used. Further, the results indicate that database design is dominated by users and systems analysts, indexes are frequently changed and a wide range of database performance and convenience factors are influential in practice. The paper concludes with some recommendations for database design support tools.},
  doi      = {https://doi.org/10.1016/0378-7206(80)90021-X},
  keywords = {Database design, inverted files, secondary indexes, database design practice, database management systems, database design tools},
  url      = {http://www.sciencedirect.com/science/article/pii/037872068090021X},
}

@Article{Vassiliadis2001,
  author   = {Panos Vassiliadis and Christoph Quix and Yannis Vassiliou and Matthias Jarke},
  title    = {Data warehouse process management},
  journal  = {Information Systems},
  year     = {2001},
  volume   = {26},
  number   = {3},
  pages    = {205 - 236},
  issn     = {0306-4379},
  note     = {12th International Conference on Advanced Systems Engineering},
  abstract = {Previous research has provided metadata models that enable the capturing of the static components of a data warehouse architecture, along with information on different quality factors over these components. This paper complements this work with the modeling of the dynamic parts of the data warehouse. The proposed metamodel of data warehouse operational processes is capable of modeling complex activities, their interrelationships, and the relationship of activities with data sources and execution details. Moreover, the metamodel complements the existing architecture and quality models in a coherent fashion, resulting in a full framework for quality-oriented data warehouse management, capable of supporting the design, administration and especially evolution of a data warehouse. Finally, we exploit our framework to revert the widespread belief that data warehouses can be treated as collections of materialized views. We have implemented this metamodel using the language Telos and the metadata repository system ConceptBase.},
  doi      = {https://doi.org/10.1016/S0306-4379(01)00018-7},
  keywords = {Data warehousing, Process modeling, Evolution, Quality},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437901000187},
}

@Article{Shah2004,
  author   = {A Shah and F Fotouhi and W Grosky and J Al-Muhtadi},
  title    = {Operators of the temporal object system and their implementation},
  journal  = {Information Sciences},
  year     = {2004},
  volume   = {158},
  pages    = {37 - 68},
  issn     = {0020-0255},
  abstract = {We proposed a temporal object system (TOS) which maintains changes to both the structure and the state of an object in a temporal fashion. Objects in TOS are referred to as temporal objects and are allowed to evolve over time. A collection of temporal objects which share the same set of common properties is grouped into a family. A temporal object that can be defined by using the local knowledge of a family is referred to as an offstage object. We also discussed the renovations of both temporal complex objects and offstage objects. This paper is a continuation of the work reported, and now we report on the operators of the TOS and their implementation. These operators are grouped into three different modules of the TOS based on their relevant functions. These modules are: object manager (or object module), family module, and root of TOS (RTOS) module. The important module is the object manager (OM) that consists of basic operators. The modules provide a facility for defining a simple temporal object and later to add a stage in the temporal object. The other operators are grouped into the two other modules and are referred to as RTOS module and family module. We have implemented these operators using the SELF version 4.0 programming language on a SUN Sparc Workstation running Solaries 2.4.},
  doi      = {https://doi.org/10.1016/j.ins.2003.08.004},
  keywords = {Object-oriented databases, Temporal database, Temporal objects, Object manager},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025503002056},
}

@Article{Rothman2005,
  author   = {Laurence S. Rothman and Nicole Jacquinet-Husson and Christian Boulet and Agnès M. Perrin},
  title    = {History and future of the molecular spectroscopic databases},
  journal  = {Comptes Rendus Physique},
  year     = {2005},
  volume   = {6},
  number   = {8},
  pages    = {897 - 907},
  issn     = {1631-0705},
  note     = {Molecular spectroscopy and planetary atmospheres},
  abstract = {A brief history and review of the development of some molecular spectroscopic databases is presented. Such databases are compilations of spectroscopic parameters whose principal purpose is to provide the necessary molecular absorption input for transmission and radiance codes. Remote sensing of the terrestrial atmosphere has advanced significantly in recent years, and this has placed greater demands on the compilations in terms of accuracy, additional species, and spectral coverage. This paper discusses current pressing issues, such as the deficiencies in line positions, intensities, and line shape, as well as the directions of future enhancements. To cite this article: L.S. Rothman et al., C. R. Physique 6 (2005).
Résumé
On présente tout d'abord un bref historique et une revue du développement de quelques banques de données spectroscopiques moléculaires. Il s'agit de compilations des paramètres spectroscopiques nécessaires au calcul du coefficient d'absorption des espèces moléculaires que l'on rencontre dans les codes de transferts radiatifs. Le sondage de l'atmosphère terrestre a, par ailleurs, beaucoup progressé dans les dernières années, avec comme conséquence une demande accrue aux banques de données d'étendre leur couverture, aussi bien en termes d'espèces moléculaires, de domaine spectral mais aussi de précision. On présente dans cet article, quelques problèmes d'actualité illustrant diverses lacunes des banques actuelles (positions des raies, intensités, profils spectraux, …) ainsi que quelques pistes d'évolutions futures. Pour citer cet article : L.S. Rothman et al., C. R. Physique 6 (2005).},
  doi      = {https://doi.org/10.1016/j.crhy.2005.09.001},
  keywords = {Spectroscopic database, Molecular spectroscopy, Line shape, Absorption parameters, Infrared cross-sections, HITRAN, GEISA, MIPAS, Banques de données spectroscopiques, Spectroscopie moléculaire, Profil spectral, Paramètres d'absorption, Sections efficaces (infrarouge), HITRAN, GEISA, MIPAS},
  url      = {http://www.sciencedirect.com/science/article/pii/S163107050500112X},
}

@Article{Agreste2014,
  author   = {Santa Agreste and Pasquale De Meo and Emilio Ferrara and Domenico Ursino},
  title    = {XML Matchers: Approaches and challenges},
  journal  = {Knowledge-Based Systems},
  year     = {2014},
  volume   = {66},
  pages    = {190 - 209},
  issn     = {0950-7051},
  abstract = {Schema Matching, i.e. the process of discovering semantic correspondences between concepts adopted in different data source schemas, has been a key topic in Database and Artificial Intelligence research areas for many years. In the past, it was largely investigated especially for classical database models (e.g., E/R schemas, relational databases, etc.). However, in the latest years, the widespread adoption of XML in the most disparate application fields pushed a growing number of researchers to design XML-specific Schema Matching approaches, called XML Matchers, aiming at finding semantic matchings between concepts defined in DTDs and XSDs. XML Matchers do not just take well-known techniques originally designed for other data models and apply them on DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical structure of a DTD/XSD) to improve the performance of the Schema Matching process. The design of XML Matchers is currently a well-established research area. The main goal of this paper is to provide a detailed description and classification of XML Matchers. We first describe to what extent the specificities of DTDs/XSDs impact on the Schema Matching task. Then we introduce a template, called XML Matcher Template, that describes the main components of an XML Matcher, their role and behavior. We illustrate how each of these components has been implemented in some popular XML Matchers. We consider our XML Matcher Template as the baseline for objectively comparing approaches that, at first glance, might appear as unrelated. The introduction of this template can be useful in the design of future XML Matchers. Finally, we analyze commercial tools implementing XML Matchers and introduce two challenging issues strictly related to this topic, namely XML source clustering and uncertainty management in XML Matchers.},
  doi      = {https://doi.org/10.1016/j.knosys.2014.04.044},
  keywords = {Schema Matching, DTD, XML Schema, XSD, XML source clustering, Uncertainty management in XML Matchers},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950705114001749},
}

@Article{Rinderle2004,
  author   = {Stefanie Rinderle and Manfred Reichert and Peter Dadam},
  title    = {Correctness criteria for dynamic changes in workflow systems––a survey},
  journal  = {Data \& Knowledge Engineering},
  year     = {2004},
  volume   = {50},
  number   = {1},
  pages    = {9 - 34},
  issn     = {0169-023X},
  note     = {Advances in business process management},
  abstract = {The capability to dynamically adapt in-progress workflows (WF) is an essential requirement for any workflow management system (WfMS). This fact has been recognized by the WF community for a long time and different approaches in the area of adaptive workflows have been developed so far. This survey systematically classifies these approaches and discusses their strengths and limitations along typical problems related to dynamic WF change. Along this classification we present important criteria for the correct adaptation of running workflows and analyze how actual approaches satisfy them. Furthermore, we provide a detailed comparison of these approaches and sketch important further issues related to dynamic change.},
  doi      = {https://doi.org/10.1016/j.datak.2004.01.002},
  keywords = {Workflow management, Adaptive systems, Dynamic workflow changes, Correctness criteria},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X04000035},
}

@Article{Mark1990,
  author   = {Leo Mark and Nick Roussopoulos},
  title    = {Information interchange between self-describing databases},
  journal  = {Information Systems},
  year     = {1990},
  volume   = {15},
  number   = {4},
  pages    = {393 - 400},
  issn     = {0306-4379},
  abstract = {Within the framework of a self-describing database system we describe a set of data management tools and a data dictionary supporting information interchange. The concepts are based on our experience from a project on standardized information interchange in NASA.},
  doi      = {https://doi.org/10.1016/0306-4379(90)90043-O},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799090043O},
}

@Article{Redondo2013,
  author   = {Jose Manuel Redondo and Francisco Ortin},
  title    = {Efficient support of dynamic inheritance for class- and prototype-based languages},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {2},
  pages    = {278 - 301},
  issn     = {0164-1212},
  abstract = {Dynamically typed languages are becoming increasingly popular for different software development scenarios where runtime adaptability is important. Therefore, existing class-based platforms such as Java and .Net have been gradually incorporating dynamic features to support the execution of these languages. The implementations of dynamic languages on these platforms commonly generate an extra layer of software over the virtual machine, which reproduces the reflective prototype-based object model provided by most dynamic languages. Simulating this model frequently involves a runtime performance penalty, and makes the interoperation between class- and prototype-based languages difficult. Instead of simulating the reflective model of dynamic languages, our approach has been to extend the object-model of an efficient class-based virtual machine with prototype-based semantics, so that it can directly support both kinds of languages. Consequently, we obtain the runtime performance improvement of using the virtual machine JIT compiler, while a direct interoperation between languages compiled to our platform is also possible. In this paper, we formalize dynamic inheritance for both class- and prototype-based languages, and implement it as an extension of an efficient virtual machine that performs JIT compilation. We also present an extensive evaluation of the runtime performance and memory consumption of the programming language implementations that provide dynamic inheritance, including ours.},
  doi      = {https://doi.org/10.1016/j.jss.2012.08.016},
  keywords = {Dynamic inheritance, Prototype-based object-oriented model, JIT compilation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002324},
}

@Article{Wong2008,
  author   = {Man Leung Wong and Yuan Yuan Guo},
  title    = {Learning Bayesian networks from incomplete databases using a novel evolutionary algorithm},
  journal  = {Decision Support Systems},
  year     = {2008},
  volume   = {45},
  number   = {2},
  pages    = {368 - 383},
  issn     = {0167-9236},
  note     = {I.T. and Value Creation},
  abstract = {This paper proposes a novel method for learning Bayesian networks from incomplete databases in the presence of missing values, which combines an evolutionary algorithm with the traditional Expectation Maximization (EM) algorithm. A data completing procedure is presented for learning and evaluating the candidate networks. Moreover, a strategy is introduced to obtain better initial networks to facilitate the method. The new method can also overcome the problem of getting stuck in sub-optimal solutions which occurs in most existing learning algorithms. The experimental results on the databases generated from several benchmark networks illustrate that the new method has better performance than some state-of-the-art algorithms. We also apply the method to a data mining problem and compare the performance of the discovered Bayesian networks with the models generated by other learning algorithms. The results demonstrate that our method outperforms other algorithms.},
  doi      = {https://doi.org/10.1016/j.dss.2008.01.002},
  keywords = {Data mining, Machine learning, Bayesian networks, Evolutionary algorithms},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923608000171},
}

@Article{Bento1997,
  author   = {J. Bento and B. Feijó and D.L. Smith},
  title    = {Engineering design knowledge representation based on logic and objects},
  journal  = {Computers \& Structures},
  year     = {1997},
  volume   = {63},
  number   = {5},
  pages    = {1015 - 1032},
  issn     = {0045-7949},
  abstract = {This paper presents a hybrid framework to integrate first-order logic into the object-oriented paradigm for representing engineering design knowledge. The object-oriented nature of engineering design activities are analysed and details of the programming environment are provided.},
  doi      = {https://doi.org/10.1016/S0045-7949(96)00247-7},
  url      = {http://www.sciencedirect.com/science/article/pii/S0045794996002477},
}

@Article{Kensche2009,
  author   = {David Kensche and Christoph Quix and Xiang Li and Yong Li and Matthias Jarke},
  title    = {Generic schema mappings for composition and query answering},
  journal  = {Data \& Knowledge Engineering},
  year     = {2009},
  volume   = {68},
  number   = {7},
  pages    = {599 - 621},
  issn     = {0169-023X},
  note     = {Special Issue: 26th International Conference on Conceptual Modeling (ER 2007) – Six selected and extended papers},
  abstract = {In this article, we present extensional mappings, that are based on second-order tuple generating dependencies between models in our Generic Role-based Metamodel GeRoMe. Our mappings support data translation between heterogeneous models, such as XML schemas, relational schemas, or OWL ontologies. The mapping language provides grouping functionalities that allow for complete restructuring of data, which is necessary for handling object oriented models and nested data structures such as XML. Furthermore, we present algorithms for mapping composition and optimization of the composition result. To verify the genericness, correctness, and composability of our approach we implemented a data translation tool and mapping export for several data manipulation languages. Furthermore, we address the question how generic schema mappings can be harnessed for answering queries against an integrated global schema.},
  doi      = {https://doi.org/10.1016/j.datak.2009.02.006},
  keywords = {Model management, Schema mappings, Mapping composition, Executable mappings, Data integration},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X09000214},
}

@Article{Eastman1996,
  author   = {Charles M Eastman},
  title    = {Managing integrity in design information flows},
  journal  = {Computer-Aided Design},
  year     = {1996},
  volume   = {28},
  number   = {6},
  pages    = {551 - 565},
  issn     = {0010-4485},
  abstract = {This paper addressses integrity rules that are embedded within engineering design applications and that apply between applications. A representation for integrity rules that are embedded in applications is presented and a set of related methods developed for: (a) maintaining the integrity condition of application developed data, (b) managing the precedence order between applications, in the context of (c) changing the schema and the associated mix of applications and (d) iterated execution of applications and change propagation. Both integrity rules literally embedded within external applications and others required to be embedded within a database are considered. The techniques are demonstrated with an extensive example.},
  doi      = {https://doi.org/10.1016/0010-4485(95)00069-0},
  keywords = {integrity, constraints, concurrent engineering, change propagation},
  url      = {http://www.sciencedirect.com/science/article/pii/0010448595000690},
}

@Article{Jarzabek1996,
  author   = {Stan Jarzabek and Tok Wang Ling},
  title    = {Model-based support for business re-engineering},
  journal  = {Information and Software Technology},
  year     = {1996},
  volume   = {38},
  number   = {5},
  pages    = {355 - 374},
  issn     = {0950-5849},
  abstract = {What do we need to know about the business in order to understand and, eventually, to improve business operations? Many business modelling methods have been described in the literature and applied in business re-engineering projects. We feel that current business modelling methods do not have a precise enough model of the underlying business knowledge. A model should be comprehensive enough to allow for a systematic study and precise formulation of re-engineering methods. It should also provide a framework for designing tools to support business re-engineering projects. We identify information requirements for business re-engineering based on the commonly used business re-engineering methods and case studies published in the literature. We formalized these requirements within the conceptual business model that is described in this paper. Business models vary from a company to company and from one business re-engineering project to another. Therefore, we build a generic model first and then we customize the generic model to the needs of a given company and a re-engineering project in hand. We build the core of a tool environment for business re-engineering around the generic business model. We achieve a required level of tool flexibility by applying meta-CASE techniques. We derive the physical schema for tool repository and generate customized tools from the customized business model specifications. Tools built around our model can support business knowledge acquisition, business process modelling, performance/quality analysis and analysis of alternative business process re-engineering solutions. In the paper, we describe our business model and an integrated computer-aided business understanding and re-engineering tool environment that we build around the business model. We illustrate benefits of building tools on a rich business model, focusing on tools and analysis methods that have not been extensively described in other sources.},
  doi      = {https://doi.org/10.1016/0950-5849(95)01047-5},
  keywords = {Business re-engineering, Conceptual modelling, Business modelling tools},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584995010475},
}

@Article{Jeong2004,
  author   = {Dongwon Jeong and Doo-Kwon Baik},
  title    = {Incremental data integration based on hierarchical metadata registry with data visibility},
  journal  = {Information Sciences},
  year     = {2004},
  volume   = {162},
  number   = {3},
  pages    = {147 - 181},
  issn     = {0020-0255},
  abstract = {A considerable number of researches have been studied on data integration based on metadata. However, existing approaches require too much cost to build an initial guideline. Most important reason is that the previous researches have not seriously considered the corresponding domain properties such as the data level and the user level. First, it is difficult in practice to create a standardized guideline on the entire data set, if there is a restricted cost given. Thus, a set of data to be integrated should be selected first. However, most databases have no statistical information that may be used to select such a set of data according to its usability. In this paper, we propose LOG (localization-based global metadata registry) methodology to build a guideline and integrate databases progressively considering the domain properties. The key idea is that the priorities of databases to be integrated are determined by the relationship to the domain properties. We also show the implementation by applying it to actual databases in Korea Institute of Science and Technology Information, which builds and manages a considerable number of databases on the science and technology in Korea. The LOG provides an incremental build method of metadata registry, and also supports progressive data integration mechanism on the existing distributed databases. It especially gives successful and efficient output on the creation of a standard guideline in the situation where the given cost is restricted.},
  doi      = {https://doi.org/10.1016/j.ins.2003.09.008},
  keywords = {Metadata registry, MDR, Data visibility, Incremental data integration, Hierarchical MDR},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025503003037},
}

@Article{Borzemski1985,
  author   = {L. Borzemski and S. Lebiediewa},
  title    = {Dipos: A Database for Computer Aided Design in Electric Power Systems},
  journal  = {IFAC Proceedings Volumes},
  year     = {1985},
  volume   = {18},
  number   = {8},
  pages    = {261 - 265},
  issn     = {1474-6670},
  note     = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
  abstract = {The database system named DIPOS intended for use in computer-aided electric power system expansion planning is presented. The CAD system containing the DIPOS database is briefly explained in the context of the transmission network expansion planning. There are many reasons that in CAD applications we should rather think about specialized or problem-oriented databases, which would give unique solutions in particular situations, than general purpose databases with maintenance and operation overheads. In the paper the user's view of data, user interface, database data model and physical database organization are discussed. The system is the relational database from the user's point of view but it employs the network database data model at the system level. This allows the user to have well known natural relational data model with its correctness and ability to perform Profitable relational operations, whereas the DIPOS database network data model gives additional advantages in the pre-definition of possible use patterns of the data. An example of database application is also presented.},
  doi      = {https://doi.org/10.1016/S1474-6670(17)60378-1},
  keywords = {Computer-aided system design, data handling, databases, power systems planning},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667017603781},
}

@InCollection{Halpin2001a,
  author    = {Terry Halpin},
  title     = {3 - Conceptual Modeling: First Steps},
  booktitle = {Information Modeling and Relational Databases},
  publisher = {Academic Press},
  year      = {2001},
  editor    = {Terry Halpin},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {55 - 107},
  address   = {San Diego},
  isbn      = {978-1-55860-672-2},
  abstract  = {Publisher Summary
Conceptual modeling portrays the application domain at a high level, using terms and concepts familiar to the application users, ignoring logical- and physical-level aspects (that is, the underlying database or programming structures used for implementation) and external-level aspects (that is, the screen forms used for data entry). This chapter discusses the desirable characteristics for any language to be used for conceptual modeling: expressibility, clarity, simplicity and orthogonality, semantic stability, semantic relevance, validation mechanisms, abstraction mechanisms, and formal foundation. With large-scale applications, the UoD can be divided into convenient modules. The conceptual schema design procedure (CSDP) is applied to each, and the resulting subschemas can be integrated into the global conceptual schema. This chapter explores three steps of CSDP: verbalizing familiar information examples as facts; refining these into formal, elementary facts, and applying quality checks; drawing the fact types, and applying a population check; and checking for entity types that should be combined. This object-role modeling approach has been used productively in industry for over 25 years.},
  doi       = {https://doi.org/10.1016/B978-155860672-2/50006-3},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558606722500063},
}

@InCollection{Li1993,
  author    = {Qing Li and Dennis McLeod},
  title     = {Managing Interdependencies among Objects in Federated Databases11Research by the two authors were supported, in part, by the Hong Kong Research Grants Council under grant DAG91/92.EG13, and by the National Science Foundation under grant IRI-9021028 respectively.},
  booktitle = {Interoperable Database Systems (Ds-5)},
  publisher = {North-Holland},
  year      = {1993},
  editor    = {DAVID K. HSIAO and ERICH J. NEUHOLD and RON SACKS-DAVIS},
  series    = {IFIP Transactions A: Computer Science and Technology},
  pages     = {331 - 347},
  address   = {Amsterdam},
  isbn      = {978-0-444-89879-1},
  abstract  = {An important issue to be addressed in supporting information sharing in federated database systems is the management of the inherent interdependencies among data in different databases. In particular, data in such databases can (implicitly and/or explicitly) exhibit various forms of interdependence, such as existence, structural, and functional/behavioral dependencies. The ability to capture and systematically support such intrinsic interdependency relationships is essential. In this paper we analyze these interdependencies in the context of an object-oriented database federation, and describe how object-oriented concepts and mechanisms can be employed to accommodate them.},
  doi       = {https://doi.org/10.1016/B978-0-444-89879-1.50024-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444898791500245},
}

@InCollection{Halpin2001b,
  author    = {Terry Halpin},
  title     = {7 - Other Constraints and Final Checks},
  booktitle = {Information Modeling and Relational Databases},
  publisher = {Academic Press},
  year      = {2001},
  editor    = {Terry Halpin},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {277 - 311},
  address   = {San Diego},
  isbn      = {978-1-55860-672-2},
  abstract  = {Publisher Summary
This chapter deals with adding other constraints and performing final checks. An occurrence frequency constraint indicates that an entry in a column (or column combination) must occur there exactly n times (n), at most n times (≤ n), at least n times (≥ n), or at least n and at most m times (n .. m). A ring constraint may apply only to a pair of roles played by the same (or a compatible) object type. The role pair may form a binary predicate or be embedded in a longer predicate. An object cardinality constraint limits the cardinality of each population of an object type. A relative closure constraint indicates that if an object in the population of the object type plays that role in the real world, it also plays that role in the model. These constraints are converted to equivalent open world constructs before mapping. At the end of the conceptual schema design procedure, final checks are made to ensure that the conceptual schema is internally consistent (its constraint pattern can be populated without contradiction), is externally consistent (agrees with original data and conditions), is redundancy free (elementary facts cannot be repeated), and is complete (covers all the requirements). Various cases of derived redundancy can be safely managed.},
  doi       = {https://doi.org/10.1016/B978-155860672-2/50010-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558606722500105},
}

@Article{Demurjian1993,
  author   = {SA Demurjian and GM Beshers and TC Ting},
  title    = {Programming versus databases in the object-oriented paradigm},
  journal  = {Information and Software Technology},
  year     = {1993},
  volume   = {35},
  number   = {2},
  pages    = {78 - 88},
  issn     = {0950-5849},
  abstract = {The object-oriented paradigm has come to the forefront of the research community in the software engineering, programming language, and database research areas. Moreover, the paradigm appears capable of supporting advanced applications such as software development environments (SDEs) that require both programming ability and persistency via a database system. However, there exists a disparity between the programming and database approaches to the object-oriented paradigm. The paper examines and discusses this disparity between the two approaches for the purpose of formulating an understanding of their commonalities and differences. This understanding has been instrumental in supporting work involving the prototyping of SDEs using the object-oriented paradigm, an examination of the techniques required to evolve a class library for persistency, and the proposal of a software architecture and functionality of a persistent programming language system. Thus, it is believed that the work presented in this paper can serve as a framework for researchers and practitioners whose efforts include the aforementioned or other, related areas. From a content perspective, this paper provides a comparative analysis between the concepts of programming and databases for the object-oriented paradigm, through a detailed presentation of system-level and model-level considerations. Both philosophical concepts and implementation pragmatics are investigated. A practical examination of the C++ programming language versus the Opal data language has been conducted, revealing many valuable insights of systems and application details and issues. Features of both approaches are also analysed and illustrated.},
  doi      = {https://doi.org/10.1016/S0950-5849(05)80002-3},
  keywords = {object-oriented paradigm, programming languages, database systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584905800023},
}

@Article{Lingat1987,
  author   = {JY Lingat and P Nobecourt and C Rolland},
  title    = {Rubis: an extended relational system managing events Part I: specification},
  journal  = {Information and Software Technology},
  year     = {1987},
  volume   = {29},
  number   = {9},
  pages    = {503 - 510},
  issn     = {0950-5849},
  abstract = {Dynamic aspects of information systems are taken into account in a lot of conceptual models. However, the dynamic concepts of these models have rarely been fully implemented in database management systems (DBMSs). Rubis is an extended relational DBMS which supports an extended relational schema (including event and operation concepts) and automatic control of the dynamic aspects of applications, i.e. event recognition, operation triggering and time handling. The first part of the paper contains a short presentation of the basic concepts and the specification language used for the extended schema. The second part focuses on the internal mechanisms: the temporal processor, which manages the temporal aspects of specifications and recognizes temporal events; and the event processor which manages events treatment and synchronization. These two mechanisms permit an automatic execution of the extended schema and so provide rapid prototyping capabilities. This last part will be covered in the December issue of this journal.},
  doi      = {https://doi.org/10.1016/0950-5849(87)90006-1},
  keywords = {data processing, information systems, databases, prototyping},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584987900061},
}

@Article{FerreiraRezende1997,
  author   = {Fernando de Ferreira Rezende and Theo Härder},
  title    = {Exploiting abstraction relationships' semantics for transaction synchronization in KBMSs},
  journal  = {Data \& Knowledge Engineering},
  year     = {1997},
  volume   = {22},
  number   = {3},
  pages    = {233 - 259},
  issn     = {0169-023X},
  abstract = {Currently, knowledge sharing is turning out to be a crucial area that needs to be supported by Knowledge Base Management Systems (KBMSs). We propose an approach for transaction synchronization in KBMSs-LARS (Locks using Abstraction Relationships' Semantics). We show how we obtain serializability of transactions thereby providing different locking granules. The main benefit of our technique is the high degree of potential concurrency, which is obtained by means of a logical partitioning of the knowledge base (KB) grounded in the abstraction relationships, and the provision of many lock types to be used on the basis of each partition. In this way, we capture the abstraction relationships' semantics which are contained in a KB graph for transaction synchronization purposes and enable the exploitation of the inherent parallelism in a knowledge representation approach.},
  doi      = {https://doi.org/10.1016/S0169-023X(96)00049-3},
  keywords = {Transaction synchronization, Concurrency control, Locking, Knowledge base management systems, Object-oriented database systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X96000493},
}

@Article{McLeod1991,
  author   = {D McLeod},
  title    = {Perspective on object databases},
  journal  = {Information and Software Technology},
  year     = {1991},
  volume   = {33},
  number   = {1},
  pages    = {13 - 21},
  issn     = {0950-5849},
  abstract = {Databases based on object-oriented and semantic database models (‘object databases’) represent significant advances over their record-based historical predecessors. In the context of general-purpose database management systems, the fundamentals of object database models are examined. Comparisons are made with record-based databases, as represented by relational database technology. The principal concepts underlying structurally object-oriented (semantic) database models are presented. Finally, a historical perspective on the evolution of object database technology is provided.},
  doi      = {https://doi.org/10.1016/0950-5849(91)90019-8},
  keywords = {databases, object databases, object-oriented databases, semantic databases, database models},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584991900198},
}

@Article{2018,
  title   = {ASGE Program},
  journal = {Gastrointestinal Endoscopy},
  year    = {2018},
  volume  = {87},
  number  = {6, Supplement},
  pages   = {AB1 - AB45},
  issn    = {0016-5107},
  note    = {DDW 2018 ASGE Program and Abstracts},
  doi     = {https://doi.org/10.1016/S0016-5107(18)30289-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S001651071830289X},
}

@Article{Brown2004,
  author   = {Dave Brown and David Leal and Chris McMahon and Rose Crossland and Janardan Devlukia},
  title    = {A Web-enabled virtual repository for supporting distributed automotive component development},
  journal  = {Advanced Engineering Informatics},
  year     = {2004},
  volume   = {18},
  number   = {3},
  pages    = {173 - 190},
  issn     = {1474-0346},
  abstract = {This paper describes a web-enabled repository system that has been designed for supporting distributed automotive component development. The repository is based on an integrated product and process life-cycle information model, derived from ISO standards and using EPISTLE generic entity modelling principles. This core model can be used with reference data libraries, such as for activities, components, documents and properties. The scope and characteristics of the model are described, along with its relation to standard product data models and classification schemes. Component design and analysis representational models are referenced as resources through document meta-data, with virtual access via the repository. The modelling approach is compatible with the current development of ontologies and topic maps for the Semantic Web, with an inter-operability route via the Resource Description Framework (RDF). The paper also provides an outline of the system architecture, the associated Web tools and the Business Object library for building client-server applications. An industrial application for fatigue studies is described, illustrating comparison of analysis and test data, repository search and provision of best practice advice. The aim is to produce a flexible system that can be populated and extended directly by engineers, shielding them from the details of the information model.},
  doi      = {https://doi.org/10.1016/j.aei.2004.12.001},
  keywords = {Product and process modelling, Generic entity core models, Engineering repository, Semantic Web, Fatigue analysis, Classification standards},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474034605000042},
}

@Article{Kim1992,
  author   = {Won Kim and Mark Scheevel and Chris Tomlinson},
  title    = {Object-oriented databases for new applications},
  journal  = {Future Generation Computer Systems},
  year     = {1992},
  volume   = {7},
  number   = {2},
  pages    = {317 - 327},
  issn     = {0167-739X},
  abstract = {As applications have grown more sophisticated, they have placed greater demands on underlying database technology. In this paper we examine some of the shortcomings of conventional database technologies in the face of these demands, and then discuss how emerging object-oriented database technology addresses these shortcomings in several specific application areas: user interface applications, multimedia databases, CAD databases, statistical and scientific databases, and management of heterogeneous database systems. We conclude with a list of remaining challenges and some suggestions for further research.},
  doi      = {https://doi.org/10.1016/0167-739X(92)90019-8},
  keywords = {Object-oriented, object-oriented databases, statistical databases, inheritance, computer-aided engineering and design},
  url      = {http://www.sciencedirect.com/science/article/pii/0167739X92900198},
}

@InCollection{Zhu2004,
  author    = {Shanzhong Zhu and Chinya V. Ravishankar},
  title     = {- Stochastic Consistency, and Scalable Pull-Based Caching for Erratic Data Stream Sources1},
  booktitle = {Proceedings 2004 VLDB Conference},
  publisher = {Morgan Kaufmann},
  year      = {2004},
  editor    = {Mario A. Nascimento and M. Tamer Özsu and Donald Kossmann and Renée J. Miller and José A. Blakeley and Berni Schiefer},
  pages     = {192 - 203},
  address   = {St Louis},
  isbn      = {978-0-12-088469-8},
  abstract  = {Publisher Summary
This chapter introduces the notion of stochastic consistency, and proposes a novel approach for achieving it for caches of highly erratic data. Erratic data sources such as stock prices and sensor data are common and important in practice. However, their erratic patterns of change make caching hard. Stochastic consistency guarantees that errors in cached values of erratic data remain within a user-specified bound, with a user-specified probability. A Brownian motion model is used to capture the behavior of data changes and its theory is used to predict when caches should initiate pulls to refresh cached copies to maintain stochastic consistency. This approach allows servers to remain totally stateless, thus achieving excellent scalability and reliability. The chapter also discusses a new real-time scheduling approach for servicing pull requests at the server. The scheduler delivers prompt response whenever possible, and minimizes the aggregate cache-source deviation due to delays during server overload.},
  doi       = {https://doi.org/10.1016/B978-012088469-8.50020-6},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780120884698500206},
}

@Article{Skevington1988,
  author  = {Craig Skevington and Cheng Hsu},
  title   = {Manufacturing architecture for integrated systems},
  journal = {Robotics and Computer-Integrated Manufacturing},
  year    = {1988},
  volume  = {4},
  number  = {3},
  pages   = {619 - 623},
  issn    = {0736-5845},
  note    = {Special Issue Manufacturing Systems and Technology of the Future},
  doi     = {https://doi.org/10.1016/0736-5845(88)90033-6},
  url     = {http://www.sciencedirect.com/science/article/pii/0736584588900336},
}

@Article{Nadkarni1995,
  author   = {Prakash M. Nadkarni and Kei-Hoi Cheung},
  title    = {SQLGEN: A framework for rapid client-server database application development},
  journal  = {Computers and Biomedical Research},
  year     = {1995},
  volume   = {28},
  number   = {6},
  pages    = {479 - 499},
  issn     = {0010-4809},
  abstract = {SQLGEN is a framework for rapid client-server relational database application development. It relies on an active data dictionary on the client machine that stores metadata on one or more database servers to which the client may be connected. The dictionary generates dynamic Structured Query Language (SQL) to perform common database operations; it also stores information about the access rights of the user at log-in time, which is used to partially self-configure the behavior of the client to disable inappropriate user actions. SQLGEN uses a microcomputer database as the client to store metadata in relational form, to transiently capture server data in tables, and to allow rapid application prototyping followed by porting to client-server mode with modest effort. SQLGEN is currently used in several production biomedical databases.},
  doi      = {https://doi.org/10.1006/cbmr.1995.1030},
  url      = {http://www.sciencedirect.com/science/article/pii/S0010480985710300},
}

@Article{Giorgini2005,
  author   = {Paolo Giorgini and John Mylopoulos and Roberto Sebastiani},
  title    = {Goal-oriented requirements analysis and reasoning in the Tropos methodology},
  journal  = {Engineering Applications of Artificial Intelligence},
  year     = {2005},
  volume   = {18},
  number   = {2},
  pages    = {159 - 171},
  issn     = {0952-1976},
  note     = {Agent-oriented Software Development},
  abstract = {Tropos is an agent-oriented software methodology proposed in (J. Autonomous Agents Multi-Agent Syst. 8(3) (2004) 203; Inf. Syst. 27(6) (2002) 365). The methodology is founded on the notions of agent and goal, and goal analysis is used extensively to support software development during different phases. This paper adopts a formal goal model defined and analyzed in (J. Data Semantics 1 (2003); Proceedings of the International Conference on Advanced Information Systems Engineering, CAISE’04, vol. 3804 of LNCS, Springer, Berlin, 2004, pp. 20–33) to make the goal analysis process concrete through the use of forward and backward reasoning for goal models. The formal goal analysis is illustrated through examples, using an implemented goal reasoning tool.},
  doi      = {https://doi.org/10.1016/j.engappai.2004.11.017},
  keywords = {Agent-oriented software development, Goal-oriented requirements analysis, Early requirements analysis, Multi-agent systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0952197604001885},
}

@Article{Alhajj1999,
  author   = {Reda Alhajj and Ashraf Elnagar},
  title    = {Incremental materialization of object-oriented views},
  journal  = {Data \& Knowledge Engineering},
  year     = {1999},
  volume   = {29},
  number   = {2},
  pages    = {121 - 145},
  issn     = {0169-023X},
  abstract = {We present an approach to handle incremental materialization of object-oriented views. Queries that define views are implemented as methods that are invoked to compute corresponding views. To avoid computation from scratch each time a view is accessed, we introduce some deferred update algorithms that reflect for a view only related modifications introduced into the database while that view was inactive. A view is updated by considering modifications performed within all classes along the inheritance and class-composition subhierarchies rooted at every class used in deriving that view. To each class, we add a modification list to keep one modification tuple per view dependent on that class. Such a tuple acts as a reference point that marks the start of the next update to the corresponding view.},
  doi      = {https://doi.org/10.1016/S0169-023X(98)00042-1},
  keywords = {Algorithms, Deferred update, Incremental update, Materialized views, Object-oriented databases},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X98000421},
}

@InCollection{STEIGER1992,
  author    = {DAVID STEIGER and RAMESH SHARDA and BRIAN LeCLAIRE},
  title     = {FUNCTIONAL DESCRIPTION OF A GRAPH-BASED INTERFACE FOR NETWORK MODELING (GIN)},
  booktitle = {Computer Science and Operations Research},
  publisher = {Pergamon},
  year      = {1992},
  editor    = {OSMAN BALCI and RAMESH SHARDA and STAVROS A. ZENIOS},
  pages     = {213 - 229},
  address   = {Amsterdam},
  isbn      = {978-0-08-040806-4},
  abstract  = {ABSTRACT
In recent years the MS/OR profession has made important advances in the solution techniques of network optimization models. However, significantly less progress has been made in 1) the interfaces between these models and the model builders and users, and 2) the documentation and validation of the model logic. Furthermore, while text-based model development systems such as GAMS do help in reducing the drudgery associated with model development and documentation, recent advances in microcomputer graphics offer an even more versatile tool for this process. The purpose of this paper is to describe a partially implemented graph-based interface for network modeling, GIN, which is designed for formulating, solving and analyzing minimum cost flow network models using the pictorial representations of NETFORMS. This system is being implemented in an interactive, graphics-based, microcomputer environment using object-oriented programming tools.},
  doi       = {https://doi.org/10.1016/B978-0-08-040806-4.50020-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080408064500204},
}

@Article{Atzeni2012,
  author   = {Paolo Atzeni and Luigi Bellomarini and Francesca Bugiotti and Fabrizio Celli and Giorgio Gianforme},
  title    = {A runtime approach to model-generic translation of schema and data},
  journal  = {Information Systems},
  year     = {2012},
  volume   = {37},
  number   = {3},
  pages    = {269 - 287},
  issn     = {0306-4379},
  abstract = {To support heterogeneity is a major requirement in current approaches to integration and transformation of data. This paper proposes a new approach to the translation of schema and data from one data model to another, and we illustrate its implementation in the tool MIDST-RT. We leverage on our previous work on MIDST, a platform conceived to perform translations in an off-line fashion. In such an approach, the source database (both schema and data) is imported into a repository, where it is stored in a universal model. Then, the translation is applied within the tool as a composition of elementary transformation steps, specified as Datalog programs. Finally, the result (again both schema and data) is exported into the operational system. Here we illustrate a new, lightweight approach where the database is not imported. MIDST-RT needs only to know the schema of the source database and the model of the target one, and generates views on the operational system that expose the underlying data according to the corresponding schema in the target model. Views are generated in an almost automatic way, on the basis of the Datalog rules for schema translation. The proposed solution can be applied to different scenarios, which include data and application migration, data interchange, and object-to-relational mapping between applications and databases.},
  doi      = {https://doi.org/10.1016/j.is.2011.11.003},
  keywords = {Model management, Data model, Schema translation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437911001542},
}

@Article{Motschnig-Pitrik1996,
  author   = {Renate Motschnig-Pitrik},
  title    = {Requirements and comparison of view mechanisms for object-oriented databases},
  journal  = {Information Systems},
  year     = {1996},
  volume   = {21},
  number   = {3},
  pages    = {229 - 252},
  issn     = {0306-4379},
  abstract = {Views in the relational data model have proved indispensable in order to provide logical data independence through external schemas that can be customized to meet the needs of individual users. Since object-oriented database systems (OODBs) are expected to provide at least the functionality of today's relational systems, view mechanisms for OODBs have recently received attention. Due to the higher expressive power of object-oriented data models when compared with the relational one, different interpretations of an object-oriented view concept coexist. This paper states the requirements and discusses the features of object-oriented view mechanisms, in order to review and systematically compare representative proposals from the literature. Individual issues concerning object-oriented views, such as view definition, typing, object-preservation versus creation, view updates, and positioning of view classes into a class/type hierarchy are addressed. The paper aims to provide insight into the state-of-the-art of object-oriented views, to discuss some trade-offs in supporting advanced features, to draw some general conclusions regarding object-oriented data models, and to indicate issues for further research.},
  doi      = {https://doi.org/10.1016/0306-4379(96)00013-0},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437996000130},
}

@Article{Stults1995,
  author   = {John T Stults},
  title    = {Matrix-assisted laser desorption/ionization mass spectrometry (MALDI-MS)},
  journal  = {Current Opinion in Structural Biology},
  year     = {1995},
  volume   = {5},
  number   = {5},
  pages    = {691 - 698},
  issn     = {0959-440X},
  abstract = {Matrix-assisted laser desorption/ionization mass spectrometry (MALDI-MS) has been responsible for solving many problems in structural biology. Mass analysis is now used routinely to confirm proper expression and processing of proteins, and to locate and identify post-translational modifications. Innovative advances in instrumentation have led to higher mass resolution and mass accuracy. New sample preparation methods are likewise yielding higher sensitivity plus greater tolerance for buffer components that have in the past suppressed signals at higher concentrations. Advancements in the technique have also led to new or improved applications in many areas, including peptide sequencing and the identification of proteins by database searching with peptide masses. Instruments with lower cost, smaller size, and higher performance are making mass measurements available to an increasing number of laboratories. MALDI-MS is poised to continue to improve in performance and in its usefulness for current and new applications.},
  doi      = {https://doi.org/10.1016/0959-440X(95)80063-8},
  url      = {http://www.sciencedirect.com/science/article/pii/0959440X95800638},
}

@Article{García-Molina2002,
  author   = {Jesús Garcı́a-Molina and Marı́a-José Ortı́n-Ibáñez and Ginés Garcı́a-Mateos},
  title    = {Extending the ODMG standard with views},
  journal  = {Information and Software Technology},
  year     = {2002},
  volume   = {44},
  number   = {3},
  pages    = {161 - 173},
  issn     = {0950-5849},
  abstract = {Views are an important functionality provided by the relational database systems. However, commercial object-oriented database systems do not support a view mechanism because defining the semantics of views in the context of an object-oriented model is more difficult than in the relational model. Indeed, views are not included in the ODMG standard. In this paper, we present a proposal aimed at including views in the ODMG, by extending the object model and the object definition language (ODL). We consider object-oriented views as having the same functionality as relational views. Views are included in the object model in such a way that (i) views make a new kind of data type definition, just as are classes, interfaces and literals, (ii) an IS-VIEW relationship is introduced in order to specify the derivation of a view from its base class, and (iii) a view instance preserves the identity of its base instance. A view can import attributes, relationships and operations from its base class, and it can also add new operations, derived attributes and derived relationships. The extent of the view is defined by an object query language (OQL) predicate. We also describe a C++ binding showing the practicability of the proposed model.},
  doi      = {https://doi.org/10.1016/S0950-5849(02)00004-6},
  keywords = {View model, View management, Object-oriented database system, ODMG standard},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584902000046},
}

@Article{Anselma2013,
  author   = {Luca Anselma and Alessio Bottrighi and Stefania Montani and Paolo Terenziani},
  title    = {Managing proposals and evaluations of updates to medical knowledge: Theory and applications},
  journal  = {Journal of Biomedical Informatics},
  year     = {2013},
  volume   = {46},
  number   = {2},
  pages    = {363 - 376},
  issn     = {1532-0464},
  abstract = {The process of keeping up-to-date the medical knowledge stored in relational databases is of paramount importance. Since quality and reliability of medical knowledge are essential, in many cases physicians’ proposals of updates must undergo experts’ evaluation before possibly becoming effective. However, until now no theoretical framework has been provided in order to cope with this phenomenon in a principled and non-ad hoc way. Indeed, such a framework is important not only in the medical domain, but in all Wikipedia-like contexts in which evaluation of update proposals is required. In this paper we propose GPVM (General Proposal Vetting Model), a general model to cope with update proposal⧹evaluation in relational databases. GPVM extends the current theory of temporal relational databases and, in particular, BCDM – Bitemporal Conceptual Data Model – “consensus” model, providing a new data model, new operations to propose and accept⧹reject updates, and new algebraic operators to query proposals. The properties of GPVM are also studied. In particular, GPVM is a consistent extension of BCDM and it is reducible to it. These properties ensure consistency with most relational temporal database frameworks, facilitating implementation on top of current frameworks and interoperability with previous approaches.},
  doi      = {https://doi.org/10.1016/j.jbi.2012.12.004},
  keywords = {Temporal clinical relational databases, BCDM semantic model, Temporal data model, algebra, proposal/evaluation operations, Proof of reducibility to standard models},
  url      = {http://www.sciencedirect.com/science/article/pii/S1532046412001876},
}

@Article{Zervakis2014,
  author   = {Georgios I. Zervakis and Spyridon Ntougias and Maria Letizia Gargano and Maria I. Besi and Elias Polemis and Milton A. Typas and Giuseppe Venturella},
  title    = {A reappraisal of the Pleurotus eryngii complex – New species and taxonomic combinations based on the application of a polyphasic approach, and an identification key to Pleurotus taxa associated with Apiaceae plants},
  journal  = {Fungal Biology},
  year     = {2014},
  volume   = {118},
  number   = {9},
  pages    = {814 - 834},
  issn     = {1878-6146},
  abstract = {The Pleurotus eryngii species-complex comprises choice edible mushrooms growing on roots and lower stem residues of Apiaceae (umbellifers) plants. Material deriving from extensive sampling was studied by mating compatibility, morphological and ecological criteria, and through analysis of ITS1-5.8S-ITS2 and IGS1 rRNA sequences. Results revealed that P. eryngii sensu stricto forms a diverse and widely distributed aggregate composed of varieties elaeoselini, eryngii, ferulae, thapsiae, and tingitanus. Pleurotuseryngii subsp. tuoliensis comb. nov. is a phylogenetically sister group to the former growing only on various Ferula species in Asia. The existence of Pleurotusnebrodensis outside of Sicily (i.e., in Greece) is reported for the first time on the basis of molecular data, while P. nebrodensis subsp. fossulatus comb. nov. is a related Asiatic taxon associated with the same plant (Prangos ferulacea). Last, Pleurotusferulaginis sp. nov. grows on Ferulago campestris in northeast Italy, Slovenia and Hungary; it occupies a distinct phylogenetic position accompanied with significant differences in spore size and mating incompatibility versus other Pleurotus populations. Coevolution with umbellifers and host/substrate specificity seem to play key roles in speciation processes within this fungal group. An identification key to the nine Pleurotus taxa growing in association with Apiaceae plants is provided.},
  doi      = {https://doi.org/10.1016/j.funbio.2014.07.001},
  keywords = {Co-evolution of plants and fungi, Fungal phylogeny, subsp. comb. nov., sp. nov., subsp. comb. nov.},
  url      = {http://www.sciencedirect.com/science/article/pii/S1878614614001044},
}

@Article{Takeshita1996,
  author   = {Toru Takeshita},
  title    = {Software technologies created in Japan},
  journal  = {Information and Software Technology},
  year     = {1996},
  volume   = {38},
  number   = {3},
  pages    = {229 - 238},
  issn     = {0950-5849},
  note     = {Information and software technology in Japan},
  abstract = {Although Japanese software technologies are generally perceived as behind the USA and Western Europe, a good number of research and development efforts are under way in this country. The author has extensively reviewed and picked those which he feels may have some Japanese originality. The topics covered in this paper include computer science theories, systems programming, languages, databases, user interfaces and software tools, most of which are more or less reflecting new paradigms and trends such as object-oriented, concurrency/parallelism, visualization and multi-media. However, because of the time and space limitations, this is by no means an exhaustive survey, nor an objective review without personal bias.},
  doi      = {https://doi.org/10.1016/0950-5849(95)01077-7},
  keywords = {Review and perspective, Software technologies, Japan},
  url      = {http://www.sciencedirect.com/science/article/pii/0950584995010777},
}

@Article{Maervoet2012,
  author   = {Joris Maervoet and Celine Vens and Greet Vanden Berghe and Hendrik Blockeel and Patrick De Causmaecker},
  title    = {Outlier detection in relational data: A case study in geographical information systems},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {5},
  pages    = {4718 - 4728},
  issn     = {0957-4174},
  abstract = {Geographical information systems are commonly used for a variety of purposes. Many of them make use of a large database of geographical data, the correctness of which strongly influences the reliability of the system. In this paper, we present an approach to quality maintenance that is based on automatic discovery of non-perfect regularities in the data. The underlying idea is that exceptions to these regularities (‘outliers’) are considered probable errors in the data, to be investigated by a human expert. A case study shows how the tool can be used for extracting valuable knowledge about outliers in real-world geographical data, in an adaptive manner to the evolving data model supporting it. While the tool aims specifically at geographical information systems, the underlying approach is more broadly applicable for quality maintenance in data-rich intelligent systems.},
  doi      = {https://doi.org/10.1016/j.eswa.2011.09.125},
  keywords = {Relational outlier detection, Geographical information systems, Quality maintenance, WARMR},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417411014485},
}

@Article{Halpin1995,
  author   = {T.A. Halpin and H.A. Proper},
  title    = {Subtyping and polymorphism in object-role modelling},
  journal  = {Data \& Knowledge Engineering},
  year     = {1995},
  volume   = {15},
  number   = {3},
  pages    = {251 - 281},
  issn     = {0169-023X},
  abstract = {Although Entity-Relationship (ER) modelling techniques are commonly used for information modelling, Object-Role Modelling (ORM) techniques are becoming increasingly popular, partly because they include detailed design procedures providing guidelines for the modeller. As with the ER approach, a number of different ORM techniques exist. In this paper, we propose an integration of two theoretically well founded ORM techniques: FORM and PSM. Our main focus is on a common terminological framework, and on the notion of subtyping. Subtyping has long been an important feature of semantic approaches to conceptual schema design. It is also the concept in which FORM and PSM differ the most in their formalization. The subtyping issue is discussed from three different viewpoints covering syntactical, identification, and population issues. Finally, a wider comparison of approaches to subtyping is made, which encompasses other ER-based and ORM-based information modelling techniques, and highlights how formal subtype definitions facilitate a comprehensive specification of subtype constraints.},
  doi      = {https://doi.org/10.1016/0169-023X(95)00005-D},
  keywords = {Object-role modelling, Conceptual modelling, Information systems, Subtyping, Polymorphism},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9500005D},
}

@Article{Thuraisingham1994,
  author   = {Bhavani Thuraisingham},
  title    = {Security issues for federated database systems},
  journal  = {Computers \& Security},
  year     = {1994},
  volume   = {13},
  number   = {6},
  pages    = {509 - 525},
  issn     = {0167-4048},
  abstract = {This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.},
  doi      = {https://doi.org/10.1016/0167-4048(91)90139-5},
  keywords = {Multilevel secure federated database system, Heterogeneity, Autonomy, Security policy, Schema, Architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/0167404891901395},
}

@Article{Nakamoto1987,
  author   = {Kent Nakamoto},
  title    = {Alternatives to information processing in consumer research New perspectives on old controversies},
  journal  = {International Journal of Research in Marketing},
  year     = {1987},
  volume   = {4},
  number   = {1},
  pages    = {11 - 27},
  issn     = {0167-8116},
  abstract = {The proposition advanced by Zajonc (1980) that affect is somehow separate from cognition has sparked heated debate over the place of these factors in consumer judgment. It seems, however, that the controversy is symptomatic of broader concerns surrounding the origins of consumer preference. Motivating many of these concerns are the limitations of the traditional information processing approach in explaining the control of judgment processes. It is noted that this issue is a venerable one, and that current thinking on the issue reflects views espoused a century ago. However, recent research on cognitive schemas, attention and automaticity, as well as studies of affective constructs can contribute to a richer understanding of the organization and expression of consumer preferences.},
  doi      = {https://doi.org/10.1016/0167-8116(87)90011-5},
  url      = {http://www.sciencedirect.com/science/article/pii/0167811687900115},
}

@Article{1991d,
  title   = {Subject index Tectonophysics volumes 191–200},
  journal = {Tectonophysics},
  year    = {1991},
  volume  = {200},
  number  = {4},
  pages   = {347 - 375},
  issn    = {0040-1951},
  doi     = {https://doi.org/10.1016/0040-1951(91)90377-5},
  url     = {http://www.sciencedirect.com/science/article/pii/0040195191903775},
}

@InCollection{Nguyen1991,
  author    = {G.T. Nguyen and D. Rieu},
  title     = {Representing design objects},
  booktitle = {Artificial Intelligence in Design '91},
  publisher = {Butterworth-Heinemann},
  year      = {1991},
  editor    = {J.S. Gero},
  pages     = {367 - 386},
  isbn      = {978-0-7506-1188-6},
  abstract  = {Engineering design applications have always been challenging to system designers. Their requirements are complex and there seem to be very few systems able to handle them globally so far. Among the requiremetns are: the definition and manipulation of composite objects, the management and control of their evolution. A detailed analysis enlighten many other issues: the definition and manipulation of semantic relationships that relate objects together, and the management of multiple representations that are simultaneously required by the designers. A proposal is made to define a data and knowledge representation model dedicated to design applications. It elaborates on object-oriented programming and knowledge representation languages. The emphasis is on the semantic relationships, the evolution and the multiple representations of design objects.},
  doi       = {https://doi.org/10.1016/B978-0-7506-1188-6.50023-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780750611886500235},
}

@InCollection{Guardiola1996,
  author    = {J. Guardiola and M. Cánovas and J.L. Iborra},
  title     = {Modelling of the biotransformation from geraniol to nerol by freely suspended and immobilised grape (Vitis vinifera) cells},
  booktitle = {Immobilized Cells},
  publisher = {Elsevier},
  year      = {1996},
  editor    = {R.H. Wijffels and R.M. Buitelaar and C. Bucke and J. Tramper},
  volume    = {11},
  series    = {Progress in Biotechnology},
  pages     = {349 - 354},
  abstract  = {Plant cell suspensions of Gamay grape cells (Vitis vinifera L. cv. Gamay Freaux) were immobilized in calcium alginate beads. Both freely suspended and immobilised cells were able to biotransform geraniol into nerol in a biphasic system based upon the culture medium and Miglyol 812N. A model was developed to describe the reaction kinetics. The model assumes that geraniol is transformed into nerol and this into geraniol. In addition both geraniol and nerol can be metabolised to other compounds. Michaelis-Menten kinetics was proposed to describe every interconversion. Once first estimates were assigned to the defined parameters (maximum specific rates of monotherpene production and degradation and saturation constants) the numeric integration of the whole set of differential equations was feasible. The resultant parameter values were useful to predict the behaviour of the immobilised system. Immobilisation did not affect, in a significant manner, the rate of interconversion. The model also gave a satisfactory representation of the reaction kinetics when Monastrell grape cell suspensions (Vitis vinifera L. cv. Monastrell) were used instead of the Gamay ones.},
  doi       = {https://doi.org/10.1016/S0921-0423(96)80047-5},
  issn      = {0921-0423},
  url       = {http://www.sciencedirect.com/science/article/pii/S0921042396800475},
}

@Article{Kondylakis2013,
  author   = {Haridimos Kondylakis and Dimitris Plexousakis},
  title    = {Ontology evolution without tears},
  journal  = {Journal of Web Semantics},
  year     = {2013},
  volume   = {19},
  pages    = {42 - 58},
  issn     = {1570-8268},
  abstract = {The evolution of ontologies is an undisputed necessity in ontology-based data integration. Yet, few research efforts have focused on addressing the need to reflect the evolution of ontologies used as global schemata onto the underlying data integration systems. In most of these approaches, when ontologies change their relations with the data sources, i.e., the mappings, are recreated manually, a process which is known to be error-prone and time-consuming. In this paper, we provide a solution that allows query answering in data integration systems under evolving ontologies without mapping redefinition. This is achieved by rewriting queries among ontology versions and then forwarding them to the underlying data integration systems to be answered. To this purpose, initially, we automatically detect and describe the changes among ontology versions using a high level language of changes. Those changes are interpreted as sound global-as-view (GAV) mappings, and they are used in order to produce equivalent rewritings among ontology versions. Whenever equivalent rewritings cannot be produced we a) guide query redefinition or b) provide the best “over-approximations”, i.e., the minimally-containing and minimally-generalized rewritings. We prove that our approach imposes only a small overhead over traditional query rewriting algorithms and it is modular and scalable. Finally, we show that it can greatly reduce human effort spent since continuous mapping redefinition is no longer necessary.},
  doi      = {https://doi.org/10.1016/j.websem.2013.01.001},
  keywords = {Ontology evolution, Data integration, Query rewriting},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826813000024},
}

@Article{Barclay1992,
  author   = {PJ Barclay and JB Kennedy},
  title    = {Semantic integrity for persistent objects},
  journal  = {Information and Software Technology},
  year     = {1992},
  volume   = {34},
  number   = {8},
  pages    = {533 - 541},
  issn     = {0950-5849},
  abstract = {Modelling constructs for specifying semantic integrity are reviewed, and their implicit execution semantics are discussed. An integrity maintenance model based on these constructs is presented. An implementation of this model in a persistent programming language is described, allowing flexible automated dynamic integrity management for applications that update a persistent store; this implementation is based on an event-driven architecture.},
  doi      = {https://doi.org/10.1016/0950-5849(92)90147-H},
  keywords = {persistent programming, conceptual modelling, semantic integrity, active object-oriented databases, code generation},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499290147H},
}

@Article{Mannino1988,
  author   = {Michael V. Mannino and Shamkant B. Navathe and Wolfgang Effelsberg},
  title    = {A rule-based approach for merging generalization hierarchies},
  journal  = {Information Systems},
  year     = {1988},
  volume   = {13},
  number   = {3},
  pages    = {257 - 272},
  issn     = {0306-4379},
  abstract = {We describe the underlying operators and rules of an interactive procedure for merging generalization hierarchies. This procedure assists a designer in defining a global view which is a view over multiple databases. The small collection of operators permit: 1.(1) connecting generalization hierarchies to form a new hierarchy,2.(2) adding and deleting subhierarchies3.(3) deleting intermediate levels. The merging procedure applies the rules in two phases. In the connecting phase, the input generalization hierarchies are connected to form a new hierarchy. The input hierarchies are typically connected at their roots but may be connected at nonroot nodes. In the subtree merging phase, the new generalization hierarchy is revised according to equivalence assertions about the attributes of the subtypes. The rules are described in detail for the case of binary merging. Extensions for the general m-way case are briefly outlined. This set of rules can form the basis of a rule-based expert system.},
  doi      = {https://doi.org/10.1016/0306-4379(88)90038-5},
  keywords = {View integration, database design, global views, generalization data models},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437988900385},
}

@Article{Baclawski1989,
  author   = {Kenneth Baclawski},
  title    = {A stochastic model of data access and communication},
  journal  = {Advances in Applied Mathematics},
  year     = {1989},
  volume   = {10},
  number   = {2},
  pages    = {175 - 200},
  issn     = {0196-8858},
  abstract = {Stochastic models are an important technique for predicting the performance of computer systems and communication networks. Although much work has been done to develop analytic and simulation models, these models usually assume that data access is uniformly distributed, that data is static, and that data access and communication occur according to the Poisson process. In practice, data access is highly skewed, does not occur at Poissonian times, and data items are constantly being created and deleted. A new stochastic model of data access is developed that includes all of these observed phenomena. The model displays a surprising richness of behavior and yet has a small number of independent parameters, is analytically tractable, and is easy to simulate. An axiomatic framework for a general class of continuous models is introduced, and a specific discrete approximation of such a model is developed in detail. The extent to which the model fits empirical observations is also discussed.},
  doi      = {https://doi.org/10.1016/0196-8858(89)90010-9},
  url      = {http://www.sciencedirect.com/science/article/pii/0196885889900109},
}

@Article{1988c,
  title   = {A geocentric approach to sponsor conducted clinical trials: Leonard Jacob and Rita Carey SK\&F labs, Philadelphia, Pennsylvania(P01)},
  journal = {Controlled Clinical Trials},
  year    = {1988},
  volume  = {9},
  number  = {3},
  pages   = {262 - 263},
  issn    = {0197-2456},
  doi     = {https://doi.org/10.1016/0197-2456(88)90135-3},
  url     = {http://www.sciencedirect.com/science/article/pii/0197245688901353},
}

@Article{Bedathur2003,
  author   = {Srikanta J Bedathur and Jayant R Haritsa and Uday S Sen},
  title    = {The building of BODHI, a bio-diversity database system},
  journal  = {Information Systems},
  year     = {2003},
  volume   = {28},
  number   = {4},
  pages    = {347 - 367},
  issn     = {0306-4379},
  note     = {Data Management in Bioinformatics},
  abstract = {We have built a database system called BODHI, intended to store plant bio-diversity information. It is based on an object-oriented modeling approach and is developed completely around public-domain software. The unique feature of BODHI is that it seamlessly integrates diverse types of data, including taxonomic characteristics, spatial distributions, and genetic sequences, thereby spanning the entire range from molecular to organism-level information. A variety of sophisticated indexing strategies are incorporated to efficiently access the various types of data, and a rule-based query processor is employed for optimizing query execution. In this paper, we report on our experiences in building BODHI and on its performance characteristics for a representative set of queries.},
  doi      = {https://doi.org/10.1016/S0306-4379(02)00073-X},
  url      = {http://www.sciencedirect.com/science/article/pii/S030643790200073X},
}

@Article{Blanken1991,
  author   = {Henk Blanken},
  title    = {Implementing version support for complex objects},
  journal  = {Data \& Knowledge Engineering},
  year     = {1991},
  volume   = {6},
  number   = {1},
  pages    = {1 - 25},
  issn     = {0169-023X},
  abstract = {New applications in the area of office information systems, computer aided design and manufacturing make new demands upon database management systems. Among others highly structured objects and their history have to be represented and manipulated. The paper discusses some general problems concerning the access and storage of complex objects with their versions and the solutions developed within the AIM/II project. Queries related to versions are distinguished in ASOF queries (asking information valid at a certain moment) and WALK-THROUGH-TIME (WTT) queries (obtaining trend information concerning a certain period). In the paper some new algorithms to handle such queries are presented. A brief analysis gives an indication about the performance of query processing in historical databases.},
  doi      = {https://doi.org/10.1016/0169-023X(91)90013-N},
  keywords = {Complex object, Historical database, Query handing},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9190013N},
}

@Article{Lanz2016,
  author   = {Andreas Lanz and Manfred Reichert and Barbara Weber},
  title    = {Process time patterns: A formal foundation},
  journal  = {Information Systems},
  year     = {2016},
  volume   = {57},
  pages    = {38 - 68},
  issn     = {0306-4379},
  abstract = {Companies increasingly adopt process-aware information systems (PAISs) to model, execute, monitor, and evolve their business processes. Though the handling of temporal constraints (e.g., deadlines or time lags between activities) is crucial for the proper support of business processes, existing PAISs vary significantly regarding the support of the temporal perspective. Both the formal specification and the operational support of temporal constraints constitute fundamental challenges in this context. In previous work, we introduced process time patterns, which facilitate the comparison and evaluation of PAISs in respect to their support of the temporal perspective. Furthermore, we provided empirical evidence for these time patterns. To avoid ambiguities and to ease the use as well as the implementation of the time patterns, this paper formally defines their semantics. To additionally foster the use of the patterns for a wide range of process modeling languages and to enable pattern integration with existing PAISs, the proposed semantics are expressed independently of a particular process meta model. Altogether, the presented pattern formalization will be fundamental for introducing the temporal perspective in PAISs.},
  doi      = {https://doi.org/10.1016/j.is.2015.10.002},
  keywords = {Process-aware Information System, Workflow Patterns, Process Time Patterns, Temporal Perspective, Temporal Constraints, Formal Semantics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915300296},
}

@InCollection{Atkinson1990,
  author    = {Malcolm Atkinson and David DeWitt and David Maier and François Bancilhon and Klaus Dittrich and Stanley Zdonik},
  title     = {The Object-Oriented Database System Manifesto},
  booktitle = {Deductive and Object-Oriented Databases},
  publisher = {North-Holland},
  year      = {1990},
  editor    = {Won KIM and Jean-Marie NICOLAS and Shojiro NISHIO},
  pages     = {223 - 240},
  address   = {Amsterdam},
  isbn      = {978-0-444-88433-6},
  abstract  = {This paper attempts to define an object-oriented database system. It describes the main features and characteristics that a system must have to qualify as an object-oriented database system. We have separated these characteristics into three groups: •Mandatory, the ones the system must satisfy in order to be termed an object-oriented database system. These are complex objects, object identity, encapsulation, types or classes, inheritance, overriding combined with late binding, extensibility, computational completeness, persistence, secondary storage management, concurrency, recovery and an ad hoc query facility.•Optional, the ones that can be added to make the system better, but which are not mandatory. These are multiple inheritance, type checking and inferencing, distribution, design transactions and versions.•Open, the points where the designer can make a number of choices. These are the programming paradigm, the representation system, the type system, and uniformity. We have taken a position, not so much expecting it to be the final word as to erect a provisional landmark to orient further debate.},
  doi       = {https://doi.org/10.1016/B978-0-444-88433-6.50020-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444884336500204},
}

@Article{Lan2003,
  author   = {Ning Lan and Gaetano T Montelione and Mark Gerstein},
  title    = {Ontologies for proteomics: towards a systematic definition of structure and function that scales to the genome level},
  journal  = {Current Opinion in Chemical Biology},
  year     = {2003},
  volume   = {7},
  number   = {1},
  pages    = {44 - 54},
  issn     = {1367-5931},
  abstract = {A principal aim of post-genomic biology is elucidating the structures, functions and biochemical properties of all gene products in a genome. However, to adequately comprehend such a large amount of information we need new descriptions of proteins that scale to the genomic level. In short, we need a unified ontology for proteomics. Much progress has been made towards this end, including a variety of approaches to systematic structural and functional classification and initial work towards developing standardized, unified descriptions for protein properties. In relation to function, there is a particularly great diversity of approaches, involving placing a protein in structured hierarchies or more-generalized networks and a recent approach based on circumscribing a protein’s function through systematic enumeration of molecular interactions.},
  doi      = {https://doi.org/10.1016/S1367-5931(02)00020-0},
  url      = {http://www.sciencedirect.com/science/article/pii/S1367593102000200},
}

@Article{Reiter1995,
  author   = {Raymond Reiter},
  title    = {On specifying database updates},
  journal  = {The Journal of Logic Programming},
  year     = {1995},
  volume   = {25},
  number   = {1},
  pages    = {53 - 91},
  issn     = {0743-1066},
  abstract = {We address the problem of formalizing the evolution of a database under the effect of an arbitrary sequence of update transactions. We do so by appealing to a first-order representation language called the situation calculus, which is a standard approach in artificial intelligence to the formalization of planning problems. We formalize database transactions in exactly the same way as actions in the artificial intelligence planning domain. This leads to a database version of the frame problem in artificial intelligence. We provide a solution to the frame problem for a special, but substantial, class of update transactions. Using the axioms corresponding to this solution, we provide procedures for determining whether a given sequence of update transactions is legal, and for query evaluation in an updated database. These procedures have the desirable property that they appeal to theorem-proving only with respect to the initial database state. We next address the problem of proving properties true in all states of the database. It turns out that mathematical induction is required for this task, and we formulate a number of suitable induction principles. Among those properties of database states that we wish to prove are the standard database notions of static and dynamic integrity constraints. In our setting, these emerge as inductive entailments of the database. Finally, we discuss various possible extensions of the approach of this paper, including transaction logs and historical queries, the complexity of query evaluation, actualized transactions, logic programming approaches to updates, database views, and state constraints.},
  doi      = {https://doi.org/10.1016/0743-1066(95)00049-P},
  url      = {http://www.sciencedirect.com/science/article/pii/074310669500049P},
}

@Article{Malinowski2008,
  author   = {E. Malinowski and E. Zimányi},
  title    = {A conceptual model for temporal data warehouses and its transformation to the ER and the object-relational models},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {64},
  number   = {1},
  pages    = {101 - 133},
  issn     = {0169-023X},
  note     = {Fourth International Conference on Business Process Management (BPM 2006) 8th International Conference on Enterprise Information Systems (ICEIS' 2006)},
  abstract = {The MultiDim model is a conceptual multidimensional model for data warehouse and OLAP applications. These applications require the presence of a time dimension to track changes in measure values. However, the time dimension cannot be used to represent changes in other dimensions. In this paper we introduce a temporal extension of the MultiDim model. This extension is based on research realized in temporal databases. We allow different temporality types: valid time, transaction time, and lifespan, which are obtained from source systems, and loading time, which is generated in the data warehouse. Our model provides temporal support for levels, attributes, hierarchies, and measures. For hierarchies we discuss different cases depending on whether the changes in levels or in the relationships between them must be kept. For measures, we give different scenarios that show the usefulness of the different temporality types. Further, since measures can be aggregated before being inserted into data warehouses, we discuss the issues related to different time granularities between source systems and data warehouses. We finish the paper presenting a transformation of the MultiDim model into the entity-relationship and the object-relational models.},
  doi      = {https://doi.org/10.1016/j.datak.2007.06.020},
  keywords = {Temporal data warehouses, Temporal multidimensional model, Conceptual multidimensional model, Logical design, Data modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001401},
}

@Article{Robinson1995,
  author   = {Vincent B. Robinson and D.Scott Mackay},
  title    = {Semantic modeling for the integration of geographic information and regional hydroecological simulation management},
  journal  = {Computers, Environment and Urban Systems},
  year     = {1995},
  volume   = {19},
  number   = {5},
  pages    = {321 - 339},
  issn     = {0198-9715},
  abstract = {Semantic modeling has proven valuable in the experimental development of the Knowledge Based Land Information Manager and Simulator (KBLIMS) which integrates the management of geographic information and a hydroecological simulation system. Semantic models provide the abstractions such as aggregation, specialization, and generalization necessary for managing the structural and behavioral properties of Regional Hydro Ecological Simulation System (RHESSys), especially the spatial and temporal elements. It is shown how semantic modeling allows for explicit consideration of semantic heterogeneity as a result of domain evolution. The seamless integration is given expression by the visual spatial query system which provides the disciplinary scientist with the ability to deal directly with landscape elements such as hillslopes, stream valleys and watersheds rather than polygons or pixels. In the end this approach provides tools that are responsive and adaptive to the demands of disciplinary science.},
  doi      = {https://doi.org/10.1016/0198-9715(95)00017-8},
  url      = {http://www.sciencedirect.com/science/article/pii/0198971595000178},
}

@Article{Valkenhoef2013,
  author   = {Gert van Valkenhoef and Tommi Tervonen and Tijs Zwinkels and Bert de Brock and Hans Hillege},
  title    = {ADDIS: A decision support system for evidence-based medicine},
  journal  = {Decision Support Systems},
  year     = {2013},
  volume   = {55},
  number   = {2},
  pages    = {459 - 475},
  issn     = {0167-9236},
  note     = {1. Analytics and Modeling for Better HealthCare 2. Decision Making in Healthcare},
  abstract = {Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit–risk decision models, and provides visualization of all results.},
  doi      = {https://doi.org/10.1016/j.dss.2012.10.005},
  keywords = {Evidence-based medicine, Evidence synthesis, Data model, Clinical trial, Decision analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923612002606},
}

@Article{Castro2002,
  author   = {Jaelson Castro and Manuel Kolp and John Mylopoulos},
  title    = {Towards requirements-driven information systems engineering: the Tropos project},
  journal  = {Information Systems},
  year     = {2002},
  volume   = {27},
  number   = {6},
  pages    = {365 - 389},
  issn     = {0306-4379},
  abstract = {Information systems of the future will have to perform well within ever-changing organizational environments. Unfortunately, existing software development methodologies (object-oriented, structured or otherwise) have traditionally been inspired by programming concepts, not organizational ones, leading to a semantic gap between the software system and its operational environment. To reduce this gap, we propose a software development methodology named Tropos which is founded on concepts used to model early requirements. Our proposal adopts the i∗ organizational modeling framework, which offers the notions of actor, goal and (actor) dependency, and uses these as a foundation to model early and late requirements, architectural and detailed design. The paper outlines Tropos phases through an e-business example, and sketches a formal language which underlies the methodology and is intended to support formal analysis. The methodology seems to complement well proposals for agent-oriented programming platforms.},
  doi      = {https://doi.org/10.1016/S0306-4379(02)00012-1},
  keywords = {Software development methodology, Requirements engineering, Information systems analysis and design, Agent-oriented systems, Software architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437902000121},
}

@Article{Rahayu2001,
  author   = {Johanna Wenny Rahayu and Elizabeth Chang and Tharam S. Dillon and David Taniar},
  title    = {Performance evaluation of the object-relational transformation methodology},
  journal  = {Data \& Knowledge Engineering},
  year     = {2001},
  volume   = {38},
  number   = {3},
  pages    = {265 - 300},
  issn     = {0169-023X},
  abstract = {The emergence of the object-oriented (OO) methodology has shown its capabilities in modelling the real world better than the earlier relational methodology. However, object-oriented databases (OODBs) are still considered immature in comparison with relational databases (RDBs) which have existed for many years. RDBs still continue to dominate the implementation of databases constituting more than 90% of all database implementations [28]. It was felt worthwhile to exploit the great modelling power of OO methodology, while still facilitating relational implementations. These reasons have led us to develop an object-relational transformation methodology [20], [21], [22], [23], [24], [25] which allows us to use the OO methodology for data modelling and to transform it into a relational logical model for implementation in relational database management systems (RDBMSs). The main purpose of this paper is to present a performance evaluation of the transformation methodology. The evaluation covers I/O cost models of different types of queries. The type of evaluation is basically comparison-based, in which the performance of SQL operations upon a set of tables derived from the relational data model is compared with the tables derived from the OO data model using the transformation methodology. The results of the evaluation show that the performance of the RDB implementation transferred from an OO conceptual model using our object-relational transformation methodology is better than the relational implementation using a conventional relational modelling. Moreover, in many cases, the relational modelling is not applicable since it cannot capture the design semantics particularly relating to collection types. Our object-relational methodology solves this problem.},
  doi      = {https://doi.org/10.1016/S0169-023X(01)00026-X},
  keywords = {Object-oriented conceptual model, Object-relational transformation, Relational databases, Performance evaluation, Quantitative analysis, Qualitative analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X0100026X},
}

@Article{Silva2000,
  author   = {Altigran S. da Silva and Alberto H.F. Laender and Marco A. Casanova},
  title    = {On the relational representation of complex specialization structures},
  journal  = {Information Systems},
  year     = {2000},
  volume   = {25},
  number   = {6},
  pages    = {399 - 415},
  issn     = {0306-4379},
  abstract = {The mapping of entity-relationship schemas (ER schemas) that contain complex specialization structures into the relational model requires the use of specific strategies to avoid inconsistent states in the final relational database. In this paper, we show that for this mapping to be correct it is required to enforce a special kind of integrity constraint, the key pairing constraint (KPC). We present a mapping strategy that use simple inclusion dependencies to enforce KPC and show that this strategy can be used to correctly map specialization structures that are more general than the simple specialization structures considered by previous strategies.},
  doi      = {https://doi.org/10.1016/S0306-4379(00)00025-9},
  keywords = {ER Model, Relational Database Design, Specialization Structures, Integrity Constraints},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437900000259},
}

@Article{Jacquinet-Husson2005,
  author   = {N. Jacquinet-Husson and N.A. Scott and A. Chédin and K. Garceran and R. Armante and A.A. Chursin and A. Barbe and M. Birk and L.R. Brown and C. Camy-Peyret and C. Claveau and C. Clerbaux and P.F. Coheur and V. Dana and L. Daumont and M.R. Debacker-Barilly and J.M. Flaud and A. Goldman and A. Hamdouni and M. Hess and D. Jacquemart and P. Köpke and J.Y. Mandin and S. Massie and S. Mikhailenko and V. Nemtchinov and A. Nikitin and D. Newnham and A. Perrin and V.I. Perevalov and L. Régalia-Jarlot and A. Rublev and F. Schreier and I. Schult and K.M. Smith and S.A. Tashkun and J.L. Teffo and R.A. Toth and Vl.G. Tyuterev and J. Vander Auwera and P. Varanasi and G. Wagner},
  title    = {The 2003 edition of the GEISA/IASI spectroscopic database},
  journal  = {Journal of Quantitative Spectroscopy and Radiative Transfer},
  year     = {2005},
  volume   = {95},
  number   = {4},
  pages    = {429 - 467},
  issn     = {0022-4073},
  abstract = {The content of the current (2003) version, GEISA/IASI-03, of the computer-accessible spectroscopic database, GEISA/IASI, is described. This “system” or database is comprised of three independent spectroscopic archives, which are (a) a database of individual spectral line parameters on 14 molecules, H2O, CO2, O3, N2O, CO, CH4, O2, NO, SO2, NO2, HNO3, OCS, C2H2, N2, and the related 51 isotopomers and isotopologues, representing 702,550 entries, in the spectral range 599–3001cm-1, (b) a database of spectral absorption cross-sections(6,572,329 entries related to six molecules, CFC-11, CFC-12, CFC-14, HCFC-22, N2O5, CCl4), and a catalogue of microphysical and optical properties (mainly, the refractive indices) of atmospheric aerosols. The modifications and improvements, which have been implemented since the earlier editions of this database, in terms of content and management, have been explained in detail. GEISA/IASI has been created with the specific purpose of assessing the capability of measurement by the IASI instrument within the designated goals of ISSWG in the frame of the CNES/EUMETSAT European Polar System preparation. All the archived data can be handled through a user-friendly associated management software, which is posted on the ARA/LMD group web site at http://ara.lmd.polytechnique.fr.},
  doi      = {https://doi.org/10.1016/j.jqsrt.2004.12.004},
  keywords = {, , Spectroscopic database, Atmospheric absorption, CFC's cross-sections, Atmospheric aerosols},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022407305000282},
}

@Article{Chee1998,
  author   = {C.L Chee and S Jarzabek and R Paul},
  title    = {F-metric: a WWW-based framework for intelligent formulation and analysis of metric queries},
  journal  = {Journal of Systems and Software},
  year     = {1998},
  volume   = {43},
  number   = {2},
  pages    = {119 - 132},
  issn     = {0164-1212},
  abstract = {As an organization matures, quantitative techniques are employed to make software project management more systematic, informed and under control. This is typically done through the collection and analysis of software metrics to measure the performance of projects. While many organizations utilize software metrics to analyze project issues and answer management queries, the manner in which such queries are answered varies from organization to organization. As the number of metrics grows, interpretation of raw metrics in the context of management goals becomes difficult. The method and tool that we developed attempts to bridge the gap between the project managers' mental model of a project and raw software metrics. Based on industrial surveys, we identified types of queries about the project progress, resources and schedule that project managers often ask during project execution. Next, we built a conceptual model for project metrics relevant to those queries, justifying our selection of a core set of test and evaluation metrics. We defined a project management query language (PMQL for short), based on concepts of an extended Entity-Relationship model, which we used to formulate project queries at a conceptual level. Finally, we present a flexible WWW-based framework which can be used by experts to formulate and update management queries in PMQL, modify heuristics based on which queries are evaluated, as well as to dynamically display solutions to existing queries. The flexibility of our framework means that new queries may be added, heuristics to queries may be changed, and display of results can be changed on-the-fly via mutual cooperation between managers and technical experts of the organization without any changes to code.},
  doi      = {https://doi.org/10.1016/S0164-1212(98)10027-4},
  keywords = {Software metrics, Software project management, WWW, Metrics queries},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121298100274},
}

@Article{Necasky2012a,
  author   = {Martin Nečaský and Irena Mlýnková and Jakub Klímek and Jakub Malý},
  title    = {When conceptual model meets grammar: A dual approach to XML data modeling},
  journal  = {Data \& Knowledge Engineering},
  year     = {2012},
  volume   = {72},
  pages    = {1 - 30},
  issn     = {0169-023X},
  abstract = {In this paper we introduce a novel approach to conceptual modeling for XML schemas. Compared to other approaches, it allows for modeling of a whole family of XML schemas related to a particular application domain. It is integrated in a well-established way of software-engineering, namely Model-Driven Development (MDD). It allows software-engineers to naturally model their application domain using a conceptual schema at the platform-independent level of the MDD hierarchy. From there they can design the desired XML schemas in a form of conceptual schemas at the platform-specific level of MDD hierarchy. Schemas at the platform-specific level are then automatically translated to particular XML schemas. Beside this forward-engineering direction, reverse-engineering direction integrating existing XML schemas into the MDD hierarchy is supported as well. We provide several theoretical results which ensure correctness of the introduced approach. We exploit regular tree grammars to formalize XML schemas. We formalize the bindings between the schemas at the two MDD levels and between schemas at the platform-specific level and XML schemas. We prove that conceptual schemas specify the target XML schemas unambiguously. We also prove the expressive power of the conceptual schemas. And, finally, we prove correctness of the introduced translation algorithms between platform-specific and XML schema levels.},
  doi      = {https://doi.org/10.1016/j.datak.2011.09.002},
  keywords = {XML schema, Conceptual modeling, Regular tree grammars, Conceptual to XML schema transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X1100125X},
}

@Article{Khoshafian1990,
  author   = {S Khoshafian},
  title    = {Insight into object-oriented databases},
  journal  = {Information and Software Technology},
  year     = {1990},
  volume   = {32},
  number   = {4},
  pages    = {274 - 289},
  issn     = {0950-5849},
  abstract = {Object orientation provides a more direct and natural representation of real-world problems. Object-oriented programming techniques allow the development of extensible and reusable modules. The object-oriented concepts are abstract data typing, inheritance, and object identity. Combining object-oriented concepts with database capabilities such as persistence, transactions, concurrency, query, etc. results in powerful systems called object-oriented databases. Object-oriented databases have become the dominant post-relational database management system and are a necessary evolutionary step towards the more powerful intelligent databases. Intelligent databases tightly couple database and object-oriented technologies with artificial intelligence, information retrieval, and multi-media data-manipulation techniques.},
  doi      = {https://doi.org/10.1016/0950-5849(90)90061-U},
  keywords = {object-oriented, databases, object-oriented databases, intelligent databases},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499090061U},
}

@Article{Han1992,
  author   = {J.-W. Han and Z.-N. Li},
  title    = {Deductive-ER: deductive entity-relationship data model and its data language},
  journal  = {Information and Software Technology},
  year     = {1992},
  volume   = {34},
  number   = {3},
  pages    = {192 - 204},
  issn     = {0950-5849},
  abstract = {The entity-relationship (ER) data model is a well known semantic data model in database design. The paper develops a deductive-ER data model, Deductive-ER, which applies the ER approach to the design of deductive databases. Based on such a model, a deductive-ER data language, called Deductive-ER, is constructed, which provides capabilities of definition and manipulation of extensional database components, intensional database components (including virtual entities, virtual relationships, and virtual attributes), hybrid components, generalization hierarchies, and integrity constraints. It supports complex data objects, including tuple-valued, list-valued, text-valued, set-valued, and null-valued attributes. Moreover, it allows recursive definitions. Deductive-ER integrates two different flavoured languages: an ER-based conventional data language and a Horn-clause-based logic data language. It takes advantage of both and provides a simple and powerful interface. The implementation of Deductive-ER is also discussed.},
  doi      = {https://doi.org/10.1016/0950-5849(92)90031-J},
  keywords = {semantic data modelling, entity-relationship model, deductive-ER model, deductive database, object-oriented database, data definition language, data manipulation language},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499290031J},
}

@Article{Durchholz1991,
  author   = {Reiner Durchholz},
  title    = {Causality, time, and deadlines},
  journal  = {Data \& Knowledge Engineering},
  year     = {1991},
  volume   = {6},
  number   = {6},
  pages    = {469 - 477},
  issn     = {0169-023X},
  abstract = {The paper explores how time-related concepts can be treated in a strictly causal way. Clocks in a local, global and regional variant are modelled within a causal frame. Perhaps the most critical use of clocks is for defining deadlines. To analyze their meaning, two relevant distinctions are made: (1) contingent vs. strict deadline requirements and (2) descriptive vs. prescriptive part of a deadline requirement. A deadline delay paradox that appears on a seemingly semantically invariant inversion of the requirement specification can be dissolved on the basis of these distinctions. The conceptual frame for discussing causality is taken from Petri net Theory.},
  doi      = {https://doi.org/10.1016/0169-023X(91)90024-R},
  keywords = {Causality, time, deadline, absolute time, relative time, contingent deadline requirement, strict deadline requirement, deadline delay paradox, descriptive part, prescriptive part, obligational aspect, Petri nets},
  url      = {http://www.sciencedirect.com/science/article/pii/0169023X9190024R},
}

@Article{Tort2011,
  author   = {Albert Tort and Antoni Olivé and Maria-Ribera Sancho},
  title    = {An approach to test-driven development of conceptual schemas},
  journal  = {Data \& Knowledge Engineering},
  year     = {2011},
  volume   = {70},
  number   = {12},
  pages    = {1088 - 1111},
  issn     = {0169-023X},
  abstract = {Test-Driven Development (TDD) is an extreme programming development method in which a software system is developed in short iterations. In this paper we present the Test-Driven Conceptual Modeling (TDCM) method, which is an application of TDD for conceptual modeling, and we show how to develop a conceptual schema using it. In TDCM, a system's conceptual schema is incrementally obtained by performing three kinds of tasks: (1) Write a test the system should pass; (2) Change the schema to pass the test; and (3) Refactor the schema to improve its qualities. We also describe an integration approach of TDCM into a broad set of software development methodologies, including the Unified Process development methodology, the MDD-based approaches, the storytest-driven agile methods and the goal and scenario-oriented requirements engineering methods. We deal with schemas written in UML/OCL, but the TDCM method could be adapted to the development of schemas in other languages.},
  doi      = {https://doi.org/10.1016/j.datak.2011.07.006},
  keywords = {Conceptual modeling, Testing, TDD, Requirements validation, UML/OCL},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X11000978},
}

@Article{Sookman1999,
  author   = {Debbie Sookman and Gilbert Pinard},
  title    = {Integrative cognitive therapy for obsessive-compulsive disorder: A focus on multiple schemas},
  journal  = {Cognitive and Behavioral Practice},
  year     = {1999},
  volume   = {6},
  number   = {4},
  pages    = {351 - 362},
  issn     = {1077-7229},
  abstract = {This paper describes integrative cognitive therapy for obsessive-compulsive disorder (OCD), which focuses on schemas. This treatment was developed by Sookman, Pinard, and Beauchemin (1994) for patients resistant to existing approaches. Multiple cognitive domains and levels of cognition, ranging from appraisals to core beliefs, are targeted. In addition to targeting domains such as inflated responsibility, perfectionism, and overimportance of thoughts (Obsessive Compulsive Cognitions Working Group, 1997), this approach also targets domains that the authors (Sookman & Pinard, 1995) have proposed to be relevant to OCD: vulnerability; response to unpredictability, newness, and change; and view of response to strong affect. Developmental theory, role of attachment experiences, notion of schemas, emotional-interpersonal foci, and the structural dimension are integrated in the conceptualization and treatment of each case. Change in OCD symptoms, secondary depression, and dysfunctional cognitions are reported for seven treatment-resistant cases. On average, symptoms improved from a moderately severe to subclinical level. In some cases the strength of dysfunctional beliefs across multiple domains was reduced to the normal range. Preliminary results support the use of this integrative approach for patients resistant to existing approaches.},
  doi      = {https://doi.org/10.1016/S1077-7229(99)80055-8},
  url      = {http://www.sciencedirect.com/science/article/pii/S1077722999800558},
}

@InCollection{McComb2003b,
  title     = {Index},
  booktitle = {Semantics in Business Systems},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Dave McComb},
  series    = {The Savvy Manager;#x0027;s Guides},
  pages     = {377 - 398},
  address   = {Burlington},
  isbn      = {978-1-55860-917-4},
  doi       = {https://doi.org/10.1016/B978-155860917-4/50021-0},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558609174500210},
}

@Article{Schneider2002,
  author   = {Ralph Schneider and Wolfgang Marquardt},
  title    = {Information technology support in the chemical process design life cycle},
  journal  = {Chemical Engineering Science},
  year     = {2002},
  volume   = {57},
  number   = {10},
  pages    = {1763 - 1792},
  issn     = {0009-2509},
  abstract = {Information technology has been becoming increasingly important in all areas of engineering during the last few years. Much of the progress achieved in chemical engineering would not have been possible without the enabling methods and tools provided by information technology. This trend will continue in the future but most likely with a considerably wider scope. While individual software tools and services have been in focus until recently, their integration into engineering work processes is an emerging and challenging area of research and development. This contribution attempts to highlight state of the art and future trends in supporting the activities during the life cycle of a chemical process by means of information technology. Emphasis will be largely on the process and plant design process rather than on procurement, manufacturing, and distribution of materials in the supply chain.},
  doi      = {https://doi.org/10.1016/S0009-2509(02)00075-1},
  keywords = {Process and plant design, Workflow, Activity modeling, Information modeling, Product data, Tool integration, Integrated design environments},
  url      = {http://www.sciencedirect.com/science/article/pii/S0009250902000751},
}

@Article{Ochuodho1992,
  author   = {S.J. Ochuodho},
  title    = {Object-oriented database support for software project management environments: data-modelling issues},
  journal  = {Information and Software Technology},
  year     = {1992},
  volume   = {34},
  number   = {5},
  pages    = {283 - 307},
  issn     = {0950-5849},
  abstract = {To keep pace with the stringent requirements of emergent applications, database technology has to change. Conventional databases have been successful inasmuch as the application domain was restricted to traditional data banks. Enhancement of existing database management systems (DBMSs) has played its part, namely, stretching the capabilities of their predecessors. Unfortunately, this extension is not unlimited. As of necessity, entirely new modelling concepts must be explored. The paper surveys conventional DBMSs, particularly with regard to their support for integrated project support environments (IPSEs). Their strengths and limitations are discussed. The problems posed by such nontraditional applications are identified. Emerging ideas that seem poised to meet this challenge are analysed. Shortcomings of these ‘advanced’ methodologies are identified, with specific implementations considered. Based on the survey, fundamental requirements of any IPSE database are presented. The object-oriented (OO) approach seems to give a good handle for satisfying these requirements. Proposal is made to investigate further how an OO-based data model suitable for IPSE support can be evolved.},
  doi      = {https://doi.org/10.1016/0950-5849(92)90059-X},
  keywords = {databases, database management systems, object-oriented, software engineering environments, IPSEs},
  url      = {http://www.sciencedirect.com/science/article/pii/095058499290059X},
}

@Article{Winarko2007,
  author   = {Edi Winarko and John F. Roddick},
  title    = {ARMADA – An algorithm for discovering richer relative temporal association rules from interval-based data},
  journal  = {Data \& Knowledge Engineering},
  year     = {2007},
  volume   = {63},
  number   = {1},
  pages    = {76 - 90},
  issn     = {0169-023X},
  note     = {Data Warehouse and Knowledge Discovery (DAWAK ’05)},
  abstract = {Temporal association rule mining promises the ability to discover time-dependent correlations or patterns between events in large volumes of data. To date, most temporal data mining research has focused on events existing at a point in time rather than over a temporal interval. In comparison to static rules, mining with respect to time points provides semantically richer rules. However, accommodating temporal intervals offers rules that are richer still. In this paper we outline a new algorithm, ARMADA, to discover frequent temporal patterns and to generate richer interval-based temporal association rules. In addition, we introduce a maximum gap time constraint that can be used to get rid of insignificant patterns and rules so that the number of generated patterns and rules can be reduced. Synthetic datasets are utilized to assess the performance of the algorithm.},
  doi      = {https://doi.org/10.1016/j.datak.2006.10.009},
  keywords = {Relative temporal data mining, Interval data, Temporal association rules, },
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X06001935},
}

@Article{Rost1994,
  author   = {Burkhard Rost and Chris Sander},
  title    = {Structure prediction of proteins—where are we now?},
  journal  = {Current Opinion in Biotechnology},
  year     = {1994},
  volume   = {5},
  number   = {4},
  pages    = {372 - 380},
  issn     = {0958-1669},
  abstract = {Although the ‘structure from sequence’ prediction problem remains fundamentally unsolved, new and promising methods in one, two and three dimensions have reopened the field. Significantly improved one-dimensional prediction of secondary structure from multiple sequence alignments is now in routine use. In the two-dimensional approach, inter-residue contacts can be detected by analysis of correlated mutations, albeit with low accuracy. Finally, three-dimensional methods, in which pseudopotentials or information values are derived from the databases, are proving their value for distinguishing between correct and incorrect models.},
  doi      = {https://doi.org/10.1016/0958-1669(94)90045-0},
  url      = {http://www.sciencedirect.com/science/article/pii/0958166994900450},
}

@Article{Belleghem1997,
  author   = {Kristof Van Belleghem and Marc Denecker and Danny De Schreye},
  title    = {On the relation between situation calculus and event calculus},
  journal  = {The Journal of Logic Programming},
  year     = {1997},
  volume   = {31},
  number   = {1},
  pages    = {3 - 37},
  issn     = {0743-1066},
  note     = {Reasoning about Action and Change},
  abstract = {In this paper we make a detailed comparison of the Situation Calculus and the Event Calculus, two logic-based temporal reasoning formalisms. We concentrate on differences between the calculi, considering the similarities sufficiently indicated in the recent literature. We illustrate the inability of Event Calculus to handle counterfactual reasoning problems, and that of Situation Calculus to deal with counterfactual statements in the presence of actions with nondeterministic effects. We present a new calculus which extends both Situation and Event Calculus. In this new calculus we define a natural and clear relation between situations and time points, which differs from those used in previous comparisons. We show the relation of this new calculus to both original calculi. We compare the original calculi using the new calculus as an analysis tool.},
  doi      = {https://doi.org/10.1016/S0743-1066(96)00123-9},
  url      = {http://www.sciencedirect.com/science/article/pii/S0743106696001239},
}

@InCollection{Halpin2008a,
  author    = {Terry Halpin and Tony Morgan},
  title     = {10 - Advanced Modeling Issues},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {399 - 472},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50014-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978012373568350014X},
}

@Article{Domb1999,
  author   = {M.A. Domb},
  title    = {Transaction independence: The road to cooperative systems},
  journal  = {Mathematical and Computer Modelling},
  year     = {1999},
  volume   = {29},
  number   = {1},
  pages    = {1 - 31},
  issn     = {0895-7177},
  abstract = {The cooperation and consistency among transactions in cooperative systems suggest that traditional transaction processing paradigms need to be modified for target applications. We propose a new framework that borrows its concepts from the evolution of database management systems. Transaction Independence and Incorporation of Workflow ideas within applications are the foundations of this framework. Cooperation is gained by TCMS, an external, domain-independent framework. This paper outlines the requirements, approach, architecture, and the implementation of TCMS.},
  doi      = {https://doi.org/10.1016/S0895-7177(98)00176-9},
  keywords = {Advanced transactions, Cooperative systems, Systems architecture, Database},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895717798001769},
}

@Article{Parent2006,
  author   = {C. Parent and S. Spaccapietra and E. Zimányi},
  title    = {The MurMur project: Modeling and querying multi-representation spatio-temporal databases},
  journal  = {Information Systems},
  year     = {2006},
  volume   = {31},
  number   = {8},
  pages    = {733 - 769},
  issn     = {0306-4379},
  abstract = {Successful information management implies the ability to design accurate representations of the real world of interest, in spite of the diversity of perceptions from the applications sharing the same database. Current database management systems do not provide representation schemes that preserve each perception while fully supporting their diversity and maintaining their consistency. This is a major hindrance for building an all-embracing view of the world while serving multiple applications, whether it is by developing a single database or by providing transparent access (e.g., via the Web) to several heterogeneous data sources (that would typically hold a great diversity of stored representations). This paper reports on results from the multiple representations and multiple resolutions in geographical databases project,11In addition to the authors' institutions, the MurMur consortium included the French Mapping Agency (Institut Géographique National, Paris, France), a public research institute on Agricultural and Environmental Engineering, (Cemagref, Grenoble, France), and a GIS provider (Star Informatic, Liège, Belgium). funded by the European Commission under the 5th Framework Programme. The objective of the project has been to enhance GIS (or DBMS) by adding functionality that supports multiple coexisting representations of the same real-word phenomena (semantic flexibility), including representations of geographic data at multiple resolutions (cartographic flexibility). The new functionality enables a semantically meaningful management of multi-scale, integrated, and temporal geo-databases.},
  doi      = {https://doi.org/10.1016/j.is.2005.01.004},
  keywords = {Spatio-temporal databases, GIS, Conceptual modeling, Multiple representations, CASE tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437905000165},
}

@Article{Groppe2006,
  author   = {Sven Groppe and Stefan Böttcher and Georg Birkenheuer and André Höing},
  title    = {Reformulating XPath queries and XSLT queries on XSLT views},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {57},
  number   = {1},
  pages    = {64 - 110},
  issn     = {0169-023X},
  abstract = {Applications using XML for data representation very often use different XML formats and thus require the transformation of XML data. The common approach transforms entire XML documents from one format into another, e.g. by using an XSLT stylesheet. Different from this approach, we use an XSLT stylesheet in order to transform a given XPath query or a given XSLT query so that we retrieve and transform only that part of the XML document, which is sufficient to answer the given query. Among other things, our approach avoids problems of replication, saves processing time, and in distributed scenarios, transportation costs.},
  doi      = {https://doi.org/10.1016/j.datak.2005.04.002},
  keywords = {XML, Semi-structured data, XSLT, XPath, Query transformation, Query reformulation, Query optimization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05000431},
}

@InCollection{Fortier2003,
  author    = {Paul J. Fortier and Howard E. Michel},
  title     = {1 - Introduction},
  booktitle = {Computer Systems Performance Evaluation and Prediction},
  publisher = {Digital Press},
  year      = {2003},
  editor    = {Paul J. Fortier and Howard E. Michel},
  pages     = {1 - 38},
  address   = {Burlington},
  isbn      = {978-1-55558-260-9},
  doi       = {https://doi.org/10.1016/B978-155558260-9/50001-1},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781555582609500011},
}

@Article{Mili2001,
  author   = {F. Mili and W. Shen and I. Martinez and Ph. Noel and M. Ram and E. Zouras},
  title    = {Knowledge modeling for design decisions},
  journal  = {Artificial Intelligence in Engineering},
  year     = {2001},
  volume   = {15},
  number   = {2},
  pages    = {153 - 164},
  issn     = {0954-1810},
  abstract = {In this paper, we share our experience in modeling and representing design knowledge relevant for engineering design decisions. We define an object model where classes are used to capture design standards and requirements relevant to designed objects. The traditional object model is customized to the representation of design knowledge in two major ways: (1) Classes representing design objects are augmented with design validation information. (2) Associations between classes are made explicit and used to reduce the redundancy and maintain the consistency of the knowledge. We define the semantics of the resulting object model and formulate the axioms that define its consistency. The object model is defined in the context of stamping design knowledge.},
  doi      = {https://doi.org/10.1016/S0954-1810(01)00013-9},
  keywords = {Engineering design, Knowledge modeling, Class constraints, Model integrity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0954181001000139},
}

@Article{Mylopoulos1998,
  author   = {John Mylopoulos},
  title    = {Information modeling in the time of the revolution},
  journal  = {Information Systems},
  year     = {1998},
  volume   = {23},
  number   = {3},
  pages    = {127 - 155},
  issn     = {0306-4379},
  note     = {Advance information systems engineering},
  abstract = {Information modeling is concerned with the construction of computer-based symbol structures which capture the meaning of information and organize it in ways that make it understandable and useful to people. Given that information is becoming an ubiquitous, abundant and precious resource, its modeling is serving as a core technology for information systems engineering. We present a brief history of information modeling techniques in Computer Science and briefly survey such techniques developed within Knowledge Representation (Artificial Intelligence), Data Modeling (Databases), and Requirements Analysis (Software Engineering and Information Systems). We then offer a characterization of information modeling techniques which classifies them according to their ontologies, i.e., the type of application for which they are intended, the set of abstraction mechanisms (or, structuring principles) they support, as well as the tools they provide for building, analyzing, and managing application models. The final component of the paper uses the proposed characterization to assess particular information modeling techniques and draw conclusions about the advances that have been achieved in the field.},
  doi      = {https://doi.org/10.1016/S0306-4379(98)00005-2},
  keywords = {Conceptual Model, Semantic Data Model, Requirements Model, Knowledge Representation Language, Ontology, Abstraction Mechanism, Classification, Generalization, Aggregation, Contextualization, Materialization, Normalization, Parameterization, Semantic Network},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437998000052},
}

@Article{Jarke1990a,
  author   = {Matthias Jarke and Manfred Jeusfeld and Thomas Rose},
  title    = {A software process data model for knowledge engineering in information systems},
  journal  = {Information Systems},
  year     = {1990},
  volume   = {15},
  number   = {1},
  pages    = {85 - 116},
  issn     = {0306-4379},
  note     = {Knowledge Engineering},
  abstract = {Knowledge engineering for information systems is a long-term, multi-person task that requires tight control and memorization not only of what knowledge is acquired but also of why and how it is acquired. We propose a software process data model as the foundation of a knowledge-based software information system that emphasizes control, support and documentation of design decision-making and tool integration in information systems environments. The model is developed along two dimensions. Firstly, it defines how to represent and integrate design objects (what), design decisions (why) and design tools (how). Secondly, it exploits the abstraction mechanisms of the extensible hybrid knowledge representation language CML/Telos to manage the evolution not only of particular software projects, but also of the software development environment in which these projects operate. Modular aggregation relates design-in-the-small and design-in-the-large support. Besides motivating and formalizing the model, we describe an operational prototype implementation called ConceptBase and report intitial application experiences in the DAIDA ESPRIT project.},
  doi      = {https://doi.org/10.1016/0306-4379(90)90018-K},
  keywords = {Software databases, software process models, information systems engineering, knowledge base management systems},
  url      = {http://www.sciencedirect.com/science/article/pii/030643799090018K},
}

@InCollection{Daum2003,
  title     = {Index},
  booktitle = {System Architecture with XML},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  editor    = {Berthold Daum and Udo Merten},
  series    = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages     = {441 - 458},
  address   = {San Francisco},
  isbn      = {978-1-55860-745-3},
  doi       = {https://doi.org/10.1016/B978-155860745-3/50015-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781558607453500155},
}

@Article{Even2010,
  author   = {Adir Even and G. Shankaranarayanan and Paul D. Berger},
  title    = {Evaluating a model for cost-effective data quality management in a real-world CRM setting},
  journal  = {Decision Support Systems},
  year     = {2010},
  volume   = {50},
  number   = {1},
  pages    = {152 - 163},
  issn     = {0167-9236},
  abstract = {Managing data resources at high quality is usually viewed as axiomatic. However, we suggest that, since the process of improving data quality should attempt to maximize economic benefits as well, high data quality is not necessarily economically-optimal. We demonstrate this argument by evaluating a microeconomic model that links the handling of data quality defects, such as outdated data and missing values, to economic outcomes: utility, cost, and net-benefit. The evaluation is set in the context of Customer Relationship Management (CRM) and uses large samples from a real-world data resource used for managing alumni relations. Within this context, our evaluation shows that all model parameters can be measured, and that all model-related assumptions are, largely, well supported. The evaluation confirms the assumption that the optimal quality level, in terms of maximizing net-benefits, is not necessarily the highest possible. Further, the evaluation process contributes some important insights for revising current data acquisition and maintenance policies.},
  doi      = {https://doi.org/10.1016/j.dss.2010.07.011},
  keywords = {Data quality, Utility, Cost–benefit analysis, Data warehouse, CRM},
  url      = {http://www.sciencedirect.com/science/article/pii/S016792361000117X},
}

@InCollection{Halpin2008b,
  author    = {Terry Halpin and Tony Morgan},
  title     = {3 - Conceptual Modeling: First Steps},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {59 - 108},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50007-2},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123735683500072},
}

@Article{2006a,
  title   = {Book reports},
  journal = {Computers \& Mathematics with Applications},
  year    = {2006},
  volume  = {51},
  number  = {6},
  pages   = {1149 - 1162},
  issn    = {0898-1221},
  doi     = {https://doi.org/10.1016/j.camwa.2006.03.015},
  url     = {http://www.sciencedirect.com/science/article/pii/S0898122106000484},
}

@Article{Jensen1996,
  author   = {Christian S. Jensen and Richard T. Snodgrass},
  title    = {Semantics of time-varying information},
  journal  = {Information Systems},
  year     = {1996},
  volume   = {21},
  number   = {4},
  pages    = {311 - 352},
  issn     = {0306-4379},
  abstract = {This paper provides a systematic and comprehensive study of the underlying semantics of temporal databases, summarizing the results of an intensive collaboration between the two authors over the last five years. We first examine how facts may be associated with time, most prominently with one or more dimensions of valid time and transaction time. One common case is that of a bitemporal relation, in which facts are associated with timestamps from exactly one valid-time and one transaction-time dimension. These two times may be related in various ways, yielding temporal specialization. Multiple transaction times arise when a fact is stored in one database, then later replicated or transferred to another database. By retaining the transaction times, termed temporal generalization, the original relation can be effectively queried by referencing only the final relation. We attempt to capture the essence of time-varying information via a very simple data model, the bitemporal conceptual data model. Emphasis is placed on the notion of snapshot equivalence of the information content of relations of different data models. The logical design of temporal databases is a natural next topic. Normal forms play a central role during the design of conventional relational databases. We show how to extend the existing relational dependency theory, including the dependencies themselves, keys, normal forms, and schema decomposition algorithms, to apply to temporal relations. However, this theory does not fully take into account the temporal semantics of the attributes of temporal relations. To address this deficiency, we study the semantics of individual attributes. One aspect is the observation and update patterns of attributes—when an attribute changes value and when the changes are recorded in the database, respectively. A related aspect is when an attribute has some value, termed its lifespan. Yet another aspect is the values themselves of attributes—how to derive a value for an attribute at any point in time from stored values, termed temporal derivation. This study of attribute semantics leads to the formulation of temporal guidelines for logical database design.},
  doi      = {https://doi.org/10.1016/0306-4379(96)00017-8},
  keywords = {Temporal Databases, Data Models, Database Design, Normal Forms, Decomposition Guidelines, Update Patterns, Lifespans, Temporal Derivation},
  url      = {http://www.sciencedirect.com/science/article/pii/0306437996000178},
}

@Article{Tansalarak2007,
  author   = {Naiyana Tansalarak and Kajal T. Claypool},
  title    = {QMatch – Using paths to match XML schemas},
  journal  = {Data \& Knowledge Engineering},
  year     = {2007},
  volume   = {60},
  number   = {2},
  pages    = {260 - 282},
  issn     = {0169-023X},
  note     = {Web data and schema management},
  abstract = {Integration of multiple heterogeneous data sources continues to be a critical problem for many application domains and a challenge for researchers world-wide. With the increasing popularity of the XML model and the proliferation of XML documents on-line, automated matching of XML documents and databases has become a critical issue. In this paper, we present a hybrid schema match algorithm, QMatch, that provides a unique path-based framework for harnessing traditional structural and semantic information, while exploiting the constraints inherent in XML documents such as the order of XML elements, to provide improved levels of matching between two given XML schemata. QMatch is based on the measurement of a unique quality of match metric, QoM, and a set of classifiers which together provide not only an effective basis for the development of a new schema match algorithm, but also a useful tool for tuning existing schema match algorithms to output at desired levels of matching. In this paper, we show via a set of experiments the benefits of the path-based QMatch over existing structural, linguistic, and hybrid algorithms such as Cupid, and provide an empirical measure of the accuracy of QMatch in terms of the true matches discovered by the algorithm.},
  doi      = {https://doi.org/10.1016/j.datak.2006.03.002},
  keywords = {Schema matching, Schema integration, Hybrid schema matching, XML schema matching},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X06000516},
}

@InCollection{Halpin2008c,
  author    = {Terry Halpin and Tony Morgan},
  title     = {16 - Other Modeling Aspects and Trends},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {835 - 892},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50020-5},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123735683500205},
}

@Article{Polat1999,
  author   = {Faruk Polat and Reda Alhajj},
  title    = {A multi-agent tuple-space based problem solving framework},
  journal  = {Journal of Systems and Software},
  year     = {1999},
  volume   = {47},
  number   = {1},
  pages    = {11 - 17},
  issn     = {0164-1212},
  abstract = {Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh, 1992, Hewitt, 1991, Gasser, 1991, Polat et al., 1993, Polat and Guvenir, 1993, Polat and Guvenir, 1994, Shoham, 1993Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.},
  doi      = {https://doi.org/10.1016/S0164-1212(99)00019-9},
  keywords = {Tuple Space, Multi-agent systems, Distributed systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121299000199},
}

@Article{Aubert2013,
  author   = {B. Aubert and R. Barate and D. Boutigny and F. Couderc and P. del Amo Sanchez and J.-M. Gaillard and A. Hicheur and Y. Karyotakis and J.P. Lees and V. Poireau and X. Prudent and P. Robbe and V. Tisserand and A. Zghiche and E. Grauges and J. Garra Tico and L. Lopez and M. Martinelli and A. Palano and M. Pappagallo and A. Pompili and G.P. Chen and J.C. Chen and N.D. Qi and G. Rong and P. Wang and Y.S. Zhu and G. Eigen and B. Stugu and L. Sun and G.S. Abrams and M. Battaglia and A.W. Borgland and A.B. Breon and D.N. Brown and J. Button-Shafer and R.N. Cahn and E. Charles and A.R. Clark and C.T. Day and M. Furman and M.S. Gill and Y. Groysman and R.G. Jacobsen and R.W. Kadel and J.A. Kadyk and L.T. Kerth and Yu.G. Kolomensky and J.F. Kral and G. Kukartsev and C. LeClerc and M.E. Levi and G. Lynch and A.M. Merchant and L.M. Mir and P.J. Oddone and T.J. Orimoto and I.L. Osipenkov and M. Pripstein and N.A. Roe and A. Romosan and M.T. Ronan and V.G. Shelkov and A. Suzuki and K. Tackmann and T. Tanabe and W.A. Wenzel and M. Zisman and M. Barrett and P.G. Bright-Thomas and K.E. Ford and T.J. Harrison and A.J. Hart and C.M. Hawkes and D.J. Knowles and S.E. Morgan and S.W. O'Neale and R.C. Penny and D. Smith and N. Soni and A.T. Watson and N.K. Watson and K. Goetzen and T. Held and H. Koch and M. Kunze and B. Lewandowski and M. Pelizaeus and K. Peters and H. Schmuecker and T. Schroeder and M. Steinke and A. Fella and E. Antonioli and J.T. Boyd and N. Chevalier and W.N. Cottingham and B. Foster and C. Mackay and D. Walker and K. Abe and D.J. Asgeirsson and T. Cuhadar-Donszelmann and B.G. Fulsom and C. Hearty and N.S. Knecht and T.S. Mattison and J.A. McKenna and D. Thiessen and A. Khan and P. Kyberd and A.K. McKemey and A. Randle-Conde and M. Saleem and D.J. Sherwood and L. Teodorescu and V.E. Blinov and A.D. Bukin and A.R. Buzykaev and V.P. Druzhinin and V.B. Golubev and A.A. Korol and E.A. Kravchenko and A.P. Onuchin and S.I. Serednyakov and Yu.I. Skovpen and E.P. Solodov and V.I. Telnov and K. Yu. Todyshev and A.N. Yushkov and D.S. Best and M. Bondioli and M. Bruinsma and M. Chao and S. Curry and I. Eschrich and D. Kirkby and A.J. Lankford and M. Mandelkern and E.C. Martin and S. McMahon and R.K. Mommsen and D.P. Stoker and S. Abachi and C. Buchanan and B.L. Hartfiel and A.J.R. Weinstein and H. Atmacan and S.D. Foulkes and J.W. Gary and J. Layter and F. Liu and O. Long and B.C. Shen and G.M. Vitug and K. Wang and Z. Yasin and L. Zhang and H.K. Hadavand and E.J. Hill and H.P. Paar and S. Rahatlou and U. Schwanke and V. Sharma and J.W. Berryhill and C. Campagnari and A. Cunha and B. Dahmes and T.M. Hong and D. Kovalskyi and N. Kuznetsova and S.L. Levy and A. Lu and M.A. Mazur and J.D. Richman and W. Verkerke and T.W. Beck and J. Beringer and A.M. Eisner and C.J. Flacco and A.A. Grillo and M. Grothe and C.A. Heusch and J. Kroseberg and W.S. Lockman and A.J. Martinez and G. Nesom and T. Schalk and R.E. Schmitz and B.A. Schumm and A. Seiden and E. Spencer and P. Spradlin and M. Turri and W. Walkowiak and L. Wang and M. Wilder and D.C. Williams and M.G. Wilson and L.O. Winstrom and E. Chen and C.H. Cheng and D.A. Doll and M.P. Dorsten and A. Dvoretskii and B. Echenard and R.J. Erwin and F. Fang and K. Flood and D.G. Hitlin and S. Metzler and I. Narsky and J. Oyang and T. Piatenko and F.C. Porter and A. Ryd and A. Samuel and S. Yang and R.Y. Zhu and R. Andreassen and S. Devmal and T.L. Geld and S. Jayatilleke and G. Mancinelli and B.T. Meadows and K. Mishra and M.D. Sokoloff and T. Abe and E.A. Antillon and T. Barillari and J. Becker and F. Blanc and P.C. Bloom and S. Chen and Z.C. Clifton and I.M. Derrington and J. Destree and M.O. Dima and W.T. Ford and A. Gaz and J.D. Gilman and J. Hachtel and J.F. Hirschauer and D.R. Johnson and A. Kreisel and M. Nagel and U. Nauenberg and A. Olivas and P. Rankin and J. Roy and W.O. Ruddick and J.G. Smith and K.A. Ulmer and W.C. van Hoek and S.R. Wagner and C.G. West and J. Zhang and R. Ayad and J. Blouw and A. Chen and E.A. Eckhart and J.L. Harton and T. Hu and W.H. Toki and R.J. Wilson and F. Winklmeier and Q.L. Zeng and D. Altenburg and E. Feltresi and A. Hauke and H. Jasper and M. Karbach and J. Merkel and A. Petzold and B. Spaan and K. Wacker and T. Brandt and J. Brose and T. Colberg and G. Dahlinger and M. Dickopp and P. Eckstein and H. Futterschneider and S. Kaiser and M.J. Kobel and R. Krause and R. Müller-Pfefferkorn and W.F. Mader and E. Maly and R. Nogowski and S. Otto and J. Schubert and K.R. Schubert and R. Schwierz and J.E. Sundermann and A. Volk and L. Wilden and D. Bernard and F. Brochard and J. Cohen-Tanugi and F. Dohou and S. Ferrag and E. Latour and A. Mathieu and C. Renard and S. Schrenk and S. T'Jampens and Ch. Thiebaux and G. Vasileiadis and M. Verderi and A. Anjomshoaa and R. Bernet and P.J. Clark and D.R. Lavin and F. Muheim and S. Playfer and A.I. Robertson and J.E. Swain and J.E. Watson and Y. Xie and D. Andreotti and M. Andreotti and D. Bettoni and C. Bozzi and R. Calabrese and V. Carassiti and A. Cecchi and G. Cibinetto and A. Cotta Ramusino and F. Evangelisti and E. Fioravanti and P. Franchini and I. Garzia and L. Landi and E. Luppi and R. Malaguti and M. Negrini and C. Padoan and A. Petrella and L. Piemontese and V. Santoro and A. Sarti and F. Anulli and R. Baldini-Ferroli and A. Calcaterra and G. Finocchiaro and S. Pacetti and P. Patteri and I.M. Peruzzi and M. Piccolo and M. Rama and R. de Sangro and M. Santoni and A. Zallo and S. Bagnasco and A. Buzzo and R. Capra and R. Contri and G. Crosetti and M. Lo Vetere and M.M. Macri and S. Minutoli and M.R. Monge and P. Musico and S. Passaggio and F.C. Pastore and C. Patrignani and M.G. Pia and E. Robutti and A. Santroni and S. Tosi and B. Bhuyan and V. Prasad and S. Bailey and G. Brandenburg and K.S. Chaisanguanthum and C.L. Lee and M. Morii and E. Won and J. Wu and A. Adametz and R.S. Dubitzky and J. Marks and S. Schenk and U. Uwer and V. Klose and H.M. Lacker and M.L. Aspinwall and W. Bhimji and D.A. Bowerman and P.D. Dauncey and U. Egede and R.L. Flack and J.R. Gaillard and N.J.W. Gunawardane and G.W. Morton and J.A. Nash and M.B. Nikolich and W. Panduro Vazquez and P. Sanders and D. Smith and G.P. Taylor and M. Tibbetts and P.K. Behera and X. Chai and M.J. Charles and G.J. Grenier and R. Hamilton and S.-J. Lee and U. Mallik and N.T. Meyer and C. Chen and J. Cochran and H.B. Crawley and L. Dong and V. Eyges and P.-A. Fischer and J. Lamsa and W.T. Meyer and S. Prell and E.I. Rosenberg and A.E. Rubin and Y.Y. Gao and A.V. Gritsan and Z.J. Guo and C.K. Lae and G. Schott and J.N. Albert and N. Arnaud and C. Beigbeder and D. Breton and M. Davier and D. Derkach and S. Dû and J. Firmino da Costa and G. Grosdidier and A. Höcker and S. Laplace and F. Le Diberder and V. Lepeltier and A.M. Lutz and B. Malaescu and J.Y. Nief and T.C. Petersen and S. Plaszczynski and S. Pruvot and P. Roudeau and M.H. Schune and J. Serrano and V. Sordini and A. Stocchi and V. Tocut and S. Trincaz-Duvoid and L.L. Wang and G. Wormser and R.M. Bionta and V. Brigljević and D.J. Lange and M.C. Simani and D.M. Wright and I. Bingham and J.P. Burke and C.A. Chavez and J.P. Coleman and I.J. Forster and J.R. Fry and E. Gabathuler and R. Gamet and M. George and D.E. Hutchcroft and M. Kay and R.J. Parry and D.J. Payne and K.C. Schofield and R.J. Sloane and C. Touramanis and D.E. Azzopardi and G. Bellodi and A.J. Bevan and C.K. Clarke and C.M. Cormack and F. Di Lodovico and P. Dixon and K.A. George and W. Menges and R.J. L. Potter and R. Sacco and H.W. Shorthouse and M. Sigamani and P. Strother and P.B. Vidal and C.L. Brown and G. Cowan and H.U. Flaecher and S. George and M.G. Green and D.A. Hopkins and P.S. Jackson and A. Kurup and C.E. Marker and P. McGrath and T.R. McMahon and S. Paramesvaran and F. Salvatore and G. Vaitsas and M.A. Winter and A.C. Wren and D.N. Brown and C.L. Davis and A.G. Denig and M. Fritsch and W. Gradl and K. Griessinger and A. Hafner and E. Prencipe and J. Allison and K.E. Alwyn and D.S. Bailey and N.R. Barlow and R.J. Barlow and Y.M. Chia and C.L. Edgar and A.C. Forti and J. Fullwood and P.A. Hart and M.C. Hodgkinson and F. Jackson and G. Jackson and M.P. Kelly and S.D. Kolya and G.D. Lafferty and A.J. Lyon and M.T. Naisbit and N. Savvas and J.H. Weatherall and T.J. West and J.C. Williams and J.I. Yi and J. Anderson and A. Farbin and W.D. Hulsbergen and A. Jawahery and V. Lillard and D.A. Roberts and J.R. Schieck and G. Simi and J.M. Tuggle and G. Blaylock and C. Dallapiccola and S.S. Hertzbach and R. Kofler and V.B. Koptchev and X. Li and T.B. Moore and E. Salvati and S. Saremi and H. Staengle and S.Y. Willocq and R. Cowan and D. Dujmic and P.H. Fisher and S.W. Henderson and K. Koeneke and M.I. Lang and G. Sciolla and M. Spitznagel and F. Taylor and R.K. Yamamoto and M. Yi and M. Zhao and Y. Zheng and M. Klemetti and D. Lindemann and D.J. J. Mangeol and S.E. Mclachlin and M. Milek and P.M. Patel and S.H. Robertson and P. Biassoni and G. Cerizza and A. Lazzaro and V. Lombardo and N. Neri and F. Palombo and R. Pellegrini and S. Stracka and J.M. Bauer and L. Cremaldi and V. Eschenburg and R. Kroeger and J. Reidy and D.A. Sanders and D.J. Summers and H.W. Zhao and R. Godang and S. Brunet and D. Cote and X. Nguyen and M. Simard and P. Taras and B. Viaud and H. Nicholson and N. Cavallo and G. De Nardo and F. Fabozzi and C. Gatto and L. Lista and D. Monorchio and G. Onorato and P. Paolucci and D. Piccolo and C. Sciacca and M.A. Baak and G. Raven and H.L. Snoek and C.P. Jessop and K.J. Knoepfel and J.M. LoSecco and W.F. Wang and T. Allmendinger and G. Benelli and B. Brau and L.A. Corwin and K.K. Gan and K. Honscheid and D. Hufnagel and H. Kagan and R. Kass and J.P. Morris and A.M. Rahimi and J.J. Regensburger and D.S. Smith and R. Ter-Antonyan and Q.K. Wong and N.L. Blount and J. Brau and R. Frey and O. Igonkina and M. Iwasaki and J.A. Kolb and M. Lu and C.T. Potter and R. Rahmat and N.B. Sinev and D. Strom and J. Strube and E. Torrence and E. Borsato and G. Castelli and F. Colecchia and A. Crescente and F. Dal Corso and A. Dorigo and C. Fanin and F. Furano and N. Gagliardi and F. Galeazzi and M. Margoni and M. Marzolla and G. Michelon and M. Morandin and M. Posocco and M. Rotondo and F. Simonetto and P. Solagna and E. Stevanato and R. Stroili and G. Tiozzo and C. Voci and S. Akar and P. Bailly and E. Ben-Haim and G. Bonneaud and H. Briand and J. Chauveau and O. Hamon and M.J.J. John and H. Lebbolo and Ph. Leruste and J. Malclès and G. Marchiori and L. Martin and J. Ocariz and A. Perez and M. Pivk and J. Prendki and L. Roos and S. Sitt and J. Stark and G. Thérin and A. Vallereau and M. Biasini and R. Covarelli and E. Manoni and S. Pennazzi and M. Pioppi and C. Angelini and G. Batignani and S. Bettarini and F. Bosi and F. Bucci and G. Calderini and M. Carpinelli and R. Cenci and A. Cervelli and F. Forti and M.A. Giorgi and A. Lusiani and G. Marchiori and M. Morganti and F. Morsani and E. Paoloni and F. Raffaelli and G. Rizzo and F. Sandrelli and G. Triggiani and J.J. Walsh and M. Haire and D. Judd and J. Biesiada and N. Danielson and P. Elmer and R.E. Fernholz and Y.P. Lau and C. Lu and V. Miftakov and J. Olsen and D. Lopes Pegna and W.R. Sands and A.J. S. Smith and A.V. Telnov and A. Tumanov and E.W. Varnes and E. Baracchini and F. Bellini and C. Bulfon and E. Buccheri and G. Cavoto and A. D'Orazio and E. Di Marco and R. Faccini and F. Ferrarotto and F. Ferroni and M. Gaspero and P.D. Jackson and E. Lamanna and E. Leonardi and L. Li Gioi and R. Lunadei and M.A. Mazzoni and S. Morganti and G. Piredda and F. Polci and D. del Re and F. Renga and F. Safai Tehrani and M. Serra and C. Voena and C. Bünger and S. Christ and T. Hartmann and T. Leddig and H. Schröder and G. Wagner and R. Waldi and T. Adye and M. Bly and C. Brew and C. Condurache and N. De Groot and B. Franek and N.I. Geddes and G.P. Gopal and E.O. Olaiya and S. Ricciardi and W. Roethel and F.F. Wilson and S.M. Xella and R. Aleksan and P. Bourgeois and S. Emery and M. Escalier and L. Esteve and A. Gaidot and S.F. Ganzhur and P.-F. Giraud and Z. Georgette and G. Graziani and G. Hamel de Monchenault and W. Kozanecki and M. Langer and M. Legendre and G.W. London and B. Mayer and P. Micout and B. Serfass and G. Vasseur and Ch. Yèche and M. Zito and M.T. Allen and R. Akre and D. Aston and T. Azemoon and D.J. Bard and J. Bartelt and R. Bartoldus and P. Bechtle and J. Becla and J.F. Benitez and N. Berger and K. Bertsche and C.T. Boeheim and K. Bouldin and A.M. Boyarski and R.F. Boyce and M. Browne and O.L. Buchmueller and W. Burgess and Y. Cai and C. Cartaro and A. Ceseracciu and R. Claus and M.R. Convery and D.P. Coupal and W.W. Craddock and G. Crane and M. Cristinziani and S. DeBarger and F.J. Decker and J.C. Dingfelder and M. Donald and J. Dorfan and G.P. Dubois-Felsmann and W. Dunwoodie and M. Ebert and S. Ecklund and R. Erickson and S. Fan and R.C. Field and A. Fisher and J. Fox and M. Franco Sevilla and B.G. Fulsom and A.M. Gabareen and I. Gaponenko and T. Glanzman and S.J. Gowdy and M.T. Graham and P. Grenier and T. Hadig and V. Halyo and G. Haller and J. Hamilton and A. Hanushevsky and A. Hasan and C. Hast and C. Hee and T. Himel and T. Hryn'ova and M.E. Huffer and T. Hung and W.R. Innes and R. Iverson and J. Kaminski and M.H. Kelsey and H. Kim and P. Kim and D. Kharakh and M.L. Kocian and A. Krasnykh and J. Krebs and W. Kroeger and A. Kulikov and N. Kurita and U. Langenegger and D.W.G.S. Leith and P. Lewis and S. Li and J. Libby and B. Lindquist and S. Luitz and V. Lüth and H.L. Lynch and D.B. MacFarlane and H. Marsiske and M. McCulloch and J. McDonald and R. Melen and S. Menke and S. Metcalfe and R. Messner and L.J. Moss and R. Mount and D.R. Muller and H. Neal and D. Nelson and S. Nelson and M. Nordby and Y. Nosochkov and A. Novokhatski and C.P. O'Grady and F.G. O'Neill and I. Ofte and V.E. Ozcan and A. Perazzo and M. Perl and S. Petrak and M. Piemontese and S. Pierson and T. Pulliam and B.N. Ratcliff and S. Ratkovsky and R. Reif and C. Rivetta and R. Rodriguez and A. Roodman and A.A. Salnikov and T. Schietinger and R.H. Schindler and H. Schwarz and J. Schwiening and J. Seeman and D. Smith and A. Snyder and A. Soha and M. Stanek and J. Stelzer and D. Su and M.K. Sullivan and K. Suzuki and S.K. Swain and H.A. Tanaka and D. Teytelman and J.M. Thompson and J.S. Tinslay and A. Trunov and J. Turner and N. van Bakel and D. van Winkle and J. Va'vra and A.P. Wagner and M. Weaver and A.J.R. Weinstein and T. Weber and C.A. West and U. Wienands and W.J. Wisniewski and M. Wittgen and W. Wittmer and D.H. Wright and H.W. Wulsin and Y. Yan and A.K. Yarritu and K. Yi and G. Yocky and C.C. Young and V. Ziegler and X.R. Chen and H. Liu and W. Park and M.V. Purohit and H. Singh and A.W. Weidemann and R.M. White and J.R. Wilson and F.X. Yumiceva and S.J. Sekula and M. Bellis and P.R. Burchat and A.J. Edwards and S.A. Majewski and T.I. Meyer and T.S. Miyashita and B.A. Petersen and C. Roat and M. Ahmed and S. Ahmed and M.S. Alam and R. Bula and J.A. Ernst and V. Jain and J. Liu and B. Pan and M.A. Saeed and F.R. Wappler and S.B. Zain and R. Gorodeisky and N. Guttman and D. Peimer and A. Soffer and A. De Silva and P. Lund and M. Krishnamurthy and G. Ragghianti and S.M. Spanier and B.J. Wogsland and R. Eckmann and J.L. Ritchie and A.M. Ruland and A. Satpathy and C.J. Schilling and R.F. Schwitters and B.C. Wray and B.W. Drummond and J.M. Izen and I. Kitayama and X.C. Lou and S. Ye and F. Bianchi and M. Bona and F. Gallo and D. Gamba and M. Pelliccioni and M. Bomben and C. Borean and L. Bosisio and F. Cossutti and G. Della Ricca and S. Dittongo and S. Grancagnolo and L. Lanceri and P. Poropat and I. Rashevskaya and L. Vitale and G. Vuagnin and P.F. Manfredi and V. Re and V. Speziali and E.D. Frank and L. Gladney and Q.H. Guo and J. Panetta and V. Azzolini and N. Lopez-March and F. Martinez-Vidal and D.A. Milanes and A. Oyanguren and A. Agarwal and J. Albert and Sw. Banerjee and F.U. Bernlochner and C.M. Brown and H.H. F. Choi and D. Fortin and K.B. Fransham and K. Hamano and R. Kowalewski and M.J. Lewczuk and I.M. Nugent and J.M. Roney and R.J. Sobie and J.J. Back and T.J. Gershon and P.F. Harrison and J. Ilic and T.E. Latham and G.B. Mohanty and E. Puccio and H.R. Band and X. Chen and B. Cheng and S. Dasu and M. Datta and A.M. Eichenbaum and J.J. Hollar and H. Hu and J.R. Johnson and P.E. Kutter and H. Li and R. Liu and B. Mellado and A. Mihalyi and A.K. Mohapatra and Y. Pan and M. Pierini and R. Prepost and I.J. Scott and P. Tan and C.O. Vuosalo and J.H. von Wimmersperg-Toeller and S.L. Wu and Z. Yu and M.G. Greene and T.M.B. Kordich},
  title    = {The BaBar detector: Upgrades, operation and performance},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2013},
  volume   = {729},
  pages    = {615 - 701},
  issn     = {0168-9002},
  abstract = {The BaBar detector operated successfully at the PEP-II asymmetric e+e− collider at the SLAC National Accelerator Laboratory from 1999 to 2008. This report covers upgrades, operation, and performance of the collider and the detector systems, as well as the trigger, online and offline computing, and aspects of event reconstruction since the beginning of data taking.},
  doi      = {https://doi.org/10.1016/j.nima.2013.05.107},
  keywords = {General-purpose detector for colliding beams, Operational experience, High-luminosity storage ring operation, Beam monitoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900213007183},
}

@Article{Aoumeur2004,
  author   = {Nasreddine Aoumeur and Gunter Saake},
  title    = {Dynamically evolving concurrent information systems specification and validation: a component-based Petri nets proposal},
  journal  = {Data \& Knowledge Engineering},
  year     = {2004},
  volume   = {50},
  number   = {2},
  pages    = {117 - 173},
  issn     = {0169-023X},
  abstract = {Besides the steady growing of size-complexity and distribution of present-day information systems, business volatility with rapid changes in users' wishes and technological upgrading are stressing an overwhelmingly need for more advanced conceptual modeling approaches. Such advanced conceptual models should coherently and soundly reflect these three crucial dimensions, namely the size, space and (evolution over) time dimensions. In contribution towards such advanced conceptual approaches, we presented in [Data Know. Eng. 42 (2) (2002) 143] a new form of integration of object-orientation with emphasize on componentization into a variety of algebraic Petri nets, we referred to as Co-nets. The purpose of the present paper is to soundly extend this proposal for coping with dynamic changing of structural and behavioral aspects of Co-nets components. To this aim, we are proposing an adequate Petri net-based meta-level that may be sketched as follows. First, we construct two `meta-nets' for each component; one concerns the modification of behavioral aspects and the other is for dealing with structural aspects. While the meta-net for behavioral dynamic enables the dynamic of any transition in a given component to be modified at runtime, the meta-net for structural aspects completes and enhances these capabilities by allowing involved messages and object signatures (i.e. structure) to be dynamically manipulated. In addition of a rigorous description of this meta-level and its illustration using a medium-complexity banking system example, we also discuss how this level brings a satisfactory solution to the well-known inheritance-anomaly problem.},
  doi      = {https://doi.org/10.1016/j.datak.2003.10.005},
  keywords = {Distributed information systems, C, Component-orientation, Rewriting logic, Runtime evolution, Inheritance-anomaly},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X03001800},
}

@Article{Ahn1988,
  author   = {Ilsoo Ahn and Richard Snodgrass},
  title    = {Partitioned storage for temporal databases},
  journal  = {Information Systems},
  year     = {1988},
  volume   = {13},
  number   = {4},
  pages    = {369 - 391},
  issn     = {0306-4379},
  abstract = {Efficiently maintaining history data on line together with current data is difficult. This paper discusses one promising approach, the temporally partitioned store. The current store contains current data and possibly some history data, whil the history store holds the rest of the data. The two stores can utilize different storage formats, and even different storage media, depending on the individual data characteristics. We discuss various issues on the temporally partitioned store, investigate several formats for the history store, and evaluate their performance on a set of sample queries.},
  doi      = {https://doi.org/10.1016/0306-4379(88)90004-X},
  url      = {http://www.sciencedirect.com/science/article/pii/030643798890004X},
}

@Article{Mazzeo2017,
  author   = {Giuseppe M. Mazzeo and Elio Masciari and Carlo Zaniolo},
  title    = {A fast and accurate algorithm for unsupervised clustering around centroids},
  journal  = {Information Sciences},
  year     = {2017},
  volume   = {400-401},
  pages    = {63 - 90},
  issn     = {0020-0255},
  abstract = {A centroid-based clustering algorithm is proposed that works in a totally unsupervised fashion and is significantly faster and more accurate than existing algorithms. The algorithm, named CLUBS+ (for CLustering Using Binary Splitting), achieves these results by combining features of hierarchical and partition-based algorithms. Thus, CLUBS+ consists of two major phases, i.e., a divisive phase and an agglomerative phase, each followed by a refinement phase. Each major phase consists of successive steps in which the samples are repartitioned using a criterion based on least quadratic distance. This criterion possesses unique analytical properties that are elucidated in the paper and exploited by the algorithm to achieve a very fast computation. The paper presents the results of the extensive experiments performed: these confirm that the new algorithm is fast, impervious to noise, and produces results of better quality than other algorithms, such as BOOL, BIRCH, and k-means++, even when the analyst can determine the correct number of clusters—a very difficult task from which users are spared by CLUBS+.},
  doi      = {https://doi.org/10.1016/j.ins.2017.03.002},
  keywords = {Unsupervised clustering, Hierarchical partitioning, Extensive experimental evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025517305765},
}

@Article{Iyer2005,
  author   = {Bala Iyer and G. Shankaranarayanan and Melanie L. Lenard},
  title    = {Model management decision environment: a Web service prototype for spreadsheet models},
  journal  = {Decision Support Systems},
  year     = {2005},
  volume   = {40},
  number   = {2},
  pages    = {283 - 304},
  issn     = {0167-9236},
  abstract = {In the modern day, digital enterprise data and models are widely distributed. Decision-making in such distributed environments needs secure and easy access to these resources, rapid integration of decision models, and the ability to deploy these in real time. This demands a different approach for model management—one that permits decision-makers to not only share/access but also evaluate/understand models, choose appropriate ones from a collection of models, and orchestrate the execution of the model(s) in real time. In this paper, we describe an architecture that defines a service-oriented, Web service-based approach to model management. We first present a classification of stakeholders from the perspective of model management and identify the layers of modeling knowledge required for managing models. We then define a formal representation for organizing the content knowledge using a graph-based representation. We have used spreadsheet model(s) as a vehicle for explaining and demonstrating our concepts in this paper. Finally, we describe an environment (virtual business environment, VBE) that is based on a Web services architecture that would help store, retrieve, and distribute the layers of modeling knowledge to the various categories of users identified.},
  doi      = {https://doi.org/10.1016/j.dss.2004.01.008},
  keywords = {Structured modeling, Model management, Spreadsheets, Knowledge layers, Web services},
  url      = {http://www.sciencedirect.com/science/article/pii/S016792360400034X},
}

@Article{Chen2006,
  author   = {Qun Chen and Andrew Lim and Kian Win Ong and Ji Qing Tang},
  title    = {Indexing graph-structured XML data for efficient structural join operation},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {58},
  number   = {2},
  pages    = {159 - 179},
  issn     = {0169-023X},
  abstract = {Structural join has been established as a primitive technique for matching the binary containment pattern, specifically the parent–child and ancestor–descendant relationship, on the tree XML data. While current indexing approaches and evaluation algorithms proposed for the structural join operation assume the tree-structured data model, the presence of reference links in XML documents may render the underlying model a graph instead. In the more general category of semi-structured data, of which XML is an example, the data model is also usually supposed to be of graph structure. In this paper, we present an indexing approach and corresponding evaluation algorithms for efficiently performing the structural join operation on graph-structured data. Our approach encodes the structural containment relationship of a graph on multiple nested tree-structured layers, probably with the exception of the last one. With each tree-structured layer indexed with the inverted technique, the structural join operation on a graph can therefore be accomplished through recursively performing structural joins on nested layer trees. Our extensive experiments on both benchmark and synthetic XML data indicate that our proposed approach has good potential to perform significantly better than existing ones in term of both the I/O and CPU cost.},
  doi      = {https://doi.org/10.1016/j.datak.2005.05.008},
  keywords = {XML, Semi-structured data, Structural join, Structural summary, Inverted indexing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05000807},
}

@Article{Singh2005,
  author   = {Amar V. Singh and Kenneth B. Knudsen and Thomas B. Knudsen},
  title    = {Computational systems analysis of developmental toxicity: design, development and implementation of a Birth Defects Systems Manager (BDSM)},
  journal  = {Reproductive Toxicology},
  year     = {2005},
  volume   = {19},
  number   = {3},
  pages    = {421 - 439},
  issn     = {0890-6238},
  note     = {Systems Biology/Computational Toxicology},
  abstract = {Birth defects and developmental disabilities remain an important public health issue worldwide. With the availability of genomic sequences from a growing number of human and model organisms and the rapid expansion of the public repositories holding large-scale gene expression datasets, a computational systems analysis of developmental toxicology can incorporate this vast digital information toward the realization of predictive models for complex disease. Here we describe the initial design, development and implementation of a Birth Defects Systems Manager (BDSM). The project was motivated by the need for a computational-bioinformatics infrastructure to manage vast digital information from functional genomics and for a new knowledge environment specifically engineered for the analysis of developmental processes and toxicities. Proof-of-concept tested BDSM using meta-analysis of gene expression data collected from different laboratories, technology platforms, and study models. The composite dataset incorporated 232 microarray comparisons of RNA samples by single or dual microarray platforms, cDNA or oligonucleotide based probes, and human or mouse sequence information. Preliminary results identified system-level features in the embryonic transcriptome as it reacted to various developmental-teratological stimuli. BDSM is open access through the worldwide web (http://systemsanalysis.louisville.edu/) and can be integrated with other bioinformatics tools and resources to advance the pace of discovery in birth defects research.},
  doi      = {https://doi.org/10.1016/j.reprotox.2004.11.008},
  keywords = {Computational, Systems biology, Development, Embryo, Toxicity, Microarray, Gene expression, Database, Bioinformatics, Mouse, Human, rat, Genomics, Network},
  url      = {http://www.sciencedirect.com/science/article/pii/S0890623804001923},
}

@Article{Shankaranarayanan2006,
  author   = {G. Shankaranarayanan and Yu Cai},
  title    = {Supporting data quality management in decision-making},
  journal  = {Decision Support Systems},
  year     = {2006},
  volume   = {42},
  number   = {1},
  pages    = {302 - 317},
  issn     = {0167-9236},
  abstract = {In the complex decision-environments that characterize e-business settings, it is important to permit decision-makers to proactively manage data quality. In this paper we propose a decision-support framework that permits decision-makers to gauge quality both in an objective (context-independent) and in a context-dependent manner. The framework is based on the information product approach and uses the Information Product Map (IPMAP). We illustrate its application in evaluating data quality using completeness—a data quality dimension that is acknowledged as important. A decision-support tool (IPView) for managing data quality that incorporates the proposed framework is also described.},
  doi      = {https://doi.org/10.1016/j.dss.2004.12.006},
  keywords = {Data quality, Completeness, Data quality dimensions, Information product, IPMAP},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923605000023},
}

@Article{Bertino2003,
  author   = {Elisa Bertino and Elena Ferrari and Giovanna Guerrini and Isabella Merlo},
  title    = {T-ODMG: an ODMG compliant temporal object model supporting multiple granularity management},
  journal  = {Information Systems},
  year     = {2003},
  volume   = {28},
  number   = {8},
  pages    = {885 - 927},
  issn     = {0306-4379},
  abstract = {In this paper we investigate some issues arising from the introduction of multiple temporal granularities in an object-oriented data model. Although issues concerning temporal granularities have been investigated in the context of temporal relational database systems, no comparable amount of work has been done in the context of object-oriented models. Moreover, the main drawback of the existing proposals is the lack of a formal basis—which we believe is essential to manage the inherent complexity of the object-oriented data model. In this paper, we define a comprehensive temporal object-oriented data model supporting multiple temporal granularities. We formally define the main notions of the data model such as types, legal values, classes, and objects. We address issues related to inheritance, type refinement, and substitutability. Finally, we describe the implementation of the presented model on top of an ODMG compliant DBMS.},
  doi      = {https://doi.org/10.1016/S0306-4379(02)00077-7},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437902000777},
}

@Article{Camporeale2015,
  author   = {Cecilia Camporeale and Antonio De Nicola and Maria Luisa Villani},
  title    = {Semantics-based services for a low carbon society: An application on emissions trading system data and scenarios management},
  journal  = {Environmental Modelling \& Software},
  year     = {2015},
  volume   = {64},
  pages    = {124 - 142},
  issn     = {1364-8152},
  abstract = {A low carbon society aims at fighting global warming by stimulating synergic efforts from governments, industry and scientific communities. Decision support systems should be adopted to provide policy makers with possible scenarios, options for prompt countermeasures in case of side effects on environment, economy and society due to low carbon society policies, and also options for information management. A necessary precondition to fulfill this agenda is to face the complexity of this multi-disciplinary domain and to reach a common understanding on it as a formal specification. Ontologies are widely accepted means to share knowledge. Together with semantic rules, they enable advanced semantic services to manage knowledge in a smarter way. Here we address the European Emissions Trading System (EU-ETS) and we present a knowledge base consisting of the EREON ontology and a catalogue of rules. Then we describe two innovative semantic services to manage ETS data and information on ETS scenarios.},
  doi      = {https://doi.org/10.1016/j.envsoft.2014.11.007},
  keywords = {Low carbon society, Emissions trading system, Ontology, Semantic rule, Semantics-based technology},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364815214003326},
}

@Article{Chen2006a,
  author   = {Qun Chen and Andrew Lim and Kian Win Ong and Jiqing Tang},
  title    = {Indexing XML documents for XPath query processing in external memory},
  journal  = {Data \& Knowledge Engineering},
  year     = {2006},
  volume   = {59},
  number   = {3},
  pages    = {681 - 699},
  issn     = {0169-023X},
  note     = {Including: ER 2003},
  abstract = {Existing encoding schemes and index structures proposed for XML query processing primarily target the containment relationship, specifically the parent–child and ancestor–descendant relationship. The presence of preceding-sibling and following-sibling location steps in the XPath specification, which is the de facto query language for XML, makes the horizontal navigation, besides the vertical navigation, among nodes of XML documents a necessity for efficient evaluation of XML queries. Our work enhances the existing range-based and prefix-based encoding schemes such that all structural relationships between XML nodes can be determined from their codes alone. Furthermore, an external-memory index structure based on the traditional B+-tree, XL+-tree(XML Location+-tree), is introduced to index element sets such that all defined location steps in the XPath language, vertical and horizontal, top-down and bottom-up, can be processed efficiently. The XL+-trees under the range or prefix encoding scheme actually share the same structure; but various search operations upon them may be slightly different as a result of the richer information provided by the prefix encoding scheme. Finally, experiments are conducted to validate the efficiency of the XL+-tree approach. We compare the query performance of XL+-tree with that of R-tree, which is capable of handling comprehensive XPath location steps and has been empirically shown to outperform other indexing approaches.},
  doi      = {https://doi.org/10.1016/j.datak.2005.11.002},
  keywords = {XML, XPath query language, External-memory index structure, +-tree},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X05001746},
}

@InCollection{Powell2007,
  title     = {Index},
  booktitle = {Oracle Performance Tuning for 10gR2 (Second Edition)},
  publisher = {Digital Press},
  year      = {2007},
  editor    = {Gavin Powell},
  pages     = {881 - 904},
  address   = {Burlington},
  edition   = {Second Edition},
  isbn      = {978-1-55558-345-3},
  doi       = {https://doi.org/10.1016/B978-155558345-3/50032-X},
  url       = {http://www.sciencedirect.com/science/article/pii/B978155558345350032X},
}

@Article{2008,
  title   = {Abstract for the 8th International congress, Molecular Epidemiology and Evolutionary Genetics of Infectious Diseases},
  journal = {Infection, Genetics and Evolution},
  year    = {2008},
  volume  = {8},
  number  = {4},
  pages   = {S1 - S49},
  issn    = {1567-1348},
  note    = {MEEGID VIII 2006},
  doi     = {https://doi.org/10.1016/j.meegid.2008.01.008},
  url     = {http://www.sciencedirect.com/science/article/pii/S1567134808000117},
}

@Article{Bonner1994,
  author   = {Anthony J. Bonner and Michael Kifer},
  title    = {An overview of transaction logic},
  journal  = {Theoretical Computer Science},
  year     = {1994},
  volume   = {133},
  number   = {2},
  pages    = {205 - 265},
  issn     = {0304-3975},
  abstract = {This paper presents an overview of Transaction Logic—a new formalism recently introduced in Bonner and Kifer (1992, 1993) and designed to deal with the phenomenon of state changes in logic programming, databases, and AI. Transaction Logic has a natural model theory and a sound and complete proof theory. Unlike many other logics, however, it is suitable for programming procedures that accomplish state transitions in a logically sound manner. Transaction logic amalgamates such features as hypothetical and committed updates, dynamic constraints on transaction execution, nondeterminism, and bulk updates. Transaction Logic also appears to be suitable as a logical model of hitherto nonlogical phenomena, including so-called procedural knowledge in AI, and the behavior of object-oriented databases, especially methods with side effects.},
  doi      = {https://doi.org/10.1016/0304-3975(94)90190-2},
  url      = {http://www.sciencedirect.com/science/article/pii/0304397594901902},
}

@Article{Libkin2011,
  author   = {Leonid Libkin and Cristina Sirangelo},
  title    = {Data exchange and schema mappings in open and closed worlds},
  journal  = {Journal of Computer and System Sciences},
  year     = {2011},
  volume   = {77},
  number   = {3},
  pages    = {542 - 571},
  issn     = {0022-0000},
  note     = {Database Theory},
  abstract = {In the study of data exchange one usually assumes an open-world semantics, making it possible to extend instances of target schemas. An alternative closed-world semantics only moves ‘as much data as needed’ from the source to the target to satisfy constraints of a schema mapping. It avoids some of the problems exhibited by the open-world semantics, but limits the expressivity of schema mappings. Here we propose a mixed approach: one can designate different attributes of target schemas as open or closed, to combine the additional expressivity of the open-world semantics with the better behavior of query answering in closed worlds. We define such schema mappings, and show that they cover a large space of data exchange solutions with two extremes being the known open and closed-world semantics. We investigate the problems of query answering and schema mapping composition, and prove two trichotomy theorems, classifying their complexity based on the number of open attributes. We find conditions under which schema mappings compose, extending known results to a wide range of closed-world mappings. We also provide results for restricted classes of queries and mappings guaranteeing lower complexity.},
  doi      = {https://doi.org/10.1016/j.jcss.2010.04.010},
  keywords = {Data exchange, Schema mappings, Closed world assumption, Open world assumption, Incomplete information},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022000010000528},
}

@Article{Meo2007,
  author   = {Pasquale De Meo and Luigi Palopoli and Giovanni Quattrone and Domenico Ursino},
  title    = {Combining Description Logics with synopses for inferring complex knowledge patterns from XML sources},
  journal  = {Information Systems},
  year     = {2007},
  volume   = {32},
  number   = {8},
  pages    = {1184 - 1224},
  issn     = {0306-4379},
  abstract = {This paper illustrates how a Description Logics fragment, combined with a specific data compression technique, can be used for inferring complex intensional knowledge patterns from a set of semantically heterogeneous XML sources. The paper first provides a detailed description of the various steps of our approach; then, it describes some experiments performed to test its accuracy; after this, it presents a wide range of applications possibly benefiting from inferred patterns; finally, it compares our approach with the related ones previously proposed in the literature.},
  doi      = {https://doi.org/10.1016/j.is.2007.03.003},
  keywords = {Description Logics, Synopses, XML, Interschema property extraction},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437907000208},
}

@Article{1991e,
  title   = {Bibliography Tectonophysics volumes 151–200},
  journal = {Tectonophysics},
  year    = {1991},
  volume  = {200},
  number  = {4},
  pages   = {377 - 422},
  issn    = {0040-1951},
  doi     = {https://doi.org/10.1016/0040-1951(91)90378-6},
  url     = {http://www.sciencedirect.com/science/article/pii/0040195191903786},
}

@Article{Chen2008,
  author   = {Qun Chen and Andrew Lim and Kian Win Ong},
  title    = {Enabling structural summaries for efficient update and workload adaptation},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {64},
  number   = {3},
  pages    = {558 - 579},
  issn     = {0169-023X},
  abstract = {To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from data and serve as the indexes for evaluating path expressions. We introduce D(k)-index, an adaptive structural summary, for general graph-structured data. Building on previous 1-index and A(k)-index, D(k)-index is also based on the concept of bisimilarity. However, as a generalization of 1-index and A(k)-index, D(k)-index possesses the adaptive ability to adjust its structure to changes in query load. It also enables efficient update algorithms, which are crucial to real applications but have not been adequately addressed in previous literatures. Our experiments show that D(k)-index is a more effective structural summary than previous static ones as a result of its query load sensitivity. In addition, the update operations on it can be performed more efficient than on its predecessors.},
  doi      = {https://doi.org/10.1016/j.datak.2007.09.012},
  keywords = {Semi-structured data, XML index, Query language, Structural summary, Bisimilarity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001802},
}

@Article{Claypool2008,
  author   = {Kajal T. Claypool},
  title    = {SUSAX: Context-specific searching in XML documents using sequence alignment techniques},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {65},
  number   = {2},
  pages    = {177 - 197},
  issn     = {0169-023X},
  note     = {Including Special Section: 3rd XML Schema and Data Management Workshop (XSDM 2006) – Five selected and extended papers},
  abstract = {Keyword searching while very successful in narrowing down the contents of the Web to the pertaining subset of information, has two primary drawbacks. First, the accuracy of the search is closely coupled with the choice of keywords. Second, keywords are limited in their expressibility. In particular, they fail to adequately capture the contextual information implicit in most searches done by users. In this paper we present an approach to efficiently address these drawbacks of keyword searching over XML documents. In particular, we present SUSAX a system for approximate contextual querying over XML documents wherein queries are represented as simple XPaths. A key contribution of our work is the novel algorithm used to match the XPath-like query with similar paths in the repository. The algorithm is based on sequence alignment algorithms prevalent in life sciences domain for discovering the similarity between genome and protein sequences. In this paper, we show an adaptation of the sequence alignment algorithm for now discovering and cataloging the similarity between two paths.},
  doi      = {https://doi.org/10.1016/j.datak.2007.09.010},
  keywords = {Schema matching, Schema integration, Hybrid schema matching, XML schema matching},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001565},
}

@Article{Cautis2009,
  author   = {Bogdan Cautis and Serge Abiteboul and Tova Milo},
  title    = {Reasoning about XML update constraints},
  journal  = {Journal of Computer and System Sciences},
  year     = {2009},
  volume   = {75},
  number   = {6},
  pages    = {336 - 358},
  issn     = {0022-0000},
  abstract = {We introduce in this paper a class of constraints for describing how an XML document can evolve, namely XML update constraints. For these constraints, we study the implication problem, giving algorithms and complexity results for constraints of varying expressive power. Besides classical constraint implication, we also consider an instance-based approach in which we take into account data. More precisely, we study implication with respect to a current tree instance, resulting from a series of unknown updates. The main motivation of our work is reasoning about data integrity under update restrictions in contexts where owners may lose control over their data, such as in publishing or exchange.},
  doi      = {https://doi.org/10.1016/j.jcss.2009.02.001},
  keywords = {Semi-structured data, XML, Update constraints, Implication, Data integrity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022000009000154},
}

@Article{Baral1997,
  author   = {Chitta Baral and Michael Gelfond},
  title    = {Reasoning about effects of concurrent actions},
  journal  = {The Journal of Logic Programming},
  year     = {1997},
  volume   = {31},
  number   = {1},
  pages    = {85 - 117},
  issn     = {0743-1066},
  note     = {Reasoning about Action and Change},
  abstract = {Gelfond and Lifschitz introduce a declarative languageAfor describing effects of actions and describe translations of theories in this language into extended logic programs. In this paper we extend the languageAand its translation to allow reasoning about the effects of concurrent actions. The logic programming formalization of situation calculus with concurrent actions presented in the paper is of independent interest and may serve as a test bed for the investigation of various transformations and logic programming inference mechanisms.},
  doi      = {https://doi.org/10.1016/S0743-1066(96)00140-9},
  url      = {http://www.sciencedirect.com/science/article/pii/S0743106696001409},
}

@InCollection{Aebersold1998,
  author    = {Ruedi Aebersold and Scott D. Patterson},
  title     = {Chapter 1 - Current Problems and Technical Solutions in Protein Biochemistry},
  booktitle = {Proteins},
  publisher = {Academic Press},
  year      = {1998},
  editor    = {Ruth Hogue Angeletti},
  pages     = {3 - 120},
  address   = {San Diego},
  isbn      = {978-0-12-058785-8},
  abstract  = {Publisher Summary
This chapter describes problems and technical solutions in protein biochemistry. The vast majority of biological processes and pathways are tightly controlled. This applies equally to well-understood and relatively simple processes such as oxygen transport and storage and two as yet molecularly poorly understood phenomena of extreme complexity such as development, cell differentiation, and signal transduction pathways that serve to elicit the appropriate intracellular responses to extracellular stimuli. Large-scale DNA mapping and sequencing efforts, aimed at deciphering the complete genomic sequences of a number of species or the expressed sequences represented by cDNAs of differentiated tissues and cells, are the most widely publicized and discussed global programs in biology. The analysis of myristoylated and palmitylated proteins follows the same principles as that of prenylated proteins, except that there is no necessity to block an endogenous enzyme to allow efficient incorporation of the radiolabel into the target proteins. It is found that depending on structural requirements of the binding site, specific protein, ligand interactions may require refolding of the polypeptide after denaturing gel electrophoresis.},
  doi       = {https://doi.org/10.1016/B978-012058785-8/50003-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780120587858500037},
}

@InCollection{SIEWIOREK1992,
  author    = {DANIEL P. SIEWIOREK and ROBERT S. SWARZ},
  title     = {8 - HIGH-AVAILABILITY SYSTEMS},
  booktitle = {Reliable Computer Systems (Second Edition)},
  publisher = {Digital Press},
  year      = {1992},
  editor    = {DANIEL P. SIEWIOREK and ROBERT S. SWARZ},
  pages     = {524 - 670},
  address   = {Boston},
  edition   = {Second Edition},
  isbn      = {978-1-55558-075-9},
  abstract  = {Publisher Summary
This chapter focuses on dynamic redundancy that is the basic approach used in high-availability systems. The common channel interface signaling provides an independent data link between telephone switching systems. The entire peripheral hardware is interfaced to the central control over AC-coupled buses. As in the AT&T switching systems, the tandem architecture is designed to take advantage of the OLTP application to simplify error detection and recovery. A network systems management program provides a set of operators that helps reduce the number of administrative errors typically encountered in complex systems. A major issue in the design of loosely coupled duplicated systems is the way by which both copies can be kept identical in the face of errors. The design goal for Stratus systems is continuous processing, which is defined as uninterrupted operation without loss of data, performance degradation, or special programming.},
  doi       = {https://doi.org/10.1016/B978-1-55558-075-9.50014-4},
  url       = {http://www.sciencedirect.com/science/article/pii/B9781555580759500144},
}

@InCollection{Jung2014,
  author    = {In-Ho Jung and Jean Lehmann and Evgueni Jak},
  title     = {Chapter 5.3 - Applications},
  booktitle = {Treatise on Process Metallurgy},
  publisher = {Elsevier},
  year      = {2014},
  editor    = {Seshadri Seetharaman},
  pages     = {675 - 798},
  address   = {Boston},
  isbn      = {978-0-08-096984-8},
  abstract  = {The computational thermodynamic databases have been applied to various pyrometallurgy processes to understand the complex chemical reaction and improve the process. In this chapter, the ferrous and nonferrous examples are presented.},
  doi       = {https://doi.org/10.1016/B978-0-08-096984-8.00032-X},
  keywords  = {Steelmaking process, Inclusion, Refractories, Slag/metal reaction, Nonferrous},
  url       = {http://www.sciencedirect.com/science/article/pii/B978008096984800032X},
}

@Article{Abazov2006,
  author   = {V.M. Abazov and B. Abbott and M. Abolins and B.S. Acharya and D.L. Adams and M. Adams and T. Adams and M. Agelou and J.-L. Agram and S.N. Ahmed and S.H. Ahn and M. Ahsan and G.D. Alexeev and G. Alkhazov and A. Alton and G. Alverson and G.A. Alves and M. Anastasoaie and T. Andeen and J.T. Anderson and S. Anderson and B. Andrieu and R. Angstadt and V. Anosov and Y. Arnoud and M. Arov and A. Askew and B. Åsman and A.C.S. Assis Jesus and O. Atramentov and C. Autermann and C. Avila and L. Babukhadia and T.C. Bacon and F. Badaud and A. Baden and S. Baffioni and L. Bagby and B. Baldin and P.W. Balm and P. Banerjee and S. Banerjee and E. Barberis and O. Bardon and W. Barg and P. Bargassa and P. Baringer and C. Barnes and J. Barreto and J.F. Bartlett and U. Bassler and M. Bhattacharjee and M.A. Baturitsky and D. Bauer and A. Bean and B. Baumbaugh and S. Beauceron and M. Begalli and F. Beaudette and M. Begel and A. Bellavance and S.B. Beri and G. Bernardi and R. Bernhard and I. Bertram and M. Besançon and A. Besson and R. Beuselinck and D. Beutel and V.A. Bezzubov and P.C. Bhat and V. Bhatnagar and M. Binder and C. Biscarat and A. Bishoff and K.M. Black and I. Blackler and G. Blazey and F. Blekman and S. Blessing and D. Bloch and U. Blumenschein and E. Bockenthien and V. Bodyagin and A. Boehnlein and O. Boeriu and T.A. Bolton and P. Bonamy and D. Bonifas and F. Borcherding and G. Borissov and K. Bos and T. Bose and C. Boswell and M. Bowden and A. Brandt and G. Briskin and R. Brock and G. Brooijmans and A. Bross and N.J. Buchanan and D. Buchholz and M. Buehler and V. Buescher and S. Burdin and S. Burke and T.H. Burnett and E. Busato and C.P. Buszello and D. Butler and J.M. Butler and J. Cammin and S. Caron and J. Bystricky and L. Canal and F. Canelli and W. Carvalho and B.C.K. Casey and D. Casey and N.M. Cason and H. Castilla-Valdez and S. Chakrabarti and D. Chakraborty and K.M. Chan and A. Chandra and D. Chapin and F. Charles and E. Cheu and L. Chevalier and E. Chi and R. Chiche and D.K. Cho and R. Choate and S. Choi and B. Choudhary and S. Chopra and J.H. Christenson and T. Christiansen and L. Christofek and I. Churin and G. Cisko and D. Claes and A.R. Clark and B. Clément and C. Clément and Y. Coadou and D.J. Colling and L. Coney and B. Connolly and M. Cooke and W.E. Cooper and D. Coppage and M. Corcoran and J. Coss and A. Cothenet and M.-C. Cousinou and B. Cox and S. Crépé-Renaudin and M. Cristetiu and M.A.C. Cummings and D. Cutts and H. da Motta and M. Das and B. Davies and G. Davies and G.A. Davis and W. Davis and K. De and P. de Jong and S.J. de Jong and E. De La Cruz-Burelo and C. De La Taille and C. De Oliveira Martins and S. Dean and J.D. Degenhardt and F. Déliot and P.A. Delsart and K. Del Signore and R. DeMaat and M. Demarteau and R. Demina and P. Demine and D. Denisov and S.P. Denisov and S. Desai and H.T. Diehl and M. Diesburg and M. Doets and M. Doidge and H. Dong and S. Doulas and L.V. Dudko and L. Duflot and S.R. Dugad and A. Duperrin and O. Dvornikov and J. Dyer and A. Dyshkant and M. Eads and D. Edmunds and T. Edwards and J. Ellison and J. Elmsheuser and J.T. Eltzroth and V.D. Elvira and S. Eno and P. Ermolov and O.V. Eroshin and J. Estrada and D. Evans and H. Evans and A. Evdokimov and V.N. Evdokimov and J. Fagan and J. Fast and S.N. Fatakia and D. Fein and L. Feligioni and A.V. Ferapontov and T. Ferbel and M.J. Ferreira and F. Fiedler and F. Filthaut and W. Fisher and H.E. Fisk and I. Fleck and T. Fitzpatrick and E. Flattum and F. Fleuret and R. Flores and J. Foglesong and M. Fortner and H. Fox and C. Franklin and W. Freeman and S. Fu and S. Fuess and T. Gadfort and C.F. Galea and E. Gallas and E. Galyaev and M. Gao and C. Garcia and A. Garcia-Bellido and J. Gardner and V. Gavrilov and A. Gay and P. Gay and D. Gelé and R. Gelhaus and K. Genser and C.E. Gerber and Y. Gershtein and D. Gillberg and G. Geurkov and G. Ginther and B. Gobbi and K. Goldmann and T. Golling and N. Gollub and V. Golovtsov and B. Gómez and G. Gomez and R. Gomez and R. Goodwin and Y. Gornushkin and K. Gounder and A. Goussiou and D. Graham and G. Graham and P.D. Grannis and K. Gray and S. Greder and D.R. Green and J. Green and J.A. Green and H. Greenlee and Z.D. Greenwood and E.M. Gregores and S. Grinstein and Ph. Gris and J.-F. Grivaz and L. Groer and S. Grünendahl and M.W. Grünewald and W. Gu and J. Guglielmo and A. Gupta and S.N. Gurzhiev and G. Gutierrez and P. Gutierrez and A. Haas and N.J. Hadley and E. Haggard and H. Haggerty and S. Hagopian and I. Hall and R.E. Hall and C. Han and L. Han and R. Hance and K. Hanagaki and P. Hanlet and S. Hansen and K. Harder and A. Harel and R. Harrington and J.M. Hauptman and R. Hauser and C. Hays and J. Hays and E. Hazen and T. Hebbeker and C. Hebert and D. Hedin and J.M. Heinmiller and A.P. Heinson and U. Heintz and C. Hensel and G. Hesketh and M.D. Hildreth and R. Hirosky and J.D. Hobbs and B. Hoeneisen and M. Hohlfeld and S.J. Hong and R. Hooper and S. Hou and P. Houben and Y. Hu and J. Huang and Y. Huang and V. Hynek and D. Huffman and I. Iashvili and R. Illingworth and A.S. Ito and S. Jabeen and Y. Jacquier and M. Jaffré and S. Jain and V. Jain and K. Jakobs and R. Jayanti and A. Jenkins and R. Jesik and Y. Jiang and K. Johns and M. Johnson and P. Johnson and A. Jonckheere and P. Jonsson and H. Jöstlein and N. Jouravlev and M. Juarez and A. Juste and A.P. Kaan and M.M. Kado and D. Käfer and W. Kahl and S. Kahn and E. Kajfasz and A.M. Kalinin and J. Kalk and S.D. Kalmani and D. Karmanov and J. Kasper and I. Katsanos and D. Kau and R. Kaur and Z. Ke and R. Kehoe and S. Kermiche and S. Kesisoglou and A. Khanov and A. Kharchilava and Y.M. Kharzheev and H. Kim and K.H. Kim and T.J. Kim and N. Kirsch and B. Klima and M. Klute and J.M. Kohli and J.-P. Konrath and E.V. Komissarov and M. Kopal and V.M. Korablev and A. Kostritski and J. Kotcher and B. Kothari and A.V. Kotwal and A. Koubarovsky and A.V. Kozelov and J. Kozminski and A. Kryemadhi and O. Kouznetsov and J. Krane and N. Kravchuk and K. Krempetz and J. Krider and M.R. Krishnaswamy and S. Krzywdzinski and M. Kubantsev and R. Kubinski and N. Kuchinsky and S. Kuleshov and Y. Kulik and A. Kumar and S. Kunori and A. Kupco and T. Kurča and J. Kvita and V.E. Kuznetsov and R. Kwarciany and S. Lager and N. Lahrichi and G. Landsberg and M. Larwill and P. Laurens and B. Lavigne and J. Lazoflores and A.-C. Le Bihan and G. Le Meur and P. Lebrun and S.W. Lee and W.M. Lee and A. Leflat and C. Leggett and F. Lehner and R. Leitner and C. Leonidopoulos and J. Leveque and P. Lewis and J. Li and Q.Z. Li and X. Li and J.G.R. Lima and D. Lincoln and C. Lindenmeyer and S.L. Linn and J. Linnemann and V.V. Lipaev and R. Lipton and M. Litmaath and J. Lizarazo and L. Lobo and A. Lobodenko and M. Lokajicek and A. Lounis and P. Love and J. Lu and H.J. Lubatti and A. Lucotte and L. Lueking and C. Luo and M. Lynker and A.L. Lyon and E. Machado and A.K.A. Maciel and R.J. Madaras and P. Mättig and C. Magass and A. Magerkurth and A.-M. Magnan and M. Maity and N. Makovec and P.K. Mal and H.B. Malbouisson and S. Malik and V.L. Malyshev and V. Manakov and H.S. Mao and Y. Maravin and D. Markley and M. Markus and T. Marshall and M. Martens and M. Martin and G. Martin-Chassard and S.E.K. Mattingly and M. Matulik and A.A. Mayorov and R. McCarthy and R. McCroskey and M. McKenna and T. McMahon and D. Meder and H.L. Melanson and A. Melnitchouk and A. Mendes and D. Mendoza and L. Mendoza and X. Meng and Y.P. Merekov and M. Merkin and K.W. Merritt and A. Meyer and J. Meyer and M. Michaut and C. Miao and H. Miettinen and D. Mihalcea and V. Mikhailov and D. Miller and J. Mitrevski and N. Mokhov and J. Molina and N.K. Mondal and H.E. Montgomery and R.W. Moore and T. Moulik and G.S. Muanza and M. Mostafa and S. Moua and M. Mulders and L. Mundim and Y.D. Mutaf and P. Nagaraj and E. Nagy and M. Naimuddin and F. Nang and M. Narain and V.S. Narasimhan and A. Narayanan and N.A. Naumann and H.A. Neal and J.P. Negret and S. Nelson and R.T. Neuenschwander and P. Neustroev and C. Noeding and A. Nomerotski and S.F. Novaes and A. Nozdrin and T. Nunnemann and A. Nurczyk and E. Nurse and V. O’Dell and D.C. O’Neil and V. Oguri and D. Olis and N. Oliveira and B. Olivier and J. Olsen and N. Oshima and B.O. Oshinowo and G.J. Otero y Garzón and P. Padley and K. Papageorgiou and N. Parashar and J. Park and S.K. Park and J. Parsons and R. Partridge and N. Parua and A. Patwa and G. Pawloski and P.M. Perea and E. Perez and O. Peters and P. Pétroff and M. Petteni and L. Phaf and R. Piegaia and M.-A. Pleier and P.L.M. Podesta-Lerma and V.M. Podstavkov and Y. Pogorelov and M.-E. Pol and A. Pompoš and P. Polosov and B.G. Pope and E. Popkov and S. Porokhovoy and W.L. Prado da Silva and W. Pritchard and I. Prokhorov and H.B. Prosper and S. Protopopescu and M.B. Przybycien and J. Qian and A. Quadt and B. Quinn and E. Ramberg and R. Ramirez-Gomez and K.J. Rani and K. Ranjan and M.V.S. Rao and P.A. Rapidis and S. Rapisarda and J. Raskowski and P.N. Ratoff and R.E. Ray and N.W. Reay and R. Rechenmacher and L.V. Reddy and T. Regan and J.-F. Renardy and S. Reucroft and J. Rha and M. Ridel and M. Rijssenbeek and I. Ripp-Baudot and F. Rizatdinova and S. Robinson and R.F. Rodrigues and M. Roco and C. Rotolo and C. Royon and P. Rubinov and R. Ruchti and R. Rucinski and V.I. Rud and N. Russakovich and P. Russo and B. Sabirov and G. Sajot and A. Sánchez-Hernández and M.P. Sanders and A. Santoro and B. Satyanarayana and G. Savage and L. Sawyer and T. Scanlon and D. Schaile and R.D. Schamberger and Y. Scheglov and H. Schellman and P. Schieferdecker and C. Schmitt and C. Schwanenberger and A.A. Schukin and A. Schwartzman and R. Schwienhorst and S. Sengupta and H. Severini and E. Shabalina and M. Shamim and H.C. Shankar and V. Shary and A.A. Shchukin and P. Sheahan and W.D. Shephard and R.K. Shivpuri and A.A. Shishkin and D. Shpakov and M. Shupe and R.A. Sidwell and V. Simak and V. Sirotenko and D. Skow and P. Skubic and P. Slattery and D.E. Smith and R.P. Smith and K. Smolek and G.R. Snow and J. Snow and S. Snyder and S. Söldner-Rembold and X. Song and Y. Song and L. Sonnenschein and A. Sopczak and V. Sorín and M. Sosebee and K. Soustruznik and M. Souza and N. Spartana and B. Spurlock and N.R. Stanton and J. Stark and J. Steele and A. Stefanik and J. Steinberg and G. Steinbrück and K. Stevenson and V. Stolin and A. Stone and D.A. Stoyanova and J. Strandberg and M.A. Strang and M. Strauss and R. Ströhmer and D. Strom and M. Strovink and L. Stutte and S. Sumowidagdo and A. Sznajder and M. Talby and S. Tentindo-Repond and P. Tamburello and W. Taylor and P. Telford and J. Temple and N. Terentyev and V. Teterin and E. Thomas and J. Thompson and B. Thooris and M. Titov and D. Toback and V.V. Tokmenin and C. Tolian and M. Tomoto and D. Tompkins and T. Toole and J. Torborg and F. Touze and S. Towers and T. Trefzger and S. Trincaz-Duvoid and T.G. Trippe and D. Tsybychev and B. Tuchming and C. Tully and A.S. Turcot and P.M. Tuts and M. Utes and L. Uvarov and S. Uvarov and S. Uzunyan and B. Vachon and P.J. van den Berg and P. van Gemmeren and R. Van Kooten and W.M. van Leeuwen and N. Varelas and E.W. Varnes and A. Vartapetian and I.A. Vasilyev and M. Vaupel and M. Vaz and P. Verdier and L.S. Vertogradov and M. Verzocchi and M. Vigneault and F. Villeneuve-Seguier and P.R. Vishwanath and J.-R. Vlimant and E. Von Toerne and A. Vorobyov and M. Vreeswijk and T. Vu Anh and V. Vysotsky and H.D. Wahl and R. Walker and N. Wallace and L. Wang and Z.-M. Wang and J. Warchol and M. Warsinsky and G. Watts and M. Wayne and M. Weber and H. Weerts and M. Wegner and N. Wermes and M. Wetstein and A. White and V. White and D. Whiteson and D. Wicke and T. Wijnen and D.A. Wijngaarden and N. Wilcer and H. Willutzki and G.W. Wilson and S.J. Wimpenny and J. Wittlin and T. Wlodek and M. Wobisch and J. Womersley and D.R. Wood and T.R. Wyatt and Z. Wu and Y. Xie and Q. Xu and N. Xuan and S. Yacoob and R. Yamada and M. Yan and R. Yarema and T. Yasuda and Y.A. Yatsunenko and Y. Yen and K. Yip and H.D. Yoo and F. Yoffe and S.W. Youn and J. Yu and A. Yurkewicz and A. Zabi and M. Zanabria and A. Zatserklyaniy and M. Zdrazil and C. Zeitnitz and B. Zhang and D. Zhang and X. Zhang and T. Zhao and Z. Zhao and H. Zheng and B. Zhou and B. Zhou and J. Zhu and M. Zielinski and D. Zieminska and A. Zieminski and R. Zitoun and T. Zmuda and V. Zutshi and S. Zviagintsev and E.G. Zverev and A. Zylberstejn},
  title    = {The upgraded DØ detector},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  year     = {2006},
  volume   = {565},
  number   = {2},
  pages    = {463 - 537},
  issn     = {0168-9002},
  abstract = {The DØ experiment enjoyed a very successful data-collection run at the Fermilab Tevatron collider between 1992 and 1996. Since then, the detector has been upgraded to take advantage of improvements to the Tevatron and to enhance its physics capabilities. We describe the new elements of the detector, including the silicon microstrip tracker, central fiber tracker, solenoidal magnet, preshower detectors, forward muon detector, and forward proton detector. The uranium/liquid-argon calorimeters and central muon detector, remaining from Run I, are discussed briefly. We also present the associated electronics, triggering, and data acquisition systems, along with the design and implementation of software specific to DØ.},
  doi      = {https://doi.org/10.1016/j.nima.2006.05.248},
  keywords = {Fermilab, DZero, D0, Tevatron Run II},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900206010357},
}

@InCollection{Du2002,
  author    = {Timon C. Du},
  title     = {1 - Emerging Database System Architectures},
  booktitle = {Database and Data Communication Network Systems},
  publisher = {Academic Press},
  year      = {2002},
  editor    = {Cornelius T. Leondes},
  pages     = {1 - 39},
  address   = {San Diego},
  isbn      = {978-0-12-443895-8},
  abstract  = {Publisher Summary
This chapter discusses the techniques and applications of emerging database-system architecture. The history of computers, information systems, and database systems is also covered. The relational data model, deductive data model, object-oriented data model, distributed data model, active database model, deductive and object-oriented database (DOOD), and the joining of active databases and object-oriented databases are discussed. It is worth noting that different data models can satisfy different needs and future system requirement are diversified. In future, database performance can be increased because of improvements in system engineering, artificial intelligence, data mining, and user interface. Many techniques will also enhance the application of database systems, such as data warehousing, online analytical processing (OLAP), decision-support systems, and engineering and production applications.},
  doi       = {https://doi.org/10.1016/B978-012443895-8/50003-9},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780124438958500039},
}

@Article{Sanz2008,
  author   = {I. Sanz and M. Mesiti and G. Guerrini and R. Berlanga},
  title    = {Fragment-based approximate retrieval in highly heterogeneous XML collections},
  journal  = {Data \& Knowledge Engineering},
  year     = {2008},
  volume   = {64},
  number   = {1},
  pages    = {266 - 293},
  issn     = {0169-023X},
  note     = {Fourth International Conference on Business Process Management (BPM 2006) 8th International Conference on Enterprise Information Systems (ICEIS' 2006)},
  abstract = {Due to the heterogeneous nature of XML data for internet applications exact matching of queries is often inadequate. The need arises to quickly identify subtrees of XML documents in a collection that are similar to a given pattern. Similarity involves both tags, that are not required to coincide, and structure, in which not all the relationships among nodes in the tree structure are strictly preserved. In this paper we present an efficient approach to the identification of similar subtrees, relying on ad-hoc indexing structures. The approach allows to quickly detect, in a heterogeneous document collection, the minimal portions that exhibit some similarity with the pattern. These candidate portions are then ranked according to their actual similarity. The approach supports different notions of similarity, thus it can be customized to different application domains. In the paper, three different similarity measures are proposed and compared. The approach is experimentally validated and the experimental results are extensively discussed.},
  doi      = {https://doi.org/10.1016/j.datak.2007.05.008},
  keywords = {XML, Approximate structural retrieval, Enhanced indexing techniques},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X07001450},
}

@Article{Bellahsene2003,
  author  = {Z. Bellahsene},
  title   = {Data integration over the Web},
  journal = {Data \& Knowledge Engineering},
  year    = {2003},
  volume  = {44},
  number  = {3},
  pages   = {265 - 266},
  issn    = {0169-023X},
  note    = {Data integration over the Web},
  doi     = {https://doi.org/10.1016/S0169-023X(02)00139-8},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169023X02001398},
}

@InCollection{Halpin2008d,
  author    = {Terry Halpin and Tony Morgan},
  title     = {14 - Schema Transformations},
  booktitle = {Information Modeling and Relational Databases (Second Edition)},
  publisher = {Morgan Kaufmann},
  year      = {2008},
  editor    = {Terry Halpin and Tony Morgan},
  series    = {The Morgan Kaufmann Series in Data Management Systems},
  pages     = {687 - 772},
  address   = {San Francisco},
  edition   = {Second Edition},
  isbn      = {978-0-12-373568-3},
  doi       = {https://doi.org/10.1016/B978-012373568-3.50018-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123735683500187},
}

@Article{Soutou1998,
  author   = {Christian Soutou},
  title    = {Relational database reverse engineering: Algorithms to extract cardinality constraints},
  journal  = {Data \& Knowledge Engineering},
  year     = {1998},
  volume   = {28},
  number   = {2},
  pages    = {161 - 207},
  issn     = {0169-023X},
  abstract = {This paper presents algorithms which build SQL queries to improve the reverse engineering of relational databases. Our process extracts the current cardinality constraints of n-ary relationships through a combination of data dictionary, data schema and data instance analysis. Our results can be applied to semantic data models with cardinalities constraints based either on the ER model or to semantic data models with cardinality constraints on participation constraints (MERISE, ECR, ERC +, OMT, ODMG). The process we propose can also refine conceptual diagrams of commercial tools with reverse-engineering options (AMC∗Designer™, ORACLETM Designer 2000, etc.). From our algorithms a PRO*C program using dynamic SQL has been implemented.},
  doi      = {https://doi.org/10.1016/S0169-023X(98)00017-2},
  keywords = {Relational database reverse engineering, Cardinality constraints, -ary},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X98000172},
}

@Article{Czajkowski2015,
  author   = {Marcin Czajkowski and Monika Czerwonka and Marek Kretowski},
  title    = {Cost-sensitive Global Model Trees applied to loan charge-off forecasting},
  journal  = {Decision Support Systems},
  year     = {2015},
  volume   = {74},
  pages    = {57 - 66},
  issn     = {0167-9236},
  abstract = {Regression learning methods in real world applications often require cost minimization instead of the reduction of various metrics of prediction errors. Currently in the literature, there is a lack of white box solutions that can deal with forecasting problems where under-prediction and over-prediction errors have different consequences. To fill this gap, we introduced the Cost-sensitive Global Model Tree (CGMT), which applies a fitness function that minimizes an average misprediction cost. Proposed specialized genetic operators improve searching for optimal tree structure and cost-sensitive linear regression models in the leaves. Experimental validation is performed on loan charge-off data. It is known to be a difficult forecasting problem for banks due to the asymmetric cost structure. Obtained results show that specialized evolutionary algorithm applied to model tree induction finds significantly more accurate predictions than tested competitors. Decisions generated by the CGMT are simple, easy to interpret, and can be applied directly.},
  doi      = {https://doi.org/10.1016/j.dss.2015.03.009},
  keywords = {Cost-sensitive regression, Model trees, Evolutionary algorithms, Asymmetric costs, Loan charge-off forecasting},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923615000639},
}

@Article{Carvalho2013,
  author   = {Moisés Gomes de Carvalho and Alberto H.F. Laender and Marcos André Gonçalves and Altigran S. da Silva},
  title    = {An evolutionary approach to complex schema matching},
  journal  = {Information Systems},
  year     = {2013},
  volume   = {38},
  number   = {3},
  pages    = {302 - 316},
  issn     = {0306-4379},
  abstract = {The schema matching problem can be defined as the task of finding semantic relationships between schema elements existing in different data repositories. Despite the existence of elaborated graphic tools for helping to find such matches, this task is usually manually done. In this paper, we propose a novel evolutionary approach to addressing the problem of automatically finding complex matches between schemas of semantically related data repositories. To the best of our knowledge, this is the first approach that is capable of discovering complex schema matches using only the data instances. Since we only exploit the data stored in the repositories for this task, we rely on matching strategies that are based on record deduplication (aka, entity-oriented strategy) and information retrieval (aka, value-oriented strategy) techniques to find complex schema matches during the evolutionary process. To demonstrate the effectiveness of our approach, we conducted an experimental evaluation using real-world and synthetic datasets. The results show that our approach is able to find complex matches with high accuracy, similar to that obtained by more elaborated (hybrid) approaches, despite using only evidence based on the data instances.},
  doi      = {https://doi.org/10.1016/j.is.2012.10.002},
  keywords = {Genetic programming, Complex schema matchings, Entity-oriented strategy, Value-oriented strategy},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437912001287},
}

@Article{Maione2003,
  author   = {Guido Maione and David Naso},
  title    = {A soft computing approach for task contracting in multi-agent manufacturing control},
  journal  = {Computers in Industry},
  year     = {2003},
  volume   = {52},
  number   = {3},
  pages    = {199 - 219},
  issn     = {0166-3615},
  note     = {Soft Computing in Industrial Applications},
  abstract = {This paper describes a new task-contracting schema for multi-agent manufacturing control based on soft computing. It aims to apply fuzzy techniques to implement a real-time multi-criteria task-contracting mechanism for part flow control in manufacturing floor. For comparison purposes, the paper also considers other recently proposed evolutionary strategies to adapt and optimize agents’ decision parameters to the changing conditions of the manufacturing floor. All the considered approaches are compared on a detailed simulation model of a hypothetical manufacturing system that was recently proposed in literature as benchmark for multi-agent control systems.},
  doi      = {https://doi.org/10.1016/S0166-3615(03)00127-1},
  keywords = {Soft computing, Manufacturing systems, Multi-agent systems, Dispatching},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361503001271},
}

@Article{Tseng2009,
  author   = {H. Chris Tseng and Bassam Almogahed},
  title    = {Modular neural networks with applications to pattern profiling problems},
  journal  = {Neurocomputing},
  year     = {2009},
  volume   = {72},
  number   = {10},
  pages    = {2093 - 2100},
  issn     = {0925-2312},
  note     = {Lattice Computing and Natural Computing (JCIS 2007) / Neural Networks in Intelligent Systems Designn (ISDA 2007)},
  abstract = {We study the feasibility and the performance of modular design concept as applied to pattern profiling problems using artificial neural network. By decomposing the given pattern profiling problem into smaller modules, it is shown that comparable performance can be achieved with improvement on computation and design complexity. A survey of typical modular neural networks shows that large-scale nonlinear problems can alleviate its dimensionality curse with modular technique. Overview of modular neural networks based on how the problem is modularized through various decomposition and subsequent aggregation is given. A pattern recognition problem for aircraft trajectory prediction using NeuroFuzzy learning with a two stage modular learning design is presented. Decoupled data are used to train respective neural network modules. A genetic algorithm is used to aggregate all the learned modules so that it is ready for online pattern recognition purpose. As compared with the non-modular approach, the modular approach offers comparable prediction performance with significantly lower overall computation time. This study validates that modular design is a promising solution for large-scale soft computing problems.},
  doi      = {https://doi.org/10.1016/j.neucom.2008.10.020},
  keywords = {Neural networks, Modular design, Soft computing, Time-series prediction, Pattern recognition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231208005444},
}

@Article{1992,
  title   = {Bibliography of the current world literature},
  journal = {Current Opinion in Neurobiology},
  year    = {1992},
  volume  = {2},
  number  = {2},
  pages   = {223 - 239},
  issn    = {0959-4388},
  doi     = {https://doi.org/10.1016/0959-4388(92)90017-F},
  url     = {http://www.sciencedirect.com/science/article/pii/095943889290017F},
}

@Article{Torchiano2011,
  author        = {Marco Torchiano and Massimiliano Di Penta and Filippo Ricca and Andrea De Lucia and Filippo Lanubile},
  title         = {Migration of information systems in the Italian industry: A state of the practice survey},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {1},
  pages         = {71 - 86},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context
Software migration—and in particular migration towards the Web and towards distributed architectures—is a challenging and complex activity, and has been particularly relevant in recent years, due to the large number of migration projects the industry had to face off because of the increasing pervasiveness of the Web and of mobile devices.
Objective
This paper reports a survey aimed at identifying the state-of-the-practice of the Italian industry for what concerns the previous experiences in software migration projects—specifically concerning information systems—the adopted tools and the emerging needs and problems.
Method
The study has been carried out among 59 Italian Information Technology companies, and for each company a representative person had to answer an on-line questionnaire concerning migration experiences, pieces of technology involved in migration projects, adopted tools, and problems occurred during the project.
Results
Indicate that migration—especially towards the Web—is highly relevant for Italian IT companies, and that companies tend to increasingly adopt free and open source solutions rather than commercial ones. Results also indicate that the adoption of specific tools for migration is still very limited, either because of the lack of skills and knowledge, or due to the lack of mature and adequate options.
Conclusions
Findings from this survey suggest the need for further technology transfer between academia and industry for the purpose of favoring the adoption of software migration techniques and tools.},
  doi           = {https://doi.org/10.1016/j.infsof.2010.08.002},
  keywords      = {Industrial survey, Migration projects, Migration tools},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910001515},
}

@Article{Varro2005,
  author        = {Gergely Varró and Katalin Friedl and Dániel Varró},
  title         = {Graph Transformation in Relational Databases},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2005},
  volume        = {127},
  number        = {1},
  pages         = {167 - 180},
  issn          = {1571-0661},
  note          = {Proceedings of the International Workshop on Graph-Based Tools (GraBaTs 2004)},
  __markedentry = {[Juliana:]},
  abstract      = {We present a novel approach to implement a graph transformation engine based on standard relational database management systems (RDBMSs). The essence of the approach is to create database views for each rules and to handle pattern matching by inner join operations while negative application conditions by left outer join operations. Furthermore, the model manipulation prescribed by the application of a graph transformation rule is also implemented using elementary data manipulation statements (such as insert, delete, update).},
  doi           = {https://doi.org/10.1016/j.entcs.2004.12.034},
  keywords      = {Tool support, Graph transformation, Pattern matching, Relational databases},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066105001131},
}

@InCollection{Halpin2008e,
  title         = {Preface},
  booktitle     = {Information Modeling and Relational Databases (Second Edition)},
  publisher     = {Morgan Kaufmann},
  year          = {2008},
  editor        = {Terry Halpin and Tony Morgan},
  series        = {The Morgan Kaufmann Series in Data Management Systems},
  pages         = {xxi - xxvi},
  address       = {San Francisco},
  edition       = {Second Edition},
  isbn          = {978-0-12-373568-3},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-012373568-3.50004-7},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123735683500047},
}

@Article{Lopez-Ortega2016,
  author        = {Eugenio López-Ortega and Damian Canales-Sanchez and Tomás Bautista-Godinez and Santiago Macias-Herrera},
  title         = {Classification of micro, small and medium enterprises (M-SME) based on their available levels of knowledge},
  journal       = {Technovation},
  year          = {2016},
  volume        = {47},
  pages         = {59 - 69},
  issn          = {0166-4972},
  __markedentry = {[Juliana:]},
  abstract      = {This paper proposes a novel and practical method of classifying micro, small and medium enterprises (M-SME) based on their available levels of knowledge. This classification considers that the specific problems that dominate a company’s operations are related to the level of knowledge available. Therefore, identifying the dominant problem groups facing the company can enable an estimate of the level of knowledge available in the enterprise. Data from 2698 Mexican M-SMEs are used to identify operational problems. The main problem groups are obtained through a cluster analysis. The proposed classification consists of three levels of knowledge based on the interpretation of the dominant problem groups in each enterprise. Each of the 2698 Mexican M-SME are classified by a discriminant analysis. Nearly half of the M-SME are classified as having a lower level of available knowledge, and only 10% are classified with the highest level. No difference is observed between the size of the company and the level of available knowledge. This means that growth in the number of employees and sales of M-SMEs is not necessarily accompanied by an accumulation of knowledge that companies can use to improve their operations.},
  doi           = {https://doi.org/10.1016/j.technovation.2015.10.001},
  keywords      = {SME classification, SME taxonomy, Knowledge available, Cluster analysis, Discriminant analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166497215000747},
}

@Article{Polo2002,
  author        = {Macario Polo and Juan Ángel Gómez and Mario Piattini and Francisco Ruiz},
  title         = {Generating three-tier applications from relational databases: a formal and practical approach},
  journal       = {Information and Software Technology},
  year          = {2002},
  volume        = {44},
  number        = {15},
  pages         = {923 - 941},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {This article describes a method for building applications with a three-tier structure (presentation, business, persistence) from an existing relational database. The method works as a transformation function that takes the relational schema as its input, producing three sets of classes (which depend on the actual system being reengineered) to represent the final application, as well as some additional auxiliary classes (which are ‘constant’ and always generated, such as an ‘About’ dialog, for example). All the classes generated are adequately placed along the three-tiers. The method is based on (1) the formalization of all the sets involved in the process, and (2) the mathematical formulation of the required functions to get the final application. For this second step, we have taken into account several well-known, widely used design and transformation patterns that produce high quality designs and highly maintainable software. The method is implemented in a tool that we have successfully used in several projects of medium size. Obviously, it is quite difficult for the obtained software to fulfill all the requirements desired by the customer, but the uniformity and understandability of its design makes very easy its modification.},
  doi           = {https://doi.org/10.1016/S0950-5849(02)00130-1},
  keywords      = {Database reengineering, Code generation, Pattern formalization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584902001301},
}

@Article{Tan1997,
  author        = {Hee Beng Kuan Tan and Tok Wang Ling},
  title         = {A method for the recovery of inclusion dependencies from data-intensive business programs},
  journal       = {Information and Software Technology},
  year          = {1997},
  volume        = {39},
  number        = {1},
  pages         = {27 - 34},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Data integrity constraints usually form a major component in a data-intensive business system. To successfully reengineer a data-intensive business system, its data integrity constraints must be understood. Inclusion dependencies constitute an important type of data integrity constraints. They are very important for migrating a system to the object-oriented technology. Many of the world's data-intensive business systems have been developed on old generation database technologies which do not support the specification of inclusion dependencies in the schemas. As a result, most of the inclusion dependencies are enforced in the programs which update the databases. However, no approach has been proposed for the recovery of inclusion dependencies from programs. We develop a theory for inferring inclusion dependencies in a database from the programs which update the database. With the use of the theory, an approach for the recovery of inclusion dependencies from programs is consequently proposed. The proposed approach can recover those inclusion dependencies which cannot be found by the existing approaches. As opposed to the existing approaches, the inclusion dependencies recovered from the proposed approach are proven by analysing the programs which update the databases.},
  doi           = {https://doi.org/10.1016/0950-5849(96)01108-1},
  keywords      = {Reverse engineering, Data-intensive business programs, Database design recovery, Inclusion dependency},
  url           = {http://www.sciencedirect.com/science/article/pii/0950584996011081},
}

@Article{Kwan1999a,
  author        = {Irene Kwan and Joseph Fong},
  title         = {Schema integration methodology and its verification by use of information capacity},
  journal       = {Information Systems},
  year          = {1999},
  volume        = {24},
  number        = {5},
  pages         = {355 - 376},
  issn          = {0306-4379},
  __markedentry = {[Juliana:]},
  abstract      = {In the past decade a considerable amount of research has been done on schema integration and translation. Until recently, most of the work has largely neglected the proof of correctness in transforming schemas. Invalid transformations can produce incomplete or inconsistent pictures that may cause damage to organisations. In this paper, we present a Schema Integration Methodology, with proof of correctness, based on the use of Information Capacity. We present our methodology as a set of steps for schema translation tasks, verify their correctness according to operational goals and derive the information capacity of the original schema in its pre-transformed and post-transformed conditions. If the information capacity of the original schema is equivalent to or dominated by the transformed schema, then information is preserved after integration. Our correctness criterion is based on the assumption that these goal(s) could be practically pursued in an operational sense and are not solely mathematical proofs.},
  doi           = {https://doi.org/10.1016/S0306-4379(99)00022-8},
  keywords      = {Schema Integration, Information Capacity Equivalence, Correctness Criteria, Schema Transformation, Information Completeness},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437999000228},
}

@Article{Jahnke2002,
  author        = {Jens H. Jahnke and Wilhelm Schäfer and Jörg P. Wadsack and Albert Zündorf},
  title         = {Supporting iterations in exploratory database reengineering processes},
  journal       = {Science of Computer Programming},
  year          = {2002},
  volume        = {45},
  number        = {2},
  pages         = {99 - 136},
  issn          = {0167-6423},
  note          = {Special Issue on Software Maintenance and Reengineering (CSMR 99)},
  __markedentry = {[Juliana:]},
  abstract      = {Key technologies like the World Wide Web, object-orientation, and distributed computing enable new applications, e.g., in the area of electronic commerce, management of information systems, and decision support systems. Today, many companies face the problem that they have to reengineer pre-existing information systems to take advantage of these technologies. Various computer-aided reengineering tools have been developed to reduce the complexity of the reengineering task. A major limitation of current approaches, however, is that they impose a strictly phase-oriented, waterfall-type reengineering process, with little support for iterations. Still, such iterations often occur in real-world examples, e.g., when additional knowledge about the legacy system becomes available or when the legacy system is modified during an ongoing migration process. In this paper, we present an approach to incremental consistency management that allows to overcome this limitation in the domain of database systems by integrating reverse and forward engineering activities in an intertwined process. The described mechanism is based on a formalization of conceptual schema translation and redesign transformations by graph rewriting rules and has been implemented and evaluated with the Varlet database reengineering environment.},
  doi           = {https://doi.org/10.1016/S0167-6423(02)00056-4},
  keywords      = {Database reengineering, Design recovery, Schema redesign, Computer-aided software engineering, Process iterations, Legacy information system, Database migration, Graph grammars},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642302000564},
}

@Article{Fong2003,
  author        = {J. Fong and H.K. Wong and Z. Cheng},
  title         = {Converting relational database into XML documents with DOM},
  journal       = {Information and Software Technology},
  year          = {2003},
  volume        = {45},
  number        = {6},
  pages         = {335 - 355},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {The revolution of XML is recognized as the trend of technology on the Internet to researchers as well as practitioners. Companies need to adopt XML technology. With investment in the current relational database systems, they want to develop new XML documents while running existing relational databases on production. They need to reengineer the relational databases into XML documents with constraints preservation. In the process, schema translation must be done before data conversion. Since the existing relational databases are usually normalized, they have to be reconstructed into XML document tree structures. This can be accomplished through denormalization by joining the normalized relations into tables according to their data dependencies constraints. The joined tables are mapped into DOMs, which are then integrated into XML document trees. The user specifies an XML document root with its relevant nodes to form a partitioned XML document tree to meet their requirements. The selected XML document tree is mapped into an XML schema in the form of DTD. We then load joined tables into DOMs, integrate them into a DOM, and transform it into an XML document.},
  doi           = {https://doi.org/10.1016/S0950-5849(03)00026-0},
  keywords      = {XML document, Denormalization, Relational database, Schema translation, Data conversion, Data dependencies, Document object model, Document type definition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584903000260},
}

@Article{Dimitrieski2015,
  author        = {Vladimir Dimitrieski and Milan Čeliković and Slavica Aleksić and Sonja Ristić and Abdalla Alargt and Ivan Luković},
  title         = {Concepts and evaluation of the extended entity-relationship approach to database design in a multi-paradigm information system modeling tool},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2015},
  volume        = {44},
  pages         = {299 - 318},
  issn          = {1477-8424},
  __markedentry = {[Juliana:]},
  abstract      = {Different approaches to information system (IS) development are based on different data models. The selection of a data model for conceptual design, among other things, depends on the problem domain, the knowledge, and the personal preferences of an IS designer. In some situations, a simultaneous usage of different approaches to the conceptual database design and IS development may lead to the most appropriate solutions. In our previous research we have developed a tool that provides an evolutive and incremental approach to IS development, which is based on the form type data model. The approaches based on the Extended Entity-Relationship (EER) and class data models are broadly accepted throughout the community of IS designers. In order to support the simultaneous usage of approaches based on the form type, EER and class data models, we have developed the Multi-Paradigm Information System Modeling Tool (MIST). In this paper, we present a part of our MIST tool that supports EER approach to a database design. MIST components currently provide a formal specification of an EER database schema specification and its transformation into the relational data model, or the class model. Also, MIST allows generation of Structured Query Language code for a database creation and procedural code for implementing database constraints. In addition, Java code that stores and processes data from the database, may be generated from the class model. In this paper, we present the evaluation study of the MIST EER domain-specific language. Users' perceptions of language quality characteristics are used for the evaluation.},
  doi           = {https://doi.org/10.1016/j.cl.2015.08.011},
  keywords      = {Entity-relationship, Domain-specific language, Model transformation, Information system design, Evaluation study},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842415000615},
}

@InCollection{Hughes2013,
  author        = {Ralph Hughes},
  title         = {Chapter 8 - Adapting Agile for Data Warehousing},
  booktitle     = {Agile Data Warehousing Project Management},
  publisher     = {Morgan Kaufmann},
  year          = {2013},
  editor        = {Ralph Hughes},
  pages         = {251 - 302},
  address       = {Boston},
  isbn          = {978-0-12-396463-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-396463-2.00008-9},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123964632000089},
}

@InCollection{Hughes2013a,
  title         = {List of Tables},
  booktitle     = {Agile Data Warehousing Project Management},
  publisher     = {Morgan Kaufmann},
  year          = {2013},
  editor        = {Ralph Hughes},
  pages         = {xv},
  address       = {Boston},
  isbn          = {978-0-12-396463-2},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-0-12-396463-2.00023-5},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780123964632000235},
}

@Article{Cleve2005,
  author        = {Anthony Cleve and Jean Henrard and Jean-Luc Hainaut},
  title         = {Co-transformations in Information System Reengineering},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2005},
  volume        = {137},
  number        = {3},
  pages         = {5 - 15},
  issn          = {1571-0661},
  note          = {Proceedings of the 2nd International Workshop on Metamodels, Schemas, and Grammars for Reverse Engineering (ateM 2004)},
  __markedentry = {[Juliana:]},
  abstract      = {Database reengineering consists of deriving a new database from a legacy database and adapting the software components accordingly. This migration process involves three main steps, namely schema conversion, data conversion and program conversion. This paper explores the feasibility of transforming the application programs through code transformation patterns that are automatically derived from the database transformations. It presents the principles of a new transformational approach coupling database and program transformations and it describes a prototype CASE tool based on this approach.},
  doi           = {https://doi.org/10.1016/j.entcs.2005.07.001},
  keywords      = {database reengineering, schema transformations, program transformations},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066105051017},
}

@Article{Politowski2018,
  author        = {Cristiano Politowski and Lisandra M. Fontoura and Fabio Petrillo and Yann-Gaël Guéhéneuc},
  title         = {Learning from the past: A process recommendation system for video game projects using postmortems experiences},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {100},
  pages         = {103 - 118},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {Context: The video game industry is a billion dollar industry that faces problems in the way games are developed. One method to address these problems is using developer aid tools, such as Recommendation Systems. These tools assist developers by generating recommendations to help them perform their tasks. Objective: This article describes a systematic approach to recommend development processes for video game projects, using postmortem knowledge extraction and a model of the context of the new project, in which “postmortems” are articles written by video game developers at the end of projects, summarizing the experience of their game development team. This approach aims to provide reflections about development processes used in the game industry as well as guidance to developers to choose the most adequate process according to the contexts they’re in. Method: Our approach is divided in three separate phases: in the first phase, we manually extracted the processes from the postmortems analysis; in the second one, we created a video game context and algorithm rules for recommendation; and finally in the third phase, we evaluated the recommended processes by using quantitative and qualitative metrics, game developers feedback, and a case study by interviewing a video game development team. Contributions: This article brings three main contributions. The first describes a database of developers’ experiences extracted from postmortems in the form of development processes. The second defines the main attributes that a video game project contain, which it uses to define the contexts of the project. The third describes and evaluates a recommendation system for video game projects, which uses the contexts of the projects to identify similar projects and suggest a set of activities in the form of a process.},
  doi           = {https://doi.org/10.1016/j.infsof.2018.04.003},
  keywords      = {Software development process, Video game development, Recommendation system},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584918300685},
}

@InCollection{Link2003,
  title         = {Glossary},
  booktitle     = {Unit Testing in Java},
  publisher     = {Morgan Kaufmann},
  year          = {2003},
  editor        = {Johannes Link},
  series        = {The Morgan Kaufmann Series in Software Engineering and Programming},
  pages         = {345 - 351},
  address       = {San Francisco},
  isbn          = {978-1-55860-868-9},
  __markedentry = {[Juliana:]},
  doi           = {https://doi.org/10.1016/B978-155860868-9/50020-1},
  url           = {http://www.sciencedirect.com/science/article/pii/B9781558608689500201},
}

@Article{Reynares2015,
  author        = {E. Reynares and M.L. Caliusco and M.R. Galli},
  title         = {A set of ontology design patterns for reengineering SBVR statements into OWL/SWRL ontologies},
  journal       = {Expert Systems with Applications},
  year          = {2015},
  volume        = {42},
  number        = {5},
  pages         = {2680 - 2690},
  issn          = {0957-4174},
  __markedentry = {[Juliana:]},
  abstract      = {The interest in the use of ontologies for creating more intelligent and effective enterprise information systems has increased considerably in recent years. The most critical aspects during the development of these systems are: (1) to identify the ontology concepts and (2) to make explicit the business rules by means of the ontology axioms. In order to address these issues, mappings of business rules expressions to ontology statements based on different languages were proposed. Despite the efforts made in this area, some work remain to be done. This work presents a set of ontology design patterns providing a way to obtain an OWL/SWRL ontology by applying metamodel transformation rules over the SBVR specification of a business domain. Patterns are rooted in the structural specification of the standards, providing a set of mappings readily usable for business people or developers concerned with the implementation of a mapping tool. Moreover, translations from SBVR to SWRL language are presented in order to fill the gap in the expressive power of SBVR and OWL. The theoretical expressions of patterns are illustrated by means of an example depicting the core structure of a fictitious company.},
  doi           = {https://doi.org/10.1016/j.eswa.2014.11.012},
  keywords      = {Ontology-driven information systems, Ontology design pattern, SBVR 1.1, OWL 2, SWRL},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417414006988},
}

@Article{Edgington2010,
  author        = {Theresa M. Edgington and T.S. Raghu and Ajay S. Vinze},
  title         = {Using process mining to identify coordination patterns in IT service management},
  journal       = {Decision Support Systems},
  year          = {2010},
  volume        = {49},
  number        = {2},
  pages         = {175 - 186},
  issn          = {0167-9236},
  __markedentry = {[Juliana:]},
  abstract      = {We empirically analyze the database used in the help desk process between a national US public agency and its global outsourcing provider. We considered the question of whether the database might reveal a deeper level of knowledge than was apparent from direct inspection. Our results reveal that four constructs underlie this process. Three are confirmed through covariance-based structural equation modeling and a fourth is implied through existing data. Our results suggest refinement in service level agreements to create a different type of governance coordination to assist in aligning the outsourcing provider's execution more closely with the client's needs.},
  doi           = {https://doi.org/10.1016/j.dss.2010.02.003},
  keywords      = {IT service management, Help desk, Coordination theory, Covariance-based Structural Equation Modeling (SEM), Adaptive structuration, EQS},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923610000400},
}

@Article{Semmel1995,
  author        = {R.D. Semmel and R.P. Winkler},
  title         = {Integrating reengineered databases to support data fusion},
  journal       = {Journal of Systems and Software},
  year          = {1995},
  volume        = {30},
  number        = {1},
  pages         = {127 - 135},
  issn          = {0164-1212},
  __markedentry = {[Juliana:]},
  abstract      = {Large information systems often require the fusion of multiple databases to achieve desired functionality. In this article, we focus on how automated query formulation capabilities may be realized over a set of fused databases. Reengineering issues related to database design and fusion are discussed, and a query formulation and design system known as QUICK is described. A case study is presented in which the logical schemas for two independent U.S. Army databases are reverse engineered into conceptual schemas that are subsequently used for data fusion and automatic query generation. In addition, enhanced methods that employ meta-level conceptual constructs to support reverse engineering, data fusion, and query formulation are described.},
  doi           = {https://doi.org/10.1016/0164-1212(94)00121-3},
  url           = {http://www.sciencedirect.com/science/article/pii/0164121294001213},
}

@Article{Zhang2000,
  author        = {Xiuzhen Zhang and Joseph Fong},
  title         = {Translating update operations from relational to object-oriented databases},
  journal       = {Information and Software Technology},
  year          = {2000},
  volume        = {42},
  number        = {3},
  pages         = {197 - 210},
  issn          = {0950-5849},
  __markedentry = {[Juliana:]},
  abstract      = {In migrating a legacy relational database system to the object-oriented (OO) platform, when database migration completes, application modules are to be migrated, where embedded relational database operations are mapped into their OO correspondents. In this paper we study mapping relational update operations to their OO equivalents, which include UPDATE11Note that we use update for general update operations and UPDATE for modifying certain data values., INSERT and DELETE operations. Relational update operation translation from relational to OO faces the touchy problem of transformation from a value-based relationship model to a reference-based model and maintaining the relational integrity constraints. Moreover, with a relational database where inheritance is expressed as attribute value subset relationship, changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy, which we call object migration. Considering all these aspects, algorithms are given mapping relational UPDATE, INSERT and DELETE operations to their OO correspondents. Our work emphasize in examining the differences in the representation of the source schema's semantics resulting from the translation process, as well as differences in the inherent semantics of the two models.},
  doi           = {https://doi.org/10.1016/S0950-5849(99)00058-0},
  keywords      = {Relational model, Object-oriented model, Query translation, Update translation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584999000580},
}

@InCollection{Haq2016,
  author        = {Qamar Shahbaz Ul Haq},
  title         = {Chapter 11 - Data Quality},
  booktitle     = {Data Mapping for Data Warehouse Design},
  publisher     = {Morgan Kaufmann},
  year          = {2016},
  editor        = {Qamar Shahbaz Ul Haq},
  pages         = {67 - 82},
  address       = {Boston},
  isbn          = {978-0-12-805185-6},
  __markedentry = {[Juliana:6]},
  abstract      = {Data warehouse is becoming common and almost every organization with good amount of data wants to get benefit of their data. However data quality issues create major disappointment for business users when end reports provide in correct and in complete view.},
  doi           = {https://doi.org/10.1016/B978-0-12-805185-6.00011-3},
  keywords      = {data quality, data cleansing, data completeness, data improvement},
  url           = {http://www.sciencedirect.com/science/article/pii/B9780128051856000113},
}

@Comment{jabref-meta: databaseType:bibtex;}
